[
  {
    "instance_id": "apache__iceberg-13345",
    "pr_id": 13345,
    "issue_id": 13343,
    "repo": "apache/iceberg",
    "problem_statement": "Creating 2 or more Iceberg Kafka Sink Connectors using slow JDBC catalog causes a startup failure on first run.\n### Apache Iceberg version\n\n1.9.0\n\n### Query engine\n\nKafka Connect\n\n### Please describe the bug üêû\n\nIf Kafka Connect is started with 2 or more Iceberg sink connectors that are using JDBC catalog and the JDBC is slow the some of the connectors will throw:\n\n```\norg.apache.iceberg.jdbc.UncheckedSQLException: Cannot initialize JDBC catalog\n\tat org.apache.iceberg.jdbc.JdbcCatalog.initializeCatalogTables(JdbcCatalog.java:206)\n\tat org.apache.iceberg.jdbc.JdbcCatalog.initialize(JdbcCatalog.java:147)\n\tat org.apache.iceberg.CatalogUtil.loadCatalog(CatalogUtil.java:277)\n\tat org.apache.iceberg.CatalogUtil.buildIcebergCatalog(CatalogUtil.java:331)\n\tat org.apache.iceberg.connect.CatalogUtils.loadCatalog(CatalogUtils.java:44)\n\tat org.apache.iceberg.connect.IcebergSinkTask.start(IcebergSinkTask.java:48)\n\tat org.apache.kafka.connect.runtime.WorkerSinkTask.initializeAndStart(WorkerSinkTask.java:323)\n\tat org.apache.kafka.connect.runtime.WorkerTask.doStart(WorkerTask.java:175)\n\tat org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:224)\n\tat org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:280)\n\tat org.apache.kafka.connect.runtime.isolation.Plugins.lambda$withClassLoader$1(Plugins.java:237)\n\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.postgresql.util.PSQLException: ERROR: duplicate key value violates unique constraint \"pg_type_typname_nsp_index\"\n  Detail: Key (typname, typnamespace)=(iceberg_tables, 2200) already exists.\n\tat org.postgresql.core.v3.QueryExecutorImpl.receiveErrorResponse(QueryExecutorImpl.java:2733)\n\tat org.postgresql.core.v3.QueryExecutorImpl.processResults(QueryExecutorImpl.java:2420)\n\tat org.postgresql.core.v3.QueryExecutorImpl.execute(QueryExecutorImpl.java:372)\n\tat org.postgresql.jdbc.PgStatement.executeInternal(PgStatement.java:517)\n\tat org.postgresql.jdbc.PgStatement.execute(PgStatement.java:434)\n\tat org.postgresql.jdbc.PgPreparedStatement.executeWithFlags(PgPreparedStatement.java:194)\n\tat org.postgresql.jdbc.PgPreparedStatement.execute(PgPreparedStatement.java:180)\n\t\tat org.apache.iceberg.jdbc.JdbcCatalog.lambda$initializeCatalogTables$0(JdbcCatalog.java:178)\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:72)\n\tat org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:65)\n\tat org.apache.iceberg.jdbc.JdbcCatalog.initializeCatalogTables(JdbcCatalog.java:162)\n\t... 15 more\n```\n\nThis is caused because the `JdbcCatalog` checks to see if the table exists and if not creates it.  However this leaves a window where a race condition can arise when another thread creates the table before the create is called.  \n\nThis only occurs the first time the connector is started as the error is that the table exists and the table existence check will detect that case.  \n\nI have a test and fix for this bug and will submit same.\n\n\n\n### Willingness to contribute\n\n- [x] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 494,
    "test_files_count": 1,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "build.gradle",
      "core/src/main/java/org/apache/iceberg/jdbc/JdbcCatalog.java",
      "core/src/test/java/org/apache/iceberg/jdbc/TestJdbcTableConcurrency.java",
      "gradle/libs.versions.toml"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/jdbc/TestJdbcTableConcurrency.java"
    ],
    "base_commit": "5b50afe8f2b4cf16af6c015625385021b44aca14",
    "head_commit": "1e58d6d6101c07b29e18554d8ad4ee69b7111ec0",
    "repo_url": "https://github.com/apache/iceberg/pull/13345",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13345",
    "dockerfile": "",
    "pr_merged_at": "2025-06-27T21:14:23.000Z",
    "patch": "diff --git a/build.gradle b/build.gradle\nindex ed0809c2f66c..9ac633c078b9 100644\n--- a/build.gradle\n+++ b/build.gradle\n@@ -366,6 +366,8 @@ project(':iceberg-core') {\n     testImplementation libs.mockserver.netty\n     testImplementation libs.mockserver.client.java\n     testImplementation libs.sqlite.jdbc\n+    testImplementation libs.derby.core\n+    testImplementation libs.derby.tools\n     testImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')\n     testImplementation libs.esotericsoftware.kryo\n     testImplementation(libs.guava.testlib) {\n\ndiff --git a/core/src/main/java/org/apache/iceberg/jdbc/JdbcCatalog.java b/core/src/main/java/org/apache/iceberg/jdbc/JdbcCatalog.java\nindex a413e6c4e29e..4d0aa08da2ba 100644\n--- a/core/src/main/java/org/apache/iceberg/jdbc/JdbcCatalog.java\n+++ b/core/src/main/java/org/apache/iceberg/jdbc/JdbcCatalog.java\n@@ -36,6 +36,7 @@\n import java.util.Set;\n import java.util.function.Consumer;\n import java.util.function.Function;\n+import java.util.function.Predicate;\n import java.util.stream.Collectors;\n import java.util.stream.Stream;\n import org.apache.iceberg.CatalogProperties;\n@@ -156,48 +157,62 @@ public void initialize(String name, Map<String, String> properties) {\n     closeableGroup.setSuppressCloseFailure(true);\n   }\n \n-  private void initializeCatalogTables() {\n-    LOG.trace(\"Creating database tables (if missing) to store iceberg catalog\");\n-    try {\n-      connections.run(\n-          conn -> {\n-            DatabaseMetaData dbMeta = conn.getMetaData();\n-            ResultSet tableExists =\n-                dbMeta.getTables(\n-                    null /* catalog name */,\n-                    null /* schemaPattern */,\n-                    JdbcUtil.CATALOG_TABLE_VIEW_NAME /* tableNamePattern */,\n-                    null /* types */);\n-            if (tableExists.next()) {\n-              return true;\n-            }\n-\n-            LOG.debug(\n-                \"Creating table {} to store iceberg catalog tables\",\n-                JdbcUtil.CATALOG_TABLE_VIEW_NAME);\n-            return conn.prepareStatement(JdbcUtil.V0_CREATE_CATALOG_SQL).execute();\n-          });\n-\n-      connections.run(\n-          conn -> {\n-            DatabaseMetaData dbMeta = conn.getMetaData();\n-            ResultSet tableExists =\n-                dbMeta.getTables(\n-                    null /* catalog name */,\n-                    null /* schemaPattern */,\n-                    JdbcUtil.NAMESPACE_PROPERTIES_TABLE_NAME /* tableNamePattern */,\n-                    null /* types */);\n-\n-            if (tableExists.next()) {\n+  private void atomicCreateTable(String tableName, String sqlCommand, String reason)\n+      throws SQLException, InterruptedException {\n+    connections.run(\n+        conn -> {\n+          DatabaseMetaData dbMeta = conn.getMetaData();\n+\n+          // check the existence of a table name\n+          Predicate<String> tableTest =\n+              name -> {\n+                try {\n+                  ResultSet result =\n+                      dbMeta.getTables(\n+                          null /* catalog name */,\n+                          null /* schemaPattern */,\n+                          name /* tableNamePattern */,\n+                          null /* types */);\n+                  return result.next();\n+                } catch (SQLException e) {\n+                  return false;\n+                }\n+              };\n+\n+          // some databases force table name to upper case -- check that last.\n+          Predicate<String> tableExists =\n+              name -> tableTest.test(name) || tableTest.test(name.toUpperCase(Locale.ROOT));\n+\n+          if (tableExists.test(tableName)) {\n+            return true;\n+          }\n+\n+          LOG.debug(\"Creating table {} {}\", tableName, reason);\n+          try {\n+            conn.prepareStatement(sqlCommand).execute();\n+            return true;\n+          } catch (SQLException e) {\n+            // see if table was created by another thread or process.\n+            if (tableExists.test(tableName)) {\n               return true;\n             }\n+            throw e;\n+          }\n+        });\n+  }\n \n-            LOG.debug(\n-                \"Creating table {} to store iceberg catalog namespace properties\",\n-                JdbcUtil.NAMESPACE_PROPERTIES_TABLE_NAME);\n-            return conn.prepareStatement(JdbcUtil.CREATE_NAMESPACE_PROPERTIES_TABLE_SQL).execute();\n-          });\n+  private void initializeCatalogTables() {\n+    LOG.trace(\"Creating database tables (if missing) to store iceberg catalog\");\n \n+    try {\n+      atomicCreateTable(\n+          JdbcUtil.CATALOG_TABLE_VIEW_NAME,\n+          JdbcUtil.V0_CREATE_CATALOG_SQL,\n+          \"to store iceberg catalog tables\");\n+      atomicCreateTable(\n+          JdbcUtil.NAMESPACE_PROPERTIES_TABLE_NAME,\n+          JdbcUtil.CREATE_NAMESPACE_PROPERTIES_TABLE_SQL,\n+          \"to store iceberg catalog namespace properties\");\n     } catch (SQLTimeoutException e) {\n       throw new UncheckedSQLException(e, \"Cannot initialize JDBC catalog: Query timed out\");\n     } catch (SQLTransientConnectionException | SQLNonTransientConnectionException e) {\n\ndiff --git a/gradle/libs.versions.toml b/gradle/libs.versions.toml\nindex 26283b266cb9..a612648ca86c 100644\n--- a/gradle/libs.versions.toml\n+++ b/gradle/libs.versions.toml\n@@ -41,6 +41,7 @@ comet = \"0.8.1\"\n datasketches = \"6.2.0\"\n delta-standalone = \"3.3.2\"\n delta-spark = \"3.3.2\"\n+derby = \"10.15.2.0\"\n esotericsoftware-kryo = \"4.0.3\"\n errorprone-annotations = \"2.38.0\"\n failsafe = \"3.3.2\"\n@@ -176,6 +177,8 @@ apiguardian = { module = \"org.apiguardian:apiguardian-api\", version.ref = \"apigu\n assertj-core = { module = \"org.assertj:assertj-core\", version.ref = \"assertj-core\" }\n awaitility = { module = \"org.awaitility:awaitility\", version.ref = \"awaitility\" }\n delta-spark = { module = \"io.delta:delta-spark_2.12\", version.ref = \"delta-spark\" }\n+derby-core = { module = \"org.apache.derby:derby\", version.ref = \"derby\"}\n+derby-tools = { module = \"org.apache.derby:derbytools\", version.ref = \"derby\"}\n esotericsoftware-kryo = { module = \"com.esotericsoftware:kryo\", version.ref = \"esotericsoftware-kryo\" }\n flink119-connector-test-utils = { module = \"org.apache.flink:flink-connector-test-utils\", version.ref = \"flink119\" }\n flink119-core = { module = \"org.apache.flink:flink-core\", version.ref = \"flink119\" }\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/jdbc/TestJdbcTableConcurrency.java b/core/src/test/java/org/apache/iceberg/jdbc/TestJdbcTableConcurrency.java\nindex 16321c1fa855..db264e4ffc18 100644\n--- a/core/src/test/java/org/apache/iceberg/jdbc/TestJdbcTableConcurrency.java\n+++ b/core/src/test/java/org/apache/iceberg/jdbc/TestJdbcTableConcurrency.java\n@@ -26,11 +26,51 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.Reader;\n+import java.math.BigDecimal;\n+import java.net.URL;\n+import java.sql.Array;\n+import java.sql.Blob;\n+import java.sql.CallableStatement;\n+import java.sql.Clob;\n+import java.sql.Connection;\n+import java.sql.DatabaseMetaData;\n+import java.sql.Date;\n+import java.sql.Driver;\n+import java.sql.DriverManager;\n+import java.sql.DriverPropertyInfo;\n+import java.sql.NClob;\n+import java.sql.ParameterMetaData;\n+import java.sql.PreparedStatement;\n+import java.sql.Ref;\n+import java.sql.ResultSet;\n+import java.sql.ResultSetMetaData;\n+import java.sql.RowId;\n+import java.sql.SQLClientInfoException;\n+import java.sql.SQLException;\n+import java.sql.SQLFeatureNotSupportedException;\n+import java.sql.SQLType;\n+import java.sql.SQLWarning;\n+import java.sql.SQLXML;\n+import java.sql.Savepoint;\n+import java.sql.ShardingKey;\n+import java.sql.Statement;\n+import java.sql.Struct;\n+import java.sql.Time;\n+import java.sql.Timestamp;\n import java.time.Duration;\n+import java.util.Calendar;\n+import java.util.List;\n import java.util.Map;\n+import java.util.Properties;\n import java.util.UUID;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.Executor;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n+import java.util.concurrent.Future;\n import java.util.concurrent.ThreadPoolExecutor;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicInteger;\n@@ -43,6 +83,7 @@\n import org.apache.iceberg.Table;\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n import org.apache.iceberg.types.Types;\n@@ -149,4 +190,1015 @@ public synchronized void testConcurrentConnections() throws InterruptedException\n     assertThat(executorService.awaitTermination(3, TimeUnit.MINUTES)).as(\"Timeout\").isTrue();\n     assertThat(Iterables.size(icebergTable.snapshots())).isEqualTo(7);\n   }\n+\n+  @Test\n+  public synchronized void testInitializeWithSlowConcurrentConnections()\n+      throws InterruptedException, SQLException, ExecutionException, ClassNotFoundException {\n+    // number of threads and requests to attempt.\n+    int parallelism = 2;\n+    // verifies that multiple calls to initialize with slow responses will not fail.\n+    Map<String, String> properties = Maps.newHashMap();\n+\n+    properties.put(CatalogProperties.WAREHOUSE_LOCATION, tableDir.getAbsolutePath());\n+    String testingDB = \"jdbc:slow:derby:memory:testDb;create=true\";\n+    new org.apache.derby.jdbc.EmbeddedDriver();\n+    properties.put(CatalogProperties.URI, testingDB);\n+    SlowDriver slowDriver = new SlowDriver(testingDB);\n+\n+    Callable<JdbcCatalog> makeCatalog =\n+        () -> {\n+          JdbcCatalog catalog = new JdbcCatalog();\n+          catalog.setConf(new Configuration());\n+          catalog.initialize(\"jdbc\", properties);\n+          return catalog;\n+        };\n+\n+    try {\n+      DriverManager.registerDriver(slowDriver);\n+      ExecutorService executorService =\n+          MoreExecutors.getExitingExecutorService(\n+              (ThreadPoolExecutor) Executors.newFixedThreadPool(parallelism));\n+\n+      List<Future<JdbcCatalog>> futures = Lists.newArrayList();\n+      for (int i = 0; i < parallelism; i++) {\n+        futures.add(executorService.submit(makeCatalog));\n+      }\n+      for (Future<JdbcCatalog> future : futures) {\n+        future.get();\n+      }\n+    } finally {\n+      DriverManager.deregisterDriver(slowDriver);\n+    }\n+  }\n+\n+  /** A Connection implementation that returns SlowPreparedStatements */\n+  private static class SlowJDBCConnection implements Connection {\n+    Connection delegate;\n+\n+    SlowJDBCConnection(Connection delegate) {\n+      this.delegate = delegate;\n+    }\n+\n+    @Override\n+    public Statement createStatement() throws SQLException {\n+      return delegate.createStatement();\n+    }\n+\n+    @Override\n+    public PreparedStatement prepareStatement(String sql) throws SQLException {\n+      return new SlowPreparedStatement(delegate.prepareStatement(sql));\n+    }\n+\n+    @Override\n+    public CallableStatement prepareCall(String sql) throws SQLException {\n+      return delegate.prepareCall(sql);\n+    }\n+\n+    @Override\n+    public String nativeSQL(String sql) throws SQLException {\n+      return delegate.nativeSQL(sql);\n+    }\n+\n+    @Override\n+    public void setAutoCommit(boolean autoCommit) throws SQLException {\n+      delegate.setAutoCommit(autoCommit);\n+    }\n+\n+    @Override\n+    public boolean getAutoCommit() throws SQLException {\n+      return delegate.getAutoCommit();\n+    }\n+\n+    @Override\n+    public void commit() throws SQLException {\n+      delegate.commit();\n+    }\n+\n+    @Override\n+    public void rollback() throws SQLException {\n+      delegate.rollback();\n+    }\n+\n+    @Override\n+    public void close() throws SQLException {\n+      delegate.close();\n+    }\n+\n+    @Override\n+    public boolean isClosed() throws SQLException {\n+      return delegate.isClosed();\n+    }\n+\n+    @Override\n+    public DatabaseMetaData getMetaData() throws SQLException {\n+      return delegate.getMetaData();\n+    }\n+\n+    @Override\n+    public void setReadOnly(boolean readOnly) throws SQLException {\n+      delegate.setReadOnly(readOnly);\n+    }\n+\n+    @Override\n+    public boolean isReadOnly() throws SQLException {\n+      return delegate.isReadOnly();\n+    }\n+\n+    @Override\n+    public void setCatalog(String catalog) throws SQLException {\n+      delegate.setCatalog(catalog);\n+    }\n+\n+    @Override\n+    public String getCatalog() throws SQLException {\n+      return delegate.getCatalog();\n+    }\n+\n+    @Override\n+    public void setTransactionIsolation(int level) throws SQLException {\n+      delegate.setTransactionIsolation(level);\n+    }\n+\n+    @Override\n+    public int getTransactionIsolation() throws SQLException {\n+      return delegate.getTransactionIsolation();\n+    }\n+\n+    @Override\n+    public SQLWarning getWarnings() throws SQLException {\n+      return delegate.getWarnings();\n+    }\n+\n+    @Override\n+    public void clearWarnings() throws SQLException {\n+      delegate.clearWarnings();\n+    }\n+\n+    @Override\n+    public Statement createStatement(int resultSetType, int resultSetConcurrency)\n+        throws SQLException {\n+      return delegate.createStatement(resultSetType, resultSetConcurrency);\n+    }\n+\n+    @Override\n+    public PreparedStatement prepareStatement(\n+        String sql, int resultSetType, int resultSetConcurrency) throws SQLException {\n+      return delegate.prepareStatement(sql, resultSetType, resultSetConcurrency);\n+    }\n+\n+    @Override\n+    public CallableStatement prepareCall(String sql, int resultSetType, int resultSetConcurrency)\n+        throws SQLException {\n+      return delegate.prepareCall(sql, resultSetType, resultSetConcurrency);\n+    }\n+\n+    @Override\n+    public Map<String, Class<?>> getTypeMap() throws SQLException {\n+      return delegate.getTypeMap();\n+    }\n+\n+    @Override\n+    public void setTypeMap(Map<String, Class<?>> map) throws SQLException {\n+      delegate.setTypeMap(map);\n+    }\n+\n+    @Override\n+    public void setHoldability(int holdability) throws SQLException {\n+      delegate.setHoldability(holdability);\n+    }\n+\n+    @Override\n+    public int getHoldability() throws SQLException {\n+      return delegate.getHoldability();\n+    }\n+\n+    @Override\n+    public Savepoint setSavepoint() throws SQLException {\n+      return delegate.setSavepoint();\n+    }\n+\n+    @Override\n+    public Savepoint setSavepoint(String name) throws SQLException {\n+      return delegate.setSavepoint(name);\n+    }\n+\n+    @Override\n+    public void rollback(Savepoint savepoint) throws SQLException {\n+      delegate.rollback(savepoint);\n+    }\n+\n+    @Override\n+    public void releaseSavepoint(Savepoint savepoint) throws SQLException {\n+      delegate.releaseSavepoint(savepoint);\n+    }\n+\n+    @Override\n+    public Statement createStatement(\n+        int resultSetType, int resultSetConcurrency, int resultSetHoldability) throws SQLException {\n+      return delegate.createStatement(resultSetType, resultSetConcurrency, resultSetHoldability);\n+    }\n+\n+    @Override\n+    public PreparedStatement prepareStatement(\n+        String sql, int resultSetType, int resultSetConcurrency, int resultSetHoldability)\n+        throws SQLException {\n+      return delegate.prepareStatement(\n+          sql, resultSetType, resultSetConcurrency, resultSetHoldability);\n+    }\n+\n+    @Override\n+    public CallableStatement prepareCall(\n+        String sql, int resultSetType, int resultSetConcurrency, int resultSetHoldability)\n+        throws SQLException {\n+      return delegate.prepareCall(sql, resultSetType, resultSetConcurrency, resultSetHoldability);\n+    }\n+\n+    @Override\n+    public PreparedStatement prepareStatement(String sql, int autoGeneratedKeys)\n+        throws SQLException {\n+      return delegate.prepareStatement(sql, autoGeneratedKeys);\n+    }\n+\n+    @Override\n+    public PreparedStatement prepareStatement(String sql, int[] columnIndexes) throws SQLException {\n+      return delegate.prepareStatement(sql, columnIndexes);\n+    }\n+\n+    @Override\n+    public PreparedStatement prepareStatement(String sql, String[] columnNames)\n+        throws SQLException {\n+      return delegate.prepareStatement(sql, columnNames);\n+    }\n+\n+    @Override\n+    public Clob createClob() throws SQLException {\n+      return delegate.createClob();\n+    }\n+\n+    @Override\n+    public Blob createBlob() throws SQLException {\n+      return delegate.createBlob();\n+    }\n+\n+    @Override\n+    public NClob createNClob() throws SQLException {\n+      return delegate.createNClob();\n+    }\n+\n+    @Override\n+    public SQLXML createSQLXML() throws SQLException {\n+      return delegate.createSQLXML();\n+    }\n+\n+    @Override\n+    public boolean isValid(int timeout) throws SQLException {\n+      return delegate.isValid(timeout);\n+    }\n+\n+    @Override\n+    public void setClientInfo(String name, String value) throws SQLClientInfoException {\n+      delegate.setClientInfo(name, value);\n+    }\n+\n+    @Override\n+    public void setClientInfo(Properties properties) throws SQLClientInfoException {\n+      delegate.setClientInfo(properties);\n+    }\n+\n+    @Override\n+    public String getClientInfo(String name) throws SQLException {\n+      return delegate.getClientInfo(name);\n+    }\n+\n+    @Override\n+    public Properties getClientInfo() throws SQLException {\n+      return delegate.getClientInfo();\n+    }\n+\n+    @Override\n+    public Array createArrayOf(String typeName, Object[] elements) throws SQLException {\n+      return delegate.createArrayOf(typeName, elements);\n+    }\n+\n+    @Override\n+    public Struct createStruct(String typeName, Object[] attributes) throws SQLException {\n+      return delegate.createStruct(typeName, attributes);\n+    }\n+\n+    @Override\n+    public void setSchema(String schema) throws SQLException {\n+      delegate.setSchema(schema);\n+    }\n+\n+    @Override\n+    public String getSchema() throws SQLException {\n+      return delegate.getSchema();\n+    }\n+\n+    @Override\n+    public void abort(Executor executor) throws SQLException {\n+      delegate.abort(executor);\n+    }\n+\n+    @Override\n+    public void setNetworkTimeout(Executor executor, int milliseconds) throws SQLException {\n+      delegate.setNetworkTimeout(executor, milliseconds);\n+    }\n+\n+    @Override\n+    public int getNetworkTimeout() throws SQLException {\n+      return delegate.getNetworkTimeout();\n+    }\n+\n+    @Override\n+    public void beginRequest() throws SQLException {\n+      delegate.beginRequest();\n+    }\n+\n+    @Override\n+    public void endRequest() throws SQLException {\n+      delegate.endRequest();\n+    }\n+\n+    @Override\n+    public boolean setShardingKeyIfValid(\n+        ShardingKey shardingKey, ShardingKey superShardingKey, int timeout) throws SQLException {\n+      return delegate.setShardingKeyIfValid(shardingKey, superShardingKey, timeout);\n+    }\n+\n+    @Override\n+    public boolean setShardingKeyIfValid(ShardingKey shardingKey, int timeout) throws SQLException {\n+      return delegate.setShardingKeyIfValid(shardingKey, timeout);\n+    }\n+\n+    @Override\n+    public void setShardingKey(ShardingKey shardingKey, ShardingKey superShardingKey)\n+        throws SQLException {\n+      delegate.setShardingKey(shardingKey, superShardingKey);\n+    }\n+\n+    @Override\n+    public void setShardingKey(ShardingKey shardingKey) throws SQLException {\n+      delegate.setShardingKey(shardingKey);\n+    }\n+\n+    @Override\n+    public <T> T unwrap(Class<T> iface) throws SQLException {\n+      return delegate.unwrap(iface);\n+    }\n+\n+    @Override\n+    public boolean isWrapperFor(Class<?> iface) throws SQLException {\n+      return delegate.isWrapperFor(iface);\n+    }\n+  }\n+\n+  /** A slow prepared statement that has a 500 ms delay before evaluating the execute() method. */\n+  private static class SlowPreparedStatement implements PreparedStatement {\n+    private final PreparedStatement delegate;\n+\n+    SlowPreparedStatement(PreparedStatement delegate) {\n+      this.delegate = delegate;\n+    }\n+\n+    @Override\n+    public ResultSet executeQuery() throws SQLException {\n+      return delegate.executeQuery();\n+    }\n+\n+    @Override\n+    public int executeUpdate() throws SQLException {\n+      return delegate.executeUpdate();\n+    }\n+\n+    @Override\n+    public void setNull(int parameterIndex, int sqlType) throws SQLException {\n+      delegate.setNull(parameterIndex, sqlType);\n+    }\n+\n+    @Override\n+    public void setBoolean(int parameterIndex, boolean x) throws SQLException {\n+      delegate.setBoolean(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setByte(int parameterIndex, byte x) throws SQLException {\n+      delegate.setByte(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setShort(int parameterIndex, short x) throws SQLException {\n+      delegate.setShort(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setInt(int parameterIndex, int x) throws SQLException {\n+      delegate.setInt(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setLong(int parameterIndex, long x) throws SQLException {\n+      delegate.setLong(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setFloat(int parameterIndex, float x) throws SQLException {\n+      delegate.setFloat(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setDouble(int parameterIndex, double x) throws SQLException {\n+      delegate.setDouble(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setBigDecimal(int parameterIndex, BigDecimal x) throws SQLException {\n+      delegate.setBigDecimal(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setString(int parameterIndex, String x) throws SQLException {\n+      delegate.setString(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setBytes(int parameterIndex, byte[] x) throws SQLException {\n+      delegate.setBytes(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setDate(int parameterIndex, Date x) throws SQLException {\n+      delegate.setDate(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setTime(int parameterIndex, Time x) throws SQLException {\n+      delegate.setTime(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setTimestamp(int parameterIndex, Timestamp x) throws SQLException {\n+      delegate.setTimestamp(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setAsciiStream(int parameterIndex, InputStream x, int length) throws SQLException {\n+      delegate.setAsciiStream(parameterIndex, x, length);\n+    }\n+\n+    @Deprecated(since = \"1.2\")\n+    @Override\n+    public void setUnicodeStream(int parameterIndex, InputStream inputStream, int length)\n+        throws SQLException {\n+      delegate.setUnicodeStream(parameterIndex, inputStream, length);\n+    }\n+\n+    @Override\n+    public void setBinaryStream(int parameterIndex, InputStream x, int length) throws SQLException {\n+      delegate.setBinaryStream(parameterIndex, x, length);\n+    }\n+\n+    @Override\n+    public void clearParameters() throws SQLException {\n+      delegate.clearParameters();\n+    }\n+\n+    @Override\n+    public void setObject(int parameterIndex, Object x, int targetSqlType) throws SQLException {\n+      delegate.setObject(parameterIndex, x, targetSqlType);\n+    }\n+\n+    @Override\n+    public void setObject(int parameterIndex, Object x) throws SQLException {\n+      delegate.setObject(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public boolean execute() throws SQLException {\n+      try {\n+        Thread.sleep(500);\n+      } catch (InterruptedException e) {\n+        throw new RuntimeException(e);\n+      }\n+      return delegate.execute();\n+    }\n+\n+    @Override\n+    public void addBatch() throws SQLException {\n+      delegate.addBatch();\n+    }\n+\n+    @Override\n+    public void setCharacterStream(int parameterIndex, Reader reader, int length)\n+        throws SQLException {\n+      delegate.setCharacterStream(parameterIndex, reader, length);\n+    }\n+\n+    @Override\n+    public void setRef(int parameterIndex, Ref x) throws SQLException {\n+      delegate.setRef(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setBlob(int parameterIndex, Blob x) throws SQLException {\n+      delegate.setBlob(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setClob(int parameterIndex, Clob x) throws SQLException {\n+      delegate.setClob(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setArray(int parameterIndex, Array x) throws SQLException {\n+      delegate.setArray(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public ResultSetMetaData getMetaData() throws SQLException {\n+      return delegate.getMetaData();\n+    }\n+\n+    @Override\n+    public void setDate(int parameterIndex, Date x, Calendar cal) throws SQLException {\n+      delegate.setDate(parameterIndex, x, cal);\n+    }\n+\n+    @Override\n+    public void setTime(int parameterIndex, Time x, Calendar cal) throws SQLException {\n+      delegate.setTime(parameterIndex, x, cal);\n+    }\n+\n+    @Override\n+    public void setTimestamp(int parameterIndex, Timestamp x, Calendar cal) throws SQLException {\n+      delegate.setTimestamp(parameterIndex, x, cal);\n+    }\n+\n+    @Override\n+    public void setNull(int parameterIndex, int sqlType, String typeName) throws SQLException {\n+      delegate.setNull(parameterIndex, sqlType, typeName);\n+    }\n+\n+    @Override\n+    public void setURL(int parameterIndex, URL x) throws SQLException {\n+      delegate.setURL(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public ParameterMetaData getParameterMetaData() throws SQLException {\n+      return delegate.getParameterMetaData();\n+    }\n+\n+    @Override\n+    public void setRowId(int parameterIndex, RowId x) throws SQLException {\n+      delegate.setRowId(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setNString(int parameterIndex, String value) throws SQLException {\n+      delegate.setNString(parameterIndex, value);\n+    }\n+\n+    @Override\n+    public void setNCharacterStream(int parameterIndex, Reader value, long length)\n+        throws SQLException {\n+      delegate.setNCharacterStream(parameterIndex, value, length);\n+    }\n+\n+    @Override\n+    public void setNClob(int parameterIndex, NClob value) throws SQLException {\n+      delegate.setNClob(parameterIndex, value);\n+    }\n+\n+    @Override\n+    public void setClob(int parameterIndex, Reader reader, long length) throws SQLException {\n+      delegate.setClob(parameterIndex, reader, length);\n+    }\n+\n+    @Override\n+    public void setBlob(int parameterIndex, InputStream inputStream, long length)\n+        throws SQLException {\n+      delegate.setBlob(parameterIndex, inputStream, length);\n+    }\n+\n+    @Override\n+    public void setNClob(int parameterIndex, Reader reader, long length) throws SQLException {\n+      delegate.setNClob(parameterIndex, reader, length);\n+    }\n+\n+    @Override\n+    public void setSQLXML(int parameterIndex, SQLXML xmlObject) throws SQLException {\n+      delegate.setSQLXML(parameterIndex, xmlObject);\n+    }\n+\n+    @Override\n+    public void setObject(int parameterIndex, Object x, int targetSqlType, int scaleOrLength)\n+        throws SQLException {\n+      delegate.setObject(parameterIndex, x, targetSqlType, scaleOrLength);\n+    }\n+\n+    @Override\n+    public void setAsciiStream(int parameterIndex, InputStream x, long length) throws SQLException {\n+      delegate.setAsciiStream(parameterIndex, x, length);\n+    }\n+\n+    @Override\n+    public void setBinaryStream(int parameterIndex, InputStream x, long length)\n+        throws SQLException {\n+      delegate.setBinaryStream(parameterIndex, x, length);\n+    }\n+\n+    @Override\n+    public void setCharacterStream(int parameterIndex, Reader reader, long length)\n+        throws SQLException {\n+      delegate.setCharacterStream(parameterIndex, reader, length);\n+    }\n+\n+    @Override\n+    public void setAsciiStream(int parameterIndex, InputStream x) throws SQLException {\n+      delegate.setAsciiStream(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setBinaryStream(int parameterIndex, InputStream x) throws SQLException {\n+      delegate.setBinaryStream(parameterIndex, x);\n+    }\n+\n+    @Override\n+    public void setCharacterStream(int parameterIndex, Reader reader) throws SQLException {\n+      delegate.setCharacterStream(parameterIndex, reader);\n+    }\n+\n+    @Override\n+    public void setNCharacterStream(int parameterIndex, Reader value) throws SQLException {\n+      delegate.setNCharacterStream(parameterIndex, value);\n+    }\n+\n+    @Override\n+    public void setClob(int parameterIndex, Reader reader) throws SQLException {\n+      delegate.setClob(parameterIndex, reader);\n+    }\n+\n+    @Override\n+    public void setBlob(int parameterIndex, InputStream inputStream) throws SQLException {\n+      delegate.setBlob(parameterIndex, inputStream);\n+    }\n+\n+    @Override\n+    public void setNClob(int parameterIndex, Reader reader) throws SQLException {\n+      delegate.setNClob(parameterIndex, reader);\n+    }\n+\n+    @Override\n+    public void setObject(int parameterIndex, Object x, SQLType targetSqlType, int scaleOrLength)\n+        throws SQLException {\n+      delegate.setObject(parameterIndex, x, targetSqlType, scaleOrLength);\n+    }\n+\n+    @Override\n+    public void setObject(int parameterIndex, Object x, SQLType targetSqlType) throws SQLException {\n+      delegate.setObject(parameterIndex, x, targetSqlType);\n+    }\n+\n+    @Override\n+    public long executeLargeUpdate() throws SQLException {\n+      return delegate.executeLargeUpdate();\n+    }\n+\n+    @Override\n+    public ResultSet executeQuery(String sql) throws SQLException {\n+      return delegate.executeQuery(sql);\n+    }\n+\n+    @Override\n+    public int executeUpdate(String sql) throws SQLException {\n+      return delegate.executeUpdate(sql);\n+    }\n+\n+    @Override\n+    public void close() throws SQLException {\n+      delegate.close();\n+    }\n+\n+    @Override\n+    public int getMaxFieldSize() throws SQLException {\n+      return delegate.getMaxFieldSize();\n+    }\n+\n+    @Override\n+    public void setMaxFieldSize(int max) throws SQLException {\n+      delegate.setMaxFieldSize(max);\n+    }\n+\n+    @Override\n+    public int getMaxRows() throws SQLException {\n+      return delegate.getMaxRows();\n+    }\n+\n+    @Override\n+    public void setMaxRows(int max) throws SQLException {\n+      delegate.setMaxRows(max);\n+    }\n+\n+    @Override\n+    public void setEscapeProcessing(boolean enable) throws SQLException {\n+      delegate.setEscapeProcessing(enable);\n+    }\n+\n+    @Override\n+    public int getQueryTimeout() throws SQLException {\n+      return delegate.getQueryTimeout();\n+    }\n+\n+    @Override\n+    public void setQueryTimeout(int seconds) throws SQLException {\n+      delegate.setQueryTimeout(seconds);\n+    }\n+\n+    @Override\n+    public void cancel() throws SQLException {\n+      delegate.cancel();\n+    }\n+\n+    @Override\n+    public SQLWarning getWarnings() throws SQLException {\n+      return delegate.getWarnings();\n+    }\n+\n+    @Override\n+    public void clearWarnings() throws SQLException {\n+      delegate.clearWarnings();\n+    }\n+\n+    @Override\n+    public void setCursorName(String name) throws SQLException {\n+      delegate.setCursorName(name);\n+    }\n+\n+    @Override\n+    public boolean execute(String sql) throws SQLException {\n+      return delegate.execute(sql);\n+    }\n+\n+    @Override\n+    public ResultSet getResultSet() throws SQLException {\n+      return delegate.getResultSet();\n+    }\n+\n+    @Override\n+    public int getUpdateCount() throws SQLException {\n+      return delegate.getUpdateCount();\n+    }\n+\n+    @Override\n+    public boolean getMoreResults() throws SQLException {\n+      return delegate.getMoreResults();\n+    }\n+\n+    @Override\n+    public void setFetchDirection(int direction) throws SQLException {\n+      delegate.setFetchDirection(direction);\n+    }\n+\n+    @Override\n+    public int getFetchDirection() throws SQLException {\n+      return delegate.getFetchDirection();\n+    }\n+\n+    @Override\n+    public void setFetchSize(int rows) throws SQLException {\n+      delegate.setFetchSize(rows);\n+    }\n+\n+    @Override\n+    public int getFetchSize() throws SQLException {\n+      return delegate.getFetchSize();\n+    }\n+\n+    @Override\n+    public int getResultSetConcurrency() throws SQLException {\n+      return delegate.getResultSetConcurrency();\n+    }\n+\n+    @Override\n+    public int getResultSetType() throws SQLException {\n+      return delegate.getResultSetType();\n+    }\n+\n+    @Override\n+    public void addBatch(String sql) throws SQLException {\n+      delegate.addBatch(sql);\n+    }\n+\n+    @Override\n+    public void clearBatch() throws SQLException {\n+      delegate.clearBatch();\n+    }\n+\n+    @Override\n+    public int[] executeBatch() throws SQLException {\n+      return delegate.executeBatch();\n+    }\n+\n+    @Override\n+    public Connection getConnection() throws SQLException {\n+      return delegate.getConnection();\n+    }\n+\n+    @Override\n+    public boolean getMoreResults(int current) throws SQLException {\n+      return delegate.getMoreResults(current);\n+    }\n+\n+    @Override\n+    public ResultSet getGeneratedKeys() throws SQLException {\n+      return delegate.getGeneratedKeys();\n+    }\n+\n+    @Override\n+    public int executeUpdate(String sql, int autoGeneratedKeys) throws SQLException {\n+      return delegate.executeUpdate(sql, autoGeneratedKeys);\n+    }\n+\n+    @Override\n+    public int executeUpdate(String sql, int[] columnIndexes) throws SQLException {\n+      return delegate.executeUpdate(sql, columnIndexes);\n+    }\n+\n+    @Override\n+    public int executeUpdate(String sql, String[] columnNames) throws SQLException {\n+      return delegate.executeUpdate(sql, columnNames);\n+    }\n+\n+    @Override\n+    public boolean execute(String sql, int autoGeneratedKeys) throws SQLException {\n+      return delegate.execute(sql, autoGeneratedKeys);\n+    }\n+\n+    @Override\n+    public boolean execute(String sql, int[] columnIndexes) throws SQLException {\n+      return delegate.execute(sql, columnIndexes);\n+    }\n+\n+    @Override\n+    public boolean execute(String sql, String[] columnNames) throws SQLException {\n+      return delegate.execute(sql, columnNames);\n+    }\n+\n+    @Override\n+    public int getResultSetHoldability() throws SQLException {\n+      return delegate.getResultSetHoldability();\n+    }\n+\n+    @Override\n+    public boolean isClosed() throws SQLException {\n+      return delegate.isClosed();\n+    }\n+\n+    @Override\n+    public void setPoolable(boolean poolable) throws SQLException {\n+      delegate.setPoolable(poolable);\n+    }\n+\n+    @Override\n+    public boolean isPoolable() throws SQLException {\n+      return delegate.isPoolable();\n+    }\n+\n+    @Override\n+    public void closeOnCompletion() throws SQLException {\n+      delegate.closeOnCompletion();\n+    }\n+\n+    @Override\n+    public boolean isCloseOnCompletion() throws SQLException {\n+      return delegate.isCloseOnCompletion();\n+    }\n+\n+    @Override\n+    public long getLargeUpdateCount() throws SQLException {\n+      return delegate.getLargeUpdateCount();\n+    }\n+\n+    @Override\n+    public void setLargeMaxRows(long max) throws SQLException {\n+      delegate.setLargeMaxRows(max);\n+    }\n+\n+    @Override\n+    public long getLargeMaxRows() throws SQLException {\n+      return delegate.getLargeMaxRows();\n+    }\n+\n+    @Override\n+    public long[] executeLargeBatch() throws SQLException {\n+      return delegate.executeLargeBatch();\n+    }\n+\n+    @Override\n+    public long executeLargeUpdate(String sql) throws SQLException {\n+      return delegate.executeLargeUpdate(sql);\n+    }\n+\n+    @Override\n+    public long executeLargeUpdate(String sql, int autoGeneratedKeys) throws SQLException {\n+      return delegate.executeLargeUpdate(sql, autoGeneratedKeys);\n+    }\n+\n+    @Override\n+    public long executeLargeUpdate(String sql, int[] columnIndexes) throws SQLException {\n+      return delegate.executeLargeUpdate(sql, columnIndexes);\n+    }\n+\n+    @Override\n+    public long executeLargeUpdate(String sql, String[] columnNames) throws SQLException {\n+      return delegate.executeLargeUpdate(sql, columnNames);\n+    }\n+\n+    @Override\n+    public String enquoteLiteral(String val) throws SQLException {\n+      return delegate.enquoteLiteral(val);\n+    }\n+\n+    @Override\n+    public String enquoteIdentifier(String identifier, boolean alwaysQuote) throws SQLException {\n+      return delegate.enquoteIdentifier(identifier, alwaysQuote);\n+    }\n+\n+    @Override\n+    public boolean isSimpleIdentifier(String identifier) throws SQLException {\n+      return delegate.isSimpleIdentifier(identifier);\n+    }\n+\n+    @Override\n+    public String enquoteNCharLiteral(String val) throws SQLException {\n+      return delegate.enquoteNCharLiteral(val);\n+    }\n+\n+    @Override\n+    public <T> T unwrap(Class<T> iface) throws SQLException {\n+      return delegate.unwrap(iface);\n+    }\n+\n+    @Override\n+    public boolean isWrapperFor(Class<?> iface) throws SQLException {\n+      return delegate.isWrapperFor(iface);\n+    }\n+  }\n+\n+  /**\n+   * A driver that wraps a true driver implementation and returns SlopPreparedStatements. URL for\n+   * this driver is \"jdbc:slow:true_driver_url\":\n+   */\n+  private static class SlowDriver implements Driver {\n+    private static final String PREFIX = \"jdbc:slow:\";\n+\n+    private Driver delegate;\n+\n+    SlowDriver(String url) throws SQLException {\n+      if (!url.startsWith(PREFIX)) {\n+        throw new SQLException(\"url must start with \" + PREFIX);\n+      }\n+      delegate = DriverManager.getDriver(rewriteUrl(url));\n+    }\n+\n+    static String rewriteUrl(String url) {\n+      return url.startsWith(PREFIX) ? \"jdbc:\" + url.substring(PREFIX.length()) : url;\n+    }\n+\n+    @Override\n+    public Connection connect(String url, Properties info) throws SQLException {\n+      return new SlowJDBCConnection(delegate.connect(rewriteUrl(url), info));\n+    }\n+\n+    @Override\n+    public boolean acceptsURL(String url) throws SQLException {\n+      return url.startsWith(PREFIX) && delegate.acceptsURL(rewriteUrl(url));\n+    }\n+\n+    @Override\n+    public DriverPropertyInfo[] getPropertyInfo(String url, Properties info) throws SQLException {\n+      return delegate.getPropertyInfo(url, info);\n+    }\n+\n+    @Override\n+    public int getMajorVersion() {\n+      return delegate.getMajorVersion();\n+    }\n+\n+    @Override\n+    public int getMinorVersion() {\n+      return delegate.getMinorVersion();\n+    }\n+\n+    @Override\n+    public boolean jdbcCompliant() {\n+      return delegate.jdbcCompliant();\n+    }\n+\n+    @Override\n+    public java.util.logging.Logger getParentLogger() throws SQLFeatureNotSupportedException {\n+      return delegate.getParentLogger();\n+    }\n+  }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13322",
    "pr_id": 13322,
    "issue_id": 13294,
    "repo": "apache/iceberg",
    "problem_statement": "Clean expired metadata doesn't work when there is no snapshot\n### Feature Request / Improvement\n\nExpireSnapshots is NOP when there is no snapshot. But I think it's more intuitive to clean expired metadata in that case, if we're told to do so. For example, I would expect this test to pass\n```java\n  @TestTemplate\n  public void test() throws Exception {\n    table.updateSchema().addColumn(\"extra_col1\", Types.StringType.get()).commit();\n    table.updateSchema().addColumn(\"extra_col2\", Types.StringType.get()).commit();\n\n    removeSnapshots(table)\n        .expireOlderThan(System.currentTimeMillis())\n        .cleanExpiredMetadata(true)\n        .commit();\n\n    assertThat(table.schemas()).hasSize(1);\n  }\n```\n\n### Query engine\n\nNone\n\n### Willingness to contribute\n\n- [x] I can contribute this improvement/feature independently\n- [x] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [ ] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 127,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/RemoveSnapshots.java",
      "core/src/test/java/org/apache/iceberg/TestRemoveSnapshots.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/TestRemoveSnapshots.java"
    ],
    "base_commit": "7ceb7402ad076b671131ad0e757d7c9a5420ea84",
    "head_commit": "2380b3262cab81a1658c921808076b4dc75c4ac6",
    "repo_url": "https://github.com/apache/iceberg/pull/13322",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13322",
    "dockerfile": "",
    "pr_merged_at": "2025-06-18T20:20:58.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java b/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java\nindex e3e8c8fb1018..f9a54d8a7bf3 100644\n--- a/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java\n+++ b/core/src/main/java/org/apache/iceberg/RemoveSnapshots.java\n@@ -178,7 +178,9 @@ public List<Snapshot> apply() {\n \n   private TableMetadata internalApply() {\n     this.base = ops.refresh();\n-    if (base.snapshots().isEmpty()) {\n+    // attempt to clean expired metadata even if there are no snapshots to expire\n+    // table metadata builder takes care of the case when this should actually be a no-op\n+    if (base.snapshots().isEmpty() && !cleanExpiredMetadata) {\n       return base;\n     }\n \n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/TestRemoveSnapshots.java b/core/src/test/java/org/apache/iceberg/TestRemoveSnapshots.java\nindex 012e72c40d7d..1318c2b3325c 100644\n--- a/core/src/test/java/org/apache/iceberg/TestRemoveSnapshots.java\n+++ b/core/src/test/java/org/apache/iceberg/TestRemoveSnapshots.java\n@@ -1799,6 +1799,36 @@ public void testExpireSnapshotsWithExecutor() {\n         .isGreaterThan(0);\n   }\n \n+  @TestTemplate\n+  public void testRemoveMetadataWithNoSnapshots() throws Exception {\n+    table.updateSchema().addColumn(\"extra_col1\", Types.StringType.get()).commit();\n+    table.updateSchema().addColumn(\"extra_col2\", Types.StringType.get()).commit();\n+    table.updateSpec().addField(\"extra_col2\").commit();\n+    assertThat(table.schemas()).hasSize(3);\n+    assertThat(table.specs()).hasSize(2);\n+\n+    removeSnapshots(table)\n+        .expireOlderThan(System.currentTimeMillis())\n+        .retainLast(1)\n+        .cleanExpiredMetadata(true)\n+        .commit();\n+    assertThat(table.schemas()).as(\"Expired schemas should be removed\").hasSize(1);\n+    assertThat(table.specs()).as(\"Expired specs should be removed\").hasSize(1);\n+  }\n+\n+  @TestTemplate\n+  public void testRemoveSnapshotsNoOp() throws Exception {\n+    TableMetadata current = table.ops().current();\n+    removeSnapshots(table)\n+        .expireOlderThan(System.currentTimeMillis())\n+        .retainLast(1)\n+        .cleanExpiredMetadata(true)\n+        .commit();\n+    assertThat(table.ops().current())\n+        .as(\"No snapshot or metadata to remove, should be a no-op\")\n+        .isSameAs(current);\n+  }\n+\n   private Set<String> manifestPaths(Snapshot snapshot, FileIO io) {\n     return snapshot.allManifests(io).stream().map(ManifestFile::path).collect(Collectors.toSet());\n   }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13244",
    "pr_id": 13244,
    "issue_id": 13237,
    "repo": "apache/iceberg",
    "problem_statement": "Parameterize tests that verify Java/Kryo serialization\n### Feature Request / Improvement\n\nIn a bunch of places around the codebase we typically test Java & Kryo serialization as following\n\n```\n@Test\n  public void testResolvingFileIOKryoSerialization() throws IOException {\n    FileIO testResolvingFileIO = new ResolvingFileIO();\n\n    // resolving fileIO should be serializable when properties are passed as immutable map\n    testResolvingFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\"));\n    FileIO roundTripSerializedFileIO =\n            TestHelpers.KryoHelpers.roundTripSerialize(testResolvingFileIO);\n    assertThat(roundTripSerializedFileIO.properties()).isEqualTo(testResolvingFileIO.properties());\n  }\n\n  @Test\n  public void testResolvingFileIOJavaSerialization() throws IOException, ClassNotFoundException {\n    FileIO testResolvingFileIO = new ResolvingFileIO();\n\n    // resolving fileIO should be serializable when properties are passed as immutable map\n    testResolvingFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\"));\n    FileIO roundTripSerializedFileIO = TestHelpers.roundTripSerialize(testResolvingFileIO);\n    assertThat(roundTripSerializedFileIO.properties()).isEqualTo(testResolvingFileIO.properties());\n  }\n```\n\nThe main difference in those tests is calling `TestHelpers.roundTripSerialize` vs `TestHelpers.KryoHelpers.roundTripSerialize`.\nIt would be good to unify those tests and make the type of serialization a test parameter\n\n### Query engine\n\nNone\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [ ] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 174,
    "test_files_count": 13,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "api/src/test/java/org/apache/iceberg/TestHelpers.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java",
      "aws/src/test/java/org/apache/iceberg/aws/TestAwsClientFactories.java",
      "aws/src/test/java/org/apache/iceberg/aws/TestAwsProperties.java",
      "azure/src/integration/java/org/apache/iceberg/azure/adlsv2/ADLSFileIOTest.java",
      "azure/src/test/java/org/apache/iceberg/azure/AzurePropertiesTest.java",
      "azure/src/test/java/org/apache/iceberg/azure/adlsv2/VendedAdlsCredentialProviderTest.java",
      "core/src/test/java/org/apache/iceberg/hadoop/HadoopFileIOTest.java",
      "core/src/test/java/org/apache/iceberg/io/TestResolvingIO.java",
      "core/src/test/java/org/apache/iceberg/io/TestStorageCredential.java",
      "core/src/test/java/org/apache/iceberg/util/TestDataFileSet.java",
      "core/src/test/java/org/apache/iceberg/util/TestDeleteFileSet.java",
      "gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSFileIOTest.java"
    ],
    "pr_changed_test_files": [
      "api/src/test/java/org/apache/iceberg/TestHelpers.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java",
      "aws/src/test/java/org/apache/iceberg/aws/TestAwsClientFactories.java",
      "aws/src/test/java/org/apache/iceberg/aws/TestAwsProperties.java",
      "azure/src/integration/java/org/apache/iceberg/azure/adlsv2/ADLSFileIOTest.java",
      "azure/src/test/java/org/apache/iceberg/azure/AzurePropertiesTest.java",
      "azure/src/test/java/org/apache/iceberg/azure/adlsv2/VendedAdlsCredentialProviderTest.java",
      "core/src/test/java/org/apache/iceberg/hadoop/HadoopFileIOTest.java",
      "core/src/test/java/org/apache/iceberg/io/TestResolvingIO.java",
      "core/src/test/java/org/apache/iceberg/io/TestStorageCredential.java",
      "core/src/test/java/org/apache/iceberg/util/TestDataFileSet.java",
      "core/src/test/java/org/apache/iceberg/util/TestDeleteFileSet.java",
      "gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSFileIOTest.java"
    ],
    "base_commit": "d3ebea5ada4e8ccf1308d22c44051e90ee1bf651",
    "head_commit": "6095aee32c5b8fdb1f246a59f5a8660be97c6ee9",
    "repo_url": "https://github.com/apache/iceberg/pull/13244",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13244",
    "dockerfile": "",
    "pr_merged_at": "2025-06-06T12:48:47.000Z",
    "patch": "",
    "test_patch": "diff --git a/api/src/test/java/org/apache/iceberg/TestHelpers.java b/api/src/test/java/org/apache/iceberg/TestHelpers.java\nindex ef83b26db469..a21e3752e84d 100644\n--- a/api/src/test/java/org/apache/iceberg/TestHelpers.java\n+++ b/api/src/test/java/org/apache/iceberg/TestHelpers.java\n@@ -41,6 +41,7 @@\n import java.util.Set;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n+import java.util.stream.Stream;\n import org.apache.iceberg.expressions.BoundPredicate;\n import org.apache.iceberg.expressions.BoundSetPredicate;\n import org.apache.iceberg.expressions.Expression;\n@@ -48,10 +49,27 @@\n import org.apache.iceberg.expressions.UnboundPredicate;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.util.ByteBuffers;\n+import org.junit.jupiter.api.Named;\n+import org.junit.jupiter.params.provider.Arguments;\n import org.objenesis.strategy.StdInstantiatorStrategy;\n \n public class TestHelpers {\n \n+  @FunctionalInterface\n+  public interface RoundTripSerializer<T> {\n+    T apply(T obj) throws IOException, ClassNotFoundException;\n+  }\n+\n+  public static <T> Stream<Arguments> serializers() {\n+    return Stream.of(\n+        Arguments.of(\n+            Named.<RoundTripSerializer<T>>of(\n+                \"KryoSerialization\", TestHelpers.KryoHelpers::roundTripSerialize)),\n+        Arguments.of(\n+            Named.<RoundTripSerializer<T>>of(\n+                \"JavaSerialization\", TestHelpers::roundTripSerialize)));\n+  }\n+\n   private TestHelpers() {}\n \n   public static final int MAX_FORMAT_VERSION = 4;\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java b/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java\nindex 5c3a7d88479d..0e84f6b95d6c 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java\n@@ -85,6 +85,8 @@\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Disabled;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.MethodSource;\n import org.mockito.Mockito;\n import org.testcontainers.containers.MinIOContainer;\n import org.testcontainers.junit.jupiter.Container;\n@@ -443,42 +445,37 @@ public void testFileIOJsonSerialization() {\n     }\n   }\n \n-  @Test\n-  public void testS3FileIOKryoSerialization() throws IOException {\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void testS3FileIOSerialization(TestHelpers.RoundTripSerializer<FileIO> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n     FileIO testS3FileIO = new S3FileIO();\n \n     // s3 fileIO should be serializable when properties are passed as immutable map\n-    testS3FileIO.initialize(ImmutableMap.of(\"k1\", \"v1\"));\n-    FileIO roundTripSerializedFileIO = TestHelpers.KryoHelpers.roundTripSerialize(testS3FileIO);\n+    testS3FileIO.initialize(ImmutableMap.of(\"k1\", \"v1\", \"k2\", \"v2\"));\n+    FileIO roundTripSerializedFileIO = roundTripSerializer.apply(testS3FileIO);\n \n     assertThat(roundTripSerializedFileIO.properties()).isEqualTo(testS3FileIO.properties());\n   }\n \n-  @Test\n-  public void testS3FileIOWithEmptyPropsKryoSerialization() throws IOException {\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void testS3FileIOWithEmptyPropsSerialization(\n+      TestHelpers.RoundTripSerializer<FileIO> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n     FileIO testS3FileIO = new S3FileIO();\n \n     // s3 fileIO should be serializable when properties passed as empty immutable map\n     testS3FileIO.initialize(ImmutableMap.of());\n-    FileIO roundTripSerializedFileIO = TestHelpers.KryoHelpers.roundTripSerialize(testS3FileIO);\n+    FileIO roundTripSerializedFileIO = roundTripSerializer.apply(testS3FileIO);\n \n     assertThat(roundTripSerializedFileIO.properties()).isEqualTo(testS3FileIO.properties());\n   }\n \n-  @Test\n-  public void fileIOWithStorageCredentialsKryoSerialization() throws IOException {\n-    S3FileIO fileIO = new S3FileIO();\n-    fileIO.setCredentials(\n-        ImmutableList.of(\n-            StorageCredential.create(\"prefix\", Map.of(\"key1\", \"val1\", \"key2\", \"val2\"))));\n-    fileIO.initialize(Map.of());\n-\n-    assertThat(TestHelpers.KryoHelpers.roundTripSerialize(fileIO).credentials())\n-        .isEqualTo(fileIO.credentials());\n-  }\n-\n-  @Test\n-  public void fileIOWithStorageCredentialsJavaSerialization()\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void fileIOWithStorageCredentialsSerialization(\n+      TestHelpers.RoundTripSerializer<S3FileIO> roundTripSerializer)\n       throws IOException, ClassNotFoundException {\n     S3FileIO fileIO = new S3FileIO();\n     fileIO.setCredentials(\n@@ -486,56 +483,15 @@ public void fileIOWithStorageCredentialsJavaSerialization()\n             StorageCredential.create(\"prefix\", Map.of(\"key1\", \"val1\", \"key2\", \"val2\"))));\n     fileIO.initialize(Map.of());\n \n-    assertThat(TestHelpers.roundTripSerialize(fileIO).credentials())\n-        .isEqualTo(fileIO.credentials());\n-  }\n-\n-  @Test\n-  public void fileIOWithPrefixedS3ClientWithoutCredentialsKryoSerialization() throws IOException {\n-    S3FileIO io = new S3FileIO();\n-    io.initialize(Map.of(AwsClientProperties.CLIENT_REGION, \"us-east-1\"));\n-\n-    assertThat(io.client()).isInstanceOf(S3Client.class);\n-    assertThat(io.asyncClient()).isInstanceOf(S3AsyncClient.class);\n-    assertThat(io.client(\"s3a://my-bucket/my-path\")).isInstanceOf(S3Client.class);\n-    assertThat(io.asyncClient(\"s3a://my-bucket/my-path\")).isInstanceOf(S3AsyncClient.class);\n-\n-    S3FileIO fileIO = TestHelpers.KryoHelpers.roundTripSerialize(io);\n-    assertThat(fileIO.credentials()).isEqualTo(io.credentials()).isEmpty();\n-\n-    assertThat(fileIO.client()).isInstanceOf(S3Client.class);\n-    assertThat(fileIO.asyncClient()).isInstanceOf(S3AsyncClient.class);\n-    assertThat(fileIO.client(\"s3a://my-bucket/my-path\")).isInstanceOf(S3Client.class);\n-    assertThat(fileIO.asyncClient(\"s3a://my-bucket/my-path\")).isInstanceOf(S3AsyncClient.class);\n+    assertThat(roundTripSerializer.apply(fileIO).credentials()).isEqualTo(fileIO.credentials());\n   }\n \n-  @Test\n-  public void fileIOWithPrefixedS3ClientKryoSerialization() throws IOException {\n-    S3FileIO io = new S3FileIO();\n-    io.setCredentials(\n-        ImmutableList.of(\n-            StorageCredential.create(\"s3://my-bucket/my-path/table1\", Map.of(\"key1\", \"val1\"))));\n-    io.initialize(Map.of(AwsClientProperties.CLIENT_REGION, \"us-east-1\"));\n-\n-    // there should be a client for the generic and specific storage prefix available\n-    assertThat(io.client()).isInstanceOf(S3Client.class);\n-    assertThat(io.asyncClient()).isInstanceOf(S3AsyncClient.class);\n-    assertThat(io.client(\"s3://my-bucket/my-path\")).isInstanceOf(S3Client.class);\n-    assertThat(io.asyncClient(\"s3://my-bucket/my-path\")).isInstanceOf(S3AsyncClient.class);\n-\n-    S3FileIO fileIO = TestHelpers.KryoHelpers.roundTripSerialize(io);\n-    assertThat(fileIO.credentials()).isEqualTo(io.credentials());\n-\n-    // make sure there's a client for the generic and specific storage prefix available after ser/de\n-    assertThat(fileIO.client()).isInstanceOf(S3Client.class);\n-    assertThat(fileIO.asyncClient()).isInstanceOf(S3AsyncClient.class);\n-    assertThat(fileIO.client(\"s3://my-bucket/my-path\")).isInstanceOf(S3Client.class);\n-    assertThat(fileIO.asyncClient(\"s3://my-bucket/my-path\")).isInstanceOf(S3AsyncClient.class);\n-  }\n-\n-  @Test\n-  public void fileIOWithPrefixedS3ClientWithoutCredentialsJavaSerialization()\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void fileIOWithPrefixedS3ClientWithoutCredentialsSerialization(\n+      TestHelpers.RoundTripSerializer<S3FileIO> roundTripSerializer)\n       throws IOException, ClassNotFoundException {\n+\n     S3FileIO io = new S3FileIO();\n     io.initialize(Map.of(AwsClientProperties.CLIENT_REGION, \"us-east-1\"));\n \n@@ -544,7 +500,7 @@ public void fileIOWithPrefixedS3ClientWithoutCredentialsJavaSerialization()\n     assertThat(io.client(\"s3a://my-bucket/my-path\")).isInstanceOf(S3Client.class);\n     assertThat(io.asyncClient(\"s3a://my-bucket/my-path\")).isInstanceOf(S3AsyncClient.class);\n \n-    S3FileIO fileIO = TestHelpers.roundTripSerialize(io);\n+    S3FileIO fileIO = roundTripSerializer.apply(io);\n     assertThat(fileIO.credentials()).isEqualTo(io.credentials()).isEmpty();\n \n     assertThat(fileIO.client()).isInstanceOf(S3Client.class);\n@@ -553,8 +509,10 @@ public void fileIOWithPrefixedS3ClientWithoutCredentialsJavaSerialization()\n     assertThat(fileIO.asyncClient(\"s3a://my-bucket/my-path\")).isInstanceOf(S3AsyncClient.class);\n   }\n \n-  @Test\n-  public void fileIOWithPrefixedS3ClientJavaSerialization()\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void fileIOWithPrefixedS3ClientSerialization(\n+      TestHelpers.RoundTripSerializer<S3FileIO> roundTripSerializer)\n       throws IOException, ClassNotFoundException {\n     S3FileIO io = new S3FileIO();\n     io.setCredentials(\n@@ -568,7 +526,7 @@ public void fileIOWithPrefixedS3ClientJavaSerialization()\n     assertThat(io.client(\"s3://my-bucket/my-path\")).isInstanceOf(S3Client.class);\n     assertThat(io.asyncClient(\"s3://my-bucket/my-path\")).isInstanceOf(S3AsyncClient.class);\n \n-    S3FileIO fileIO = TestHelpers.roundTripSerialize(io);\n+    S3FileIO fileIO = roundTripSerializer.apply(io);\n     assertThat(fileIO.credentials()).isEqualTo(io.credentials());\n \n     // make sure there's a client for the generic and specific storage prefix available after ser/de\n@@ -578,17 +536,6 @@ public void fileIOWithPrefixedS3ClientJavaSerialization()\n     assertThat(fileIO.asyncClient(\"s3://my-bucket/my-path\")).isInstanceOf(S3AsyncClient.class);\n   }\n \n-  @Test\n-  public void testS3FileIOJavaSerialization() throws IOException, ClassNotFoundException {\n-    FileIO testS3FileIO = new S3FileIO();\n-\n-    // s3 fileIO should be serializable when properties are passed as immutable map\n-    testS3FileIO.initialize(ImmutableMap.of(\"k1\", \"v1\"));\n-    FileIO roundTripSerializedFileIO = TestHelpers.roundTripSerialize(testS3FileIO);\n-\n-    assertThat(roundTripSerializedFileIO.properties()).isEqualTo(testS3FileIO.properties());\n-  }\n-\n   @Test\n   public void testResolvingFileIOLoad() {\n     ResolvingFileIO resolvingFileIO = new ResolvingFileIO();\n@@ -662,8 +609,10 @@ public void testInputFileWithManifest() throws IOException {\n     verify(s3mock, never()).headObject(any(HeadObjectRequest.class));\n   }\n \n-  @Test\n-  public void resolvingFileIOLoadWithoutStorageCredentials()\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void resolvingFileIOLoadWithoutStorageCredentials(\n+      TestHelpers.RoundTripSerializer<ResolvingFileIO> roundTripSerializer)\n       throws IOException, ClassNotFoundException {\n     ResolvingFileIO resolvingFileIO = new ResolvingFileIO();\n     resolvingFileIO.initialize(ImmutableMap.of(AwsClientProperties.CLIENT_REGION, \"us-east-1\"));\n@@ -686,8 +635,8 @@ public void resolvingFileIOLoadWithoutStorageCredentials()\n                   .isInstanceOf(S3AsyncClient.class);\n             });\n \n-    // make sure credentials can be accessed after kryo serde\n-    ResolvingFileIO resolvingIO = TestHelpers.KryoHelpers.roundTripSerialize(resolvingFileIO);\n+    // make sure credentials can be accessed after serde\n+    ResolvingFileIO resolvingIO = roundTripSerializer.apply(resolvingFileIO);\n     assertThat(resolvingIO.credentials()).isEmpty();\n     result =\n         DynMethods.builder(\"io\")\n@@ -706,31 +655,12 @@ public void resolvingFileIOLoadWithoutStorageCredentials()\n                   .isSameAs(fileIO.asyncClient())\n                   .isInstanceOf(S3AsyncClient.class);\n             });\n-\n-    // make sure credentials can be accessed after java serde\n-    resolvingIO = TestHelpers.roundTripSerialize(resolvingFileIO);\n-    assertThat(resolvingIO.credentials()).isEmpty();\n-    result =\n-        DynMethods.builder(\"io\")\n-            .hiddenImpl(ResolvingFileIO.class, String.class)\n-            .build(resolvingIO)\n-            .invoke(\"s3://foo/bar\");\n-    assertThat(result)\n-        .isInstanceOf(S3FileIO.class)\n-        .asInstanceOf(InstanceOfAssertFactories.type(S3FileIO.class))\n-        .satisfies(\n-            fileIO -> {\n-              assertThat(fileIO.client(\"s3://foo/bar\"))\n-                  .isSameAs(fileIO.client())\n-                  .isInstanceOf(S3Client.class);\n-              assertThat(fileIO.asyncClient(\"s3://foo/bar\"))\n-                  .isSameAs(fileIO.asyncClient())\n-                  .isInstanceOf(S3AsyncClient.class);\n-            });\n   }\n \n-  @Test\n-  public void resolvingFileIOLoadWithStorageCredentials()\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void resolvingFileIOLoadWithStorageCredentials(\n+      TestHelpers.RoundTripSerializer<ResolvingFileIO> roundTripSerializer)\n       throws IOException, ClassNotFoundException {\n     StorageCredential credential = StorageCredential.create(\"s3://foo/bar\", Map.of(\"key1\", \"val1\"));\n     List<StorageCredential> storageCredentials = ImmutableList.of(credential);\n@@ -760,33 +690,8 @@ public void resolvingFileIOLoadWithStorageCredentials()\n               .isInstanceOf(S3AsyncClient.class);\n         });\n \n-    // make sure credentials are still present after kryo serde\n-    ResolvingFileIO resolvingIO = TestHelpers.KryoHelpers.roundTripSerialize(resolvingFileIO);\n-    assertThat(resolvingIO.credentials()).isEqualTo(storageCredentials);\n-    result =\n-        DynMethods.builder(\"io\")\n-            .hiddenImpl(ResolvingFileIO.class, String.class)\n-            .build(resolvingIO)\n-            .invoke(\"s3://foo/bar\");\n-    io =\n-        assertThat(result)\n-            .isInstanceOf(S3FileIO.class)\n-            .asInstanceOf(InstanceOfAssertFactories.type(S3FileIO.class));\n-    io.extracting(S3FileIO::credentials).isEqualTo(storageCredentials);\n-    io.satisfies(\n-        fileIO -> {\n-          // make sure there are two separate S3 clients for different prefixes and that the\n-          // underlying sync/async client is set\n-          assertThat(fileIO.client(\"s3://foo/bar\"))\n-              .isNotSameAs(fileIO.client())\n-              .isInstanceOf(S3Client.class);\n-          assertThat(fileIO.asyncClient(\"s3://foo/bar\"))\n-              .isNotSameAs(fileIO.asyncClient())\n-              .isInstanceOf(S3AsyncClient.class);\n-        });\n-\n-    // make sure credentials are still present after java serde\n-    resolvingIO = TestHelpers.roundTripSerialize(resolvingFileIO);\n+    // make sure credentials are still present after serde\n+    ResolvingFileIO resolvingIO = roundTripSerializer.apply(resolvingFileIO);\n     assertThat(resolvingIO.credentials()).isEqualTo(storageCredentials);\n     result =\n         DynMethods.builder(\"io\")\n\ndiff --git a/aws/src/test/java/org/apache/iceberg/aws/TestAwsClientFactories.java b/aws/src/test/java/org/apache/iceberg/aws/TestAwsClientFactories.java\nindex 1e421535b8d6..fe95f9364673 100644\n--- a/aws/src/test/java/org/apache/iceberg/aws/TestAwsClientFactories.java\n+++ b/aws/src/test/java/org/apache/iceberg/aws/TestAwsClientFactories.java\n@@ -33,6 +33,8 @@\n import org.apache.iceberg.util.SerializationUtil;\n import org.assertj.core.api.ThrowableAssert;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.MethodSource;\n import software.amazon.awssdk.auth.credentials.AwsBasicCredentials;\n import software.amazon.awssdk.auth.credentials.AwsCredentials;\n import software.amazon.awssdk.auth.credentials.AwsCredentialsProvider;\n@@ -133,12 +135,14 @@ public void testS3FileIoCredentialsVerification() {\n         .hasMessage(\"S3 client access key ID and secret access key must be set at the same time\");\n   }\n \n-  @Test\n-  public void testDefaultAwsClientFactorySerializable() throws IOException {\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void testDefaultAwsClientFactorySerializable(\n+      TestHelpers.RoundTripSerializer<AwsClientFactory> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n     Map<String, String> properties = Maps.newHashMap();\n     AwsClientFactory defaultAwsClientFactory = AwsClientFactories.from(properties);\n-    AwsClientFactory roundTripResult =\n-        TestHelpers.KryoHelpers.roundTripSerialize(defaultAwsClientFactory);\n+    AwsClientFactory roundTripResult = roundTripSerializer.apply(defaultAwsClientFactory);\n     assertThat(roundTripResult).isInstanceOf(AwsClientFactories.DefaultAwsClientFactory.class);\n \n     byte[] serializedFactoryBytes = SerializationUtil.serializeToBytes(defaultAwsClientFactory);\n@@ -148,15 +152,17 @@ public void testDefaultAwsClientFactorySerializable() throws IOException {\n         .isInstanceOf(AwsClientFactories.DefaultAwsClientFactory.class);\n   }\n \n-  @Test\n-  public void testAssumeRoleAwsClientFactorySerializable() throws IOException {\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void testAssumeRoleAwsClientFactorySerializable(\n+      TestHelpers.RoundTripSerializer<AwsClientFactory> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n     Map<String, String> properties = Maps.newHashMap();\n     properties.put(AwsProperties.CLIENT_FACTORY, AssumeRoleAwsClientFactory.class.getName());\n     properties.put(AwsProperties.CLIENT_ASSUME_ROLE_ARN, \"arn::test\");\n     properties.put(AwsProperties.CLIENT_ASSUME_ROLE_REGION, \"us-east-1\");\n     AwsClientFactory assumeRoleAwsClientFactory = AwsClientFactories.from(properties);\n-    AwsClientFactory roundTripResult =\n-        TestHelpers.KryoHelpers.roundTripSerialize(assumeRoleAwsClientFactory);\n+    AwsClientFactory roundTripResult = roundTripSerializer.apply(assumeRoleAwsClientFactory);\n     assertThat(roundTripResult).isInstanceOf(AssumeRoleAwsClientFactory.class);\n \n     byte[] serializedFactoryBytes = SerializationUtil.serializeToBytes(assumeRoleAwsClientFactory);\n@@ -165,8 +171,11 @@ public void testAssumeRoleAwsClientFactorySerializable() throws IOException {\n     assertThat(deserializedClientFactory).isInstanceOf(AssumeRoleAwsClientFactory.class);\n   }\n \n-  @Test\n-  public void testLakeFormationAwsClientFactorySerializable() throws IOException {\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void testLakeFormationAwsClientFactorySerializable(\n+      TestHelpers.RoundTripSerializer<AwsClientFactory> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n     Map<String, String> properties = Maps.newHashMap();\n     properties.put(AwsProperties.CLIENT_FACTORY, LakeFormationAwsClientFactory.class.getName());\n     properties.put(AwsProperties.CLIENT_ASSUME_ROLE_ARN, \"arn::test\");\n@@ -176,8 +185,7 @@ public void testLakeFormationAwsClientFactorySerializable() throws IOException {\n             + LakeFormationAwsClientFactory.LF_AUTHORIZED_CALLER,\n         \"emr\");\n     AwsClientFactory lakeFormationAwsClientFactory = AwsClientFactories.from(properties);\n-    AwsClientFactory roundTripResult =\n-        TestHelpers.KryoHelpers.roundTripSerialize(lakeFormationAwsClientFactory);\n+    AwsClientFactory roundTripResult = roundTripSerializer.apply(lakeFormationAwsClientFactory);\n     assertThat(roundTripResult).isInstanceOf(LakeFormationAwsClientFactory.class);\n \n     byte[] serializedFactoryBytes =\n\ndiff --git a/aws/src/test/java/org/apache/iceberg/aws/TestAwsProperties.java b/aws/src/test/java/org/apache/iceberg/aws/TestAwsProperties.java\nindex 8a0dabcaffa8..c30d56eb385c 100644\n--- a/aws/src/test/java/org/apache/iceberg/aws/TestAwsProperties.java\n+++ b/aws/src/test/java/org/apache/iceberg/aws/TestAwsProperties.java\n@@ -25,16 +25,19 @@\n import java.io.IOException;\n import org.apache.iceberg.TestHelpers;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.MethodSource;\n \n public class TestAwsProperties {\n \n-  @Test\n-  public void testKryoSerialization() throws IOException {\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void testSerialization(TestHelpers.RoundTripSerializer<AwsProperties> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n     AwsProperties awsPropertiesWithProps =\n         new AwsProperties(ImmutableMap.of(GLUE_CATALOG_ID, \"foo\", DYNAMODB_TABLE_NAME, \"ice\"));\n     AwsProperties deSerializedAwsPropertiesWithProps =\n-        TestHelpers.KryoHelpers.roundTripSerialize(awsPropertiesWithProps);\n+        roundTripSerializer.apply(awsPropertiesWithProps);\n     assertThat(deSerializedAwsPropertiesWithProps.glueCatalogId())\n         .isEqualTo(awsPropertiesWithProps.glueCatalogId());\n     assertThat(deSerializedAwsPropertiesWithProps.dynamoDbTableName())\n\ndiff --git a/azure/src/integration/java/org/apache/iceberg/azure/adlsv2/ADLSFileIOTest.java b/azure/src/integration/java/org/apache/iceberg/azure/adlsv2/ADLSFileIOTest.java\nindex 8f24ca9de368..398a7aed09a3 100644\n--- a/azure/src/integration/java/org/apache/iceberg/azure/adlsv2/ADLSFileIOTest.java\n+++ b/azure/src/integration/java/org/apache/iceberg/azure/adlsv2/ADLSFileIOTest.java\n@@ -49,6 +49,8 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.MethodSource;\n \n public class ADLSFileIOTest extends BaseAzuriteTest {\n \n@@ -176,24 +178,15 @@ public void testDeletePrefixOperations() {\n     verify(client).deleteDirectoryWithResponse(eq(\"dir\"), eq(true), any(), any(), any());\n   }\n \n-  @Test\n-  public void testKryoSerialization() throws IOException {\n-    FileIO testFileIO = new ADLSFileIO();\n-\n-    // gcs fileIO should be serializable when properties are passed as immutable map\n-    testFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\"));\n-    FileIO roundTripSerializedFileIO = TestHelpers.KryoHelpers.roundTripSerialize(testFileIO);\n-\n-    assertThat(testFileIO.properties()).isEqualTo(roundTripSerializedFileIO.properties());\n-  }\n-\n-  @Test\n-  public void testJavaSerialization() throws IOException, ClassNotFoundException {\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void testSerialization(TestHelpers.RoundTripSerializer<FileIO> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n     FileIO testFileIO = new ADLSFileIO();\n \n     // gcs fileIO should be serializable when properties are passed as immutable map\n-    testFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\"));\n-    FileIO roundTripSerializedFileIO = TestHelpers.roundTripSerialize(testFileIO);\n+    testFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\", \"k2\", \"v2\"));\n+    FileIO roundTripSerializedFileIO = roundTripSerializer.apply(testFileIO);\n \n     assertThat(testFileIO.properties()).isEqualTo(roundTripSerializedFileIO.properties());\n   }\n\ndiff --git a/azure/src/test/java/org/apache/iceberg/azure/AzurePropertiesTest.java b/azure/src/test/java/org/apache/iceberg/azure/AzurePropertiesTest.java\nindex 153c088c4f84..2e2c41aebd03 100644\n--- a/azure/src/test/java/org/apache/iceberg/azure/AzurePropertiesTest.java\n+++ b/azure/src/test/java/org/apache/iceberg/azure/AzurePropertiesTest.java\n@@ -39,17 +39,22 @@\n import com.azure.identity.DefaultAzureCredential;\n import com.azure.storage.common.StorageSharedKeyCredential;\n import com.azure.storage.file.datalake.DataLakeFileSystemClientBuilder;\n+import java.io.IOException;\n import java.util.Optional;\n import org.apache.iceberg.CatalogProperties;\n import org.apache.iceberg.TestHelpers;\n import org.apache.iceberg.azure.adlsv2.VendedAdlsCredentialProvider;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.MethodSource;\n \n public class AzurePropertiesTest {\n \n-  @Test\n-  public void testSerializable() throws Exception {\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void testSerializable(TestHelpers.RoundTripSerializer<AzureProperties> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n     AzureProperties props =\n         new AzureProperties(\n             ImmutableMap.<String, String>builder()\n@@ -61,7 +66,7 @@ public void testSerializable() throws Exception {\n                 .put(ADLS_SHARED_KEY_ACCOUNT_KEY, \"secret\")\n                 .build());\n \n-    AzureProperties serdedProps = TestHelpers.roundTripSerialize(props);\n+    AzureProperties serdedProps = roundTripSerializer.apply(props);\n     assertThat(serdedProps.adlsReadBlockSize()).isEqualTo(props.adlsReadBlockSize());\n     assertThat(serdedProps.adlsWriteBlockSize()).isEqualTo(props.adlsWriteBlockSize());\n   }\n\ndiff --git a/azure/src/test/java/org/apache/iceberg/azure/adlsv2/VendedAdlsCredentialProviderTest.java b/azure/src/test/java/org/apache/iceberg/azure/adlsv2/VendedAdlsCredentialProviderTest.java\nindex f17b1ea5a685..678f1189f748 100644\n--- a/azure/src/test/java/org/apache/iceberg/azure/adlsv2/VendedAdlsCredentialProviderTest.java\n+++ b/azure/src/test/java/org/apache/iceberg/azure/adlsv2/VendedAdlsCredentialProviderTest.java\n@@ -40,6 +40,8 @@\n import org.apache.iceberg.rest.responses.LoadCredentialsResponse;\n import org.apache.iceberg.rest.responses.LoadCredentialsResponseParser;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.MethodSource;\n import org.mockserver.model.HttpRequest;\n import org.mockserver.model.HttpResponse;\n import org.mockserver.verify.VerificationTimes;\n@@ -273,8 +275,11 @@ public void multipleStorageAccounts() {\n     }\n   }\n \n-  @Test\n-  public void serializableTest() throws IOException, ClassNotFoundException {\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void serializableTest(\n+      TestHelpers.RoundTripSerializer<VendedAdlsCredentialProvider> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n     HttpRequest mockRequest = request(\"/v1/credentials\").withMethod(HttpMethod.GET.name());\n     Credential credential =\n         ImmutableCredential.builder()\n@@ -297,7 +302,7 @@ public void serializableTest() throws IOException, ClassNotFoundException {\n       assertThat(azureSasCredential)\n           .isEqualTo(credential.config().get(ADLS_SAS_TOKEN_PREFIX + STORAGE_ACCOUNT));\n \n-      VendedAdlsCredentialProvider deserializedProvider = TestHelpers.roundTripSerialize(provider);\n+      VendedAdlsCredentialProvider deserializedProvider = roundTripSerializer.apply(provider);\n       String reGeneratedAzureSasCredential =\n           deserializedProvider.credentialForAccount(STORAGE_ACCOUNT);\n \n\ndiff --git a/core/src/test/java/org/apache/iceberg/hadoop/HadoopFileIOTest.java b/core/src/test/java/org/apache/iceberg/hadoop/HadoopFileIOTest.java\nindex f92966f3e36b..e79fdda60cfe 100644\n--- a/core/src/test/java/org/apache/iceberg/hadoop/HadoopFileIOTest.java\n+++ b/core/src/test/java/org/apache/iceberg/hadoop/HadoopFileIOTest.java\n@@ -45,6 +45,8 @@\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.io.TempDir;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.MethodSource;\n \n public class HadoopFileIOTest {\n   private final Random random = new Random(1);\n@@ -143,24 +145,17 @@ public void testDeleteFilesErrorHandling() {\n         .hasMessage(\"Failed to delete 2 files\");\n   }\n \n-  @Test\n-  public void testHadoopFileIOKryoSerialization() throws IOException {\n-    FileIO testHadoopFileIO = new HadoopFileIO();\n-\n-    // hadoop fileIO should be serializable when properties are passed as immutable map\n-    testHadoopFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\"));\n-    FileIO roundTripSerializedFileIO = TestHelpers.KryoHelpers.roundTripSerialize(testHadoopFileIO);\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void testHadoopFileIOSerialization(\n+      TestHelpers.RoundTripSerializer<FileIO> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n \n-    assertThat(roundTripSerializedFileIO.properties()).isEqualTo(testHadoopFileIO.properties());\n-  }\n-\n-  @Test\n-  public void testHadoopFileIOJavaSerialization() throws IOException, ClassNotFoundException {\n     FileIO testHadoopFileIO = new HadoopFileIO();\n \n     // hadoop fileIO should be serializable when properties are passed as immutable map\n-    testHadoopFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\"));\n-    FileIO roundTripSerializedFileIO = TestHelpers.roundTripSerialize(testHadoopFileIO);\n+    testHadoopFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\", \"k2\", \"v2\"));\n+    FileIO roundTripSerializedFileIO = roundTripSerializer.apply(testHadoopFileIO);\n \n     assertThat(roundTripSerializedFileIO.properties()).isEqualTo(testHadoopFileIO.properties());\n   }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/io/TestResolvingIO.java b/core/src/test/java/org/apache/iceberg/io/TestResolvingIO.java\nindex 1d9886b3730a..bc526c18e8cf 100644\n--- a/core/src/test/java/org/apache/iceberg/io/TestResolvingIO.java\n+++ b/core/src/test/java/org/apache/iceberg/io/TestResolvingIO.java\n@@ -43,63 +43,40 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.io.TempDir;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.MethodSource;\n \n public class TestResolvingIO {\n \n   @TempDir private java.nio.file.Path temp;\n \n-  @Test\n-  public void testResolvingFileIOKryoSerialization() throws IOException {\n-    FileIO testResolvingFileIO = new ResolvingFileIO();\n-\n-    // resolving fileIO should be serializable when properties are passed as immutable map\n-    testResolvingFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\"));\n-    FileIO roundTripSerializedFileIO =\n-        TestHelpers.KryoHelpers.roundTripSerialize(testResolvingFileIO);\n-    assertThat(roundTripSerializedFileIO.properties()).isEqualTo(testResolvingFileIO.properties());\n-  }\n-\n-  @Test\n-  public void testResolvingFileIOWithHadoopFileIOKryoSerialization() throws IOException {\n-    ResolvingFileIO resolvingFileIO = new ResolvingFileIO();\n-    Configuration conf = new Configuration();\n-    resolvingFileIO.setConf(conf);\n-    resolvingFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\"));\n-\n-    assertThat(resolvingFileIO.ioClass(temp.toString())).isEqualTo(HadoopFileIO.class);\n-    assertThat(resolvingFileIO.newInputFile(temp.toString())).isNotNull();\n-\n-    ResolvingFileIO roundTripSerializedFileIO =\n-        TestHelpers.KryoHelpers.roundTripSerialize(resolvingFileIO);\n-    roundTripSerializedFileIO.setConf(conf);\n-    assertThat(roundTripSerializedFileIO.properties()).isEqualTo(resolvingFileIO.properties());\n-\n-    assertThat(roundTripSerializedFileIO.ioClass(temp.toString())).isEqualTo(HadoopFileIO.class);\n-    assertThat(roundTripSerializedFileIO.newInputFile(temp.toString())).isNotNull();\n-  }\n-\n-  @Test\n-  public void testResolvingFileIOJavaSerialization() throws IOException, ClassNotFoundException {\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void testResolvingFileIOSerialization(\n+      TestHelpers.RoundTripSerializer<FileIO> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n     FileIO testResolvingFileIO = new ResolvingFileIO();\n \n     // resolving fileIO should be serializable when properties are passed as immutable map\n-    testResolvingFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\"));\n-    FileIO roundTripSerializedFileIO = TestHelpers.roundTripSerialize(testResolvingFileIO);\n+    testResolvingFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\", \"k2\", \"v2\"));\n+    FileIO roundTripSerializedFileIO = roundTripSerializer.apply(testResolvingFileIO);\n     assertThat(roundTripSerializedFileIO.properties()).isEqualTo(testResolvingFileIO.properties());\n   }\n \n-  @Test\n-  public void testResolvingFileIOWithHadoopFileIOJavaSerialization()\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void testResolvingFileIOWithHadoopFileIOSerialization(\n+      TestHelpers.RoundTripSerializer<ResolvingFileIO> roundTripSerializer)\n       throws IOException, ClassNotFoundException {\n     ResolvingFileIO resolvingFileIO = new ResolvingFileIO();\n     Configuration conf = new Configuration();\n     resolvingFileIO.setConf(conf);\n-    resolvingFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\"));\n+    resolvingFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\", \"k2\", \"v2\"));\n \n     assertThat(resolvingFileIO.ioClass(temp.toString())).isEqualTo(HadoopFileIO.class);\n     assertThat(resolvingFileIO.newInputFile(temp.toString())).isNotNull();\n \n-    ResolvingFileIO roundTripSerializedFileIO = TestHelpers.roundTripSerialize(resolvingFileIO);\n+    ResolvingFileIO roundTripSerializedFileIO = roundTripSerializer.apply(resolvingFileIO);\n     roundTripSerializedFileIO.setConf(conf);\n     assertThat(roundTripSerializedFileIO.properties()).isEqualTo(resolvingFileIO.properties());\n \n@@ -186,25 +163,10 @@ public void delegateFileIOWithAndWithoutMixins() {\n     assertThat(resolvingFileIO.newInputFile(\"/file\")).isNull();\n   }\n \n-  @Test\n-  public void resolvingFileIOWithStorageCredentialsKryoSerialization() throws IOException {\n-    StorageCredential credential = StorageCredential.create(\"prefix\", Map.of(\"key1\", \"val1\"));\n-    List<StorageCredential> storageCredentials = ImmutableList.of(credential);\n-    ResolvingFileIO resolvingFileIO =\n-        (ResolvingFileIO)\n-            CatalogUtil.loadFileIO(\n-                ResolvingFileIO.class.getName(),\n-                ImmutableMap.of(),\n-                new Configuration(),\n-                storageCredentials);\n-\n-    assertThat(TestHelpers.KryoHelpers.roundTripSerialize(resolvingFileIO).credentials())\n-        .isEqualTo(storageCredentials)\n-        .isEqualTo(resolvingFileIO.credentials());\n-  }\n-\n-  @Test\n-  public void resolvingFileIOWithStorageCredentialsJavaSerialization()\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void resolvingFileIOWithStorageCredentialsSerialization(\n+      TestHelpers.RoundTripSerializer<ResolvingFileIO> roundTripSerializer)\n       throws IOException, ClassNotFoundException {\n     StorageCredential credential = StorageCredential.create(\"prefix\", Map.of(\"key1\", \"val1\"));\n     List<StorageCredential> storageCredentials = ImmutableList.of(credential);\n@@ -216,7 +178,7 @@ public void resolvingFileIOWithStorageCredentialsJavaSerialization()\n                 new Configuration(),\n                 storageCredentials);\n \n-    assertThat(TestHelpers.roundTripSerialize(resolvingFileIO).credentials())\n+    assertThat(roundTripSerializer.apply(resolvingFileIO).credentials())\n         .isEqualTo(storageCredentials)\n         .isEqualTo(resolvingFileIO.credentials());\n   }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/io/TestStorageCredential.java b/core/src/test/java/org/apache/iceberg/io/TestStorageCredential.java\nindex fcf177060fec..63e5dd304d9b 100644\n--- a/core/src/test/java/org/apache/iceberg/io/TestStorageCredential.java\n+++ b/core/src/test/java/org/apache/iceberg/io/TestStorageCredential.java\n@@ -26,6 +26,8 @@\n import org.apache.iceberg.TestHelpers;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.MethodSource;\n \n public class TestStorageCredential {\n   @Test\n@@ -42,22 +44,16 @@ public void invalidConfig() {\n         .hasMessage(\"Invalid config: must be non-empty\");\n   }\n \n-  @Test\n-  public void kryoSerDe() throws IOException {\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void serialization(TestHelpers.RoundTripSerializer<StorageCredential> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n     // using a single element in the map will create a singleton map, which will work with Kryo.\n     // However, creating two config elements will fail if the config in StorageCredential isn't a\n     // SerializableMap\n     StorageCredential credential =\n         StorageCredential.create(\n             \"randomPrefix\", ImmutableMap.of(\"token1\", \"storageToken1\", \"token2\", \"storageToken2\"));\n-    assertThat(TestHelpers.KryoHelpers.roundTripSerialize(credential)).isEqualTo(credential);\n-  }\n-\n-  @Test\n-  public void javaSerDe() throws IOException, ClassNotFoundException {\n-    StorageCredential credential =\n-        StorageCredential.create(\n-            \"randomPrefix\", ImmutableMap.of(\"token\", \"storageToken\", \"token2\", \"storageToken2\"));\n-    assertThat(TestHelpers.roundTripSerialize(credential)).isEqualTo(credential);\n+    assertThat(roundTripSerializer.apply(credential)).isEqualTo(credential);\n   }\n }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/util/TestDataFileSet.java b/core/src/test/java/org/apache/iceberg/util/TestDataFileSet.java\nindex 0f298ad82e9d..26b76dd68a77 100644\n--- a/core/src/test/java/org/apache/iceberg/util/TestDataFileSet.java\n+++ b/core/src/test/java/org/apache/iceberg/util/TestDataFileSet.java\n@@ -21,6 +21,7 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import java.io.IOException;\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.Set;\n@@ -30,6 +31,8 @@\n import org.apache.iceberg.TestHelpers;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.MethodSource;\n \n /**\n  * Testing {@link DataFileSet} is easier in iceberg-core since the data file builders are located\n@@ -288,16 +291,11 @@ public void equalsAndHashCode() {\n     assertThat(set1.hashCode()).isEqualTo(set2.hashCode()).isEqualTo(set3.hashCode());\n   }\n \n-  @Test\n-  public void kryoSerialization() throws Exception {\n-    DataFileSet dataFiles = DataFileSet.of(ImmutableList.of(FILE_C, FILE_B, FILE_A));\n-    assertThat(TestHelpers.KryoHelpers.roundTripSerialize(dataFiles)).isEqualTo(dataFiles);\n-  }\n-\n-  @Test\n-  public void javaSerialization() throws Exception {\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void serialization(TestHelpers.RoundTripSerializer<DataFileSet> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n     DataFileSet dataFiles = DataFileSet.of(ImmutableList.of(FILE_C, FILE_B, FILE_A));\n-    DataFileSet deserialized = TestHelpers.deserialize(TestHelpers.serialize(dataFiles));\n-    assertThat(deserialized).isEqualTo(dataFiles);\n+    assertThat(roundTripSerializer.apply(dataFiles)).isEqualTo(dataFiles);\n   }\n }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/util/TestDeleteFileSet.java b/core/src/test/java/org/apache/iceberg/util/TestDeleteFileSet.java\nindex 5f4488a3a1d5..8865a8a8fb1b 100644\n--- a/core/src/test/java/org/apache/iceberg/util/TestDeleteFileSet.java\n+++ b/core/src/test/java/org/apache/iceberg/util/TestDeleteFileSet.java\n@@ -21,6 +21,7 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import java.io.IOException;\n import java.util.Arrays;\n import java.util.Collections;\n import java.util.Set;\n@@ -30,6 +31,8 @@\n import org.apache.iceberg.TestHelpers;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.MethodSource;\n \n /**\n  * Testing {@link DeleteFileSet} is easier in iceberg-core since the delete file builders are\n@@ -304,18 +307,12 @@ public void equalsAndHashCode() {\n     assertThat(set1.hashCode()).isEqualTo(set2.hashCode()).isEqualTo(set3.hashCode());\n   }\n \n-  @Test\n-  public void kryoSerialization() throws Exception {\n-    DeleteFileSet deleteFiles =\n-        DeleteFileSet.of(ImmutableList.of(FILE_C_DELETES, FILE_B_DELETES, FILE_A_DELETES));\n-    assertThat(TestHelpers.KryoHelpers.roundTripSerialize(deleteFiles)).isEqualTo(deleteFiles);\n-  }\n-\n-  @Test\n-  public void javaSerialization() throws Exception {\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void serialization(TestHelpers.RoundTripSerializer<DeleteFileSet> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n     DeleteFileSet deleteFiles =\n         DeleteFileSet.of(ImmutableList.of(FILE_C_DELETES, FILE_B_DELETES, FILE_A_DELETES));\n-    DeleteFileSet deserialize = TestHelpers.deserialize(TestHelpers.serialize(deleteFiles));\n-    assertThat(deserialize).isEqualTo(deleteFiles);\n+    assertThat(roundTripSerializer.apply(deleteFiles)).isEqualTo(deleteFiles);\n   }\n }\n\ndiff --git a/gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSFileIOTest.java b/gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSFileIOTest.java\nindex 97fe7cc40fc1..a536c638f475 100644\n--- a/gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSFileIOTest.java\n+++ b/gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSFileIOTest.java\n@@ -62,6 +62,8 @@\n import org.assertj.core.api.InstanceOfAssertFactories;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.MethodSource;\n \n public class GCSFileIOTest {\n   private static final String TEST_BUCKET = \"TEST_BUCKET\";\n@@ -202,24 +204,17 @@ public void testDeletePrefix() {\n         .isEqualTo(1);\n   }\n \n-  @Test\n-  public void testGCSFileIOKryoSerialization() throws IOException {\n-    FileIO testGCSFileIO = new GCSFileIO();\n-\n-    // gcs fileIO should be serializable when properties are passed as immutable map\n-    testGCSFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\"));\n-    FileIO roundTripSerializedFileIO = TestHelpers.KryoHelpers.roundTripSerialize(testGCSFileIO);\n-\n-    assertThat(testGCSFileIO.properties()).isEqualTo(roundTripSerializedFileIO.properties());\n-  }\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void testGCSFileIOSerialization(\n+      TestHelpers.RoundTripSerializer<FileIO> roundTripSerializer)\n+      throws IOException, ClassNotFoundException {\n \n-  @Test\n-  public void testGCSFileIOJavaSerialization() throws IOException, ClassNotFoundException {\n     FileIO testGCSFileIO = new GCSFileIO();\n \n     // gcs fileIO should be serializable when properties are passed as immutable map\n-    testGCSFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\"));\n-    FileIO roundTripSerializedFileIO = TestHelpers.roundTripSerialize(testGCSFileIO);\n+    testGCSFileIO.initialize(ImmutableMap.of(\"k1\", \"v1\", \"k2\", \"v2\"));\n+    FileIO roundTripSerializedFileIO = roundTripSerializer.apply(testGCSFileIO);\n \n     assertThat(testGCSFileIO.properties()).isEqualTo(roundTripSerializedFileIO.properties());\n   }\n@@ -410,84 +405,10 @@ public void multipleStorageCredentialsConfigured() {\n     }\n   }\n \n-  @Test\n-  public void fileIOWithStorageCredentialsKryoSerialization() throws IOException {\n-    GCSFileIO fileIO = new GCSFileIO();\n-    fileIO.setCredentials(\n-        ImmutableList.of(\n-            StorageCredential.create(\"prefix\", Map.of(\"key1\", \"val1\", \"key2\", \"val2\"))));\n-    fileIO.initialize(Map.of());\n-\n-    assertThat(TestHelpers.KryoHelpers.roundTripSerialize(fileIO).credentials())\n-        .isEqualTo(fileIO.credentials());\n-  }\n-\n-  @Test\n-  public void fileIOWithPrefixedStorageClientWithoutCredentialsKryoSerialization()\n-      throws IOException {\n-    GCSFileIO fileIO = new GCSFileIO();\n-    fileIO.initialize(\n-        Map.of(GCS_OAUTH2_TOKEN, \"gcsTokenFromProperties\", GCS_OAUTH2_TOKEN_EXPIRES_AT, \"1000\"));\n-\n-    assertThat(fileIO.client(\"gs\")).isInstanceOf(Storage.class);\n-    assertThat(fileIO.client(\"gs://bucket1/my-path/tableX\")).isInstanceOf(Storage.class);\n-    assertThat(fileIO.client(\"gs://bucket1/my-path/tableX\").getOptions().getCredentials())\n-        .isInstanceOf(OAuth2Credentials.class)\n-        .extracting(\"value\")\n-        .extracting(\"temporaryAccess\")\n-        .isEqualTo(new AccessToken(\"gcsTokenFromProperties\", new Date(1000L)));\n-\n-    GCSFileIO roundTripIO = TestHelpers.KryoHelpers.roundTripSerialize(fileIO);\n-    assertThat(roundTripIO).isNotNull();\n-    assertThat(roundTripIO.credentials()).isEqualTo(fileIO.credentials()).isEmpty();\n-\n-    assertThat(roundTripIO.client(\"gs\")).isInstanceOf(Storage.class);\n-    assertThat(roundTripIO.client(\"gs://bucket1/my-path/tableX\")).isInstanceOf(Storage.class);\n-    assertThat(roundTripIO.client(\"gs://bucket1/my-path/tableX\").getOptions().getCredentials())\n-        .isInstanceOf(OAuth2Credentials.class)\n-        .extracting(\"value\")\n-        .extracting(\"temporaryAccess\")\n-        .isEqualTo(new AccessToken(\"gcsTokenFromProperties\", new Date(1000L)));\n-  }\n-\n-  @Test\n-  public void fileIOWithPrefixedStorageClientKryoSerialization() throws IOException {\n-    GCSFileIO fileIO = new GCSFileIO();\n-    fileIO.setCredentials(\n-        ImmutableList.of(\n-            StorageCredential.create(\n-                \"gs://bucket1\",\n-                ImmutableMap.of(\n-                    \"gcs.oauth2.token\",\n-                    \"gcsTokenFromCredential\",\n-                    \"gcs.oauth2.token-expires-at\",\n-                    \"2000\"))));\n-    fileIO.initialize(\n-        Map.of(GCS_OAUTH2_TOKEN, \"gcsTokenFromProperties\", GCS_OAUTH2_TOKEN_EXPIRES_AT, \"1000\"));\n-\n-    assertThat(fileIO.client(\"gs\")).isInstanceOf(Storage.class);\n-    assertThat(fileIO.client(\"gs://bucket1/my-path/tableX\")).isInstanceOf(Storage.class);\n-    assertThat(fileIO.client(\"gs://bucket1/my-path/tableX\").getOptions().getCredentials())\n-        .isInstanceOf(OAuth2Credentials.class)\n-        .extracting(\"value\")\n-        .extracting(\"temporaryAccess\")\n-        .isEqualTo(new AccessToken(\"gcsTokenFromCredential\", new Date(2000L)));\n-\n-    GCSFileIO roundTripIO = TestHelpers.KryoHelpers.roundTripSerialize(fileIO);\n-    assertThat(roundTripIO).isNotNull();\n-    assertThat(roundTripIO.credentials()).isEqualTo(fileIO.credentials());\n-\n-    assertThat(roundTripIO.client(\"gs\")).isInstanceOf(Storage.class);\n-    assertThat(roundTripIO.client(\"gs://bucket1/my-path/tableX\")).isInstanceOf(Storage.class);\n-    assertThat(roundTripIO.client(\"gs://bucket1/my-path/tableX\").getOptions().getCredentials())\n-        .isInstanceOf(OAuth2Credentials.class)\n-        .extracting(\"value\")\n-        .extracting(\"temporaryAccess\")\n-        .isEqualTo(new AccessToken(\"gcsTokenFromCredential\", new Date(2000L)));\n-  }\n-\n-  @Test\n-  public void fileIOWithStorageCredentialsJavaSerialization()\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void fileIOWithStorageCredentialsSerialization(\n+      TestHelpers.RoundTripSerializer<GCSFileIO> roundTripSerializer)\n       throws IOException, ClassNotFoundException {\n     GCSFileIO fileIO = new GCSFileIO();\n     fileIO.setCredentials(\n@@ -495,12 +416,13 @@ public void fileIOWithStorageCredentialsJavaSerialization()\n             StorageCredential.create(\"prefix\", Map.of(\"key1\", \"val1\", \"key2\", \"val2\"))));\n     fileIO.initialize(Map.of());\n \n-    assertThat(TestHelpers.roundTripSerialize(fileIO).credentials())\n-        .isEqualTo(fileIO.credentials());\n+    assertThat(roundTripSerializer.apply(fileIO).credentials()).isEqualTo(fileIO.credentials());\n   }\n \n-  @Test\n-  public void fileIOWithPrefixedStorageClientWithoutCredentialsJavaSerialization()\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void fileIOWithPrefixedStorageClientWithoutCredentialsSerialization(\n+      TestHelpers.RoundTripSerializer<GCSFileIO> roundTripSerializer)\n       throws IOException, ClassNotFoundException {\n     GCSFileIO fileIO = new GCSFileIO();\n     fileIO.initialize(\n@@ -514,7 +436,8 @@ public void fileIOWithPrefixedStorageClientWithoutCredentialsJavaSerialization()\n         .extracting(\"temporaryAccess\")\n         .isEqualTo(new AccessToken(\"gcsTokenFromProperties\", new Date(1000L)));\n \n-    GCSFileIO roundTripIO = TestHelpers.roundTripSerialize(fileIO);\n+    GCSFileIO roundTripIO = roundTripSerializer.apply(fileIO);\n+    assertThat(roundTripIO).isNotNull();\n     assertThat(roundTripIO.credentials()).isEqualTo(fileIO.credentials()).isEmpty();\n \n     assertThat(roundTripIO.client(\"gs\")).isInstanceOf(Storage.class);\n@@ -526,8 +449,10 @@ public void fileIOWithPrefixedStorageClientWithoutCredentialsJavaSerialization()\n         .isEqualTo(new AccessToken(\"gcsTokenFromProperties\", new Date(1000L)));\n   }\n \n-  @Test\n-  public void fileIOWithPrefixedStorageClientJavaSerialization()\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void fileIOWithPrefixedStorageClientSerialization(\n+      TestHelpers.RoundTripSerializer<GCSFileIO> roundTripSerializer)\n       throws IOException, ClassNotFoundException {\n     GCSFileIO fileIO = new GCSFileIO();\n     fileIO.setCredentials(\n@@ -550,7 +475,8 @@ public void fileIOWithPrefixedStorageClientJavaSerialization()\n         .extracting(\"temporaryAccess\")\n         .isEqualTo(new AccessToken(\"gcsTokenFromCredential\", new Date(2000L)));\n \n-    GCSFileIO roundTripIO = TestHelpers.roundTripSerialize(fileIO);\n+    GCSFileIO roundTripIO = roundTripSerializer.apply(fileIO);\n+    assertThat(roundTripIO).isNotNull();\n     assertThat(roundTripIO.credentials()).isEqualTo(fileIO.credentials());\n \n     assertThat(roundTripIO.client(\"gs\")).isInstanceOf(Storage.class);\n@@ -562,21 +488,22 @@ public void fileIOWithPrefixedStorageClientJavaSerialization()\n         .isEqualTo(new AccessToken(\"gcsTokenFromCredential\", new Date(2000L)));\n   }\n \n-  @Test\n-  public void resolvingFileIOLoadWithoutStorageCredentials()\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void resolvingFileIOLoadWithoutStorageCredentials(\n+      TestHelpers.RoundTripSerializer<ResolvingFileIO> roundTripSerializer)\n       throws IOException, ClassNotFoundException {\n     ResolvingFileIO resolvingFileIO = new ResolvingFileIO();\n     resolvingFileIO.initialize(ImmutableMap.of());\n \n-    ResolvingFileIO fileIO = TestHelpers.KryoHelpers.roundTripSerialize(resolvingFileIO);\n-    assertThat(fileIO.credentials()).isEmpty();\n-\n-    fileIO = TestHelpers.roundTripSerialize(resolvingFileIO);\n+    ResolvingFileIO fileIO = roundTripSerializer.apply(resolvingFileIO);\n     assertThat(fileIO.credentials()).isEmpty();\n   }\n \n-  @Test\n-  public void resolvingFileIOLoadWithStorageCredentials()\n+  @ParameterizedTest\n+  @MethodSource(\"org.apache.iceberg.TestHelpers#serializers\")\n+  public void resolvingFileIOLoadWithStorageCredentials(\n+      TestHelpers.RoundTripSerializer<ResolvingFileIO> roundTripSerializer)\n       throws IOException, ClassNotFoundException {\n     StorageCredential credential = StorageCredential.create(\"prefix\", Map.of(\"key1\", \"val1\"));\n     List<StorageCredential> storageCredentials = ImmutableList.of(credential);\n@@ -595,22 +522,8 @@ public void resolvingFileIOLoadWithStorageCredentials()\n         .extracting(GCSFileIO::credentials)\n         .isEqualTo(storageCredentials);\n \n-    // make sure credentials are still present after kryo serde\n-    ResolvingFileIO fileIO = TestHelpers.KryoHelpers.roundTripSerialize(resolvingFileIO);\n-    assertThat(fileIO.credentials()).isEqualTo(storageCredentials);\n-    result =\n-        DynMethods.builder(\"io\")\n-            .hiddenImpl(ResolvingFileIO.class, String.class)\n-            .build(fileIO)\n-            .invoke(\"gs://foo/bar\");\n-    assertThat(result)\n-        .isInstanceOf(GCSFileIO.class)\n-        .asInstanceOf(InstanceOfAssertFactories.type(GCSFileIO.class))\n-        .extracting(GCSFileIO::credentials)\n-        .isEqualTo(storageCredentials);\n-\n-    // make sure credentials are still present after java serde\n-    fileIO = TestHelpers.roundTripSerialize(resolvingFileIO);\n+    // make sure credentials are still present after serde\n+    ResolvingFileIO fileIO = roundTripSerializer.apply(resolvingFileIO);\n     assertThat(fileIO.credentials()).isEqualTo(storageCredentials);\n     result =\n         DynMethods.builder(\"io\")\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13216",
    "pr_id": 13216,
    "issue_id": 13174,
    "repo": "apache/iceberg",
    "problem_statement": "S3 client for storage path not available\n### Apache Iceberg version\n\nmain (development)\n\n### Query engine\n\nSpark\n\n### Please describe the bug üêû\n\nHey! I had a problem with the commit https://github.com/apache/iceberg/pull/12799#issuecomment-2913084781\nSpark 3.5_2.12 with jupyter-notebook.\n \nWhen I add these props for enabling S3FileIO, problem appears:\n``` \n  spark.sql.defaultCatalog: iceberg\n  spark.sql.catalog.iceberg.io-impl: org.apache.iceberg.aws.s3.S3FileIO\n  spark.sql.catalog.iceberg.s3.endpoint: http://...\n  spark.sql.catalog.iceberg.client.credentials-provider: software.amazon.awssdk.auth.credentials.EnvironmentVariableCredentialsProvider  \n  spark.executorEnv.AWS_ENDPOINT: ${env:AWS_ENDPOINT}\n  spark.executorEnv.AWS_ACCESS_KEY_ID: ${env:AWS_ACCESS_KEY_ID}\n  spark.executorEnv.AWS_SECRET_ACCESS_KEY: ${env:AWS_SECRET_ACCESS_KEY}\n  spark.executorEnv.AWS_REGION: ${env:AWS_REGION}\n  spark.executorEnv.AWS_DEFAULT_REGION: ${env:AWS_DEFAULT_REGION}\n  spark.serializer: org.apache.spark.serializer.KryoSerializer\n```\n\nStackTrace:\n```\n25/05/28 19:34:40 ERROR org.apache.spark.executor.Executor: Exception in task 7.0 in stage 1.0 (TID 8)\njava.lang.IllegalStateException: [BUG] S3 client for storage path not available: s3a://spark-nt/load_test_delete_warehouse/gen_transaction_icb_month/data/TRANSACTION_DATE_month=2010-06/00000-0-ac46d814-2605-4a4c-9dae-74d4a22f072a-00135.parquet\n\tat org.apache.iceberg.relocated.com.google.common.base.Preconditions.checkState(Preconditions.java:603) ~[iceberg-spark-runtime-3.5_2.12-apache-main-raw.jar:?]\n\tat org.apache.iceberg.aws.s3.S3FileIO.clientForStoragePath(S3FileIO.java:427) ~[iceberg-spark-runtime-3.5_2.12-apache-main-raw.jar:?]\n\tat org.apache.iceberg.aws.s3.S3FileIO.newInputFile(S3FileIO.java:184) ~[iceberg-spark-runtime-3.5_2.12-apache-main-raw.jar:?]\n\tat org.apache.iceberg.encryption.EncryptingFileIO.wrap(EncryptingFileIO.java:150) ~[iceberg-spark-runtime-3.5_2.12-apache-main-raw.jar:?]\n\tat org.apache.iceberg.relocated.com.google.common.collect.Iterators$6.transform(Iterators.java:828) ~[iceberg-spark-runtime-3.5_2.12-apache-main-raw.jar:?]\n\tat org.apache.iceberg.relocated.com.google.common.collect.TransformedIterator.next(TransformedIterator.java:51) ~[iceberg-spark-runtime-3.5_2.12-apache-main-raw.jar:?]\n\tat org.apache.iceberg.relocated.com.google.common.collect.TransformedIterator.next(TransformedIterator.java:51) ~[iceberg-spark-runtime-3.5_2.12-apache-main-raw.jar:?]\n\tat org.apache.iceberg.encryption.EncryptingFileIO.bulkDecrypt(EncryptingFileIO.java:63) ~[iceberg-spark-runtime-3.5_2.12-apache-main-raw.jar:?]\n\tat org.apache.iceberg.spark.source.BaseReader.inputFiles(BaseReader.java:177) ~[iceberg-spark-runtime-3.5_2.12-apache-main-raw.jar:?]\n\tat org.apache.iceberg.spark.source.BaseReader.getInputFile(BaseReader.java:170) ~[iceberg-spark-runtime-3.5_2.12-apache-main-raw.jar:?]\n\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:100) ~[iceberg-spark-runtime-3.5_2.12-apache-main-raw.jar:?]\n\tat org.apache.iceberg.spark.source.BatchDataReader.open(BatchDataReader.java:43) ~[iceberg-spark-runtime-3.5_2.12-apache-main-raw.jar:?]\n\tat org.apache.iceberg.spark.source.BaseReader.next(BaseReader.java:134) ~[iceberg-spark-runtime-3.5_2.12-apache-main-raw.jar:?]\n\tat org.apache.spark.sql.execution.datasources.v2.PartitionIterator.hasNext(DataSourceRDD.scala:120) ~[spark-sql_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.sql.execution.datasources.v2.MetricsIterator.hasNext(DataSourceRDD.scala:158) ~[spark-sql_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1(DataSourceRDD.scala:63) ~[spark-sql_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.$anonfun$hasNext$1$adapted(DataSourceRDD.scala:63) ~[spark-sql_2.12-3.5.4.jar:3.5.4]\n\tat scala.Option.exists(Option.scala:376) ~[scala-library-2.12.18.jar:?]\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63) ~[spark-sql_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.advanceToNextIter(DataSourceRDD.scala:97) ~[spark-sql_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceRDD$$anon$1.hasNext(DataSourceRDD.scala:63) ~[spark-sql_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.18.jar:?]\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source) ~[?:?]\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) ~[?:?]\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) ~[spark-sql_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43) ~[spark-sql_2.12-3.5.4.jar:3.5.4]\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.18.jar:?]\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460) ~[scala-library-2.12.18.jar:?]\n\tat org.apache.spark.util.random.SamplingUtils$.reservoirSampleAndCount(SamplingUtils.scala:41) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1(Partitioner.scala:322) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.RangePartitioner$.$anonfun$sketch$1$adapted(Partitioner.scala:320) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:910) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:910) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64) ~[spark-common-utils_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61) ~[spark-common-utils_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94) ~[spark-core_2.12-3.5.4.jar:3.5.4]\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623) [spark-core_2.12-3.5.4.jar:3.5.4]\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136) [?:?]\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635) [?:?]\n\tat java.lang.Thread.run(Thread.java:840) [?:?]\n```\n\n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [x] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 1185,
    "test_files_count": 2,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java",
      "aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java",
      "core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java",
      "gcp/src/main/java/org/apache/iceberg/gcp/gcs/GCSFileIO.java",
      "gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSFileIOTest.java"
    ],
    "pr_changed_test_files": [
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java",
      "gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSFileIOTest.java"
    ],
    "base_commit": "ee1dea0a4995c61c48f2df889a77b9a19ffff02d",
    "head_commit": "6bcd1afe34a6b883e977c831874c8e6ef0109e48",
    "repo_url": "https://github.com/apache/iceberg/pull/13216",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13216",
    "dockerfile": "",
    "pr_merged_at": "2025-06-05T18:26:59.000Z",
    "patch": "diff --git a/aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java b/aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java\nindex 43a09f465441..e00356a94742 100644\n--- a/aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java\n+++ b/aws/src/main/java/org/apache/iceberg/aws/s3/S3FileIO.java\n@@ -103,7 +103,8 @@ public class S3FileIO\n   private MetricsContext metrics = MetricsContext.nullMetrics();\n   private final AtomicBoolean isResourceClosed = new AtomicBoolean(false);\n   private transient StackTraceElement[] createStack;\n-  private List<StorageCredential> storageCredentials = ImmutableList.of();\n+  // use modifiable collection for Kryo serde\n+  private List<StorageCredential> storageCredentials = Lists.newArrayList();\n   private transient volatile Map<String, PrefixedS3Client> clientByPrefix;\n \n   /**\n\ndiff --git a/core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java b/core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java\nindex 381f4785001c..9815a459f0fe 100644\n--- a/core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java\n+++ b/core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java\n@@ -73,7 +73,8 @@ public class ResolvingFileIO\n   private final transient StackTraceElement[] createStack;\n   private SerializableMap<String, String> properties;\n   private SerializableSupplier<Configuration> hadoopConf;\n-  private List<StorageCredential> storageCredentials = List.of();\n+  // use modifiable collection for Kryo serde\n+  private List<StorageCredential> storageCredentials = Lists.newArrayList();\n \n   /**\n    * No-arg constructor to load the FileIO dynamically.\n\ndiff --git a/gcp/src/main/java/org/apache/iceberg/gcp/gcs/GCSFileIO.java b/gcp/src/main/java/org/apache/iceberg/gcp/gcs/GCSFileIO.java\nindex d541adf99344..0c76d1258c16 100644\n--- a/gcp/src/main/java/org/apache/iceberg/gcp/gcs/GCSFileIO.java\n+++ b/gcp/src/main/java/org/apache/iceberg/gcp/gcs/GCSFileIO.java\n@@ -70,7 +70,8 @@ public class GCSFileIO implements DelegateFileIO, SupportsStorageCredentials {\n   private MetricsContext metrics = MetricsContext.nullMetrics();\n   private final AtomicBoolean isResourceClosed = new AtomicBoolean(false);\n   private SerializableMap<String, String> properties = null;\n-  private List<StorageCredential> storageCredentials = ImmutableList.of();\n+  // use modifiable collection for Kryo serde\n+  private List<StorageCredential> storageCredentials = Lists.newArrayList();\n   private transient volatile Map<String, PrefixedStorage> storageByPrefix;\n \n   /**\n",
    "test_patch": "diff --git a/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java b/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java\nindex e96f38ecde64..5c3a7d88479d 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java\n@@ -490,6 +490,25 @@ public void fileIOWithStorageCredentialsJavaSerialization()\n         .isEqualTo(fileIO.credentials());\n   }\n \n+  @Test\n+  public void fileIOWithPrefixedS3ClientWithoutCredentialsKryoSerialization() throws IOException {\n+    S3FileIO io = new S3FileIO();\n+    io.initialize(Map.of(AwsClientProperties.CLIENT_REGION, \"us-east-1\"));\n+\n+    assertThat(io.client()).isInstanceOf(S3Client.class);\n+    assertThat(io.asyncClient()).isInstanceOf(S3AsyncClient.class);\n+    assertThat(io.client(\"s3a://my-bucket/my-path\")).isInstanceOf(S3Client.class);\n+    assertThat(io.asyncClient(\"s3a://my-bucket/my-path\")).isInstanceOf(S3AsyncClient.class);\n+\n+    S3FileIO fileIO = TestHelpers.KryoHelpers.roundTripSerialize(io);\n+    assertThat(fileIO.credentials()).isEqualTo(io.credentials()).isEmpty();\n+\n+    assertThat(fileIO.client()).isInstanceOf(S3Client.class);\n+    assertThat(fileIO.asyncClient()).isInstanceOf(S3AsyncClient.class);\n+    assertThat(fileIO.client(\"s3a://my-bucket/my-path\")).isInstanceOf(S3Client.class);\n+    assertThat(fileIO.asyncClient(\"s3a://my-bucket/my-path\")).isInstanceOf(S3AsyncClient.class);\n+  }\n+\n   @Test\n   public void fileIOWithPrefixedS3ClientKryoSerialization() throws IOException {\n     S3FileIO io = new S3FileIO();\n@@ -514,6 +533,26 @@ public void fileIOWithPrefixedS3ClientKryoSerialization() throws IOException {\n     assertThat(fileIO.asyncClient(\"s3://my-bucket/my-path\")).isInstanceOf(S3AsyncClient.class);\n   }\n \n+  @Test\n+  public void fileIOWithPrefixedS3ClientWithoutCredentialsJavaSerialization()\n+      throws IOException, ClassNotFoundException {\n+    S3FileIO io = new S3FileIO();\n+    io.initialize(Map.of(AwsClientProperties.CLIENT_REGION, \"us-east-1\"));\n+\n+    assertThat(io.client()).isInstanceOf(S3Client.class);\n+    assertThat(io.asyncClient()).isInstanceOf(S3AsyncClient.class);\n+    assertThat(io.client(\"s3a://my-bucket/my-path\")).isInstanceOf(S3Client.class);\n+    assertThat(io.asyncClient(\"s3a://my-bucket/my-path\")).isInstanceOf(S3AsyncClient.class);\n+\n+    S3FileIO fileIO = TestHelpers.roundTripSerialize(io);\n+    assertThat(fileIO.credentials()).isEqualTo(io.credentials()).isEmpty();\n+\n+    assertThat(fileIO.client()).isInstanceOf(S3Client.class);\n+    assertThat(fileIO.asyncClient()).isInstanceOf(S3AsyncClient.class);\n+    assertThat(fileIO.client(\"s3a://my-bucket/my-path\")).isInstanceOf(S3Client.class);\n+    assertThat(fileIO.asyncClient(\"s3a://my-bucket/my-path\")).isInstanceOf(S3AsyncClient.class);\n+  }\n+\n   @Test\n   public void fileIOWithPrefixedS3ClientJavaSerialization()\n       throws IOException, ClassNotFoundException {\n@@ -623,6 +662,73 @@ public void testInputFileWithManifest() throws IOException {\n     verify(s3mock, never()).headObject(any(HeadObjectRequest.class));\n   }\n \n+  @Test\n+  public void resolvingFileIOLoadWithoutStorageCredentials()\n+      throws IOException, ClassNotFoundException {\n+    ResolvingFileIO resolvingFileIO = new ResolvingFileIO();\n+    resolvingFileIO.initialize(ImmutableMap.of(AwsClientProperties.CLIENT_REGION, \"us-east-1\"));\n+\n+    FileIO result =\n+        DynMethods.builder(\"io\")\n+            .hiddenImpl(ResolvingFileIO.class, String.class)\n+            .build(resolvingFileIO)\n+            .invoke(\"s3://foo/bar\");\n+    assertThat(result)\n+        .isInstanceOf(S3FileIO.class)\n+        .asInstanceOf(InstanceOfAssertFactories.type(S3FileIO.class))\n+        .satisfies(\n+            fileIO -> {\n+              assertThat(fileIO.client(\"s3://foo/bar\"))\n+                  .isSameAs(fileIO.client())\n+                  .isInstanceOf(S3Client.class);\n+              assertThat(fileIO.asyncClient(\"s3://foo/bar\"))\n+                  .isSameAs(fileIO.asyncClient())\n+                  .isInstanceOf(S3AsyncClient.class);\n+            });\n+\n+    // make sure credentials can be accessed after kryo serde\n+    ResolvingFileIO resolvingIO = TestHelpers.KryoHelpers.roundTripSerialize(resolvingFileIO);\n+    assertThat(resolvingIO.credentials()).isEmpty();\n+    result =\n+        DynMethods.builder(\"io\")\n+            .hiddenImpl(ResolvingFileIO.class, String.class)\n+            .build(resolvingIO)\n+            .invoke(\"s3a://foo/bar\");\n+    assertThat(result)\n+        .isInstanceOf(S3FileIO.class)\n+        .asInstanceOf(InstanceOfAssertFactories.type(S3FileIO.class))\n+        .satisfies(\n+            fileIO -> {\n+              assertThat(fileIO.client(\"s3://foo/bar\"))\n+                  .isSameAs(fileIO.client())\n+                  .isInstanceOf(S3Client.class);\n+              assertThat(fileIO.asyncClient(\"s3://foo/bar\"))\n+                  .isSameAs(fileIO.asyncClient())\n+                  .isInstanceOf(S3AsyncClient.class);\n+            });\n+\n+    // make sure credentials can be accessed after java serde\n+    resolvingIO = TestHelpers.roundTripSerialize(resolvingFileIO);\n+    assertThat(resolvingIO.credentials()).isEmpty();\n+    result =\n+        DynMethods.builder(\"io\")\n+            .hiddenImpl(ResolvingFileIO.class, String.class)\n+            .build(resolvingIO)\n+            .invoke(\"s3://foo/bar\");\n+    assertThat(result)\n+        .isInstanceOf(S3FileIO.class)\n+        .asInstanceOf(InstanceOfAssertFactories.type(S3FileIO.class))\n+        .satisfies(\n+            fileIO -> {\n+              assertThat(fileIO.client(\"s3://foo/bar\"))\n+                  .isSameAs(fileIO.client())\n+                  .isInstanceOf(S3Client.class);\n+              assertThat(fileIO.asyncClient(\"s3://foo/bar\"))\n+                  .isSameAs(fileIO.asyncClient())\n+                  .isInstanceOf(S3AsyncClient.class);\n+            });\n+  }\n+\n   @Test\n   public void resolvingFileIOLoadWithStorageCredentials()\n       throws IOException, ClassNotFoundException {\n\ndiff --git a/gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSFileIOTest.java b/gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSFileIOTest.java\nindex 4b5776aee846..97fe7cc40fc1 100644\n--- a/gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSFileIOTest.java\n+++ b/gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSFileIOTest.java\n@@ -422,6 +422,34 @@ public void fileIOWithStorageCredentialsKryoSerialization() throws IOException {\n         .isEqualTo(fileIO.credentials());\n   }\n \n+  @Test\n+  public void fileIOWithPrefixedStorageClientWithoutCredentialsKryoSerialization()\n+      throws IOException {\n+    GCSFileIO fileIO = new GCSFileIO();\n+    fileIO.initialize(\n+        Map.of(GCS_OAUTH2_TOKEN, \"gcsTokenFromProperties\", GCS_OAUTH2_TOKEN_EXPIRES_AT, \"1000\"));\n+\n+    assertThat(fileIO.client(\"gs\")).isInstanceOf(Storage.class);\n+    assertThat(fileIO.client(\"gs://bucket1/my-path/tableX\")).isInstanceOf(Storage.class);\n+    assertThat(fileIO.client(\"gs://bucket1/my-path/tableX\").getOptions().getCredentials())\n+        .isInstanceOf(OAuth2Credentials.class)\n+        .extracting(\"value\")\n+        .extracting(\"temporaryAccess\")\n+        .isEqualTo(new AccessToken(\"gcsTokenFromProperties\", new Date(1000L)));\n+\n+    GCSFileIO roundTripIO = TestHelpers.KryoHelpers.roundTripSerialize(fileIO);\n+    assertThat(roundTripIO).isNotNull();\n+    assertThat(roundTripIO.credentials()).isEqualTo(fileIO.credentials()).isEmpty();\n+\n+    assertThat(roundTripIO.client(\"gs\")).isInstanceOf(Storage.class);\n+    assertThat(roundTripIO.client(\"gs://bucket1/my-path/tableX\")).isInstanceOf(Storage.class);\n+    assertThat(roundTripIO.client(\"gs://bucket1/my-path/tableX\").getOptions().getCredentials())\n+        .isInstanceOf(OAuth2Credentials.class)\n+        .extracting(\"value\")\n+        .extracting(\"temporaryAccess\")\n+        .isEqualTo(new AccessToken(\"gcsTokenFromProperties\", new Date(1000L)));\n+  }\n+\n   @Test\n   public void fileIOWithPrefixedStorageClientKryoSerialization() throws IOException {\n     GCSFileIO fileIO = new GCSFileIO();\n@@ -471,6 +499,33 @@ public void fileIOWithStorageCredentialsJavaSerialization()\n         .isEqualTo(fileIO.credentials());\n   }\n \n+  @Test\n+  public void fileIOWithPrefixedStorageClientWithoutCredentialsJavaSerialization()\n+      throws IOException, ClassNotFoundException {\n+    GCSFileIO fileIO = new GCSFileIO();\n+    fileIO.initialize(\n+        Map.of(GCS_OAUTH2_TOKEN, \"gcsTokenFromProperties\", GCS_OAUTH2_TOKEN_EXPIRES_AT, \"1000\"));\n+\n+    assertThat(fileIO.client(\"gs\")).isInstanceOf(Storage.class);\n+    assertThat(fileIO.client(\"gs://bucket1/my-path/tableX\")).isInstanceOf(Storage.class);\n+    assertThat(fileIO.client(\"gs://bucket1/my-path/tableX\").getOptions().getCredentials())\n+        .isInstanceOf(OAuth2Credentials.class)\n+        .extracting(\"value\")\n+        .extracting(\"temporaryAccess\")\n+        .isEqualTo(new AccessToken(\"gcsTokenFromProperties\", new Date(1000L)));\n+\n+    GCSFileIO roundTripIO = TestHelpers.roundTripSerialize(fileIO);\n+    assertThat(roundTripIO.credentials()).isEqualTo(fileIO.credentials()).isEmpty();\n+\n+    assertThat(roundTripIO.client(\"gs\")).isInstanceOf(Storage.class);\n+    assertThat(roundTripIO.client(\"gs://bucket1/my-path/tableX\")).isInstanceOf(Storage.class);\n+    assertThat(roundTripIO.client(\"gs://bucket1/my-path/tableX\").getOptions().getCredentials())\n+        .isInstanceOf(OAuth2Credentials.class)\n+        .extracting(\"value\")\n+        .extracting(\"temporaryAccess\")\n+        .isEqualTo(new AccessToken(\"gcsTokenFromProperties\", new Date(1000L)));\n+  }\n+\n   @Test\n   public void fileIOWithPrefixedStorageClientJavaSerialization()\n       throws IOException, ClassNotFoundException {\n@@ -507,6 +562,19 @@ public void fileIOWithPrefixedStorageClientJavaSerialization()\n         .isEqualTo(new AccessToken(\"gcsTokenFromCredential\", new Date(2000L)));\n   }\n \n+  @Test\n+  public void resolvingFileIOLoadWithoutStorageCredentials()\n+      throws IOException, ClassNotFoundException {\n+    ResolvingFileIO resolvingFileIO = new ResolvingFileIO();\n+    resolvingFileIO.initialize(ImmutableMap.of());\n+\n+    ResolvingFileIO fileIO = TestHelpers.KryoHelpers.roundTripSerialize(resolvingFileIO);\n+    assertThat(fileIO.credentials()).isEmpty();\n+\n+    fileIO = TestHelpers.roundTripSerialize(resolvingFileIO);\n+    assertThat(fileIO.credentials()).isEmpty();\n+  }\n+\n   @Test\n   public void resolvingFileIOLoadWithStorageCredentials()\n       throws IOException, ClassNotFoundException {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13208",
    "pr_id": 13208,
    "issue_id": 12889,
    "repo": "apache/iceberg",
    "problem_statement": "Flaky test: TestRewriteDataFilesAction.testParallelPartialProgressWithMaxFailedCommitsLargerThanTotalFileGroup()\n### Apache Iceberg version\n\nmain (development)\n\n### Query engine\n\nNone\n\n### Please describe the bug üêû\n\n```\nTestRewriteDataFilesAction > testParallelPartialProgressWithMaxFailedCommitsLargerThanTotalFileGroup() > formatVersion = 3 FAILED\n    java.lang.RuntimeException: partial-progress.enabled is true but 1 rewrite commits failed. This is more than the maximum allowed failures of 0. Check the logs to determine why the individual commits failed. If this is persistent it may help to increase partial-progress.max-commits which will split the rewrite operation into smaller commits.\n        at org.apache.iceberg.spark.actions.RewriteDataFilesSparkAction.doExecuteWithPartialProgress(RewriteDataFilesSparkAction.java:400)\n        at org.apache.iceberg.spark.actions.RewriteDataFilesSparkAction.execute(RewriteDataFilesSparkAction.java:187)\n        at org.apache.iceberg.spark.actions.TestRewriteDataFilesAction.testParallelPartialProgressWithMaxFailedCommitsLargerThanTotalFileGroup(TestRewriteDataFilesAction.java:1340)\n```\n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 149,
    "test_files_count": 3,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java"
    ],
    "base_commit": "931865ecaf40a827f9081dddb675bf1c95c05461",
    "head_commit": "3e6f2f25696730eedf0aa060b2c345a6f164eea6",
    "repo_url": "https://github.com/apache/iceberg/pull/13208",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13208",
    "dockerfile": "",
    "pr_merged_at": "2025-06-04T10:08:52.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\nindex 83eb53eb65f4..baf6a0dd26ac 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\n@@ -1326,7 +1326,7 @@ public void testParallelPartialProgressWithMaxFailedCommits() {\n   }\n \n   @TestTemplate\n-  public void testParallelPartialProgressWithMaxFailedCommitsLargerThanTotalFileGroup() {\n+  public void testParallelPartialProgressWithMaxCommitsLargerThanTotalGroupCount() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n \n@@ -1341,7 +1341,8 @@ public void testParallelPartialProgressWithMaxFailedCommitsLargerThanTotalFileGr\n             // Since we can have at most one commit per file group and there are only 10 file\n             // groups, actual number of commits is 10\n             .option(RewriteDataFiles.PARTIAL_PROGRESS_MAX_COMMITS, \"20\")\n-            .option(RewriteDataFiles.PARTIAL_PROGRESS_MAX_FAILED_COMMITS, \"0\");\n+            // Setting max-failed-commits to 1 to tolerate random commit failure\n+            .option(RewriteDataFiles.PARTIAL_PROGRESS_MAX_FAILED_COMMITS, \"1\");\n     rewrite.execute();\n \n     table.refresh();\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\nindex 83eb53eb65f4..baf6a0dd26ac 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\n@@ -1326,7 +1326,7 @@ public void testParallelPartialProgressWithMaxFailedCommits() {\n   }\n \n   @TestTemplate\n-  public void testParallelPartialProgressWithMaxFailedCommitsLargerThanTotalFileGroup() {\n+  public void testParallelPartialProgressWithMaxCommitsLargerThanTotalGroupCount() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n \n@@ -1341,7 +1341,8 @@ public void testParallelPartialProgressWithMaxFailedCommitsLargerThanTotalFileGr\n             // Since we can have at most one commit per file group and there are only 10 file\n             // groups, actual number of commits is 10\n             .option(RewriteDataFiles.PARTIAL_PROGRESS_MAX_COMMITS, \"20\")\n-            .option(RewriteDataFiles.PARTIAL_PROGRESS_MAX_FAILED_COMMITS, \"0\");\n+            // Setting max-failed-commits to 1 to tolerate random commit failure\n+            .option(RewriteDataFiles.PARTIAL_PROGRESS_MAX_FAILED_COMMITS, \"1\");\n     rewrite.execute();\n \n     table.refresh();\n\ndiff --git a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\nindex 87ce48adea13..c93ca27803e3 100644\n--- a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\n+++ b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\n@@ -1326,7 +1326,7 @@ public void testParallelPartialProgressWithMaxFailedCommits() {\n   }\n \n   @TestTemplate\n-  public void testParallelPartialProgressWithMaxFailedCommitsLargerThanTotalFileGroup() {\n+  public void testParallelPartialProgressWithMaxCommitsLargerThanTotalGroupCount() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n \n@@ -1341,7 +1341,8 @@ public void testParallelPartialProgressWithMaxFailedCommitsLargerThanTotalFileGr\n             // Since we can have at most one commit per file group and there are only 10 file\n             // groups, actual number of commits is 10\n             .option(RewriteDataFiles.PARTIAL_PROGRESS_MAX_COMMITS, \"20\")\n-            .option(RewriteDataFiles.PARTIAL_PROGRESS_MAX_FAILED_COMMITS, \"0\");\n+            // Setting max-failed-commits to 1 to tolerate random commit failure\n+            .option(RewriteDataFiles.PARTIAL_PROGRESS_MAX_FAILED_COMMITS, \"1\");\n     rewrite.execute();\n \n     table.refresh();\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13163",
    "pr_id": 13163,
    "issue_id": 13155,
    "repo": "apache/iceberg",
    "problem_statement": "Incrementally computing partition stats can miss deleted files\n### Apache Iceberg version\n\nmain (development)\n\n### Query engine\n\nNone\n\n### Please describe the bug üêû\n\nIt seems stats diff is computed only from the current snapshot, which may not track deleted files. For example the following test case would fail:\n```java\n  @Test\n  public void test() throws Exception {\n    Table testTable =\n        TestTables.create(tempDir(\"my_test\"), \"my_test\", SCHEMA, SPEC, 2, fileFormatProperty);\n\n    DataFile dataFile1 =\n        DataFiles.builder(SPEC)\n            .withPath(\"/df1.parquet\")\n            .withPartitionPath(\"c2=a/c3=a\")\n            .withFileSizeInBytes(10)\n            .withRecordCount(1)\n            .build();\n    DataFile dataFile2 =\n        DataFiles.builder(SPEC)\n            .withPath(\"/df2.parquet\")\n            .withPartitionPath(\"c2=b/c3=b\")\n            .withFileSizeInBytes(10)\n            .withRecordCount(1)\n            .build();\n\n    testTable.newAppend().appendFile(dataFile1).appendFile(dataFile2).commit();\n\n    testTable\n        .updatePartitionStatistics()\n        .setPartitionStatistics(PartitionStatsHandler.computeAndWriteStatsFile(testTable))\n        .commit();\n\n    testTable.newDelete().deleteFile(dataFile1).commit();\n    testTable.newDelete().deleteFile(dataFile2).commit();\n\n    PartitionStatisticsFile statsFile = PartitionStatsHandler.computeAndWriteStatsFile(testTable);\n\n    assertThat(\n            PartitionStatsHandler.readPartitionStatsFile(\n                PartitionStatsHandler.schema(Partitioning.partitionType(testTable)),\n                Files.localInput(statsFile.path())))\n        .allMatch(s -> s.dataRecordCount() == 0);\n  }\n```\n\n### Willingness to contribute\n\n- [x] I can contribute a fix for this bug independently\n- [x] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 187,
    "test_files_count": 2,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/PartitionStatsHandler.java",
      "core/src/test/java/org/apache/iceberg/PartitionStatsHandlerTestBase.java",
      "orc/src/test/java/org/apache/iceberg/TestOrcPartitionStatsHandler.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/PartitionStatsHandlerTestBase.java",
      "orc/src/test/java/org/apache/iceberg/TestOrcPartitionStatsHandler.java"
    ],
    "base_commit": "cab0decbb0e32bf314039e30807eb033c50665d5",
    "head_commit": "d29d2b46bdab3f781a6839a506b638e15340bd02",
    "repo_url": "https://github.com/apache/iceberg/pull/13163",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13163",
    "dockerfile": "",
    "pr_merged_at": "2025-05-29T11:20:49.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/PartitionStatsHandler.java b/core/src/main/java/org/apache/iceberg/PartitionStatsHandler.java\nindex aeb6b8b4c7e7..3fb2e790a2bf 100644\n--- a/core/src/main/java/org/apache/iceberg/PartitionStatsHandler.java\n+++ b/core/src/main/java/org/apache/iceberg/PartitionStatsHandler.java\n@@ -29,9 +29,9 @@\n import java.util.Locale;\n import java.util.Map;\n import java.util.Queue;\n-import java.util.Set;\n import java.util.UUID;\n import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n import org.apache.iceberg.data.GenericRecord;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.io.FileAppender;\n@@ -39,10 +39,8 @@\n import org.apache.iceberg.io.OutputFile;\n import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n-import org.apache.iceberg.relocated.com.google.common.base.Predicate;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Queues;\n-import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.types.Comparators;\n import org.apache.iceberg.types.Types.IntegerType;\n import org.apache.iceberg.types.Types.LongType;\n@@ -163,7 +161,8 @@ public static PartitionStatisticsFile computeAndWriteStatsFile(Table table, long\n     if (statisticsFile == null) {\n       LOG.info(\n           \"Using full compute as previous statistics file is not present for incremental compute.\");\n-      stats = computeStats(table, snapshot, file -> true, false /* incremental */).values();\n+      stats =\n+          computeStats(table, snapshot.allManifests(table.io()), false /* incremental */).values();\n     } else {\n       stats = computeAndMergeStatsIncremental(table, snapshot, partitionType, statisticsFile);\n     }\n@@ -276,7 +275,8 @@ private static Collection<PartitionStats> computeAndMergeStatsIncremental(\n     PartitionMap<PartitionStats> statsMap = PartitionMap.create(table.specs());\n     // read previous stats, note that partition field will be read as GenericRecord\n     try (CloseableIterable<PartitionStats> oldStats =\n-        readPartitionStatsFile(schema(partitionType), Files.localInput(previousStatsFile.path()))) {\n+        readPartitionStatsFile(\n+            schema(partitionType), table.io().newInputFile(previousStatsFile.path()))) {\n       oldStats.forEach(\n           partitionStats ->\n               statsMap.put(partitionStats.specId(), partitionStats.partition(), partitionStats));\n@@ -325,28 +325,34 @@ static PartitionStatisticsFile latestStatsFile(Table table, long snapshotId) {\n       }\n     }\n \n-    // This is unlikely to happen.\n-    throw new RuntimeException(\n-        \"Unable to find previous stats with valid snapshot. Invalidate partition stats for all the snapshots to use full compute.\");\n+    // A stats file exists but isn't accessible from the current snapshot chain.\n+    // It may belong to a different snapshot reference (like a branch or tag).\n+    // Falling back to full computation for current snapshot.\n+    return null;\n   }\n \n   private static PartitionMap<PartitionStats> computeStatsDiff(\n       Table table, Snapshot fromSnapshot, Snapshot toSnapshot) {\n-    Set<Long> snapshotIdsRange =\n-        Sets.newHashSet(\n-            SnapshotUtil.ancestorIdsBetween(\n-                toSnapshot.snapshotId(), fromSnapshot.snapshotId(), table::snapshot));\n-    Predicate<ManifestFile> manifestFilePredicate =\n-        manifestFile -> snapshotIdsRange.contains(manifestFile.snapshotId());\n-    return computeStats(table, toSnapshot, manifestFilePredicate, true /* incremental */);\n+    Iterable<Snapshot> snapshots =\n+        SnapshotUtil.ancestorsBetween(\n+            toSnapshot.snapshotId(), fromSnapshot.snapshotId(), table::snapshot);\n+    // DELETED manifest entries are not carried over to subsequent snapshots.\n+    // So, for incremental computation, gather the manifests added by each snapshot\n+    // instead of relying solely on those from the latest snapshot.\n+    List<ManifestFile> manifests =\n+        StreamSupport.stream(snapshots.spliterator(), false)\n+            .flatMap(\n+                snapshot ->\n+                    snapshot.allManifests(table.io()).stream()\n+                        .filter(file -> file.snapshotId().equals(snapshot.snapshotId())))\n+            .collect(Collectors.toList());\n+\n+    return computeStats(table, manifests, true /* incremental */);\n   }\n \n   private static PartitionMap<PartitionStats> computeStats(\n-      Table table, Snapshot snapshot, Predicate<ManifestFile> predicate, boolean incremental) {\n+      Table table, List<ManifestFile> manifests, boolean incremental) {\n     StructType partitionType = Partitioning.partitionType(table);\n-    List<ManifestFile> manifests =\n-        snapshot.allManifests(table.io()).stream().filter(predicate).collect(Collectors.toList());\n-\n     Queue<PartitionMap<PartitionStats>> statsByManifest = Queues.newConcurrentLinkedQueue();\n     Tasks.foreach(manifests)\n         .stopOnFailure()\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/PartitionStatsHandlerTestBase.java b/core/src/test/java/org/apache/iceberg/PartitionStatsHandlerTestBase.java\nindex a00ad76e1daa..1c56fae2de01 100644\n--- a/core/src/test/java/org/apache/iceberg/PartitionStatsHandlerTestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/PartitionStatsHandlerTestBase.java\n@@ -204,7 +204,7 @@ public void testAllDatatypePartitionWriting() throws Exception {\n     List<PartitionStats> written;\n     try (CloseableIterable<PartitionStats> recordIterator =\n         PartitionStatsHandler.readPartitionStatsFile(\n-            dataSchema, Files.localInput(statisticsFile.path()))) {\n+            dataSchema, testTable.io().newInputFile(statisticsFile.path()))) {\n       written = Lists.newArrayList(recordIterator);\n     }\n \n@@ -273,7 +273,7 @@ public void testOptionalFieldsWriting() throws Exception {\n     List<PartitionStats> written;\n     try (CloseableIterable<PartitionStats> recordIterator =\n         PartitionStatsHandler.readPartitionStatsFile(\n-            dataSchema, Files.localInput(statisticsFile.path()))) {\n+            dataSchema, testTable.io().newInputFile(statisticsFile.path()))) {\n       written = Lists.newArrayList(recordIterator);\n     }\n \n@@ -441,6 +441,52 @@ public void testPartitionStats() throws Exception {\n             snapshot1.snapshotId()));\n   }\n \n+  @Test\n+  public void testCopyOnWriteDelete() throws Exception {\n+    Table testTable =\n+        TestTables.create(tempDir(\"my_test\"), \"my_test\", SCHEMA, SPEC, 2, fileFormatProperty);\n+\n+    DataFile dataFile1 =\n+        DataFiles.builder(SPEC)\n+            .withPath(\"/df1.parquet\")\n+            .withPartitionPath(\"c2=a/c3=a\")\n+            .withFileSizeInBytes(10)\n+            .withRecordCount(1)\n+            .build();\n+    DataFile dataFile2 =\n+        DataFiles.builder(SPEC)\n+            .withPath(\"/df2.parquet\")\n+            .withPartitionPath(\"c2=b/c3=b\")\n+            .withFileSizeInBytes(10)\n+            .withRecordCount(1)\n+            .build();\n+\n+    testTable.newAppend().appendFile(dataFile1).appendFile(dataFile2).commit();\n+\n+    PartitionStatisticsFile statisticsFile =\n+        PartitionStatsHandler.computeAndWriteStatsFile(testTable);\n+    testTable.updatePartitionStatistics().setPartitionStatistics(statisticsFile).commit();\n+\n+    assertThat(\n+            PartitionStatsHandler.readPartitionStatsFile(\n+                PartitionStatsHandler.schema(Partitioning.partitionType(testTable)),\n+                testTable.io().newInputFile(statisticsFile.path())))\n+        .allMatch(s -> (s.dataRecordCount() != 0 && s.dataFileCount() != 0));\n+\n+    testTable.newDelete().deleteFile(dataFile1).commit();\n+    testTable.newDelete().deleteFile(dataFile2).commit();\n+\n+    PartitionStatisticsFile statisticsFileNew =\n+        PartitionStatsHandler.computeAndWriteStatsFile(testTable);\n+\n+    // stats must be decremented to zero as all the files removed from table.\n+    assertThat(\n+            PartitionStatsHandler.readPartitionStatsFile(\n+                PartitionStatsHandler.schema(Partitioning.partitionType(testTable)),\n+                testTable.io().newInputFile(statisticsFileNew.path())))\n+        .allMatch(s -> (s.dataRecordCount() == 0 && s.dataFileCount() == 0));\n+  }\n+\n   @Test\n   public void testLatestStatsFile() throws Exception {\n     Table testTable =\n@@ -476,6 +522,46 @@ public void testLatestStatsFile() throws Exception {\n     assertThat(latestStatsFile).isEqualTo(statisticsFile);\n   }\n \n+  @Test\n+  public void testLatestStatsFileWithBranch() throws Exception {\n+    Table testTable =\n+        TestTables.create(\n+            tempDir(\"stats_file_branch\"), \"stats_file_branch\", SCHEMA, SPEC, 2, fileFormatProperty);\n+    DataFile dataFile =\n+        FileGenerationUtil.generateDataFile(testTable, TestHelpers.Row.of(\"foo\", \"A\"));\n+\n+    /*\n+                                             * [statsMainB]\n+          ---- snapshotA  ------ snapshotMainB\n+                        \\\n+                         \\\n+                          \\\n+                           snapshotBranchB(branch:b1)\n+    */\n+\n+    testTable.newAppend().appendFile(dataFile).commit();\n+    long snapshotAId = testTable.currentSnapshot().snapshotId();\n+\n+    testTable.newAppend().appendFile(dataFile).commit();\n+    long snapshotMainBId = testTable.currentSnapshot().snapshotId();\n+\n+    String branchName = \"b1\";\n+    testTable.manageSnapshots().createBranch(branchName, snapshotAId).commit();\n+    testTable.newAppend().appendFile(dataFile).commit();\n+    long snapshotBranchBId = testTable.snapshot(branchName).snapshotId();\n+\n+    PartitionStatisticsFile statsMainB =\n+        PartitionStatsHandler.computeAndWriteStatsFile(testTable, snapshotMainBId);\n+    testTable.updatePartitionStatistics().setPartitionStatistics(statsMainB).commit();\n+\n+    // should find latest stats for snapshotMainB\n+    assertThat(PartitionStatsHandler.latestStatsFile(testTable, snapshotMainBId))\n+        .isEqualTo(statsMainB);\n+\n+    // should not find latest stats for snapshotBranchB\n+    assertThat(PartitionStatsHandler.latestStatsFile(testTable, snapshotBranchBId)).isNull();\n+  }\n+\n   private static StructLike partitionRecord(\n       Types.StructType partitionType, String val1, String val2) {\n     GenericRecord record = GenericRecord.create(partitionType);\n@@ -496,7 +582,7 @@ private static void computeAndValidatePartitionStats(\n     List<PartitionStats> partitionStats;\n     try (CloseableIterable<PartitionStats> recordIterator =\n         PartitionStatsHandler.readPartitionStatsFile(\n-            recordSchema, Files.localInput(result.path()))) {\n+            recordSchema, testTable.io().newInputFile(result.path()))) {\n       partitionStats = Lists.newArrayList(recordIterator);\n     }\n \n\ndiff --git a/orc/src/test/java/org/apache/iceberg/TestOrcPartitionStatsHandler.java b/orc/src/test/java/org/apache/iceberg/TestOrcPartitionStatsHandler.java\nindex c26c0042976a..f91b86b2345c 100644\n--- a/orc/src/test/java/org/apache/iceberg/TestOrcPartitionStatsHandler.java\n+++ b/orc/src/test/java/org/apache/iceberg/TestOrcPartitionStatsHandler.java\n@@ -53,4 +53,18 @@ public void testLatestStatsFile() throws Exception {\n         .isInstanceOf(UnsupportedOperationException.class)\n         .hasMessage(\"Cannot write using unregistered internal data format: ORC\");\n   }\n+\n+  @Override\n+  public void testLatestStatsFileWithBranch() throws Exception {\n+    assertThatThrownBy(super::testLatestStatsFileWithBranch)\n+        .isInstanceOf(UnsupportedOperationException.class)\n+        .hasMessage(\"Cannot write using unregistered internal data format: ORC\");\n+  }\n+\n+  @Override\n+  public void testCopyOnWriteDelete() throws Exception {\n+    assertThatThrownBy(super::testCopyOnWriteDelete)\n+        .isInstanceOf(UnsupportedOperationException.class)\n+        .hasMessage(\"Cannot write using unregistered internal data format: ORC\");\n+  }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13130",
    "pr_id": 13130,
    "issue_id": 12874,
    "repo": "apache/iceberg",
    "problem_statement": "Hive should throw a NoSuchNamespaceException when listing a non-existing namespace\n### Feature Request / Improvement\n\nOnce this is implemented, `TestHiveCatalog.testListNonExistingNamespace()` can be enabled again\n\n### Query engine\n\nNone\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [x] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 62,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java"
    ],
    "pr_changed_test_files": [
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java"
    ],
    "base_commit": "14493784a6d59d7d1dfee84fe619fe349518e537",
    "head_commit": "4eb0d57d708ffaa4f5ff7ad0690d586491b56147",
    "repo_url": "https://github.com/apache/iceberg/pull/13130",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13130",
    "dockerfile": "",
    "pr_merged_at": "2025-06-03T05:49:18.000Z",
    "patch": "diff --git a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java\nindex bed1772d8e52..b0d9e224634d 100644\n--- a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java\n+++ b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveCatalog.java\n@@ -508,7 +508,7 @@ public void createNamespace(Namespace namespace, Map<String, String> meta) {\n \n   @Override\n   public List<Namespace> listNamespaces(Namespace namespace) {\n-    if (!isValidateNamespace(namespace) && !namespace.isEmpty()) {\n+    if (!namespace.isEmpty() && (!isValidateNamespace(namespace) || !namespaceExists(namespace))) {\n       throw new NoSuchNamespaceException(\"Namespace does not exist: %s\", namespace);\n     }\n     if (!namespace.isEmpty()) {\n",
    "test_patch": "diff --git a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java\nindex 25a6410ffb81..5c3907670c52 100644\n--- a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java\n+++ b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java\n@@ -83,7 +83,6 @@\n import org.apache.thrift.TException;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Disabled;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.extension.RegisterExtension;\n import org.junit.jupiter.api.io.TempDir;\n@@ -1209,11 +1208,4 @@ public void testDatabaseLocationWithSlashInWarehouseDir() {\n \n     assertThat(database.getLocationUri()).isEqualTo(\"s3://bucket/database.db\");\n   }\n-\n-  @Test\n-  @Override\n-  @Disabled(\"Hive currently returns an empty list instead of throwing a NoSuchNamespaceException\")\n-  public void testListNonExistingNamespace() {\n-    super.testListNonExistingNamespace();\n-  }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13101",
    "pr_id": 13101,
    "issue_id": 13097,
    "repo": "apache/iceberg",
    "problem_statement": "Modify REST Tests to Bind Loopback instead of Localhost\n### Feature Request / Improvement\n\nCurrently a lot of our tests use \n\nhttps://github.com/apache/iceberg/blob/a7f3dc79a2f42a4875ac35eec2137ecff15204fc/core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java#L169\nhttps://github.com/apache/iceberg/blob/a7f3dc79a2f42a4875ac35eec2137ecff15204fc/aws/src/integration/java/org/apache/iceberg/aws/s3/signer/TestS3RestSigner.java#L190\n\nOr something like that which binds all available interfaces\n\nWhen we set up the clients we will inevitable call for the following for the client URI\n```java\nhttpServer.getURI().toString(),\n```\n\nDown the chain this means we essentially are use InetAddress.getLocalHost() to get the URI.\n\nFor some developers who are working in more locked down firewall environments this means we attempt to communicate with the embedded services on a network accessible IP which is not allowed. \n\nWhile we can kind of work around this by adding a loopback alias to our hostname though this isn't exactly a great long term solution.\n```bash\n/etc/hosts\n\n127.0.0.1 your_hostname\n```\n\nI think it probably makes sense to change all instances of test server creation to use loopback instead of localhost as their bind address to default clients to only speak on localhost. We could also make the test bind host settable as an ENV var or as grade build property.\n\nReally any solution here is fine, but ideally we want to be able to have tests communicate only on loopback if possible.\n\n### Query engine\n\nNone\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [x] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 277,
    "test_files_count": 10,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "aws/src/integration/java/org/apache/iceberg/aws/s3/signer/TestS3RestSigner.java",
      "core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java",
      "core/src/test/java/org/apache/iceberg/rest/TestRESTViewCatalog.java",
      "core/src/test/java/org/apache/iceberg/rest/TestRESTViewCatalogWithAssumedViewSupport.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/TestBase.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java"
    ],
    "pr_changed_test_files": [
      "aws/src/integration/java/org/apache/iceberg/aws/s3/signer/TestS3RestSigner.java",
      "core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java",
      "core/src/test/java/org/apache/iceberg/rest/TestRESTViewCatalog.java",
      "core/src/test/java/org/apache/iceberg/rest/TestRESTViewCatalogWithAssumedViewSupport.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/TestBase.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java"
    ],
    "base_commit": "09a531740ccec41d69c166ec484be6da16a09947",
    "head_commit": "255ff0d0ac68b2c432f9392eb60efeb8bf1a8330",
    "repo_url": "https://github.com/apache/iceberg/pull/13101",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13101",
    "dockerfile": "",
    "pr_merged_at": "2025-05-27T18:33:01.000Z",
    "patch": "",
    "test_patch": "diff --git a/aws/src/integration/java/org/apache/iceberg/aws/s3/signer/TestS3RestSigner.java b/aws/src/integration/java/org/apache/iceberg/aws/s3/signer/TestS3RestSigner.java\nindex 34f5f2c710c8..3026a3dce30d 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/s3/signer/TestS3RestSigner.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/s3/signer/TestS3RestSigner.java\n@@ -21,6 +21,8 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.InstanceOfAssertFactories.type;\n \n+import java.net.InetAddress;\n+import java.net.InetSocketAddress;\n import java.net.URI;\n import java.nio.file.Path;\n import java.nio.file.Paths;\n@@ -187,7 +189,7 @@ private static Server initHttpServer() throws Exception {\n     servletContext.addServlet(new ServletHolder(servlet), \"/*\");\n     servletContext.setHandler(new GzipHandler());\n \n-    Server server = new Server(0);\n+    Server server = new Server(new InetSocketAddress(InetAddress.getLoopbackAddress(), 0));\n     server.setHandler(servletContext);\n     server.start();\n     return server;\n\ndiff --git a/core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java b/core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java\nindex f8f5148f79a8..30b1ddef5b17 100644\n--- a/core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java\n+++ b/core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java\n@@ -34,6 +34,8 @@\n import com.fasterxml.jackson.databind.ObjectMapper;\n import java.io.File;\n import java.io.IOException;\n+import java.net.InetAddress;\n+import java.net.InetSocketAddress;\n import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n@@ -166,7 +168,7 @@ public <T extends RESTResponse> T execute(\n     servletContext.addServlet(new ServletHolder(new RESTCatalogServlet(adaptor)), \"/*\");\n     servletContext.setHandler(new GzipHandler());\n \n-    this.httpServer = new Server(0);\n+    this.httpServer = new Server(new InetSocketAddress(InetAddress.getLoopbackAddress(), 0));\n     httpServer.setHandler(servletContext);\n     httpServer.start();\n \n\ndiff --git a/core/src/test/java/org/apache/iceberg/rest/TestRESTViewCatalog.java b/core/src/test/java/org/apache/iceberg/rest/TestRESTViewCatalog.java\nindex 3abb40fad04a..418b9f9cc298 100644\n--- a/core/src/test/java/org/apache/iceberg/rest/TestRESTViewCatalog.java\n+++ b/core/src/test/java/org/apache/iceberg/rest/TestRESTViewCatalog.java\n@@ -27,6 +27,8 @@\n import com.fasterxml.jackson.core.JsonProcessingException;\n import com.fasterxml.jackson.databind.ObjectMapper;\n import java.io.File;\n+import java.net.InetAddress;\n+import java.net.InetSocketAddress;\n import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n@@ -105,7 +107,7 @@ public <T extends RESTResponse> T execute(\n     servletContext.addServlet(new ServletHolder(new RESTCatalogServlet(adaptor)), \"/*\");\n     servletContext.setHandler(new GzipHandler());\n \n-    this.httpServer = new Server(0);\n+    this.httpServer = new Server(new InetSocketAddress(InetAddress.getLoopbackAddress(), 0));\n     httpServer.setHandler(servletContext);\n     httpServer.start();\n \n\ndiff --git a/core/src/test/java/org/apache/iceberg/rest/TestRESTViewCatalogWithAssumedViewSupport.java b/core/src/test/java/org/apache/iceberg/rest/TestRESTViewCatalogWithAssumedViewSupport.java\nindex b6e2076934e9..4e543d3fa22e 100644\n--- a/core/src/test/java/org/apache/iceberg/rest/TestRESTViewCatalogWithAssumedViewSupport.java\n+++ b/core/src/test/java/org/apache/iceberg/rest/TestRESTViewCatalogWithAssumedViewSupport.java\n@@ -21,6 +21,8 @@\n import static org.apache.iceberg.rest.RESTCatalogAdapter.Route.CONFIG;\n \n import java.io.File;\n+import java.net.InetAddress;\n+import java.net.InetSocketAddress;\n import java.util.Map;\n import java.util.UUID;\n import org.apache.iceberg.CatalogProperties;\n@@ -66,7 +68,7 @@ public <T extends RESTResponse> T handleRequest(\n     servletContext.addServlet(new ServletHolder(new RESTCatalogServlet(adaptor)), \"/*\");\n     servletContext.setHandler(new GzipHandler());\n \n-    this.httpServer = new Server(0);\n+    this.httpServer = new Server(new InetSocketAddress(InetAddress.getLoopbackAddress(), 0));\n     httpServer.setHandler(servletContext);\n     httpServer.start();\n \n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBase.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBase.java\nindex 2d327519e027..3c32b4693684 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBase.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBase.java\n@@ -23,6 +23,7 @@\n \n import java.io.IOException;\n import java.io.UncheckedIOException;\n+import java.net.InetAddress;\n import java.net.URI;\n import java.nio.file.Files;\n import java.nio.file.Path;\n@@ -74,6 +75,7 @@ public static void startMetastoreAndSpark() {\n     TestBase.spark =\n         SparkSession.builder()\n             .master(\"local[2]\")\n+            .config(\"spark.driver.host\", InetAddress.getLoopbackAddress().getHostAddress())\n             .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n             .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n             .config(\"spark.sql.legacy.respectNullabilityInTextDatasetConversion\", \"true\")\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\nindex 06d5e0c44fb3..20def386bbb1 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\n@@ -22,6 +22,7 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.net.InetAddress;\n import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n@@ -57,7 +58,11 @@ public abstract class ScanTestBase extends AvroDataTest {\n \n   @BeforeAll\n   public static void startSpark() {\n-    ScanTestBase.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+    ScanTestBase.spark =\n+        SparkSession.builder()\n+            .config(\"spark.driver.host\", InetAddress.getLoopbackAddress().getHostAddress())\n+            .master(\"local[2]\")\n+            .getOrCreate();\n     ScanTestBase.sc = JavaSparkContext.fromSparkContext(spark.sparkContext());\n   }\n \n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestBase.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestBase.java\nindex 3e9f3334ef6e..daf4e29ac075 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestBase.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestBase.java\n@@ -23,6 +23,7 @@\n \n import java.io.IOException;\n import java.io.UncheckedIOException;\n+import java.net.InetAddress;\n import java.net.URI;\n import java.nio.file.Files;\n import java.nio.file.Path;\n@@ -74,6 +75,7 @@ public static void startMetastoreAndSpark() {\n     TestBase.spark =\n         SparkSession.builder()\n             .master(\"local[2]\")\n+            .config(\"spark.driver.host\", InetAddress.getLoopbackAddress().getHostAddress())\n             .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n             .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n             .config(\"spark.sql.legacy.respectNullabilityInTextDatasetConversion\", \"true\")\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\nindex 0886df957db0..216d418f1830 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\n@@ -22,6 +22,7 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.net.InetAddress;\n import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n@@ -57,7 +58,11 @@ public abstract class ScanTestBase extends AvroDataTest {\n \n   @BeforeAll\n   public static void startSpark() {\n-    ScanTestBase.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+    ScanTestBase.spark =\n+        SparkSession.builder()\n+            .config(\"spark.driver.host\", InetAddress.getLoopbackAddress().getHostAddress())\n+            .master(\"local[2]\")\n+            .getOrCreate();\n     ScanTestBase.sc = JavaSparkContext.fromSparkContext(spark.sparkContext());\n   }\n \n\ndiff --git a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/TestBase.java b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/TestBase.java\nindex 3e9f3334ef6e..daf4e29ac075 100644\n--- a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/TestBase.java\n+++ b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/TestBase.java\n@@ -23,6 +23,7 @@\n \n import java.io.IOException;\n import java.io.UncheckedIOException;\n+import java.net.InetAddress;\n import java.net.URI;\n import java.nio.file.Files;\n import java.nio.file.Path;\n@@ -74,6 +75,7 @@ public static void startMetastoreAndSpark() {\n     TestBase.spark =\n         SparkSession.builder()\n             .master(\"local[2]\")\n+            .config(\"spark.driver.host\", InetAddress.getLoopbackAddress().getHostAddress())\n             .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n             .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n             .config(\"spark.sql.legacy.respectNullabilityInTextDatasetConversion\", \"true\")\n\ndiff --git a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\nindex 0886df957db0..216d418f1830 100644\n--- a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\n+++ b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\n@@ -22,6 +22,7 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.net.InetAddress;\n import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n@@ -57,7 +58,11 @@ public abstract class ScanTestBase extends AvroDataTest {\n \n   @BeforeAll\n   public static void startSpark() {\n-    ScanTestBase.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n+    ScanTestBase.spark =\n+        SparkSession.builder()\n+            .config(\"spark.driver.host\", InetAddress.getLoopbackAddress().getHostAddress())\n+            .master(\"local[2]\")\n+            .getOrCreate();\n     ScanTestBase.sc = JavaSparkContext.fromSparkContext(spark.sparkContext());\n   }\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13072",
    "pr_id": 13072,
    "issue_id": 13054,
    "repo": "apache/iceberg",
    "problem_statement": "Handle leftover deprecations\n### Feature Request / Improvement\n\nWhile working on #13053, I just looked up `@deprecated` and found still too many places need clean up or update. \n\nExample: `MetricsConfig.fromProperties()` need to be removed. \n`org.apache.iceberg.data.GenericAppenderHelper` need to be removed?\nFlinkSchemaUtil.convert(TableSchema schema) need to be removed. \netc.\n\nNote: \nLookup `@deprecated`, please go through the java doc before removing or updating.  Should not remove/update contents that are meant for 2.0.0 or next version 1.11.0. \n\nWork on the things that doesn't have version mentioned in the javadoc (maybe need javadoc update if it is meant for 2.0.0)\n\n### Query engine\n\nNone\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [ ] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 144,
    "test_files_count": 26,
    "non_test_files_count": 12,
    "pr_changed_files": [
      "LICENSE",
      "flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java",
      "flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkDynamicTableFactory.java",
      "flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java",
      "flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java",
      "flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java",
      "flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSink.java",
      "flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSinkBuilder.java",
      "flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/IcebergSource.java",
      "flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java",
      "flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowConverter.java",
      "flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/util/FlinkCompatibilityUtil.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkFilters.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSinkCompaction.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestBucketPartitionerFlinkIcebergSink.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestCompressionSettings.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkBase.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkBranch.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkDistributionMode.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkExtended.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2Base.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2Branch.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2DistributionMode.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSinkBranch.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSinkV2.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergStreamWriter.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestTaskWriters.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/dynamic/TestDynamicIcebergSink.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/BoundedTableFactory.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceBounded.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceBoundedRow.java"
    ],
    "pr_changed_test_files": [
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkFilters.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSinkCompaction.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestBucketPartitionerFlinkIcebergSink.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestCompressionSettings.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkBase.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkBranch.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkDistributionMode.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkExtended.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2Base.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2Branch.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2DistributionMode.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSinkBranch.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSinkV2.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergStreamWriter.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestTaskWriters.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/dynamic/TestDynamicIcebergSink.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/BoundedTableFactory.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceBounded.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceBoundedRow.java"
    ],
    "base_commit": "6e432fcb24fd55c9024a3192b17a364623886047",
    "head_commit": "5b907c381a8ad32950be7f5c4bf9e937a1967097",
    "repo_url": "https://github.com/apache/iceberg/pull/13072",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13072",
    "dockerfile": "",
    "pr_merged_at": "2025-06-25T08:34:10.000Z",
    "patch": "diff --git a/LICENSE b/LICENSE\nindex 76f6113d9811..34511297cd8a 100644\n--- a/LICENSE\n+++ b/LICENSE\n@@ -331,6 +331,7 @@ This product includes code from Apache Flink.\n * Parameterized test at class level logic in ParameterizedTestExtension.java\n * Parameter provider annotation for parameterized tests in Parameters.java\n * Parameter field annotation for parameterized tests in Parameter.java\n+* Primary key validation logic in FlinkSchemaUtil.java\n \n Copyright: 1999-2022 The Apache Software Foundation.\n Home page: https://flink.apache.org/\n\ndiff --git a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\nindex b61e770e389f..4bb235b811d0 100644\n--- a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n+++ b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkCatalog.java\n@@ -38,6 +38,7 @@\n import org.apache.flink.table.catalog.CatalogTable;\n import org.apache.flink.table.catalog.ObjectPath;\n import org.apache.flink.table.catalog.ResolvedCatalogTable;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.catalog.TableChange;\n import org.apache.flink.table.catalog.exceptions.CatalogException;\n import org.apache.flink.table.catalog.exceptions.DatabaseAlreadyExistException;\n@@ -51,7 +52,6 @@\n import org.apache.flink.table.catalog.stats.CatalogTableStatistics;\n import org.apache.flink.table.expressions.Expression;\n import org.apache.flink.table.factories.Factory;\n-import org.apache.flink.table.legacy.api.TableSchema;\n import org.apache.flink.util.StringUtils;\n import org.apache.iceberg.CachingCatalog;\n import org.apache.iceberg.DataFile;\n@@ -435,7 +435,7 @@ void createIcebergTable(ObjectPath tablePath, ResolvedCatalogTable table, boolea\n     validateFlinkTable(table);\n \n     Schema icebergSchema = FlinkSchemaUtil.convert(table.getResolvedSchema());\n-    PartitionSpec spec = toPartitionSpec(((CatalogTable) table).getPartitionKeys(), icebergSchema);\n+    PartitionSpec spec = toPartitionSpec(table.getPartitionKeys(), icebergSchema);\n     ImmutableMap.Builder<String, String> properties = ImmutableMap.builder();\n     String location = null;\n     for (Map.Entry<String, String> entry : table.getOptions().entrySet()) {\n@@ -467,22 +467,7 @@ private boolean isReservedProperty(String prop) {\n   }\n \n   private static void validateTableSchemaAndPartition(CatalogTable ct1, CatalogTable ct2) {\n-    TableSchema ts1 = ct1.getSchema();\n-    TableSchema ts2 = ct2.getSchema();\n-    boolean equalsPrimary = false;\n-\n-    if (ts1.getPrimaryKey().isPresent() && ts2.getPrimaryKey().isPresent()) {\n-      equalsPrimary =\n-          Objects.equals(ts1.getPrimaryKey().get().getType(), ts2.getPrimaryKey().get().getType())\n-              && Objects.equals(\n-                  ts1.getPrimaryKey().get().getColumns(), ts2.getPrimaryKey().get().getColumns());\n-    } else if (!ts1.getPrimaryKey().isPresent() && !ts2.getPrimaryKey().isPresent()) {\n-      equalsPrimary = true;\n-    }\n-\n-    if (!(Objects.equals(ts1.getTableColumns(), ts2.getTableColumns())\n-        && Objects.equals(ts1.getWatermarkSpecs(), ts2.getWatermarkSpecs())\n-        && equalsPrimary)) {\n+    if (!Objects.equals(ct1.getUnresolvedSchema(), ct2.getUnresolvedSchema())) {\n       throw new UnsupportedOperationException(\n           \"Altering schema is not supported in the old alterTable API. \"\n               + \"To alter schema, use the other alterTable API and provide a list of TableChange's.\");\n@@ -632,9 +617,9 @@ private static void validateFlinkTable(CatalogBaseTable table) {\n     Preconditions.checkArgument(\n         table instanceof CatalogTable, \"The Table should be a CatalogTable.\");\n \n-    TableSchema schema = table.getSchema();\n+    org.apache.flink.table.api.Schema schema = table.getUnresolvedSchema();\n     schema\n-        .getTableColumns()\n+        .getColumns()\n         .forEach(\n             column -> {\n               if (!FlinkCompatibilityUtil.isPhysicalColumn(column)) {\n@@ -671,16 +656,17 @@ private static List<String> toPartitionKeys(PartitionSpec spec, Schema icebergSc\n   }\n \n   static CatalogTable toCatalogTableWithProps(Table table, Map<String, String> props) {\n-    TableSchema schema = FlinkSchemaUtil.toSchema(table.schema());\n+    ResolvedSchema resolvedSchema = FlinkSchemaUtil.toResolvedSchema(table.schema());\n     List<String> partitionKeys = toPartitionKeys(table.spec(), table.schema());\n \n     // NOTE: We can not create a IcebergCatalogTable extends CatalogTable, because Flink optimizer\n-    // may use\n-    // CatalogTableImpl to copy a new catalog table.\n+    // may use DefaultCatalogTable to copy a new catalog table.\n     // Let's re-loading table from Iceberg catalog when creating source/sink operators.\n-    // Iceberg does not have Table comment, so pass a null (Default comment value in Flink).\n     return CatalogTable.newBuilder()\n-        .schema(schema.toSchema())\n+        .schema(\n+            org.apache.flink.table.api.Schema.newBuilder()\n+                .fromResolvedSchema(resolvedSchema)\n+                .build())\n         .partitionKeys(partitionKeys)\n         .options(props)\n         .build();\n\ndiff --git a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkDynamicTableFactory.java b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkDynamicTableFactory.java\nindex 344e4d2753a5..bd79c1156090 100644\n--- a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkDynamicTableFactory.java\n+++ b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkDynamicTableFactory.java\n@@ -21,20 +21,21 @@\n import java.util.Collections;\n import java.util.Map;\n import java.util.Set;\n+import java.util.stream.Collectors;\n import org.apache.flink.configuration.ConfigOption;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.table.catalog.CatalogDatabaseImpl;\n+import org.apache.flink.table.catalog.Column;\n import org.apache.flink.table.catalog.ObjectIdentifier;\n import org.apache.flink.table.catalog.ObjectPath;\n import org.apache.flink.table.catalog.ResolvedCatalogTable;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.catalog.exceptions.DatabaseAlreadyExistException;\n import org.apache.flink.table.catalog.exceptions.TableAlreadyExistException;\n import org.apache.flink.table.connector.sink.DynamicTableSink;\n import org.apache.flink.table.connector.source.DynamicTableSource;\n import org.apache.flink.table.factories.DynamicTableSinkFactory;\n import org.apache.flink.table.factories.DynamicTableSourceFactory;\n-import org.apache.flink.table.legacy.api.TableSchema;\n-import org.apache.flink.table.utils.TableSchemaUtils;\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.exceptions.AlreadyExistsException;\n import org.apache.iceberg.flink.source.IcebergTableSource;\n@@ -60,7 +61,11 @@ public DynamicTableSource createDynamicTableSource(Context context) {\n     ObjectIdentifier objectIdentifier = context.getObjectIdentifier();\n     ResolvedCatalogTable resolvedCatalogTable = context.getCatalogTable();\n     Map<String, String> tableProps = resolvedCatalogTable.getOptions();\n-    TableSchema tableSchema = TableSchemaUtils.getPhysicalSchema(resolvedCatalogTable.getSchema());\n+    ResolvedSchema resolvedSchema =\n+        ResolvedSchema.of(\n+            resolvedCatalogTable.getResolvedSchema().getColumns().stream()\n+                .filter(Column::isPhysical)\n+                .collect(Collectors.toList()));\n \n     TableLoader tableLoader;\n     if (catalog != null) {\n@@ -74,7 +79,8 @@ public DynamicTableSource createDynamicTableSource(Context context) {\n               objectIdentifier.getObjectName());\n     }\n \n-    return new IcebergTableSource(tableLoader, tableSchema, tableProps, context.getConfiguration());\n+    return new IcebergTableSource(\n+        tableLoader, resolvedSchema, tableProps, context.getConfiguration());\n   }\n \n   @Override\n@@ -82,7 +88,11 @@ public DynamicTableSink createDynamicTableSink(Context context) {\n     ObjectIdentifier objectIdentifier = context.getObjectIdentifier();\n     ResolvedCatalogTable resolvedCatalogTable = context.getCatalogTable();\n     Map<String, String> writeProps = resolvedCatalogTable.getOptions();\n-    TableSchema tableSchema = TableSchemaUtils.getPhysicalSchema(resolvedCatalogTable.getSchema());\n+    ResolvedSchema resolvedSchema =\n+        ResolvedSchema.of(\n+            resolvedCatalogTable.getResolvedSchema().getColumns().stream()\n+                .filter(Column::isPhysical)\n+                .collect(Collectors.toList()));\n \n     TableLoader tableLoader;\n     if (catalog != null) {\n@@ -96,7 +106,8 @@ public DynamicTableSink createDynamicTableSink(Context context) {\n               objectIdentifier.getObjectName());\n     }\n \n-    return new IcebergTableSink(tableLoader, tableSchema, context.getConfiguration(), writeProps);\n+    return new IcebergTableSink(\n+        tableLoader, resolvedSchema, context.getConfiguration(), writeProps);\n   }\n \n   @Override\n\ndiff --git a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java\nindex a6afa0436bac..7f55d4b07bb1 100644\n--- a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java\n+++ b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/FlinkSchemaUtil.java\n@@ -18,11 +18,18 @@\n  */\n package org.apache.iceberg.flink;\n \n+import java.util.Collections;\n import java.util.List;\n+import java.util.Map;\n import java.util.Set;\n+import java.util.UUID;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.api.ValidationException;\n import org.apache.flink.table.catalog.Column;\n import org.apache.flink.table.catalog.ResolvedSchema;\n+import org.apache.flink.table.catalog.UniqueConstraint;\n import org.apache.flink.table.legacy.api.TableSchema;\n import org.apache.flink.table.types.logical.LogicalType;\n import org.apache.flink.table.types.logical.RowType;\n@@ -59,7 +66,7 @@ public class FlinkSchemaUtil {\n   private FlinkSchemaUtil() {}\n \n   /**\n-   * @deprecated Use {@link #convert(ResolvedSchema)} instead.\n+   * @deprecated will be removed in 2.0.0; use {@link #convert(ResolvedSchema)} instead.\n    */\n   @Deprecated\n   public static Schema convert(TableSchema schema) {\n@@ -102,11 +109,10 @@ public static Schema convert(ResolvedSchema flinkSchema) {\n     RowType root = (RowType) schemaType;\n     Type converted = root.accept(new FlinkTypeToType(root));\n     Schema icebergSchema = new Schema(converted.asStructType().fields());\n-    if (flinkSchema.getPrimaryKey().isPresent()) {\n-      return freshIdentifierFieldIds(icebergSchema, flinkSchema.getPrimaryKey().get().getColumns());\n-    } else {\n-      return icebergSchema;\n-    }\n+    return flinkSchema\n+        .getPrimaryKey()\n+        .map(pk -> freshIdentifierFieldIds(icebergSchema, pk.getColumns()))\n+        .orElse(icebergSchema);\n   }\n \n   private static Schema freshIdentifierFieldIds(Schema icebergSchema, List<String> primaryKeys) {\n@@ -137,7 +143,10 @@ private static Schema freshIdentifierFieldIds(Schema icebergSchema, List<String>\n    * @param flinkSchema a Flink TableSchema\n    * @return the equivalent Schema\n    * @throws IllegalArgumentException if the type cannot be converted or there are missing ids\n+   * @deprecated since 1.10.0, will be removed in 2.0.0. Use {@link #convert(Schema,\n+   *     ResolvedSchema)} instead.\n    */\n+  @Deprecated\n   public static Schema convert(Schema baseSchema, TableSchema flinkSchema) {\n     // convert to a type with fresh ids\n     Types.StructType struct = convert(flinkSchema).asStruct();\n@@ -155,6 +164,35 @@ public static Schema convert(Schema baseSchema, TableSchema flinkSchema) {\n     }\n   }\n \n+  /**\n+   * Convert a Flink {@link ResolvedSchema} to a {@link Schema} based on the given schema.\n+   *\n+   * <p>This conversion does not assign new ids; it uses ids from the base schema.\n+   *\n+   * <p>Data types, field order, and nullability will match the Flink type. This conversion may\n+   * return a schema that is not compatible with base schema.\n+   *\n+   * @param baseSchema a Schema on which conversion is based\n+   * @param flinkSchema a Flink ResolvedSchema\n+   * @return the equivalent Schema\n+   * @throws IllegalArgumentException if the type cannot be converted or there are missing ids\n+   */\n+  public static Schema convert(Schema baseSchema, ResolvedSchema flinkSchema) {\n+    // convert to a type with fresh ids\n+    Types.StructType struct = convert(flinkSchema).asStruct();\n+    // reassign ids to match the base schema\n+    Schema schema = TypeUtil.reassignIds(new Schema(struct.fields()), baseSchema);\n+    // reassign doc to match the base schema\n+    schema = TypeUtil.reassignDoc(schema, baseSchema);\n+\n+    // fix types that can't be represented in Flink (UUID)\n+    Schema fixedSchema = FlinkFixupTypes.fixup(schema, baseSchema);\n+    return flinkSchema\n+        .getPrimaryKey()\n+        .map(pk -> freshIdentifierFieldIds(fixedSchema, pk.getColumns()))\n+        .orElse(fixedSchema);\n+  }\n+\n   /**\n    * Convert a {@link Schema} to a {@link RowType Flink type}.\n    *\n@@ -192,7 +230,10 @@ public static Type convert(LogicalType flinkType) {\n    *\n    * @param rowType a RowType\n    * @return Flink TableSchema\n+   * @deprecated since 1.10.0, will be removed in 2.0.0. Use {@link #toResolvedSchema(RowType)}\n+   *     instead\n    */\n+  @Deprecated\n   public static TableSchema toSchema(RowType rowType) {\n     TableSchema.Builder builder = TableSchema.builder();\n     for (RowType.RowField field : rowType.getFields()) {\n@@ -201,12 +242,31 @@ public static TableSchema toSchema(RowType rowType) {\n     return builder.build();\n   }\n \n+  /**\n+   * Convert a {@link RowType} to a {@link ResolvedSchema}.\n+   *\n+   * @param rowType a RowType\n+   * @return Flink ResolvedSchema\n+   */\n+  public static ResolvedSchema toResolvedSchema(RowType rowType) {\n+    List<Column> columns = Lists.newArrayListWithExpectedSize(rowType.getFieldCount());\n+    for (RowType.RowField field : rowType.getFields()) {\n+      columns.add(\n+          Column.physical(field.getName(), TypeConversions.fromLogicalToDataType(field.getType())));\n+    }\n+\n+    return ResolvedSchema.of(columns);\n+  }\n+\n   /**\n    * Convert a {@link Schema} to a {@link TableSchema}.\n    *\n    * @param schema iceberg schema to convert.\n    * @return Flink TableSchema.\n+   * @deprecated since 1.10.0, will be removed in 2.0.0. Use {@link #toResolvedSchema(Schema)}\n+   *     instead\n    */\n+  @Deprecated\n   public static TableSchema toSchema(Schema schema) {\n     TableSchema.Builder builder = TableSchema.builder();\n \n@@ -231,4 +291,90 @@ public static TableSchema toSchema(Schema schema) {\n \n     return builder.build();\n   }\n+\n+  /**\n+   * Convert a {@link Schema} to a {@link ResolvedSchema}.\n+   *\n+   * @param schema iceberg schema to convert.\n+   * @return Flink ResolvedSchema.\n+   */\n+  public static ResolvedSchema toResolvedSchema(Schema schema) {\n+    RowType rowType = convert(schema);\n+    List<Column> columns = Lists.newArrayListWithExpectedSize(rowType.getFieldCount());\n+\n+    // Add columns.\n+    for (RowType.RowField field : rowType.getFields()) {\n+      columns.add(\n+          Column.physical(field.getName(), TypeConversions.fromLogicalToDataType(field.getType())));\n+    }\n+\n+    // Add primary key.\n+    Set<Integer> identifierFieldIds = schema.identifierFieldIds();\n+    UniqueConstraint uniqueConstraint = null;\n+    if (!identifierFieldIds.isEmpty()) {\n+      List<String> primaryKeyColumns =\n+          Lists.newArrayListWithExpectedSize(identifierFieldIds.size());\n+      for (Integer identifierFieldId : identifierFieldIds) {\n+        String columnName = schema.findColumnName(identifierFieldId);\n+        Preconditions.checkNotNull(\n+            columnName, \"Cannot find field with id %s in schema %s\", identifierFieldId, schema);\n+\n+        primaryKeyColumns.add(columnName);\n+      }\n+\n+      uniqueConstraint =\n+          UniqueConstraint.primaryKey(UUID.randomUUID().toString(), primaryKeyColumns);\n+\n+      validatePrimaryKey(uniqueConstraint, columns);\n+    }\n+\n+    return new ResolvedSchema(columns, Collections.emptyList(), uniqueConstraint);\n+  }\n+\n+  /**\n+   * Copied from\n+   * org.apache.flink.table.catalog.DefaultSchemaResolver#validatePrimaryKey(org.apache.flink.table.catalog.UniqueConstraint,\n+   * java.util.List)\n+   */\n+  private static void validatePrimaryKey(UniqueConstraint primaryKey, List<Column> columns) {\n+    final Map<String, Column> columnsByNameLookup =\n+        columns.stream().collect(Collectors.toMap(Column::getName, Function.identity()));\n+\n+    final Set<String> duplicateColumns =\n+        primaryKey.getColumns().stream()\n+            .filter(name -> Collections.frequency(primaryKey.getColumns(), name) > 1)\n+            .collect(Collectors.toSet());\n+\n+    if (!duplicateColumns.isEmpty()) {\n+      throw new ValidationException(\n+          String.format(\n+              \"Invalid primary key '%s'. A primary key must not contain duplicate columns. Found: %s\",\n+              primaryKey.getName(), duplicateColumns));\n+    }\n+\n+    for (String columnName : primaryKey.getColumns()) {\n+      Column column = columnsByNameLookup.get(columnName);\n+      if (column == null) {\n+        throw new ValidationException(\n+            String.format(\n+                \"Invalid primary key '%s'. Column '%s' does not exist.\",\n+                primaryKey.getName(), columnName));\n+      }\n+\n+      if (!column.isPhysical()) {\n+        throw new ValidationException(\n+            String.format(\n+                \"Invalid primary key '%s'. Column '%s' is not a physical column.\",\n+                primaryKey.getName(), columnName));\n+      }\n+\n+      final LogicalType columnType = column.getDataType().getLogicalType();\n+      if (columnType.isNullable()) {\n+        throw new ValidationException(\n+            String.format(\n+                \"Invalid primary key '%s'. Column '%s' is nullable.\",\n+                primaryKey.getName(), columnName));\n+      }\n+    }\n+  }\n }\n\ndiff --git a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\nindex a5195630629c..c8c11474177c 100644\n--- a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n+++ b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/IcebergTableSink.java\n@@ -21,17 +21,14 @@\n import java.util.List;\n import java.util.Map;\n import org.apache.flink.configuration.ReadableConfig;\n-import org.apache.flink.streaming.api.datastream.DataStream;\n-import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n+import org.apache.flink.table.catalog.UniqueConstraint;\n import org.apache.flink.table.connector.ChangelogMode;\n-import org.apache.flink.table.connector.ProviderContext;\n import org.apache.flink.table.connector.sink.DataStreamSinkProvider;\n import org.apache.flink.table.connector.sink.DynamicTableSink;\n import org.apache.flink.table.connector.sink.abilities.SupportsOverwrite;\n import org.apache.flink.table.connector.sink.abilities.SupportsPartitioning;\n-import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.legacy.api.TableSchema;\n-import org.apache.flink.table.legacy.api.constraints.UniqueConstraint;\n import org.apache.flink.types.RowKind;\n import org.apache.iceberg.flink.sink.FlinkSink;\n import org.apache.iceberg.flink.sink.IcebergSink;\n@@ -40,7 +37,8 @@\n \n public class IcebergTableSink implements DynamicTableSink, SupportsPartitioning, SupportsOverwrite {\n   private final TableLoader tableLoader;\n-  private final TableSchema tableSchema;\n+  @Deprecated private final TableSchema tableSchema;\n+  private final ResolvedSchema resolvedSchema;\n   private final ReadableConfig readableConfig;\n   private final Map<String, String> writeProps;\n \n@@ -49,11 +47,17 @@ public class IcebergTableSink implements DynamicTableSink, SupportsPartitioning,\n   private IcebergTableSink(IcebergTableSink toCopy) {\n     this.tableLoader = toCopy.tableLoader;\n     this.tableSchema = toCopy.tableSchema;\n+    this.resolvedSchema = toCopy.resolvedSchema;\n     this.overwrite = toCopy.overwrite;\n     this.readableConfig = toCopy.readableConfig;\n     this.writeProps = toCopy.writeProps;\n   }\n \n+  /**\n+   * @deprecated since 1.10.0, will be removed in 2.0.0. Use {@link #IcebergTableSink(TableLoader,\n+   *     ResolvedSchema, ReadableConfig, Map)} instead\n+   */\n+  @Deprecated\n   public IcebergTableSink(\n       TableLoader tableLoader,\n       TableSchema tableSchema,\n@@ -61,6 +65,19 @@ public IcebergTableSink(\n       Map<String, String> writeProps) {\n     this.tableLoader = tableLoader;\n     this.tableSchema = tableSchema;\n+    this.resolvedSchema = null;\n+    this.readableConfig = readableConfig;\n+    this.writeProps = writeProps;\n+  }\n+\n+  public IcebergTableSink(\n+      TableLoader tableLoader,\n+      ResolvedSchema resolvedSchema,\n+      ReadableConfig readableConfig,\n+      Map<String, String> writeProps) {\n+    this.tableLoader = tableLoader;\n+    this.tableSchema = null;\n+    this.resolvedSchema = resolvedSchema;\n     this.readableConfig = readableConfig;\n     this.writeProps = writeProps;\n   }\n@@ -71,34 +88,66 @@ public SinkRuntimeProvider getSinkRuntimeProvider(Context context) {\n         !overwrite || context.isBounded(),\n         \"Unbounded data stream doesn't support overwrite operation.\");\n \n-    List<String> equalityColumns =\n-        tableSchema.getPrimaryKey().map(UniqueConstraint::getColumns).orElseGet(ImmutableList::of);\n+    if (resolvedSchema != null) {\n+      List<String> equalityColumns =\n+          resolvedSchema\n+              .getPrimaryKey()\n+              .map(UniqueConstraint::getColumns)\n+              .orElseGet(ImmutableList::of);\n \n-    return new DataStreamSinkProvider() {\n-      @Override\n-      public DataStreamSink<?> consumeDataStream(\n-          ProviderContext providerContext, DataStream<RowData> dataStream) {\n-        if (readableConfig.get(FlinkConfigOptions.TABLE_EXEC_ICEBERG_USE_V2_SINK)) {\n-          return IcebergSink.forRowData(dataStream)\n-              .tableLoader(tableLoader)\n-              .tableSchema(tableSchema)\n-              .equalityFieldColumns(equalityColumns)\n-              .overwrite(overwrite)\n-              .setAll(writeProps)\n-              .flinkConf(readableConfig)\n-              .append();\n-        } else {\n-          return FlinkSink.forRowData(dataStream)\n-              .tableLoader(tableLoader)\n-              .tableSchema(tableSchema)\n-              .equalityFieldColumns(equalityColumns)\n-              .overwrite(overwrite)\n-              .setAll(writeProps)\n-              .flinkConf(readableConfig)\n-              .append();\n-        }\n-      }\n-    };\n+      return (DataStreamSinkProvider)\n+          (providerContext, dataStream) -> {\n+            if (Boolean.TRUE.equals(\n+                readableConfig.get(FlinkConfigOptions.TABLE_EXEC_ICEBERG_USE_V2_SINK))) {\n+              return IcebergSink.forRowData(dataStream)\n+                  .tableLoader(tableLoader)\n+                  .resolvedSchema(resolvedSchema)\n+                  .equalityFieldColumns(equalityColumns)\n+                  .overwrite(overwrite)\n+                  .setAll(writeProps)\n+                  .flinkConf(readableConfig)\n+                  .append();\n+            } else {\n+              return FlinkSink.forRowData(dataStream)\n+                  .tableLoader(tableLoader)\n+                  .resolvedSchema(resolvedSchema)\n+                  .equalityFieldColumns(equalityColumns)\n+                  .overwrite(overwrite)\n+                  .setAll(writeProps)\n+                  .flinkConf(readableConfig)\n+                  .append();\n+            }\n+          };\n+    } else {\n+      List<String> equalityColumns =\n+          tableSchema\n+              .getPrimaryKey()\n+              .map(org.apache.flink.table.legacy.api.constraints.UniqueConstraint::getColumns)\n+              .orElseGet(ImmutableList::of);\n+\n+      return (DataStreamSinkProvider)\n+          (providerContext, dataStream) -> {\n+            if (readableConfig.get(FlinkConfigOptions.TABLE_EXEC_ICEBERG_USE_V2_SINK)) {\n+              return IcebergSink.forRowData(dataStream)\n+                  .tableLoader(tableLoader)\n+                  .tableSchema(tableSchema)\n+                  .equalityFieldColumns(equalityColumns)\n+                  .overwrite(overwrite)\n+                  .setAll(writeProps)\n+                  .flinkConf(readableConfig)\n+                  .append();\n+            } else {\n+              return FlinkSink.forRowData(dataStream)\n+                  .tableLoader(tableLoader)\n+                  .tableSchema(tableSchema)\n+                  .equalityFieldColumns(equalityColumns)\n+                  .overwrite(overwrite)\n+                  .setAll(writeProps)\n+                  .flinkConf(readableConfig)\n+                  .append();\n+            }\n+          };\n+    }\n   }\n \n   @Override\n\ndiff --git a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\nindex 8da97df037de..d83a11d0f462 100644\n--- a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n+++ b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n@@ -38,6 +38,7 @@\n import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n import org.apache.flink.streaming.api.functions.sink.v2.DiscardingSink;\n import org.apache.flink.streaming.api.operators.OneInputStreamOperatorFactory;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.data.util.DataFormatConverters;\n import org.apache.flink.table.legacy.api.TableSchema;\n@@ -109,7 +110,10 @@ public static <T> Builder builderFor(\n    * @param input the source input data stream with {@link Row}s.\n    * @param tableSchema defines the {@link TypeInformation} for input data.\n    * @return {@link Builder} to connect the iceberg table.\n+   * @deprecated since 1.10.0, will be removed in 2.0.0. Use {@link #forRow(DataStream,\n+   *     ResolvedSchema)} instead.\n    */\n+  @Deprecated\n   public static Builder forRow(DataStream<Row> input, TableSchema tableSchema) {\n     RowType rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n     DataType[] fieldDataTypes = tableSchema.getFieldDataTypes();\n@@ -120,6 +124,26 @@ public static Builder forRow(DataStream<Row> input, TableSchema tableSchema) {\n         .tableSchema(tableSchema);\n   }\n \n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link Row}s into\n+   * iceberg table. We use {@link RowData} inside the sink connector, so users need to provide a\n+   * {@link ResolvedSchema} for builder to convert those {@link Row}s to a {@link RowData}\n+   * DataStream.\n+   *\n+   * @param input the source input data stream with {@link Row}s.\n+   * @param resolvedSchema defines the {@link TypeInformation} for input data.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRow(DataStream<Row> input, ResolvedSchema resolvedSchema) {\n+    RowType rowType = (RowType) resolvedSchema.toSinkRowDataType().getLogicalType();\n+    DataType[] fieldDataTypes = resolvedSchema.getColumnDataTypes().toArray(DataType[]::new);\n+\n+    DataFormatConverters.RowConverter rowConverter =\n+        new DataFormatConverters.RowConverter(fieldDataTypes);\n+    return builderFor(input, rowConverter::toInternal, FlinkCompatibilityUtil.toTypeInfo(rowType))\n+        .resolvedSchema(resolvedSchema);\n+  }\n+\n   /**\n    * Initialize a {@link Builder} to export the data from input data stream with {@link RowData}s\n    * into iceberg table.\n@@ -135,7 +159,8 @@ public static class Builder implements IcebergSinkBuilder<Builder> {\n     private Function<String, DataStream<RowData>> inputCreator = null;\n     private TableLoader tableLoader;\n     private Table table;\n-    private TableSchema tableSchema;\n+    @Deprecated private TableSchema tableSchema;\n+    private ResolvedSchema resolvedSchema;\n     private List<String> equalityFieldColumns = null;\n     private String uidPrefix = null;\n     private final Map<String, String> snapshotProperties = Maps.newHashMap();\n@@ -221,6 +246,12 @@ public Builder tableSchema(TableSchema newTableSchema) {\n       return this;\n     }\n \n+    @Override\n+    public Builder resolvedSchema(ResolvedSchema newResolvedSchema) {\n+      this.resolvedSchema = newResolvedSchema;\n+      return this;\n+    }\n+\n     @Override\n     public Builder overwrite(boolean newOverwrite) {\n       writeOptions.put(FlinkWriteOptions.OVERWRITE_MODE.key(), Boolean.toString(newOverwrite));\n@@ -416,7 +447,10 @@ private DataStreamSink<Void> chainIcebergOperators() {\n       Set<Integer> equalityFieldIds =\n           SinkUtil.checkAndGetEqualityFieldIds(table, equalityFieldColumns);\n \n-      RowType flinkRowType = toFlinkRowType(table.schema(), tableSchema);\n+      RowType flinkRowType =\n+          resolvedSchema != null\n+              ? toFlinkRowType(table.schema(), resolvedSchema)\n+              : toFlinkRowType(table.schema(), tableSchema);\n       int writerParallelism =\n           flinkWriteConf.writeParallelism() == null\n               ? rowDataInput.getParallelism()\n@@ -684,6 +718,13 @@ private DataStream<RowData> distributeDataStream(\n     }\n   }\n \n+  /**\n+   * Clean up after removing {@link Builder#tableSchema}\n+   *\n+   * @deprecated since 1.10.0, will be removed in 2.0.0. Use {@link #toFlinkRowType(Schema,\n+   *     ResolvedSchema)} instead.\n+   */\n+  @Deprecated\n   static RowType toFlinkRowType(Schema schema, TableSchema requestedSchema) {\n     if (requestedSchema != null) {\n       // Convert the flink schema to iceberg schema firstly, then reassign ids to match the existing\n@@ -701,6 +742,23 @@ static RowType toFlinkRowType(Schema schema, TableSchema requestedSchema) {\n     }\n   }\n \n+  static RowType toFlinkRowType(Schema schema, ResolvedSchema requestedSchema) {\n+    if (requestedSchema != null) {\n+      // Convert the flink schema to iceberg schema firstly, then reassign ids to match the existing\n+      // iceberg schema.\n+      Schema writeSchema = TypeUtil.reassignIds(FlinkSchemaUtil.convert(requestedSchema), schema);\n+      TypeUtil.validateWriteSchema(schema, writeSchema, true, true);\n+\n+      // We use this flink schema to read values from RowData. The flink's TINYINT and SMALLINT will\n+      // be promoted to iceberg INTEGER, that means if we use iceberg's table schema to read TINYINT\n+      // (backend by 1 'byte'), we will read 4 bytes rather than 1 byte, it will mess up the\n+      // byte array in BinaryRowData. So here we must use flink schema.\n+      return (RowType) requestedSchema.toSinkRowDataType().getLogicalType();\n+    } else {\n+      return FlinkSchemaUtil.convert(schema);\n+    }\n+  }\n+\n   static IcebergStreamWriter<RowData> createStreamWriter(\n       SerializableSupplier<Table> tableSupplier,\n       FlinkWriteConf flinkWriteConf,\n\ndiff --git a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSink.java b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSink.java\nindex 9ab7a7730c36..3121b6ffb065 100644\n--- a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSink.java\n+++ b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSink.java\n@@ -50,6 +50,7 @@\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamSink;\n import org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.data.util.DataFormatConverters;\n import org.apache.flink.table.legacy.api.TableSchema;\n@@ -320,7 +321,8 @@ public static class Builder implements IcebergSinkBuilder<Builder> {\n     private TableLoader tableLoader;\n     private String uidSuffix = \"\";\n     private Function<String, DataStream<RowData>> inputCreator = null;\n-    private TableSchema tableSchema;\n+    @Deprecated private TableSchema tableSchema;\n+    private ResolvedSchema resolvedSchema;\n     private SerializableTable table;\n     private final Map<String, String> writeOptions = Maps.newHashMap();\n     private final Map<String, String> snapshotSummary = Maps.newHashMap();\n@@ -334,6 +336,13 @@ private Builder forRowData(DataStream<RowData> newRowDataInput) {\n       return this;\n     }\n \n+    /**\n+     * Clean up after removing {@link IcebergSink#forRow(DataStream, TableSchema)}\n+     *\n+     * @deprecated since 1.10.0, will be removed in 2.0.0. Use {@link #forRow(DataStream,\n+     *     ResolvedSchema)} instead.\n+     */\n+    @Deprecated\n     private Builder forRow(DataStream<Row> input, TableSchema inputTableSchema) {\n       RowType rowType = (RowType) inputTableSchema.toRowDataType().getLogicalType();\n       DataType[] fieldDataTypes = inputTableSchema.getFieldDataTypes();\n@@ -345,6 +354,17 @@ private Builder forRow(DataStream<Row> input, TableSchema inputTableSchema) {\n           .tableSchema(inputTableSchema);\n     }\n \n+    private Builder forRow(DataStream<Row> input, ResolvedSchema inputResolvedSchema) {\n+      RowType rowType = (RowType) inputResolvedSchema.toSinkRowDataType().getLogicalType();\n+      DataType[] fieldDataTypes = inputResolvedSchema.getColumnDataTypes().toArray(DataType[]::new);\n+\n+      DataFormatConverters.RowConverter rowConverter =\n+          new DataFormatConverters.RowConverter(fieldDataTypes);\n+      return forMapperOutputType(\n+              input, rowConverter::toInternal, FlinkCompatibilityUtil.toTypeInfo(rowType))\n+          .resolvedSchema(inputResolvedSchema);\n+    }\n+\n     private <T> Builder forMapperOutputType(\n         DataStream<T> input, MapFunction<T, RowData> mapper, TypeInformation<RowData> outputType) {\n       this.inputCreator =\n@@ -422,6 +442,12 @@ public Builder tableSchema(TableSchema newTableSchema) {\n       return this;\n     }\n \n+    @Override\n+    public Builder resolvedSchema(ResolvedSchema newResolvedSchema) {\n+      this.resolvedSchema = newResolvedSchema;\n+      return this;\n+    }\n+\n     @Override\n     public Builder overwrite(boolean newOverwrite) {\n       writeOptions.put(FlinkWriteOptions.OVERWRITE_MODE.key(), Boolean.toString(newOverwrite));\n@@ -644,7 +670,9 @@ IcebergSink build() {\n           snapshotSummary,\n           uidSuffix,\n           SinkUtil.writeProperties(flinkWriteConf.dataFileFormat(), flinkWriteConf, table),\n-          toFlinkRowType(table.schema(), tableSchema),\n+          resolvedSchema != null\n+              ? toFlinkRowType(table.schema(), resolvedSchema)\n+              : toFlinkRowType(table.schema(), tableSchema),\n           tableSupplier,\n           flinkWriteConf,\n           equalityFieldIds,\n@@ -705,6 +733,13 @@ private static SerializableTable checkAndGetTable(TableLoader tableLoader, Table\n     return (SerializableTable) SerializableTable.copyOf(table);\n   }\n \n+  /**\n+   * Clean up after removing {@link Builder#tableSchema}\n+   *\n+   * @deprecated since 1.10.0, will be removed in 2.0.0. Use {@link #toFlinkRowType(Schema,\n+   *     ResolvedSchema)} instead.\n+   */\n+  @Deprecated\n   private static RowType toFlinkRowType(Schema schema, TableSchema requestedSchema) {\n     if (requestedSchema != null) {\n       // Convert the flink schema to iceberg schema firstly, then reassign ids to match the existing\n@@ -722,6 +757,23 @@ private static RowType toFlinkRowType(Schema schema, TableSchema requestedSchema\n     }\n   }\n \n+  private static RowType toFlinkRowType(Schema schema, ResolvedSchema requestedSchema) {\n+    if (requestedSchema != null) {\n+      // Convert the flink schema to iceberg schema firstly, then reassign ids to match the existing\n+      // iceberg schema.\n+      Schema writeSchema = TypeUtil.reassignIds(FlinkSchemaUtil.convert(requestedSchema), schema);\n+      TypeUtil.validateWriteSchema(schema, writeSchema, true, true);\n+\n+      // We use this flink schema to read values from RowData. The flink's TINYINT and SMALLINT will\n+      // be promoted to iceberg INTEGER, that means if we use iceberg's table schema to read TINYINT\n+      // (backend by 1 'byte'), we will read 4 bytes rather than 1 byte, it will mess up the byte\n+      // array in BinaryRowData. So here we must use flink schema.\n+      return (RowType) requestedSchema.toSinkRowDataType().getLogicalType();\n+    } else {\n+      return FlinkSchemaUtil.convert(schema);\n+    }\n+  }\n+\n   private DataStream<RowData> distributeDataStream(DataStream<RowData> input) {\n     DistributionMode mode = flinkWriteConf.distributionMode();\n     Schema schema = table.schema();\n@@ -884,11 +936,27 @@ public static <T> Builder builderFor(\n    * @param input the source input data stream with {@link Row}s.\n    * @param tableSchema defines the {@link TypeInformation} for input data.\n    * @return {@link Builder} to connect the iceberg table.\n+   * @deprecated Use {@link #forRow(DataStream, ResolvedSchema)} instead.\n    */\n+  @Deprecated\n   public static Builder forRow(DataStream<Row> input, TableSchema tableSchema) {\n     return new Builder().forRow(input, tableSchema);\n   }\n \n+  /**\n+   * Initialize a {@link Builder} to export the data from input data stream with {@link Row}s into\n+   * iceberg table. We use {@link RowData} inside the sink connector, so users need to provide a\n+   * {@link ResolvedSchema} for builder to convert those {@link Row}s to a {@link RowData}\n+   * DataStream.\n+   *\n+   * @param input the source input data stream with {@link Row}s.\n+   * @param resolvedSchema defines the {@link TypeInformation} for input data.\n+   * @return {@link Builder} to connect the iceberg table.\n+   */\n+  public static Builder forRow(DataStream<Row> input, ResolvedSchema resolvedSchema) {\n+    return new Builder().forRow(input, resolvedSchema);\n+  }\n+\n   /**\n    * Initialize a {@link Builder} to export the data from input data stream with {@link RowData}s\n    * into iceberg table.\n\ndiff --git a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSinkBuilder.java b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSinkBuilder.java\nindex 68121fbb471e..577b2b9a4227 100644\n--- a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSinkBuilder.java\n+++ b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergSinkBuilder.java\n@@ -24,6 +24,7 @@\n import org.apache.flink.configuration.ReadableConfig;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamSink;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.legacy.api.TableSchema;\n import org.apache.flink.types.Row;\n@@ -40,8 +41,14 @@\n @Internal\n interface IcebergSinkBuilder<T extends IcebergSinkBuilder<?>> {\n \n+  /**\n+   * @deprecated Use {@link #resolvedSchema(ResolvedSchema)} instead.\n+   */\n+  @Deprecated\n   T tableSchema(TableSchema newTableSchema);\n \n+  T resolvedSchema(ResolvedSchema newResolvedSchema);\n+\n   T tableLoader(TableLoader newTableLoader);\n \n   T equalityFieldColumns(List<String> columns);\n@@ -64,6 +71,10 @@ interface IcebergSinkBuilder<T extends IcebergSinkBuilder<?>> {\n \n   DataStreamSink<?> append();\n \n+  /**\n+   * @deprecated Use {@link #forRow(DataStream, ResolvedSchema, boolean)} instead.\n+   */\n+  @Deprecated\n   static IcebergSinkBuilder<?> forRow(\n       DataStream<Row> input, TableSchema tableSchema, boolean useV2Sink) {\n     if (useV2Sink) {\n@@ -73,6 +84,15 @@ static IcebergSinkBuilder<?> forRow(\n     }\n   }\n \n+  static IcebergSinkBuilder<?> forRow(\n+      DataStream<Row> input, ResolvedSchema resolvedSchema, boolean useV2Sink) {\n+    if (useV2Sink) {\n+      return IcebergSink.forRow(input, resolvedSchema);\n+    } else {\n+      return FlinkSink.forRow(input, resolvedSchema);\n+    }\n+  }\n+\n   static IcebergSinkBuilder<?> forRowData(DataStream<RowData> input, boolean useV2Sink) {\n     if (useV2Sink) {\n       return IcebergSink.forRowData(input);\n\ndiff --git a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/IcebergSource.java b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/IcebergSource.java\nindex 7b2ac2cd7451..ec7cb010b6be 100644\n--- a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/IcebergSource.java\n+++ b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/IcebergSource.java\n@@ -41,6 +41,7 @@\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.datastream.DataStreamSource;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.legacy.api.TableSchema;\n import org.apache.iceberg.BaseMetadataTable;\n@@ -291,7 +292,8 @@ public static class Builder<T> {\n     private RowDataConverter<T> converter;\n     private ReadableConfig flinkConfig = new Configuration();\n     private final ScanContext.Builder contextBuilder = ScanContext.builder();\n-    private TableSchema projectedFlinkSchema;\n+    private TableSchema projectedTableSchema;\n+    private ResolvedSchema projectedFlinkSchema;\n     private Boolean exposeLocality;\n \n     private final Map<String, String> readOptions = Maps.newHashMap();\n@@ -458,7 +460,17 @@ public Builder<T> project(Schema newProjectedSchema) {\n       return this;\n     }\n \n+    /**\n+     * @deprecated since 1.10.0, will be removed in 2.0.0. Use {@link #project(ResolvedSchema)}\n+     *     instead.\n+     */\n+    @Deprecated\n     public Builder<T> project(TableSchema newProjectedFlinkSchema) {\n+      this.projectedTableSchema = newProjectedFlinkSchema;\n+      return this;\n+    }\n+\n+    public Builder<T> project(ResolvedSchema newProjectedFlinkSchema) {\n       this.projectedFlinkSchema = newProjectedFlinkSchema;\n       return this;\n     }\n@@ -546,7 +558,7 @@ public Builder<T> watermarkColumnTimeUnit(TimeUnit timeUnit) {\n     }\n \n     /**\n-     * @deprecated Use {@link #setAll} instead.\n+     * @deprecated will be removed in 2.0.0; use {@link #setAll} instead.\n      */\n     @Deprecated\n     public Builder<T> properties(Map<String, String> properties) {\n@@ -572,6 +584,8 @@ public IcebergSource<T> build() {\n       Schema icebergSchema = table.schema();\n       if (projectedFlinkSchema != null) {\n         contextBuilder.project(FlinkSchemaUtil.convert(icebergSchema, projectedFlinkSchema));\n+      } else if (projectedTableSchema != null) {\n+        contextBuilder.project(FlinkSchemaUtil.convert(icebergSchema, projectedTableSchema));\n       }\n \n       SerializableRecordEmitter<T> emitter = SerializableRecordEmitter.defaultEmitter();\n\ndiff --git a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java\nindex d6077751ec2e..dc5e6675b3e2 100644\n--- a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java\n+++ b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java\n@@ -22,10 +22,13 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Optional;\n+import java.util.stream.Collectors;\n import org.apache.flink.annotation.Internal;\n import org.apache.flink.configuration.ReadableConfig;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.catalog.Column;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.connector.ChangelogMode;\n import org.apache.flink.table.connector.ProviderContext;\n import org.apache.flink.table.connector.source.DataStreamScanProvider;\n@@ -63,7 +66,7 @@ public class IcebergTableSource\n   private List<Expression> filters;\n \n   private final TableLoader loader;\n-  private final TableSchema schema;\n+  private final ResolvedSchema schema;\n   private final Map<String, String> properties;\n   private final boolean isLimitPushDown;\n   private final ReadableConfig readableConfig;\n@@ -81,7 +84,7 @@ private IcebergTableSource(IcebergTableSource toCopy) {\n \n   public IcebergTableSource(\n       TableLoader loader,\n-      TableSchema schema,\n+      ResolvedSchema schema,\n       Map<String, String> properties,\n       ReadableConfig readableConfig) {\n     this(loader, schema, properties, null, false, null, ImmutableList.of(), readableConfig);\n@@ -89,7 +92,7 @@ public IcebergTableSource(\n \n   private IcebergTableSource(\n       TableLoader loader,\n-      TableSchema schema,\n+      ResolvedSchema schema,\n       Map<String, String> properties,\n       int[] projectedFields,\n       boolean isLimitPushDown,\n@@ -120,8 +123,8 @@ private DataStream<RowData> createDataStream(StreamExecutionEnvironment execEnv)\n     return FlinkSource.forRowData()\n         .env(execEnv)\n         .tableLoader(loader)\n-        .properties(properties)\n-        .project(getProjectedSchema())\n+        .setAll(properties)\n+        .project(TableSchema.fromResolvedSchema(getProjectedSchema()))\n         .limit(limit)\n         .filters(filters)\n         .flinkConf(readableConfig)\n@@ -134,7 +137,7 @@ private DataStream<RowData> createFLIP27Stream(StreamExecutionEnvironment env) {\n     return IcebergSource.forRowData()\n         .tableLoader(loader)\n         .assignerFactory(assignerType.factory())\n-        .properties(properties)\n+        .setAll(properties)\n         .project(getProjectedSchema())\n         .limit(limit)\n         .filters(filters)\n@@ -142,17 +145,13 @@ private DataStream<RowData> createFLIP27Stream(StreamExecutionEnvironment env) {\n         .buildStream(env);\n   }\n \n-  private TableSchema getProjectedSchema() {\n+  private ResolvedSchema getProjectedSchema() {\n     if (projectedFields == null) {\n       return schema;\n     } else {\n-      String[] fullNames = schema.getFieldNames();\n-      DataType[] fullTypes = schema.getFieldDataTypes();\n-      return TableSchema.builder()\n-          .fields(\n-              Arrays.stream(projectedFields).mapToObj(i -> fullNames[i]).toArray(String[]::new),\n-              Arrays.stream(projectedFields).mapToObj(i -> fullTypes[i]).toArray(DataType[]::new))\n-          .build();\n+      List<Column> fullColumns = schema.getColumns();\n+      return ResolvedSchema.of(\n+          Arrays.stream(projectedFields).mapToObj(fullColumns::get).collect(Collectors.toList()));\n     }\n   }\n \n\ndiff --git a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowConverter.java b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowConverter.java\nindex 85346565a45a..0e028ff91b87 100644\n--- a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowConverter.java\n+++ b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/source/reader/RowConverter.java\n@@ -18,13 +18,12 @@\n  */\n package org.apache.iceberg.flink.source.reader;\n \n-import java.util.stream.Stream;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.java.typeutils.RowTypeInfo;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.data.conversion.DataStructureConverter;\n import org.apache.flink.table.data.conversion.DataStructureConverters;\n-import org.apache.flink.table.legacy.api.TableSchema;\n import org.apache.flink.table.runtime.typeutils.ExternalTypeInfo;\n import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.table.types.utils.TypeConversions;\n@@ -43,12 +42,13 @@ private RowConverter(RowType rowType, TypeInformation<Row> rowTypeInfo) {\n \n   public static RowConverter fromIcebergSchema(org.apache.iceberg.Schema icebergSchema) {\n     RowType rowType = FlinkSchemaUtil.convert(icebergSchema);\n-    TableSchema tableSchema = FlinkSchemaUtil.toSchema(icebergSchema);\n-    TypeInformation[] typeInformations =\n-        Stream.of(tableSchema.getFieldDataTypes())\n+    ResolvedSchema resolvedSchema = FlinkSchemaUtil.toResolvedSchema(icebergSchema);\n+    TypeInformation<?>[] types =\n+        resolvedSchema.getColumnDataTypes().stream()\n             .map(ExternalTypeInfo::of)\n             .toArray(TypeInformation[]::new);\n-    RowTypeInfo rowTypeInfo = new RowTypeInfo(typeInformations, tableSchema.getFieldNames());\n+    String[] fieldNames = resolvedSchema.getColumnNames().toArray(String[]::new);\n+    RowTypeInfo rowTypeInfo = new RowTypeInfo(types, fieldNames);\n     return new RowConverter(rowType, rowTypeInfo);\n   }\n \n\ndiff --git a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/util/FlinkCompatibilityUtil.java b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/util/FlinkCompatibilityUtil.java\nindex 245e4489c7fc..38bd73b87127 100644\n--- a/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/util/FlinkCompatibilityUtil.java\n+++ b/flink/v2.0/flink/src/main/java/org/apache/iceberg/flink/util/FlinkCompatibilityUtil.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg.flink.util;\n \n import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.table.api.Schema;\n import org.apache.flink.table.catalog.Column;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.legacy.api.TableColumn;\n@@ -37,6 +38,10 @@ public static TypeInformation<RowData> toTypeInfo(RowType rowType) {\n     return InternalTypeInfo.of(rowType);\n   }\n \n+  /**\n+   * @deprecated since 1.10.0, will be removed in 2.0.0.\n+   */\n+  @Deprecated\n   public static boolean isPhysicalColumn(TableColumn column) {\n     return column.isPhysical();\n   }\n@@ -44,4 +49,8 @@ public static boolean isPhysicalColumn(TableColumn column) {\n   public static boolean isPhysicalColumn(Column column) {\n     return column.isPhysical();\n   }\n+\n+  public static boolean isPhysicalColumn(Schema.UnresolvedColumn column) {\n+    return column instanceof Schema.UnresolvedPhysicalColumn;\n+  }\n }\n",
    "test_patch": "diff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java\nindex 0d39a665cfe8..0071abfd9a3f 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/SimpleDataUtil.java\n@@ -27,6 +27,8 @@\n import java.util.Map;\n import java.util.stream.Collectors;\n import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.catalog.Column;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.data.GenericRowData;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.data.StringData;\n@@ -88,10 +90,14 @@ private SimpleDataUtil() {}\n           Types.NestedField.optional(2, \"data\", Types.StringType.get()),\n           Types.NestedField.optional(3, \"extra\", Types.StringType.get()));\n \n-  public static final TableSchema FLINK_SCHEMA =\n-      TableSchema.builder().field(\"id\", DataTypes.INT()).field(\"data\", DataTypes.STRING()).build();\n+  public static final ResolvedSchema FLINK_SCHEMA =\n+      ResolvedSchema.of(\n+          Column.physical(\"id\", DataTypes.INT()), Column.physical(\"data\", DataTypes.STRING()));\n \n-  public static final RowType ROW_TYPE = (RowType) FLINK_SCHEMA.toRowDataType().getLogicalType();\n+  public static final TableSchema FLINK_TABLE_SCHEMA = TableSchema.fromResolvedSchema(FLINK_SCHEMA);\n+\n+  public static final RowType ROW_TYPE =\n+      (RowType) FLINK_SCHEMA.toSourceRowDataType().getLogicalType();\n \n   public static final Record RECORD = GenericRecord.create(SCHEMA);\n   public static final Record RECORD2 = GenericRecord.create(SCHEMA2);\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java\nindex 0bd2b5501128..f7848a5d22ef 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java\n@@ -103,7 +103,7 @@ public void testRenameTable() {\n         .isInstanceOf(ValidationException.class)\n         .hasMessage(\"Table `tl` was not found.\");\n \n-    Schema actualSchema = FlinkSchemaUtil.convert(getTableEnv().from(\"tl2\").getSchema());\n+    Schema actualSchema = FlinkSchemaUtil.convert(getTableEnv().from(\"tl2\").getResolvedSchema());\n     assertThat(tableSchema.asStruct()).isEqualTo(actualSchema.asStruct());\n   }\n \n@@ -236,10 +236,11 @@ public void testCreateTableLikeInFlinkCatalog() throws TableNotExistException {\n     String srcCatalogProps =\n         FlinkCreateTableOptions.toJson(catalogName, DATABASE, \"tl\", filteredOptions);\n     Map<String, String> options = catalogTable.getOptions();\n-    assertThat(options.get(FlinkCreateTableOptions.CONNECTOR_PROPS_KEY))\n-        .isEqualTo(FlinkDynamicTableFactory.FACTORY_IDENTIFIER);\n-    assertThat(options.get(FlinkCreateTableOptions.SRC_CATALOG_PROPS_KEY))\n-        .isEqualTo(srcCatalogProps);\n+    assertThat(options)\n+        .containsEntry(\n+            FlinkCreateTableOptions.CONNECTOR_PROPS_KEY,\n+            FlinkDynamicTableFactory.FACTORY_IDENTIFIER)\n+        .containsEntry(FlinkCreateTableOptions.SRC_CATALOG_PROPS_KEY, srcCatalogProps);\n   }\n \n   @TestTemplate\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkFilters.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkFilters.java\nindex b47a7920fe0c..59b868ea1ef1 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkFilters.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkFilters.java\n@@ -31,6 +31,8 @@\n import java.util.stream.Collectors;\n import org.apache.flink.table.api.DataTypes;\n import org.apache.flink.table.api.Expressions;\n+import org.apache.flink.table.catalog.Column;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.expressions.ApiExpressionUtils;\n import org.apache.flink.table.expressions.CallExpression;\n import org.apache.flink.table.expressions.Expression;\n@@ -41,8 +43,6 @@\n import org.apache.flink.table.expressions.ValueLiteralExpression;\n import org.apache.flink.table.expressions.utils.ApiExpressionDefaultVisitor;\n import org.apache.flink.table.functions.BuiltInFunctionDefinitions;\n-import org.apache.flink.table.legacy.api.TableColumn;\n-import org.apache.flink.table.legacy.api.TableSchema;\n import org.apache.iceberg.expressions.And;\n import org.apache.iceberg.expressions.BoundLiteralPredicate;\n import org.apache.iceberg.expressions.Not;\n@@ -55,21 +55,20 @@\n \n public class TestFlinkFilters {\n \n-  private static final TableSchema TABLE_SCHEMA =\n-      TableSchema.builder()\n-          .field(\"field1\", DataTypes.INT())\n-          .field(\"field2\", DataTypes.BIGINT())\n-          .field(\"field3\", DataTypes.FLOAT())\n-          .field(\"field4\", DataTypes.DOUBLE())\n-          .field(\"field5\", DataTypes.STRING())\n-          .field(\"field6\", DataTypes.BOOLEAN())\n-          .field(\"field7\", DataTypes.BINARY(2))\n-          .field(\"field8\", DataTypes.DECIMAL(10, 2))\n-          .field(\"field9\", DataTypes.DATE())\n-          .field(\"field10\", DataTypes.TIME())\n-          .field(\"field11\", DataTypes.TIMESTAMP())\n-          .field(\"field12\", DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE())\n-          .build();\n+  private static final ResolvedSchema RESOLVED_SCHEMA =\n+      ResolvedSchema.of(\n+          Column.physical(\"field1\", DataTypes.INT()),\n+          Column.physical(\"field2\", DataTypes.BIGINT()),\n+          Column.physical(\"field3\", DataTypes.FLOAT()),\n+          Column.physical(\"field4\", DataTypes.DOUBLE()),\n+          Column.physical(\"field5\", DataTypes.STRING()),\n+          Column.physical(\"field6\", DataTypes.BOOLEAN()),\n+          Column.physical(\"field7\", DataTypes.BINARY(2)),\n+          Column.physical(\"field8\", DataTypes.DECIMAL(10, 2)),\n+          Column.physical(\"field9\", DataTypes.DATE()),\n+          Column.physical(\"field10\", DataTypes.TIME()),\n+          Column.physical(\"field11\", DataTypes.TIMESTAMP()),\n+          Column.physical(\"field12\", DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE()));\n \n   // A map list of fields and values used to verify the conversion of flink expression to iceberg\n   // expression\n@@ -397,7 +396,7 @@ private <T> void matchLiteral(String fieldName, Object flinkLiteral, T icebergLi\n     UnboundPredicate<T> unboundPredicate = (UnboundPredicate<T>) expression;\n \n     org.apache.iceberg.expressions.Expression expression1 =\n-        unboundPredicate.bind(FlinkSchemaUtil.convert(TABLE_SCHEMA).asStruct(), false);\n+        unboundPredicate.bind(FlinkSchemaUtil.convert(RESOLVED_SCHEMA).asStruct(), false);\n     assertThat(expression1)\n         .as(\"The expression should be a BoundLiteralPredicate\")\n         .isInstanceOf(BoundLiteralPredicate.class);\n@@ -408,17 +407,19 @@ private <T> void matchLiteral(String fieldName, Object flinkLiteral, T icebergLi\n \n   private static Expression resolve(Expression originalExpression) {\n     return originalExpression.accept(\n-        new ApiExpressionDefaultVisitor<Expression>() {\n+        new ApiExpressionDefaultVisitor<>() {\n           @Override\n           public Expression visit(UnresolvedReferenceExpression unresolvedReference) {\n             String name = unresolvedReference.getName();\n-            Optional<TableColumn> field = TABLE_SCHEMA.getTableColumn(name);\n-            if (field.isPresent()) {\n-              int index = TABLE_SCHEMA.getTableColumns().indexOf(field.get());\n-              return new FieldReferenceExpression(name, field.get().getType(), 0, index);\n-            } else {\n-              return null;\n-            }\n+            return RESOLVED_SCHEMA\n+                .getColumn(name)\n+                .map(\n+                    column -> {\n+                      int columnIndex = RESOLVED_SCHEMA.getColumns().indexOf(column);\n+                      return new FieldReferenceExpression(\n+                          name, column.getDataType(), 0, columnIndex);\n+                    })\n+                .orElse(null);\n           }\n \n           @Override\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java\nindex e505de519fea..ce18a1bb3d50 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkSchemaUtil.java\n@@ -21,8 +21,13 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import java.util.Collections;\n+import java.util.List;\n import org.apache.flink.table.api.DataTypes;\n import org.apache.flink.table.api.ValidationException;\n+import org.apache.flink.table.catalog.Column;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n+import org.apache.flink.table.catalog.UniqueConstraint;\n import org.apache.flink.table.legacy.api.TableSchema;\n import org.apache.flink.table.types.logical.BinaryType;\n import org.apache.flink.table.types.logical.CharType;\n@@ -33,48 +38,60 @@\n import org.apache.flink.table.types.logical.TimestampType;\n import org.apache.flink.table.types.logical.VarBinaryType;\n import org.apache.flink.table.types.logical.VarCharType;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.Types;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestFlinkSchemaUtil {\n \n-  @Test\n+  @Parameter private boolean isTableSchema;\n+\n+  @Parameters(name = \"isTableSchema={0}\")\n+  private static Object[][] parameters() {\n+    return new Object[][] {{true}, {false}};\n+  }\n+\n+  @TestTemplate\n   public void testConvertFlinkSchemaToIcebergSchema() {\n-    TableSchema flinkSchema =\n-        TableSchema.builder()\n-            .field(\"id\", DataTypes.INT().notNull())\n-            .field(\"name\", DataTypes.STRING()) /* optional by default */\n-            .field(\"salary\", DataTypes.DOUBLE().notNull())\n-            .field(\n+    ResolvedSchema flinkSchema =\n+        ResolvedSchema.of(\n+            Column.physical(\"id\", DataTypes.INT().notNull()),\n+            Column.physical(\"name\", DataTypes.STRING()) /* optional by default */,\n+            Column.physical(\"salary\", DataTypes.DOUBLE().notNull()),\n+            Column.physical(\n                 \"locations\",\n                 DataTypes.MAP(\n                     DataTypes.STRING(),\n                     DataTypes.ROW(\n                         DataTypes.FIELD(\"posX\", DataTypes.DOUBLE().notNull(), \"X field\"),\n-                        DataTypes.FIELD(\"posY\", DataTypes.DOUBLE().notNull(), \"Y field\"))))\n-            .field(\"strArray\", DataTypes.ARRAY(DataTypes.STRING()).nullable())\n-            .field(\"intArray\", DataTypes.ARRAY(DataTypes.INT()).nullable())\n-            .field(\"char\", DataTypes.CHAR(10).notNull())\n-            .field(\"varchar\", DataTypes.VARCHAR(10).notNull())\n-            .field(\"boolean\", DataTypes.BOOLEAN().nullable())\n-            .field(\"tinyint\", DataTypes.TINYINT())\n-            .field(\"smallint\", DataTypes.SMALLINT())\n-            .field(\"bigint\", DataTypes.BIGINT())\n-            .field(\"varbinary\", DataTypes.VARBINARY(10))\n-            .field(\"binary\", DataTypes.BINARY(10))\n-            .field(\"time\", DataTypes.TIME())\n-            .field(\"timestampWithoutZone\", DataTypes.TIMESTAMP())\n-            .field(\"timestampWithZone\", DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE())\n-            .field(\"date\", DataTypes.DATE())\n-            .field(\"decimal\", DataTypes.DECIMAL(2, 2))\n-            .field(\"decimal2\", DataTypes.DECIMAL(38, 2))\n-            .field(\"decimal3\", DataTypes.DECIMAL(10, 1))\n-            .field(\"multiset\", DataTypes.MULTISET(DataTypes.STRING().notNull()))\n-            .build();\n+                        DataTypes.FIELD(\"posY\", DataTypes.DOUBLE().notNull(), \"Y field\")))),\n+            Column.physical(\"strArray\", DataTypes.ARRAY(DataTypes.STRING()).nullable()),\n+            Column.physical(\"intArray\", DataTypes.ARRAY(DataTypes.INT()).nullable()),\n+            Column.physical(\"char\", DataTypes.CHAR(10).notNull()),\n+            Column.physical(\"varchar\", DataTypes.VARCHAR(10).notNull()),\n+            Column.physical(\"boolean\", DataTypes.BOOLEAN().nullable()),\n+            Column.physical(\"tinyint\", DataTypes.TINYINT()),\n+            Column.physical(\"smallint\", DataTypes.SMALLINT()),\n+            Column.physical(\"bigint\", DataTypes.BIGINT()),\n+            Column.physical(\"varbinary\", DataTypes.VARBINARY(10)),\n+            Column.physical(\"binary\", DataTypes.BINARY(10)),\n+            Column.physical(\"time\", DataTypes.TIME()),\n+            Column.physical(\"timestampWithoutZone\", DataTypes.TIMESTAMP()),\n+            Column.physical(\"timestampWithZone\", DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE()),\n+            Column.physical(\"date\", DataTypes.DATE()),\n+            Column.physical(\"decimal\", DataTypes.DECIMAL(2, 2)),\n+            Column.physical(\"decimal2\", DataTypes.DECIMAL(38, 2)),\n+            Column.physical(\"decimal3\", DataTypes.DECIMAL(10, 1)),\n+            Column.physical(\"multiset\", DataTypes.MULTISET(DataTypes.STRING().notNull())));\n \n     Schema icebergSchema =\n         new Schema(\n@@ -120,19 +137,19 @@ public void testConvertFlinkSchemaToIcebergSchema() {\n     checkSchema(flinkSchema, icebergSchema);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMapField() {\n-    TableSchema flinkSchema =\n-        TableSchema.builder()\n-            .field(\n+    ResolvedSchema flinkSchema =\n+        ResolvedSchema.of(\n+            Column.physical(\n                 \"map_int_long\",\n-                DataTypes.MAP(DataTypes.INT(), DataTypes.BIGINT()).notNull()) /* Required */\n-            .field(\n+                DataTypes.MAP(DataTypes.INT(), DataTypes.BIGINT()).notNull()) /* Required */,\n+            Column.physical(\n                 \"map_int_array_string\",\n-                DataTypes.MAP(DataTypes.ARRAY(DataTypes.INT()), DataTypes.STRING()))\n-            .field(\n-                \"map_decimal_string\", DataTypes.MAP(DataTypes.DECIMAL(10, 2), DataTypes.STRING()))\n-            .field(\n+                DataTypes.MAP(DataTypes.ARRAY(DataTypes.INT()), DataTypes.STRING())),\n+            Column.physical(\n+                \"map_decimal_string\", DataTypes.MAP(DataTypes.DECIMAL(10, 2), DataTypes.STRING())),\n+            Column.physical(\n                 \"map_fields_fields\",\n                 DataTypes.MAP(\n                         DataTypes.ROW(\n@@ -145,8 +162,7 @@ public void testMapField() {\n                                     DataTypes.ARRAY(DataTypes.STRING()),\n                                     \"doc - array\"))\n                             .notNull() /* Required */)\n-                    .notNull() /* Required */)\n-            .build();\n+                    .notNull() /* Required */));\n \n     Schema icebergSchema =\n         new Schema(\n@@ -190,11 +206,11 @@ public void testMapField() {\n     checkSchema(flinkSchema, icebergSchema);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testStructField() {\n-    TableSchema flinkSchema =\n-        TableSchema.builder()\n-            .field(\n+    ResolvedSchema flinkSchema =\n+        ResolvedSchema.of(\n+            Column.physical(\n                 \"struct_int_string_decimal\",\n                 DataTypes.ROW(\n                         DataTypes.FIELD(\"field_int\", DataTypes.INT()),\n@@ -208,14 +224,13 @@ public void testStructField() {\n                                         \"inner_struct_float_array\",\n                                         DataTypes.ARRAY(DataTypes.FLOAT())))\n                                 .notNull()) /* Row is required */)\n-                    .notNull()) /* Required */\n-            .field(\n+                    .notNull()) /* Required */,\n+            Column.physical(\n                 \"struct_map_int_int\",\n                 DataTypes.ROW(\n                         DataTypes.FIELD(\n                             \"field_map\", DataTypes.MAP(DataTypes.INT(), DataTypes.INT())))\n-                    .nullable()) /* Optional */\n-            .build();\n+                    .nullable()) /* Optional */);\n \n     Schema icebergSchema =\n         new Schema(\n@@ -249,23 +264,23 @@ public void testStructField() {\n     checkSchema(flinkSchema, icebergSchema);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testListField() {\n-    TableSchema flinkSchema =\n-        TableSchema.builder()\n-            .field(\n+    ResolvedSchema flinkSchema =\n+        ResolvedSchema.of(\n+            Column.physical(\n                 \"list_struct_fields\",\n                 DataTypes.ARRAY(DataTypes.ROW(DataTypes.FIELD(\"field_int\", DataTypes.INT())))\n-                    .notNull()) /* Required */\n-            .field(\n+                    .notNull()) /* Required */,\n+            Column.physical(\n                 \"list_optional_struct_fields\",\n                 DataTypes.ARRAY(\n                         DataTypes.ROW(\n                             DataTypes.FIELD(\n                                 \"field_timestamp_with_local_time_zone\",\n                                 DataTypes.TIMESTAMP_WITH_LOCAL_TIME_ZONE())))\n-                    .nullable()) /* Optional */\n-            .field(\n+                    .nullable()) /* Optional */,\n+            Column.physical(\n                 \"list_map_fields\",\n                 DataTypes.ARRAY(\n                         DataTypes.MAP(\n@@ -274,8 +289,7 @@ public void testListField() {\n                                 DataTypes.ROW(\n                                     DataTypes.FIELD(\"field_0\", DataTypes.INT(), \"doc - int\")))\n                             .notNull())\n-                    .notNull()) /* Required */\n-            .build();\n+                    .notNull()) /* Required */);\n \n     Schema icebergSchema =\n         new Schema(\n@@ -312,14 +326,26 @@ public void testListField() {\n     checkSchema(flinkSchema, icebergSchema);\n   }\n \n-  private void checkSchema(TableSchema flinkSchema, Schema icebergSchema) {\n-    assertThat(FlinkSchemaUtil.convert(flinkSchema).asStruct()).isEqualTo(icebergSchema.asStruct());\n-    // The conversion is not a 1:1 mapping, so we just check iceberg types.\n-    assertThat(\n-            FlinkSchemaUtil.convert(\n-                    FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(icebergSchema)))\n-                .asStruct())\n-        .isEqualTo(icebergSchema.asStruct());\n+  private void checkSchema(ResolvedSchema flinkSchema, Schema icebergSchema) {\n+    if (isTableSchema) {\n+      assertThat(FlinkSchemaUtil.convert(TableSchema.fromResolvedSchema(flinkSchema)).asStruct())\n+          .isEqualTo(icebergSchema.asStruct());\n+      // The conversion is not a 1:1 mapping, so we just check iceberg types.\n+      assertThat(\n+              FlinkSchemaUtil.convert(\n+                      FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(icebergSchema)))\n+                  .asStruct())\n+          .isEqualTo(icebergSchema.asStruct());\n+    } else {\n+      assertThat(FlinkSchemaUtil.convert(flinkSchema).asStruct())\n+          .isEqualTo(icebergSchema.asStruct());\n+      // The conversion is not a 1:1 mapping, so we just check iceberg types.\n+      assertThat(\n+              FlinkSchemaUtil.convert(\n+                      FlinkSchemaUtil.toResolvedSchema(FlinkSchemaUtil.convert(icebergSchema)))\n+                  .asStruct())\n+          .isEqualTo(icebergSchema.asStruct());\n+    }\n   }\n \n   @Test\n@@ -358,9 +384,13 @@ private void checkInconsistentType(\n     assertThat(FlinkSchemaUtil.convert(icebergType)).isEqualTo(flinkExpectedType);\n     assertThat(FlinkSchemaUtil.convert(FlinkSchemaUtil.toSchema(RowType.of(flinkType))).asStruct())\n         .isEqualTo(Types.StructType.of(Types.NestedField.optional(0, \"f0\", icebergExpectedType)));\n+    assertThat(\n+            FlinkSchemaUtil.convert(FlinkSchemaUtil.toResolvedSchema(RowType.of(flinkType)))\n+                .asStruct())\n+        .isEqualTo(Types.StructType.of(Types.NestedField.optional(0, \"f0\", icebergExpectedType)));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testConvertFlinkSchemaBaseOnIcebergSchema() {\n     Schema baseSchema =\n         new Schema(\n@@ -369,18 +399,31 @@ public void testConvertFlinkSchemaBaseOnIcebergSchema() {\n                 Types.NestedField.optional(102, \"string\", Types.StringType.get())),\n             Sets.newHashSet(101));\n \n-    TableSchema flinkSchema =\n-        TableSchema.builder()\n-            .field(\"int\", DataTypes.INT().notNull())\n-            .field(\"string\", DataTypes.STRING().nullable())\n-            .primaryKey(\"int\")\n-            .build();\n-    Schema convertedSchema = FlinkSchemaUtil.convert(baseSchema, flinkSchema);\n+    Schema convertedSchema;\n+    if (isTableSchema) {\n+      TableSchema flinkSchema =\n+          TableSchema.builder()\n+              .field(\"int\", DataTypes.INT().notNull())\n+              .field(\"string\", DataTypes.STRING().nullable())\n+              .primaryKey(\"int\")\n+              .build();\n+      convertedSchema = FlinkSchemaUtil.convert(baseSchema, flinkSchema);\n+    } else {\n+      ResolvedSchema flinkSchema =\n+          new ResolvedSchema(\n+              List.of(\n+                  Column.physical(\"int\", DataTypes.INT().notNull()),\n+                  Column.physical(\"string\", DataTypes.STRING().nullable())),\n+              Collections.emptyList(),\n+              UniqueConstraint.primaryKey(\"pk\", List.of(\"int\")));\n+      convertedSchema = FlinkSchemaUtil.convert(baseSchema, flinkSchema);\n+    }\n+\n     assertThat(convertedSchema.asStruct()).isEqualTo(baseSchema.asStruct());\n     assertThat(convertedSchema.identifierFieldIds()).containsExactly(101);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testConvertFlinkSchemaWithPrimaryKeys() {\n     Schema icebergSchema =\n         new Schema(\n@@ -389,14 +432,22 @@ public void testConvertFlinkSchemaWithPrimaryKeys() {\n                 Types.NestedField.required(2, \"string\", Types.StringType.get())),\n             Sets.newHashSet(1, 2));\n \n-    TableSchema tableSchema = FlinkSchemaUtil.toSchema(icebergSchema);\n-    assertThat(tableSchema.getPrimaryKey())\n-        .isPresent()\n-        .get()\n-        .satisfies(k -> assertThat(k.getColumns()).containsExactly(\"int\", \"string\"));\n+    if (isTableSchema) {\n+      TableSchema tableSchema = FlinkSchemaUtil.toSchema(icebergSchema);\n+      assertThat(tableSchema.getPrimaryKey())\n+          .isPresent()\n+          .get()\n+          .satisfies(k -> assertThat(k.getColumns()).containsExactly(\"int\", \"string\"));\n+    } else {\n+      ResolvedSchema resolvedSchema = FlinkSchemaUtil.toResolvedSchema(icebergSchema);\n+      assertThat(resolvedSchema.getPrimaryKey())\n+          .isPresent()\n+          .get()\n+          .satisfies(k -> assertThat(k.getColumns()).containsExactly(\"int\", \"string\"));\n+    }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testConvertFlinkSchemaWithNestedColumnInPrimaryKeys() {\n     Schema icebergSchema =\n         new Schema(\n@@ -408,9 +459,16 @@ public void testConvertFlinkSchemaWithNestedColumnInPrimaryKeys() {\n                         Types.NestedField.required(2, \"inner\", Types.IntegerType.get())))),\n             Sets.newHashSet(2));\n \n-    assertThatThrownBy(() -> FlinkSchemaUtil.toSchema(icebergSchema))\n-        .isInstanceOf(ValidationException.class)\n-        .hasMessageStartingWith(\"Could not create a PRIMARY KEY\")\n-        .hasMessageContaining(\"Column 'struct.inner' does not exist.\");\n+    if (isTableSchema) {\n+      assertThatThrownBy(() -> FlinkSchemaUtil.toSchema(icebergSchema))\n+          .isInstanceOf(ValidationException.class)\n+          .hasMessageStartingWith(\"Could not create a PRIMARY KEY\")\n+          .hasMessageContaining(\"Column 'struct.inner' does not exist.\");\n+    } else {\n+      assertThatThrownBy(() -> FlinkSchemaUtil.toResolvedSchema(icebergSchema))\n+          .isInstanceOf(ValidationException.class)\n+          .hasMessageStartingWith(\"Invalid primary key\")\n+          .hasMessageContaining(\"Column 'struct.inner' does not exist.\");\n+    }\n   }\n }\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\nindex fad30f9c1e67..d99f657a11cc 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSink.java\n@@ -142,7 +142,7 @@ public void testInsertFromSourceTable() throws Exception {\n             \"sourceTable\",\n             getTableEnv()\n                 .fromValues(\n-                    SimpleDataUtil.FLINK_SCHEMA.toRowDataType(),\n+                    SimpleDataUtil.FLINK_SCHEMA.toSourceRowDataType(),\n                     Expressions.row(1, \"hello\"),\n                     Expressions.row(2, \"world\"),\n                     Expressions.row(3, (String) null),\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSinkCompaction.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSinkCompaction.java\nindex 55d8a627d583..03d96ac2c573 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSinkCompaction.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/TestFlinkTableSinkCompaction.java\n@@ -31,6 +31,8 @@\n import org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.ExternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n import org.apache.flink.types.Row;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.ManifestFile;\n@@ -53,10 +55,14 @@\n public class TestFlinkTableSinkCompaction extends CatalogTestBase {\n \n   private static final TypeInformation<Row> ROW_TYPE_INFO =\n-      new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+      new RowTypeInfo(\n+          SimpleDataUtil.FLINK_SCHEMA.getColumnDataTypes().stream()\n+              .map(ExternalTypeInfo::of)\n+              .toArray(TypeInformation[]::new));\n \n   private static final DataFormatConverters.RowConverter CONVERTER =\n-      new DataFormatConverters.RowConverter(SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+      new DataFormatConverters.RowConverter(\n+          SimpleDataUtil.FLINK_SCHEMA.getColumnDataTypes().toArray(DataType[]::new));\n \n   private static final String TABLE_NAME = \"test_table\";\n   private StreamTableEnvironment tEnv;\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestBucketPartitionerFlinkIcebergSink.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestBucketPartitionerFlinkIcebergSink.java\nindex 243c50a72b20..69afba12026a 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestBucketPartitionerFlinkIcebergSink.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestBucketPartitionerFlinkIcebergSink.java\n@@ -34,6 +34,8 @@\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.data.util.DataFormatConverters;\n+import org.apache.flink.table.runtime.typeutils.ExternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n import org.apache.flink.test.junit5.MiniClusterExtension;\n import org.apache.flink.types.Row;\n import org.apache.iceberg.DistributionMode;\n@@ -76,7 +78,10 @@ public class TestBucketPartitionerFlinkIcebergSink {\n       new HadoopCatalogExtension(DATABASE, TestFixtures.TABLE);\n \n   private static final TypeInformation<Row> ROW_TYPE_INFO =\n-      new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+      new RowTypeInfo(\n+          SimpleDataUtil.FLINK_SCHEMA.getColumnDataTypes().stream()\n+              .map(ExternalTypeInfo::of)\n+              .toArray(TypeInformation[]::new));\n \n   // Parallelism = 8 (parallelism > numBuckets) throughout the test suite\n   private final int parallelism = NUMBER_TASK_MANAGERS * SLOTS_PER_TASK_MANAGER;\n@@ -107,7 +112,8 @@ private void setupEnvironment(TableSchemaType tableSchemaType) {\n \n   private void appendRowsToTable(List<RowData> allRows) throws Exception {\n     DataFormatConverters.RowConverter converter =\n-        new DataFormatConverters.RowConverter(SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+        new DataFormatConverters.RowConverter(\n+            SimpleDataUtil.FLINK_SCHEMA.getColumnDataTypes().toArray(DataType[]::new));\n \n     DataStream<RowData> dataStream =\n         env.addSource(\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestCompressionSettings.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestCompressionSettings.java\nindex b7bca839c141..5a74db5713a5 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestCompressionSettings.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestCompressionSettings.java\n@@ -26,8 +26,8 @@\n import java.nio.file.Path;\n import java.util.Map;\n import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.legacy.api.TableSchema;\n import org.apache.flink.table.types.logical.RowType;\n import org.apache.iceberg.Parameter;\n import org.apache.iceberg.ParameterizedTestExtension;\n@@ -211,7 +211,7 @@ public void testCompressionOrc() throws Exception {\n \n   private static OneInputStreamOperatorTestHarness<RowData, FlinkWriteResult>\n       createIcebergStreamWriter(\n-          Table icebergTable, TableSchema flinkSchema, Map<String, String> override)\n+          Table icebergTable, ResolvedSchema flinkSchema, Map<String, String> override)\n           throws Exception {\n     RowType flinkRowType = FlinkSink.toFlinkRowType(icebergTable.schema(), flinkSchema);\n     FlinkWriteConf flinkWriteConfig =\n@@ -230,7 +230,7 @@ public void testCompressionOrc() throws Exception {\n   }\n \n   private static Map<String, String> appenderProperties(\n-      Table table, TableSchema schema, Map<String, String> override) throws Exception {\n+      Table table, ResolvedSchema schema, Map<String, String> override) throws Exception {\n     try (OneInputStreamOperatorTestHarness<RowData, FlinkWriteResult> testHarness =\n         createIcebergStreamWriter(table, schema, override)) {\n       testHarness.processElement(SimpleDataUtil.createRowData(1, \"hello\"), 1);\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java\nindex b778037c559c..fc1236ed8855 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSink.java\n@@ -52,21 +52,39 @@ public class TestFlinkIcebergSink extends TestFlinkIcebergSinkBase {\n   @Parameter(index = 2)\n   private boolean partitioned;\n \n-  @Parameters(name = \"format={0}, parallelism = {1}, partitioned = {2}\")\n+  @Parameter(index = 3)\n+  private boolean isTableSchema;\n+\n+  @Parameters(name = \"format={0}, parallelism = {1}, partitioned = {2}, isTableSchema = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {FileFormat.AVRO, 1, true},\n-      {FileFormat.AVRO, 1, false},\n-      {FileFormat.AVRO, 2, true},\n-      {FileFormat.AVRO, 2, false},\n-      {FileFormat.ORC, 1, true},\n-      {FileFormat.ORC, 1, false},\n-      {FileFormat.ORC, 2, true},\n-      {FileFormat.ORC, 2, false},\n-      {FileFormat.PARQUET, 1, true},\n-      {FileFormat.PARQUET, 1, false},\n-      {FileFormat.PARQUET, 2, true},\n-      {FileFormat.PARQUET, 2, false}\n+      // Remove after the deprecation of TableSchema - BEGIN\n+      {FileFormat.AVRO, 1, true, true},\n+      {FileFormat.AVRO, 1, false, true},\n+      {FileFormat.AVRO, 2, true, true},\n+      {FileFormat.AVRO, 2, false, true},\n+      {FileFormat.ORC, 1, true, true},\n+      {FileFormat.ORC, 1, false, true},\n+      {FileFormat.ORC, 2, true, true},\n+      {FileFormat.ORC, 2, false, true},\n+      {FileFormat.PARQUET, 1, true, true},\n+      {FileFormat.PARQUET, 1, false, true},\n+      {FileFormat.PARQUET, 2, true, true},\n+      {FileFormat.PARQUET, 2, false, true},\n+      // Remove after the deprecation of TableSchema - END\n+\n+      {FileFormat.AVRO, 1, true, false},\n+      {FileFormat.AVRO, 1, false, false},\n+      {FileFormat.AVRO, 2, true, false},\n+      {FileFormat.AVRO, 2, false, false},\n+      {FileFormat.ORC, 1, true, false},\n+      {FileFormat.ORC, 1, false, false},\n+      {FileFormat.ORC, 2, true, false},\n+      {FileFormat.ORC, 2, false, false},\n+      {FileFormat.PARQUET, 1, true, false},\n+      {FileFormat.PARQUET, 1, false, false},\n+      {FileFormat.PARQUET, 2, true, false},\n+      {FileFormat.PARQUET, 2, false, false},\n     };\n   }\n \n@@ -115,11 +133,11 @@ public void testWriteRowData() throws Exception {\n \n   @TestTemplate\n   public void testWriteRow() throws Exception {\n-    testWriteRow(parallelism, null, DistributionMode.NONE);\n+    testWriteRow(parallelism, null, DistributionMode.NONE, isTableSchema);\n   }\n \n   @TestTemplate\n-  public void testWriteRowWithTableSchema() throws Exception {\n-    testWriteRow(parallelism, SimpleDataUtil.FLINK_SCHEMA, DistributionMode.NONE);\n+  public void testWriteRowWithFlinkSchema() throws Exception {\n+    testWriteRow(parallelism, SimpleDataUtil.FLINK_SCHEMA, DistributionMode.NONE, isTableSchema);\n   }\n }\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkBase.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkBase.java\nindex de098f826d7d..55e00d39b316 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkBase.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkBase.java\n@@ -28,9 +28,12 @@\n import org.apache.flink.api.java.typeutils.RowTypeInfo;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.data.util.DataFormatConverters;\n import org.apache.flink.table.legacy.api.TableSchema;\n+import org.apache.flink.table.runtime.typeutils.ExternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n import org.apache.flink.test.junit5.MiniClusterExtension;\n import org.apache.flink.types.Row;\n import org.apache.iceberg.DistributionMode;\n@@ -56,10 +59,14 @@ public class TestFlinkIcebergSinkBase {\n       new HadoopCatalogExtension(DATABASE, TestFixtures.TABLE);\n \n   protected static final TypeInformation<Row> ROW_TYPE_INFO =\n-      new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+      new RowTypeInfo(\n+          SimpleDataUtil.FLINK_SCHEMA.getColumnDataTypes().stream()\n+              .map(ExternalTypeInfo::of)\n+              .toArray(TypeInformation[]::new));\n \n   protected static final DataFormatConverters.RowConverter CONVERTER =\n-      new DataFormatConverters.RowConverter(SimpleDataUtil.FLINK_SCHEMA.getFieldDataTypes());\n+      new DataFormatConverters.RowConverter(\n+          SimpleDataUtil.FLINK_SCHEMA.getColumnDataTypes().toArray(DataType[]::new));\n \n   protected TableLoader tableLoader;\n   protected Table table;\n@@ -87,18 +94,32 @@ protected List<RowData> convertToRowData(List<Row> rows) {\n   }\n \n   protected void testWriteRow(\n-      int writerParallelism, TableSchema tableSchema, DistributionMode distributionMode)\n+      int writerParallelism,\n+      ResolvedSchema resolvedSchema,\n+      DistributionMode distributionMode,\n+      boolean isTableSchema)\n       throws Exception {\n     List<Row> rows = createRows(\"\");\n     DataStream<Row> dataStream = env.addSource(createBoundedSource(rows), ROW_TYPE_INFO);\n \n-    FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .tableSchema(tableSchema)\n-        .writeParallelism(writerParallelism)\n-        .distributionMode(distributionMode)\n-        .append();\n+    if (isTableSchema) {\n+      FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .tableSchema(\n+              resolvedSchema != null ? TableSchema.fromResolvedSchema(resolvedSchema) : null)\n+          .writeParallelism(writerParallelism)\n+          .distributionMode(distributionMode)\n+          .append();\n+    } else {\n+      FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .resolvedSchema(resolvedSchema)\n+          .writeParallelism(writerParallelism)\n+          .distributionMode(distributionMode)\n+          .append();\n+    }\n \n     // Execute the program.\n     env.execute(\"Test Iceberg DataStream.\");\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkBranch.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkBranch.java\nindex 06ea0bc887e8..a77ddead3003 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkBranch.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkBranch.java\n@@ -25,6 +25,7 @@\n import java.util.List;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.legacy.api.TableSchema;\n import org.apache.flink.types.Row;\n import org.apache.iceberg.DistributionMode;\n@@ -59,15 +60,25 @@ public class TestFlinkIcebergSinkBranch extends TestFlinkIcebergSinkBase {\n   @Parameter(index = 1)\n   private String branch;\n \n+  @Parameter(index = 2)\n+  private boolean isTableSchema;\n+\n   private TableLoader tableLoader;\n \n-  @Parameters(name = \"formatVersion = {0}, branch = {1}\")\n+  @Parameters(name = \"formatVersion = {0}, branch = {1}, isTableSchema = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {\"1\", \"main\"},\n-      {\"1\", \"testBranch\"},\n-      {\"2\", \"main\"},\n-      {\"2\", \"testBranch\"}\n+      // Remove after the deprecation of TableSchema - BEGIN\n+      {\"1\", \"main\", true},\n+      {\"1\", \"testBranch\", true},\n+      {\"2\", \"main\", true},\n+      {\"2\", \"testBranch\", true},\n+      // Remove after the deprecation of TableSchema - END\n+\n+      {\"1\", \"main\", false},\n+      {\"1\", \"testBranch\", false},\n+      {\"2\", \"main\", false},\n+      {\"2\", \"testBranch\", false},\n     };\n   }\n \n@@ -95,23 +106,33 @@ public void before() throws IOException {\n   }\n \n   @TestTemplate\n-  public void testWriteRowWithTableSchema() throws Exception {\n+  public void testWriteRowWithFlinkSchema() throws Exception {\n     testWriteRow(SimpleDataUtil.FLINK_SCHEMA, DistributionMode.NONE);\n     verifyOtherBranchUnmodified();\n   }\n \n-  private void testWriteRow(TableSchema tableSchema, DistributionMode distributionMode)\n+  private void testWriteRow(ResolvedSchema resolvedSchema, DistributionMode distributionMode)\n       throws Exception {\n     List<Row> rows = createRows(\"\");\n     DataStream<Row> dataStream = env.addSource(createBoundedSource(rows), ROW_TYPE_INFO);\n \n-    FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .tableSchema(tableSchema)\n-        .toBranch(branch)\n-        .distributionMode(distributionMode)\n-        .append();\n+    if (isTableSchema) {\n+      FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .tableSchema(TableSchema.fromResolvedSchema(resolvedSchema))\n+          .toBranch(branch)\n+          .distributionMode(distributionMode)\n+          .append();\n+    } else {\n+      FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .resolvedSchema(resolvedSchema)\n+          .toBranch(branch)\n+          .distributionMode(distributionMode)\n+          .append();\n+    }\n \n     // Execute the program.\n     env.execute(\"Test Iceberg DataStream.\");\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkDistributionMode.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkDistributionMode.java\nindex 84fa48e38b70..a75d9b61c755 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkDistributionMode.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkDistributionMode.java\n@@ -85,15 +85,28 @@ public class TestFlinkIcebergSinkDistributionMode extends TestFlinkIcebergSinkBa\n   @Parameter(index = 2)\n   private int writeParallelism;\n \n-  @Parameters(name = \"parallelism = {0}, partitioned = {1}, writeParallelism = {2}\")\n+  @Parameter(index = 3)\n+  private boolean isTableSchema;\n+\n+  @Parameters(\n+      name = \"parallelism = {0}, partitioned = {1}, writeParallelism = {2}, isTableSchema = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {1, true, 1},\n-      {1, false, 1},\n-      {2, true, 2},\n-      {2, false, 2},\n-      {1, true, 2},\n-      {1, false, 2},\n+      // Remove after the deprecation of TableSchema - BEGIN\n+      {1, true, 1, true},\n+      {1, false, 1, true},\n+      {2, true, 2, true},\n+      {2, false, 2, true},\n+      {1, true, 2, true},\n+      {1, false, 2, true},\n+      // Remove after the deprecation of TableSchema - END\n+\n+      {1, true, 1, false},\n+      {1, false, 1, false},\n+      {2, true, 2, false},\n+      {2, false, 2, false},\n+      {1, true, 2, false},\n+      {1, false, 2, false},\n     };\n   }\n \n@@ -122,7 +135,7 @@ public void before() throws IOException {\n \n   @TestTemplate\n   public void testShuffleByPartitionWithSchema() throws Exception {\n-    testWriteRow(parallelism, SimpleDataUtil.FLINK_SCHEMA, DistributionMode.HASH);\n+    testWriteRow(parallelism, SimpleDataUtil.FLINK_SCHEMA, DistributionMode.HASH, isTableSchema);\n     if (partitioned) {\n       assertThat(partitionFiles(\"aaa\")).isEqualTo(1);\n       assertThat(partitionFiles(\"bbb\")).isEqualTo(1);\n@@ -137,7 +150,7 @@ public void testJobNoneDistributeMode() throws Exception {\n         .set(TableProperties.WRITE_DISTRIBUTION_MODE, DistributionMode.HASH.modeName())\n         .commit();\n \n-    testWriteRow(parallelism, null, DistributionMode.NONE);\n+    testWriteRow(parallelism, null, DistributionMode.NONE, isTableSchema);\n \n     if (parallelism > 1) {\n       if (partitioned) {\n@@ -154,7 +167,7 @@ public void testJobNullDistributionMode() throws Exception {\n         .set(TableProperties.WRITE_DISTRIBUTION_MODE, DistributionMode.HASH.modeName())\n         .commit();\n \n-    testWriteRow(parallelism, null, null);\n+    testWriteRow(parallelism, null, null, isTableSchema);\n \n     if (partitioned) {\n       assertThat(partitionFiles(\"aaa\")).isEqualTo(1);\n@@ -165,7 +178,7 @@ public void testJobNullDistributionMode() throws Exception {\n \n   @TestTemplate\n   public void testPartitionWriteMode() throws Exception {\n-    testWriteRow(parallelism, null, DistributionMode.HASH);\n+    testWriteRow(parallelism, null, DistributionMode.HASH, isTableSchema);\n     if (partitioned) {\n       assertThat(partitionFiles(\"aaa\")).isEqualTo(1);\n       assertThat(partitionFiles(\"bbb\")).isEqualTo(1);\n@@ -182,11 +195,17 @@ public void testOverrideWriteConfigWithUnknownDistributionMode() {\n     DataStream<Row> dataStream = env.addSource(createBoundedSource(rows), ROW_TYPE_INFO);\n \n     FlinkSink.Builder builder =\n-        FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-            .table(table)\n-            .tableLoader(tableLoader)\n-            .writeParallelism(writeParallelism)\n-            .setAll(newProps);\n+        isTableSchema\n+            ? FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .writeParallelism(writeParallelism)\n+                .setAll(newProps)\n+            : FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .writeParallelism(writeParallelism)\n+                .setAll(newProps);\n \n     assertThatThrownBy(builder::append)\n         .isInstanceOf(IllegalArgumentException.class)\n@@ -194,7 +213,7 @@ public void testOverrideWriteConfigWithUnknownDistributionMode() {\n   }\n \n   @TestTemplate\n-  public void testRangeDistributionWithoutSortOrderUnpartitioned() throws Exception {\n+  public void testRangeDistributionWithoutSortOrderUnpartitioned() {\n     assumeThat(partitioned).isFalse();\n \n     table\n@@ -208,10 +227,15 @@ public void testRangeDistributionWithoutSortOrderUnpartitioned() throws Exceptio\n             createRangeDistributionBoundedSource(createCharRows(numOfCheckpoints, 10)),\n             ROW_TYPE_INFO);\n     FlinkSink.Builder builder =\n-        FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-            .table(table)\n-            .tableLoader(tableLoader)\n-            .writeParallelism(writeParallelism);\n+        isTableSchema\n+            ? FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .writeParallelism(writeParallelism)\n+            : FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .writeParallelism(writeParallelism);\n \n     // Range distribution requires either sort order or partition spec defined\n     assertThatThrownBy(builder::append)\n@@ -235,10 +259,15 @@ public void testRangeDistributionWithoutSortOrderPartitioned() throws Exception\n             createRangeDistributionBoundedSource(createCharRows(numOfCheckpoints, 10)),\n             ROW_TYPE_INFO);\n     FlinkSink.Builder builder =\n-        FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-            .table(table)\n-            .tableLoader(tableLoader)\n-            .writeParallelism(writeParallelism);\n+        isTableSchema\n+            ? FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .writeParallelism(writeParallelism)\n+            : FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .writeParallelism(writeParallelism);\n \n     // sort based on partition columns\n     builder.append();\n@@ -273,10 +302,15 @@ public void testRangeDistributionWithNullValue() throws Exception {\n     DataStream<Row> dataStream =\n         env.addSource(createRangeDistributionBoundedSource(charRows), ROW_TYPE_INFO);\n     FlinkSink.Builder builder =\n-        FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-            .table(table)\n-            .tableLoader(tableLoader)\n-            .writeParallelism(parallelism);\n+        isTableSchema\n+            ? FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .writeParallelism(parallelism)\n+            : FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .writeParallelism(parallelism);\n \n     // sort based on partition columns\n     builder.append();\n@@ -309,12 +343,22 @@ public void testRangeDistributionWithSortOrder() throws Exception {\n         env.addSource(\n             createRangeDistributionBoundedSource(createCharRows(numOfCheckpoints, 10)),\n             ROW_TYPE_INFO);\n-    FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .writeParallelism(writeParallelism)\n-        .rangeDistributionStatisticsType(StatisticsType.Map)\n-        .append();\n+    if (isTableSchema) {\n+      FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .rangeDistributionStatisticsType(StatisticsType.Map)\n+          .append();\n+    } else {\n+      FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .rangeDistributionStatisticsType(StatisticsType.Map)\n+          .append();\n+    }\n+\n     env.execute(getClass().getSimpleName());\n \n     table.refresh();\n@@ -370,12 +414,22 @@ public void testRangeDistributionSketchWithSortOrder() throws Exception {\n         env.addSource(\n             createRangeDistributionBoundedSource(createIntRows(numOfCheckpoints, 1_000)),\n             ROW_TYPE_INFO);\n-    FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .writeParallelism(writeParallelism)\n-        .rangeDistributionStatisticsType(StatisticsType.Sketch)\n-        .append();\n+    if (isTableSchema) {\n+      FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .rangeDistributionStatisticsType(StatisticsType.Sketch)\n+          .append();\n+    } else {\n+      FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .rangeDistributionStatisticsType(StatisticsType.Sketch)\n+          .append();\n+    }\n+\n     env.execute(getClass().getSimpleName());\n \n     table.refresh();\n@@ -439,12 +493,22 @@ public void testRangeDistributionStatisticsMigration() throws Exception {\n \n     DataStream<Row> dataStream =\n         env.addSource(createRangeDistributionBoundedSource(rowsPerCheckpoint), ROW_TYPE_INFO);\n-    FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .writeParallelism(writeParallelism)\n-        .rangeDistributionStatisticsType(StatisticsType.Auto)\n-        .append();\n+    if (isTableSchema) {\n+      FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .rangeDistributionStatisticsType(StatisticsType.Auto)\n+          .append();\n+    } else {\n+      FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .rangeDistributionStatisticsType(StatisticsType.Auto)\n+          .append();\n+    }\n+\n     env.execute(getClass().getSimpleName());\n \n     table.refresh();\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkExtended.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkExtended.java\nindex 36a59b20431c..f3d735982f54 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkExtended.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkExtended.java\n@@ -32,6 +32,9 @@\n import org.apache.flink.types.Row;\n import org.apache.iceberg.DistributionMode;\n import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n@@ -47,17 +50,27 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n /**\n  * This class tests the more extended features of Flink sink. Extract them separately since it is\n  * unnecessary to test all the parameters combinations in {@link TestFlinkIcebergSink}. Each test\n  * method in {@link TestFlinkIcebergSink} runs 12 combinations, which are expensive and slow.\n  */\n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestFlinkIcebergSinkExtended extends TestFlinkIcebergSinkBase {\n   private final boolean partitioned = true;\n   private final int parallelism = 2;\n   private final FileFormat format = FileFormat.PARQUET;\n \n+  @Parameter private boolean isTableSchema;\n+\n+  @Parameters(name = \"isTableSchema={0}\")\n+  private static Object[][] parameters() {\n+    return new Object[][] {{true}, {false}};\n+  }\n+\n   @BeforeEach\n   public void before() throws IOException {\n     this.table =\n@@ -81,7 +94,7 @@ public void before() throws IOException {\n     this.tableLoader = CATALOG_EXTENSION.tableLoader();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTwoSinksInDisjointedDAG() throws Exception {\n     Map<String, String> props = ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, format.name());\n \n@@ -124,29 +137,52 @@ public void testTwoSinksInDisjointedDAG() throws Exception {\n         env.fromCollection(leftRows, ROW_TYPE_INFO)\n             .name(\"leftCustomSource\")\n             .uid(\"leftCustomSource\");\n-    FlinkSink.forRow(leftStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(leftTable)\n-        .tableLoader(leftTableLoader)\n-        .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n-        .distributionMode(DistributionMode.NONE)\n-        .uidPrefix(\"leftIcebergSink\")\n-        .append();\n+    if (isTableSchema) {\n+      FlinkSink.forRow(leftStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(leftTable)\n+          .tableLoader(leftTableLoader)\n+          .tableSchema(SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .distributionMode(DistributionMode.NONE)\n+          .uidPrefix(\"leftIcebergSink\")\n+          .append();\n+    } else {\n+      FlinkSink.forRow(leftStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(leftTable)\n+          .tableLoader(leftTableLoader)\n+          .resolvedSchema(SimpleDataUtil.FLINK_SCHEMA)\n+          .distributionMode(DistributionMode.NONE)\n+          .uidPrefix(\"leftIcebergSink\")\n+          .append();\n+    }\n \n     List<Row> rightRows = createRows(\"right-\");\n     DataStream<Row> rightStream =\n         env.fromCollection(rightRows, ROW_TYPE_INFO)\n             .name(\"rightCustomSource\")\n             .uid(\"rightCustomSource\");\n-    FlinkSink.forRow(rightStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(rightTable)\n-        .tableLoader(rightTableLoader)\n-        .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n-        .writeParallelism(parallelism)\n-        .distributionMode(DistributionMode.HASH)\n-        .uidPrefix(\"rightIcebergSink\")\n-        .setSnapshotProperty(\"flink.test\", TestFlinkIcebergSink.class.getName())\n-        .setSnapshotProperties(Collections.singletonMap(\"direction\", \"rightTable\"))\n-        .append();\n+    if (isTableSchema) {\n+      FlinkSink.forRow(rightStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(rightTable)\n+          .tableLoader(rightTableLoader)\n+          .tableSchema(SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .writeParallelism(parallelism)\n+          .distributionMode(DistributionMode.HASH)\n+          .uidPrefix(\"rightIcebergSink\")\n+          .setSnapshotProperty(\"flink.test\", TestFlinkIcebergSink.class.getName())\n+          .setSnapshotProperties(Collections.singletonMap(\"direction\", \"rightTable\"))\n+          .append();\n+    } else {\n+      FlinkSink.forRow(rightStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(rightTable)\n+          .tableLoader(rightTableLoader)\n+          .resolvedSchema(SimpleDataUtil.FLINK_SCHEMA)\n+          .writeParallelism(parallelism)\n+          .distributionMode(DistributionMode.HASH)\n+          .uidPrefix(\"rightIcebergSink\")\n+          .setSnapshotProperty(\"flink.test\", TestFlinkIcebergSink.class.getName())\n+          .setSnapshotProperties(Collections.singletonMap(\"direction\", \"rightTable\"))\n+          .append();\n+    }\n \n     // Execute the program.\n     env.execute(\"Test Iceberg DataStream.\");\n@@ -162,7 +198,7 @@ public void testTwoSinksInDisjointedDAG() throws Exception {\n         .containsEntry(\"direction\", \"rightTable\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOverrideWriteConfigWithUnknownFileFormat() {\n     Map<String, String> newProps = Maps.newHashMap();\n     newProps.put(FlinkWriteOptions.WRITE_FORMAT.key(), \"UNRECOGNIZED\");\n@@ -171,11 +207,17 @@ public void testOverrideWriteConfigWithUnknownFileFormat() {\n     DataStream<Row> dataStream = env.addSource(createBoundedSource(rows), ROW_TYPE_INFO);\n \n     FlinkSink.Builder builder =\n-        FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-            .table(table)\n-            .tableLoader(tableLoader)\n-            .writeParallelism(parallelism)\n-            .setAll(newProps);\n+        isTableSchema\n+            ? FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .writeParallelism(parallelism)\n+                .setAll(newProps)\n+            : FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .writeParallelism(parallelism)\n+                .setAll(newProps);\n \n     assertThatThrownBy(builder::append)\n         .isInstanceOf(IllegalArgumentException.class)\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java\nindex 44b1c57eda64..ffd40b6cdc95 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2.java\n@@ -109,7 +109,9 @@ public void testCheckAndGetEqualityFieldIds() {\n     DataStream<Row> dataStream =\n         env.addSource(new BoundedTestSource<>(ImmutableList.of()), ROW_TYPE_INFO);\n     FlinkSink.Builder builder =\n-        FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA).table(table);\n+        isTableSchema\n+            ? FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA).table(table)\n+            : FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA).table(table);\n \n     // Use schema identifier field IDs as equality field id list by default\n     assertThat(builder.checkAndGetEqualityFieldIds())\n@@ -165,15 +167,21 @@ public void testChangeLogOnSameKey() throws Exception {\n   }\n \n   @TestTemplate\n-  public void testUpsertModeCheck() throws Exception {\n+  public void testUpsertModeCheck() {\n     DataStream<Row> dataStream =\n         env.addSource(new BoundedTestSource<>(ImmutableList.of()), ROW_TYPE_INFO);\n     FlinkSink.Builder builder =\n-        FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-            .tableLoader(tableLoader)\n-            .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n-            .writeParallelism(parallelism)\n-            .upsert(true);\n+        isTableSchema\n+            ? FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+                .tableLoader(tableLoader)\n+                .tableSchema(SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+                .writeParallelism(parallelism)\n+                .upsert(true)\n+            : FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+                .tableLoader(tableLoader)\n+                .resolvedSchema(SimpleDataUtil.FLINK_SCHEMA)\n+                .writeParallelism(parallelism)\n+                .upsert(true);\n \n     assertThatThrownBy(\n             () ->\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2Base.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2Base.java\nindex edee493460cf..12a4593d039e 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2Base.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2Base.java\n@@ -29,6 +29,7 @@\n import org.apache.flink.api.java.typeutils.RowTypeInfo;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.runtime.typeutils.ExternalTypeInfo;\n import org.apache.flink.types.Row;\n import org.apache.flink.types.RowKind;\n import org.apache.iceberg.FileFormat;\n@@ -52,7 +53,10 @@ class TestFlinkIcebergSinkV2Base {\n \n   static final int FORMAT_V2 = 2;\n   static final TypeInformation<Row> ROW_TYPE_INFO =\n-      new RowTypeInfo(SimpleDataUtil.FLINK_SCHEMA.getFieldTypes());\n+      new RowTypeInfo(\n+          SimpleDataUtil.FLINK_SCHEMA.getColumnDataTypes().stream()\n+              .map(ExternalTypeInfo::of)\n+              .toArray(TypeInformation[]::new));\n \n   static final int ROW_ID_POS = 0;\n   static final int ROW_DATA_POS = 1;\n@@ -73,21 +77,41 @@ class TestFlinkIcebergSinkV2Base {\n   @Parameter(index = 3)\n   String writeDistributionMode;\n \n-  @Parameters(name = \"FileFormat={0}, Parallelism={1}, Partitioned={2}, WriteDistributionMode={3}\")\n+  @Parameter(index = 4)\n+  boolean isTableSchema;\n+\n+  @Parameters(\n+      name =\n+          \"FileFormat={0}, Parallelism={1}, Partitioned={2}, WriteDistributionMode={3}, IsTableSchema={4}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      new Object[] {FileFormat.AVRO, 1, true, TableProperties.WRITE_DISTRIBUTION_MODE_NONE},\n-      new Object[] {FileFormat.AVRO, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_NONE},\n-      new Object[] {FileFormat.AVRO, 4, true, TableProperties.WRITE_DISTRIBUTION_MODE_NONE},\n-      new Object[] {FileFormat.AVRO, 4, false, TableProperties.WRITE_DISTRIBUTION_MODE_NONE},\n-      new Object[] {FileFormat.ORC, 1, true, TableProperties.WRITE_DISTRIBUTION_MODE_HASH},\n-      new Object[] {FileFormat.ORC, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_HASH},\n-      new Object[] {FileFormat.ORC, 4, true, TableProperties.WRITE_DISTRIBUTION_MODE_HASH},\n-      new Object[] {FileFormat.ORC, 4, false, TableProperties.WRITE_DISTRIBUTION_MODE_HASH},\n-      new Object[] {FileFormat.PARQUET, 1, true, TableProperties.WRITE_DISTRIBUTION_MODE_RANGE},\n-      new Object[] {FileFormat.PARQUET, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_RANGE},\n-      new Object[] {FileFormat.PARQUET, 4, true, TableProperties.WRITE_DISTRIBUTION_MODE_RANGE},\n-      new Object[] {FileFormat.PARQUET, 4, false, TableProperties.WRITE_DISTRIBUTION_MODE_RANGE}\n+      // Remove after the deprecation of TableSchema - BEGIN\n+      {FileFormat.AVRO, 1, true, TableProperties.WRITE_DISTRIBUTION_MODE_NONE, true},\n+      {FileFormat.AVRO, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_NONE, true},\n+      {FileFormat.AVRO, 4, true, TableProperties.WRITE_DISTRIBUTION_MODE_NONE, true},\n+      {FileFormat.AVRO, 4, false, TableProperties.WRITE_DISTRIBUTION_MODE_NONE, true},\n+      {FileFormat.ORC, 1, true, TableProperties.WRITE_DISTRIBUTION_MODE_HASH, true},\n+      {FileFormat.ORC, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_HASH, true},\n+      {FileFormat.ORC, 4, true, TableProperties.WRITE_DISTRIBUTION_MODE_HASH, true},\n+      {FileFormat.ORC, 4, false, TableProperties.WRITE_DISTRIBUTION_MODE_HASH, true},\n+      {FileFormat.PARQUET, 1, true, TableProperties.WRITE_DISTRIBUTION_MODE_RANGE, true},\n+      {FileFormat.PARQUET, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_RANGE, true},\n+      {FileFormat.PARQUET, 4, true, TableProperties.WRITE_DISTRIBUTION_MODE_RANGE, true},\n+      {FileFormat.PARQUET, 4, false, TableProperties.WRITE_DISTRIBUTION_MODE_RANGE, true},\n+      // Remove after the deprecation of TableSchema - END\n+\n+      {FileFormat.AVRO, 1, true, TableProperties.WRITE_DISTRIBUTION_MODE_NONE, false},\n+      {FileFormat.AVRO, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_NONE, false},\n+      {FileFormat.AVRO, 4, true, TableProperties.WRITE_DISTRIBUTION_MODE_NONE, false},\n+      {FileFormat.AVRO, 4, false, TableProperties.WRITE_DISTRIBUTION_MODE_NONE, false},\n+      {FileFormat.ORC, 1, true, TableProperties.WRITE_DISTRIBUTION_MODE_HASH, false},\n+      {FileFormat.ORC, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_HASH, false},\n+      {FileFormat.ORC, 4, true, TableProperties.WRITE_DISTRIBUTION_MODE_HASH, false},\n+      {FileFormat.ORC, 4, false, TableProperties.WRITE_DISTRIBUTION_MODE_HASH, false},\n+      {FileFormat.PARQUET, 1, true, TableProperties.WRITE_DISTRIBUTION_MODE_RANGE, false},\n+      {FileFormat.PARQUET, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_RANGE, false},\n+      {FileFormat.PARQUET, 4, true, TableProperties.WRITE_DISTRIBUTION_MODE_RANGE, false},\n+      {FileFormat.PARQUET, 4, false, TableProperties.WRITE_DISTRIBUTION_MODE_RANGE, false},\n     };\n   }\n \n@@ -332,14 +356,25 @@ void testChangeLogs(\n     DataStream<Row> dataStream =\n         env.addSource(new BoundedTestSource<>(elementsPerCheckpoint), ROW_TYPE_INFO);\n \n-    FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .tableLoader(tableLoader)\n-        .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n-        .writeParallelism(parallelism)\n-        .equalityFieldColumns(equalityFieldColumns)\n-        .upsert(insertAsUpsert)\n-        .toBranch(branch)\n-        .append();\n+    if (isTableSchema) {\n+      FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .tableLoader(tableLoader)\n+          .tableSchema(SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .writeParallelism(parallelism)\n+          .equalityFieldColumns(equalityFieldColumns)\n+          .upsert(insertAsUpsert)\n+          .toBranch(branch)\n+          .append();\n+    } else {\n+      FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .tableLoader(tableLoader)\n+          .resolvedSchema(SimpleDataUtil.FLINK_SCHEMA)\n+          .writeParallelism(parallelism)\n+          .equalityFieldColumns(equalityFieldColumns)\n+          .upsert(insertAsUpsert)\n+          .toBranch(branch)\n+          .append();\n+    }\n \n     // Execute the program.\n     env.execute(\"Test Iceberg Change-Log DataStream.\");\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2Branch.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2Branch.java\nindex 56cba8f460e2..8ce3e1886f40 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2Branch.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2Branch.java\n@@ -46,20 +46,23 @@ public class TestFlinkIcebergSinkV2Branch extends TestFlinkIcebergSinkV2Base {\n   static final HadoopCatalogExtension CATALOG_EXTENSION =\n       new HadoopCatalogExtension(DATABASE, TestFixtures.TABLE);\n \n-  @Parameter(index = 4)\n+  @Parameter(index = 5)\n   protected String branch;\n \n   @Parameters(\n       name =\n-          \"FileFormat={0}, Parallelism={1}, Partitioned={2}, WriteDistributionMode={3}, Branch={4}\")\n+          \"FileFormat={0}, Parallelism={1}, Partitioned={2}, WriteDistributionMode={3}, IsTableSchema={4}, Branch={5}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      new Object[] {\n-        FileFormat.AVRO, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_NONE, \"main\"\n+      // Remove after the deprecation of TableSchema - BEGIN\n+      {FileFormat.AVRO, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_NONE, true, \"main\"},\n+      {FileFormat.AVRO, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_NONE, true, \"testBranch\"},\n+      // Remove after the deprecation of TableSchema - END\n+\n+      {FileFormat.AVRO, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_NONE, false, \"main\"},\n+      {\n+        FileFormat.AVRO, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_NONE, false, \"testBranch\"\n       },\n-      new Object[] {\n-        FileFormat.AVRO, 1, false, TableProperties.WRITE_DISTRIBUTION_MODE_NONE, \"testBranch\"\n-      }\n     };\n   }\n \n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2DistributionMode.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2DistributionMode.java\nindex 47217db206ba..60706db38f08 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2DistributionMode.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkV2DistributionMode.java\n@@ -85,15 +85,28 @@ public class TestFlinkIcebergSinkV2DistributionMode extends TestFlinkIcebergSink\n   @Parameter(index = 2)\n   private int writeParallelism;\n \n-  @Parameters(name = \"parallelism = {0}, partitioned = {1}, writeParallelism = {2}\")\n+  @Parameter(index = 3)\n+  private boolean isTableSchema;\n+\n+  @Parameters(\n+      name = \"parallelism = {0}, partitioned = {1}, writeParallelism = {2}, isTableSchema = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {1, true, 1},\n-      {1, false, 1},\n-      {2, true, 2},\n-      {2, false, 2},\n-      {1, true, 2},\n-      {1, false, 2},\n+      // Remove after the deprecation of TableSchema - BEGIN\n+      {1, true, 1, true},\n+      {1, false, 1, true},\n+      {2, true, 2, true},\n+      {2, false, 2, true},\n+      {1, true, 2, true},\n+      {1, false, 2, true},\n+      // Remove after the deprecation of TableSchema - END\n+\n+      {1, true, 1, false},\n+      {1, false, 1, false},\n+      {2, true, 2, false},\n+      {2, false, 2, false},\n+      {1, true, 2, false},\n+      {1, false, 2, false},\n     };\n   }\n \n@@ -122,7 +135,7 @@ public void before() throws IOException {\n \n   @TestTemplate\n   public void testShuffleByPartitionWithSchema() throws Exception {\n-    testWriteRow(parallelism, SimpleDataUtil.FLINK_SCHEMA, DistributionMode.HASH);\n+    testWriteRow(parallelism, SimpleDataUtil.FLINK_SCHEMA, DistributionMode.HASH, isTableSchema);\n     if (partitioned) {\n       assertThat(partitionFiles(\"aaa\")).isEqualTo(1);\n       assertThat(partitionFiles(\"bbb\")).isEqualTo(1);\n@@ -137,7 +150,7 @@ public void testJobNoneDistributeMode() throws Exception {\n         .set(TableProperties.WRITE_DISTRIBUTION_MODE, DistributionMode.HASH.modeName())\n         .commit();\n \n-    testWriteRow(parallelism, null, DistributionMode.NONE);\n+    testWriteRow(parallelism, null, DistributionMode.NONE, isTableSchema);\n \n     if (parallelism > 1) {\n       if (partitioned) {\n@@ -154,7 +167,7 @@ public void testJobNullDistributionMode() throws Exception {\n         .set(TableProperties.WRITE_DISTRIBUTION_MODE, DistributionMode.HASH.modeName())\n         .commit();\n \n-    testWriteRow(parallelism, null, null);\n+    testWriteRow(parallelism, null, null, isTableSchema);\n \n     if (partitioned) {\n       assertThat(partitionFiles(\"aaa\")).isEqualTo(1);\n@@ -165,7 +178,7 @@ public void testJobNullDistributionMode() throws Exception {\n \n   @TestTemplate\n   public void testPartitionWriteMode() throws Exception {\n-    testWriteRow(parallelism, null, DistributionMode.HASH);\n+    testWriteRow(parallelism, null, DistributionMode.HASH, isTableSchema);\n     if (partitioned) {\n       assertThat(partitionFiles(\"aaa\")).isEqualTo(1);\n       assertThat(partitionFiles(\"bbb\")).isEqualTo(1);\n@@ -181,12 +194,21 @@ public void testOverrideWriteConfigWithUnknownDistributionMode() {\n     List<Row> rows = createRows(\"\");\n     DataStream<Row> dataStream = env.addSource(createBoundedSource(rows), ROW_TYPE_INFO);\n \n-    IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .writeParallelism(writeParallelism)\n-        .setAll(newProps)\n-        .append();\n+    if (isTableSchema) {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .setAll(newProps)\n+          .append();\n+    } else {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .setAll(newProps)\n+          .append();\n+    }\n \n     assertThatThrownBy(env::execute)\n         .isInstanceOf(IllegalArgumentException.class)\n@@ -208,11 +230,19 @@ public void testRangeDistributionWithoutSortOrderUnpartitioned() throws Exceptio\n             createRangeDistributionBoundedSource(createCharRows(numOfCheckpoints, 10)),\n             ROW_TYPE_INFO);\n \n-    IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .writeParallelism(writeParallelism)\n-        .append();\n+    if (isTableSchema) {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .append();\n+    } else {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .append();\n+    }\n \n     // Range distribution requires either sort order or partition spec defined\n     assertThatThrownBy(env::execute)\n@@ -236,11 +266,19 @@ public void testRangeDistributionWithoutSortOrderPartitioned() throws Exception\n             createRangeDistributionBoundedSource(createCharRows(numOfCheckpoints, 10)),\n             ROW_TYPE_INFO);\n \n-    IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .writeParallelism(writeParallelism)\n-        .append();\n+    if (isTableSchema) {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .append();\n+    } else {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .append();\n+    }\n \n     // sort based on partition columns\n     env.execute(getClass().getSimpleName());\n@@ -274,11 +312,19 @@ public void testRangeDistributionWithNullValue() throws Exception {\n     DataStream<Row> dataStream =\n         env.addSource(createRangeDistributionBoundedSource(charRows), ROW_TYPE_INFO);\n \n-    IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .writeParallelism(parallelism)\n-        .append();\n+    if (isTableSchema) {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(parallelism)\n+          .append();\n+    } else {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(parallelism)\n+          .append();\n+    }\n \n     // sort based on partition columns\n     env.execute(getClass().getSimpleName());\n@@ -310,12 +356,23 @@ public void testRangeDistributionWithSortOrder() throws Exception {\n         env.addSource(\n             createRangeDistributionBoundedSource(createCharRows(numOfCheckpoints, 10)),\n             ROW_TYPE_INFO);\n-    IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .writeParallelism(writeParallelism)\n-        .rangeDistributionStatisticsType(StatisticsType.Map)\n-        .append();\n+\n+    if (isTableSchema) {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .rangeDistributionStatisticsType(StatisticsType.Map)\n+          .append();\n+    } else {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .rangeDistributionStatisticsType(StatisticsType.Map)\n+          .append();\n+    }\n+\n     env.execute(getClass().getSimpleName());\n \n     table.refresh();\n@@ -371,12 +428,23 @@ public void testRangeDistributionSketchWithSortOrder() throws Exception {\n         env.addSource(\n             createRangeDistributionBoundedSource(createIntRows(numOfCheckpoints, 1_000)),\n             ROW_TYPE_INFO);\n-    IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .writeParallelism(writeParallelism)\n-        .rangeDistributionStatisticsType(StatisticsType.Sketch)\n-        .append();\n+\n+    if (isTableSchema) {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .rangeDistributionStatisticsType(StatisticsType.Sketch)\n+          .append();\n+    } else {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .rangeDistributionStatisticsType(StatisticsType.Sketch)\n+          .append();\n+    }\n+\n     env.execute(getClass().getSimpleName());\n \n     table.refresh();\n@@ -440,12 +508,23 @@ public void testRangeDistributionStatisticsMigration() throws Exception {\n \n     DataStream<Row> dataStream =\n         env.addSource(createRangeDistributionBoundedSource(rowsPerCheckpoint), ROW_TYPE_INFO);\n-    IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .writeParallelism(writeParallelism)\n-        .rangeDistributionStatisticsType(StatisticsType.Auto)\n-        .append();\n+\n+    if (isTableSchema) {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .rangeDistributionStatisticsType(StatisticsType.Auto)\n+          .append();\n+    } else {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .writeParallelism(writeParallelism)\n+          .rangeDistributionStatisticsType(StatisticsType.Auto)\n+          .append();\n+    }\n+\n     env.execute(getClass().getSimpleName());\n \n     table.refresh();\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\nindex ccc620ea75c4..dc9b9db45fe2 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\n@@ -31,6 +31,7 @@\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.legacy.api.TableSchema;\n import org.apache.flink.types.Row;\n@@ -74,21 +75,39 @@ public class TestIcebergSink extends TestFlinkIcebergSinkBase {\n   @Parameter(index = 2)\n   private boolean partitioned;\n \n-  @Parameters(name = \"format={0}, parallelism={1}, partitioned={2}\")\n+  @Parameter(index = 3)\n+  private boolean isTableSchema;\n+\n+  @Parameters(name = \"format={0}, parallelism={1}, partitioned={2}, isTableSchema={3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {FileFormat.AVRO, 1, true},\n-      {FileFormat.AVRO, 1, false},\n-      {FileFormat.AVRO, 2, true},\n-      {FileFormat.AVRO, 2, false},\n-      {FileFormat.ORC, 1, true},\n-      {FileFormat.ORC, 1, false},\n-      {FileFormat.ORC, 2, true},\n-      {FileFormat.ORC, 2, false},\n-      {FileFormat.PARQUET, 1, true},\n-      {FileFormat.PARQUET, 1, false},\n-      {FileFormat.PARQUET, 2, true},\n-      {FileFormat.PARQUET, 2, false}\n+      // Remove after the deprecation of TableSchema - BEGIN\n+      {FileFormat.AVRO, 1, true, true},\n+      {FileFormat.AVRO, 1, false, true},\n+      {FileFormat.AVRO, 2, true, true},\n+      {FileFormat.AVRO, 2, false, true},\n+      {FileFormat.ORC, 1, true, true},\n+      {FileFormat.ORC, 1, false, true},\n+      {FileFormat.ORC, 2, true, true},\n+      {FileFormat.ORC, 2, false, true},\n+      {FileFormat.PARQUET, 1, true, true},\n+      {FileFormat.PARQUET, 1, false, true},\n+      {FileFormat.PARQUET, 2, true, true},\n+      {FileFormat.PARQUET, 2, false, true},\n+      // Remove after the deprecation of TableSchema - END\n+\n+      {FileFormat.AVRO, 1, true, false},\n+      {FileFormat.AVRO, 1, false, false},\n+      {FileFormat.AVRO, 2, true, false},\n+      {FileFormat.AVRO, 2, false, false},\n+      {FileFormat.ORC, 1, true, false},\n+      {FileFormat.ORC, 1, false, false},\n+      {FileFormat.ORC, 2, true, false},\n+      {FileFormat.ORC, 2, false, false},\n+      {FileFormat.PARQUET, 1, true, false},\n+      {FileFormat.PARQUET, 1, false, false},\n+      {FileFormat.PARQUET, 2, true, false},\n+      {FileFormat.PARQUET, 2, false, false},\n     };\n   }\n \n@@ -220,29 +239,54 @@ void testTwoSinksInDisjointedDAG() throws Exception {\n         env.fromCollection(leftRows, ROW_TYPE_INFO)\n             .name(\"leftCustomSource\")\n             .uid(\"leftCustomSource\");\n-    IcebergSink.forRow(leftStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(leftTable)\n-        .tableLoader(leftTableLoader)\n-        .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n-        .distributionMode(DistributionMode.NONE)\n-        .uidSuffix(\"leftIcebergSink\")\n-        .append();\n+\n+    if (isTableSchema) {\n+      IcebergSink.forRow(leftStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(leftTable)\n+          .tableLoader(leftTableLoader)\n+          .tableSchema(SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .distributionMode(DistributionMode.NONE)\n+          .uidSuffix(\"leftIcebergSink\")\n+          .append();\n+    } else {\n+      IcebergSink.forRow(leftStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(leftTable)\n+          .tableLoader(leftTableLoader)\n+          .resolvedSchema(SimpleDataUtil.FLINK_SCHEMA)\n+          .distributionMode(DistributionMode.NONE)\n+          .uidSuffix(\"leftIcebergSink\")\n+          .append();\n+    }\n \n     List<Row> rightRows = createRows(\"right-\");\n     DataStream<Row> rightStream =\n         env.fromCollection(rightRows, ROW_TYPE_INFO)\n             .name(\"rightCustomSource\")\n             .uid(\"rightCustomSource\");\n-    IcebergSink.forRow(rightStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(rightTable)\n-        .tableLoader(rightTableLoader)\n-        .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n-        .writeParallelism(parallelism)\n-        .distributionMode(DistributionMode.HASH)\n-        .uidSuffix(\"rightIcebergSink\")\n-        .setSnapshotProperty(\"flink.test\", TestIcebergSink.class.getName())\n-        .snapshotProperties(Collections.singletonMap(\"direction\", \"rightTable\"))\n-        .append();\n+\n+    if (isTableSchema) {\n+      IcebergSink.forRow(rightStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(rightTable)\n+          .tableLoader(rightTableLoader)\n+          .tableSchema(SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .writeParallelism(parallelism)\n+          .distributionMode(DistributionMode.HASH)\n+          .uidSuffix(\"rightIcebergSink\")\n+          .setSnapshotProperty(\"flink.test\", TestIcebergSink.class.getName())\n+          .snapshotProperties(Collections.singletonMap(\"direction\", \"rightTable\"))\n+          .append();\n+    } else {\n+      IcebergSink.forRow(rightStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(rightTable)\n+          .tableLoader(rightTableLoader)\n+          .resolvedSchema(SimpleDataUtil.FLINK_SCHEMA)\n+          .writeParallelism(parallelism)\n+          .distributionMode(DistributionMode.HASH)\n+          .uidSuffix(\"rightIcebergSink\")\n+          .setSnapshotProperty(\"flink.test\", TestIcebergSink.class.getName())\n+          .snapshotProperties(Collections.singletonMap(\"direction\", \"rightTable\"))\n+          .append();\n+    }\n \n     // Execute the program.\n     env.execute(\"Test Iceberg DataStream.\");\n@@ -269,12 +313,19 @@ void testOverrideWriteConfigWithUnknownFileFormat() {\n     DataStream<Row> dataStream = env.addSource(createBoundedSource(rows), ROW_TYPE_INFO);\n \n     Builder builder =\n-        IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-            .table(table)\n-            .tableLoader(tableLoader)\n-            .writeParallelism(parallelism)\n-            .setAll(newProps)\n-            .uidSuffix(\"ingestion\");\n+        isTableSchema\n+            ? IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .writeParallelism(parallelism)\n+                .setAll(newProps)\n+                .uidSuffix(\"ingestion\")\n+            : IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .writeParallelism(parallelism)\n+                .setAll(newProps)\n+                .uidSuffix(\"ingestion\");\n     assertThatThrownBy(builder::append)\n         .isInstanceOf(IllegalArgumentException.class)\n         .hasMessage(\"Invalid file format: UNRECOGNIZED\");\n@@ -305,18 +356,28 @@ void testWriteRowWithTableRefreshInterval() throws Exception {\n   }\n \n   @TestTemplate\n-  void testOperatorsUidNameNoUidSuffix() throws Exception {\n+  void testOperatorsUidNameNoUidSuffix() {\n     List<Row> rows = createRows(\"\");\n     DataStream<Row> dataStream =\n         env.addSource(createBoundedSource(rows), ROW_TYPE_INFO).uid(\"mySourceId\");\n \n-    IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n-        .writeParallelism(parallelism)\n-        .distributionMode(DistributionMode.HASH)\n-        .append();\n+    if (isTableSchema) {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .tableSchema(SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .writeParallelism(parallelism)\n+          .distributionMode(DistributionMode.HASH)\n+          .append();\n+    } else {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .resolvedSchema(SimpleDataUtil.FLINK_SCHEMA)\n+          .writeParallelism(parallelism)\n+          .distributionMode(DistributionMode.HASH)\n+          .append();\n+    }\n \n     Transformation firstTransformation = env.getTransformations().get(0);\n     Transformation secondTransformation = env.getTransformations().get(1);\n@@ -327,19 +388,30 @@ void testOperatorsUidNameNoUidSuffix() throws Exception {\n   }\n \n   @TestTemplate\n-  void testOperatorsUidNameWitUidSuffix() throws Exception {\n+  void testOperatorsUidNameWitUidSuffix() {\n     List<Row> rows = createRows(\"\");\n     DataStream<Row> dataStream =\n         env.addSource(createBoundedSource(rows), ROW_TYPE_INFO).uid(\"mySourceId\");\n \n-    IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n-        .writeParallelism(parallelism)\n-        .distributionMode(DistributionMode.HASH)\n-        .uidSuffix(\"data-ingestion\")\n-        .append();\n+    if (isTableSchema) {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .tableSchema(SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .writeParallelism(parallelism)\n+          .distributionMode(DistributionMode.HASH)\n+          .uidSuffix(\"data-ingestion\")\n+          .append();\n+    } else {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .resolvedSchema(SimpleDataUtil.FLINK_SCHEMA)\n+          .writeParallelism(parallelism)\n+          .distributionMode(DistributionMode.HASH)\n+          .uidSuffix(\"data-ingestion\")\n+          .append();\n+    }\n \n     Transformation firstTransformation = env.getTransformations().get(0);\n     Transformation secondTransformation = env.getTransformations().get(1);\n@@ -350,7 +422,7 @@ void testOperatorsUidNameWitUidSuffix() throws Exception {\n   }\n \n   @TestTemplate\n-  void testErrorOnNullForRequiredField() throws Exception {\n+  void testErrorOnNullForRequiredField() {\n     assumeThat(format)\n         .as(\"ORC file format supports null values even for required fields.\")\n         .isNotEqualTo(FileFormat.ORC);\n@@ -377,13 +449,23 @@ void testErrorOnNullForRequiredField() throws Exception {\n     DataStream<Row> dataStream =\n         env.addSource(createBoundedSource(rows), ROW_TYPE_INFO).uid(\"mySourceId\");\n \n-    TableSchema flinkSchema = FlinkSchemaUtil.toSchema(icebergSchema);\n-    IcebergSink.forRow(dataStream, flinkSchema)\n-        .table(table2)\n-        .tableLoader(TableLoader.fromCatalog(CATALOG_EXTENSION.catalogLoader(), tableIdentifier))\n-        .tableSchema(flinkSchema)\n-        .writeParallelism(parallelism)\n-        .append();\n+    if (isTableSchema) {\n+      TableSchema flinkSchema = FlinkSchemaUtil.toSchema(icebergSchema);\n+      IcebergSink.forRow(dataStream, flinkSchema)\n+          .table(table2)\n+          .tableLoader(TableLoader.fromCatalog(CATALOG_EXTENSION.catalogLoader(), tableIdentifier))\n+          .tableSchema(flinkSchema)\n+          .writeParallelism(parallelism)\n+          .append();\n+    } else {\n+      ResolvedSchema flinkSchema = FlinkSchemaUtil.toResolvedSchema(icebergSchema);\n+      IcebergSink.forRow(dataStream, flinkSchema)\n+          .table(table2)\n+          .tableLoader(TableLoader.fromCatalog(CATALOG_EXTENSION.catalogLoader(), tableIdentifier))\n+          .resolvedSchema(flinkSchema)\n+          .writeParallelism(parallelism)\n+          .append();\n+    }\n \n     assertThatThrownBy(() -> env.execute()).hasRootCauseInstanceOf(NullPointerException.class);\n   }\n@@ -395,12 +477,19 @@ void testDefaultWriteParallelism() {\n         env.addSource(createBoundedSource(rows), ROW_TYPE_INFO).uid(\"mySourceId\");\n \n     var sink =\n-        IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-            .table(table)\n-            .tableLoader(tableLoader)\n-            .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n-            .distributionMode(DistributionMode.NONE)\n-            .append();\n+        isTableSchema\n+            ? IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .tableSchema(SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+                .distributionMode(DistributionMode.NONE)\n+                .append()\n+            : IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .resolvedSchema(SimpleDataUtil.FLINK_SCHEMA)\n+                .distributionMode(DistributionMode.NONE)\n+                .append();\n \n     // since the sink write parallelism was null, it asserts that the default parallelism used was\n     // the input source parallelism.\n@@ -418,13 +507,21 @@ void testWriteParallelism() {\n         env.addSource(createBoundedSource(rows), ROW_TYPE_INFO).uid(\"mySourceId\");\n \n     var sink =\n-        IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-            .table(table)\n-            .tableLoader(tableLoader)\n-            .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n-            .distributionMode(DistributionMode.NONE)\n-            .writeParallelism(parallelism)\n-            .append();\n+        isTableSchema\n+            ? IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .tableSchema(SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+                .distributionMode(DistributionMode.NONE)\n+                .writeParallelism(parallelism)\n+                .append()\n+            : IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+                .table(table)\n+                .tableLoader(tableLoader)\n+                .resolvedSchema(SimpleDataUtil.FLINK_SCHEMA)\n+                .distributionMode(DistributionMode.NONE)\n+                .writeParallelism(parallelism)\n+                .append();\n \n     // The parallelism has been properly specified when creating the IcebergSink, so this asserts\n     // that its value is the same as the parallelism TestTemplate parameter\n@@ -433,19 +530,30 @@ void testWriteParallelism() {\n     assertThat(sink.getTransformation().getParallelism()).isEqualTo(parallelism);\n   }\n \n-  private void testWriteRow(TableSchema tableSchema, DistributionMode distributionMode)\n+  private void testWriteRow(ResolvedSchema resolvedSchema, DistributionMode distributionMode)\n       throws Exception {\n     List<Row> rows = createRows(\"\");\n     DataStream<Row> dataStream =\n         env.addSource(createBoundedSource(rows), ROW_TYPE_INFO).uid(\"mySourceId\");\n \n-    IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .tableSchema(tableSchema)\n-        .writeParallelism(parallelism)\n-        .distributionMode(distributionMode)\n-        .append();\n+    if (isTableSchema) {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .tableSchema(\n+              resolvedSchema == null ? null : TableSchema.fromResolvedSchema(resolvedSchema))\n+          .writeParallelism(parallelism)\n+          .distributionMode(distributionMode)\n+          .append();\n+    } else {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .resolvedSchema(resolvedSchema)\n+          .writeParallelism(parallelism)\n+          .distributionMode(distributionMode)\n+          .append();\n+    }\n \n     // Execute the program.\n     env.execute(\"Test Iceberg DataStream.\");\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSinkBranch.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSinkBranch.java\nindex 72d5e95ff98f..ddcb57f6ca33 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSinkBranch.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSinkBranch.java\n@@ -24,6 +24,7 @@\n import java.util.List;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.legacy.api.TableSchema;\n import org.apache.flink.types.Row;\n import org.apache.iceberg.DistributionMode;\n@@ -49,9 +50,20 @@ public class TestIcebergSinkBranch extends TestFlinkIcebergSinkBase {\n   @Parameter(index = 0)\n   private String branch;\n \n-  @Parameters(name = \"branch = {0}\")\n+  @Parameter(index = 1)\n+  private boolean isTableSchema;\n+\n+  @Parameters(name = \"branch = {0}, isTableSchema = {1}\")\n   public static Object[][] parameters() {\n-    return new Object[][] {new Object[] {\"main\"}, new Object[] {\"testBranch\"}};\n+    return new Object[][] {\n+      // Remove after the deprecation of TableSchema - BEGIN\n+      {\"main\", true},\n+      {\"testBranch\", true},\n+      // Remove after the deprecation of TableSchema - END\n+\n+      {\"main\", false},\n+      {\"testBranch\", false},\n+    };\n   }\n \n   @BeforeEach\n@@ -83,18 +95,28 @@ public void testWriteRowWithTableSchema() throws Exception {\n     verifyOtherBranchUnmodified();\n   }\n \n-  private void testWriteRow(TableSchema tableSchema, DistributionMode distributionMode)\n+  private void testWriteRow(ResolvedSchema resolvedSchema, DistributionMode distributionMode)\n       throws Exception {\n     List<Row> rows = createRows(\"\");\n     DataStream<Row> dataStream = env.addSource(createBoundedSource(rows), ROW_TYPE_INFO);\n \n-    IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .table(table)\n-        .tableLoader(tableLoader)\n-        .tableSchema(tableSchema)\n-        .toBranch(branch)\n-        .distributionMode(distributionMode)\n-        .append();\n+    if (isTableSchema) {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .tableSchema(TableSchema.fromResolvedSchema(resolvedSchema))\n+          .toBranch(branch)\n+          .distributionMode(distributionMode)\n+          .append();\n+    } else {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .table(table)\n+          .tableLoader(tableLoader)\n+          .resolvedSchema(resolvedSchema)\n+          .toBranch(branch)\n+          .distributionMode(distributionMode)\n+          .append();\n+    }\n \n     // Execute the program.\n     env.execute(\"Test Iceberg DataStream.\");\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSinkV2.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSinkV2.java\nindex 69f39fba5130..f873dcd99c06 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSinkV2.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSinkV2.java\n@@ -110,7 +110,9 @@ public void testCheckAndGetEqualityFieldIds() {\n     DataStream<Row> dataStream =\n         env.addSource(new BoundedTestSource<>(ImmutableList.of()), ROW_TYPE_INFO);\n     IcebergSink.Builder builder =\n-        IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA).table(table);\n+        isTableSchema\n+            ? IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA).table(table)\n+            : IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA).table(table);\n \n     // Use user-provided equality field column as equality field id list\n     builder.equalityFieldColumns(Lists.newArrayList(\"id\"));\n@@ -158,15 +160,21 @@ public void testChangeLogOnSameKey() throws Exception {\n   }\n \n   @TestTemplate\n-  public void testUpsertModeCheck() throws Exception {\n+  public void testUpsertModeCheck() {\n     DataStream<Row> dataStream =\n         env.addSource(new BoundedTestSource<>(ImmutableList.of()), ROW_TYPE_INFO);\n     IcebergSink.Builder builder =\n-        IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-            .tableLoader(tableLoader)\n-            .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n-            .writeParallelism(parallelism)\n-            .upsert(true);\n+        isTableSchema\n+            ? IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+                .tableLoader(tableLoader)\n+                .tableSchema(SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+                .writeParallelism(parallelism)\n+                .upsert(true)\n+            : IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+                .tableLoader(tableLoader)\n+                .resolvedSchema(SimpleDataUtil.FLINK_SCHEMA)\n+                .writeParallelism(parallelism)\n+                .upsert(true);\n \n     assertThatThrownBy(\n             () ->\n@@ -238,15 +246,27 @@ protected void testChangeLogs(\n     DataStream<Row> dataStream =\n         env.addSource(new BoundedTestSource<>(elementsPerCheckpoint), ROW_TYPE_INFO);\n \n-    IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n-        .tableLoader(tableLoader)\n-        .tableSchema(SimpleDataUtil.FLINK_SCHEMA)\n-        .writeParallelism(parallelism)\n-        .equalityFieldColumns(equalityFieldColumns)\n-        .upsert(insertAsUpsert)\n-        .toBranch(branch)\n-        .uidSuffix(\"sink\")\n-        .append();\n+    if (isTableSchema) {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .tableLoader(tableLoader)\n+          .tableSchema(SimpleDataUtil.FLINK_TABLE_SCHEMA)\n+          .writeParallelism(parallelism)\n+          .equalityFieldColumns(equalityFieldColumns)\n+          .upsert(insertAsUpsert)\n+          .toBranch(branch)\n+          .uidSuffix(\"sink\")\n+          .append();\n+    } else {\n+      IcebergSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n+          .tableLoader(tableLoader)\n+          .resolvedSchema(SimpleDataUtil.FLINK_SCHEMA)\n+          .writeParallelism(parallelism)\n+          .equalityFieldColumns(equalityFieldColumns)\n+          .upsert(insertAsUpsert)\n+          .toBranch(branch)\n+          .uidSuffix(\"sink\")\n+          .append();\n+    }\n \n     // Execute the program.\n     env.execute(\"Test Iceberg Change-Log DataStream.\");\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergStreamWriter.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergStreamWriter.java\nindex 2e6531573d83..7f4f7758e519 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergStreamWriter.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergStreamWriter.java\n@@ -32,9 +32,10 @@\n import org.apache.flink.streaming.api.operators.BoundedOneInput;\n import org.apache.flink.streaming.util.OneInputStreamOperatorTestHarness;\n import org.apache.flink.table.api.DataTypes;\n+import org.apache.flink.table.catalog.Column;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.data.GenericRowData;\n import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.legacy.api.TableSchema;\n import org.apache.flink.table.types.logical.RowType;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n@@ -321,12 +322,11 @@ public void testPromotedFlinkDataType() throws Exception {\n             Types.NestedField.required(1, \"tinyint\", Types.IntegerType.get()),\n             Types.NestedField.required(2, \"smallint\", Types.IntegerType.get()),\n             Types.NestedField.optional(3, \"int\", Types.IntegerType.get()));\n-    TableSchema flinkSchema =\n-        TableSchema.builder()\n-            .field(\"tinyint\", DataTypes.TINYINT().notNull())\n-            .field(\"smallint\", DataTypes.SMALLINT().notNull())\n-            .field(\"int\", DataTypes.INT().nullable())\n-            .build();\n+    ResolvedSchema flinkSchema =\n+        ResolvedSchema.of(\n+            Column.physical(\"tinyint\", DataTypes.TINYINT().notNull()),\n+            Column.physical(\"smallint\", DataTypes.SMALLINT().notNull()),\n+            Column.physical(\"int\", DataTypes.INT().nullable()));\n \n     PartitionSpec spec;\n     if (partitioned) {\n@@ -390,7 +390,7 @@ private OneInputStreamOperatorTestHarness<RowData, FlinkWriteResult> createIcebe\n   }\n \n   private OneInputStreamOperatorTestHarness<RowData, FlinkWriteResult> createIcebergStreamWriter(\n-      Table icebergTable, TableSchema flinkSchema) throws Exception {\n+      Table icebergTable, ResolvedSchema flinkSchema) throws Exception {\n     RowType flinkRowType = FlinkSink.toFlinkRowType(icebergTable.schema(), flinkSchema);\n     FlinkWriteConf flinkWriteConfig =\n         new FlinkWriteConf(\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestTaskWriters.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestTaskWriters.java\nindex a01926783d05..6b7b0d4c35a3 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestTaskWriters.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestTaskWriters.java\n@@ -26,7 +26,6 @@\n import java.util.List;\n import java.util.Map;\n import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.types.logical.RowType;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n@@ -230,7 +229,7 @@ private TaskWriter<RowData> createTaskWriter(long targetFileSize) {\n     TaskWriterFactory<RowData> taskWriterFactory =\n         new RowDataTaskWriterFactory(\n             SerializableTable.copyOf(table),\n-            (RowType) SimpleDataUtil.FLINK_SCHEMA.toRowDataType().getLogicalType(),\n+            SimpleDataUtil.ROW_TYPE,\n             targetFileSize,\n             format,\n             table.properties(),\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/dynamic/TestDynamicIcebergSink.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/dynamic/TestDynamicIcebergSink.java\nindex fb077ffff508..4ee4ede46d79 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/dynamic/TestDynamicIcebergSink.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/dynamic/TestDynamicIcebergSink.java\n@@ -42,9 +42,10 @@\n import org.apache.flink.runtime.client.JobExecutionException;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.data.util.DataFormatConverters;\n-import org.apache.flink.table.legacy.api.TableSchema;\n+import org.apache.flink.table.types.DataType;\n import org.apache.flink.table.types.logical.RowType;\n import org.apache.flink.types.Row;\n import org.apache.flink.util.Collector;\n@@ -200,8 +201,9 @@ public void generate(DynamicIcebergDataImpl row, Collector<DynamicRecord> out) {\n \n   private static DataFormatConverters.RowConverter converter(Schema schema) {\n     RowType rowType = FlinkSchemaUtil.convert(schema);\n-    TableSchema tableSchema = FlinkSchemaUtil.toSchema(rowType);\n-    return new DataFormatConverters.RowConverter(tableSchema.getFieldDataTypes());\n+    ResolvedSchema resolvedSchema = FlinkSchemaUtil.toResolvedSchema(rowType);\n+    return new DataFormatConverters.RowConverter(\n+        resolvedSchema.getColumnDataTypes().toArray(DataType[]::new));\n   }\n \n   @Test\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/BoundedTableFactory.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/BoundedTableFactory.java\nindex 7e9e1982991d..134858f5055e 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/BoundedTableFactory.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/BoundedTableFactory.java\n@@ -23,7 +23,9 @@\n import java.util.Set;\n import java.util.concurrent.atomic.AtomicInteger;\n import java.util.function.Supplier;\n+import java.util.stream.Collectors;\n import java.util.stream.Stream;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.java.typeutils.RowTypeInfo;\n import org.apache.flink.configuration.ConfigOption;\n import org.apache.flink.configuration.ConfigOptions;\n@@ -31,6 +33,8 @@\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.streaming.api.functions.source.legacy.SourceFunction;\n+import org.apache.flink.table.catalog.Column;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.connector.ChangelogMode;\n import org.apache.flink.table.connector.ProviderContext;\n import org.apache.flink.table.connector.source.DataStreamScanProvider;\n@@ -39,9 +43,9 @@\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.data.util.DataFormatConverters;\n import org.apache.flink.table.factories.DynamicTableSourceFactory;\n-import org.apache.flink.table.legacy.api.TableSchema;\n+import org.apache.flink.table.runtime.typeutils.ExternalTypeInfo;\n+import org.apache.flink.table.types.DataType;\n import org.apache.flink.table.types.logical.RowType;\n-import org.apache.flink.table.utils.TableSchemaUtils;\n import org.apache.flink.types.Row;\n import org.apache.flink.types.RowKind;\n import org.apache.iceberg.flink.util.FlinkCompatibilityUtil;\n@@ -68,15 +72,18 @@ public static void clearDataSets() {\n \n   @Override\n   public DynamicTableSource createDynamicTableSource(Context context) {\n-    TableSchema tableSchema =\n-        TableSchemaUtils.getPhysicalSchema(context.getCatalogTable().getSchema());\n+    ResolvedSchema resolvedSchema =\n+        ResolvedSchema.of(\n+            context.getCatalogTable().getResolvedSchema().getColumns().stream()\n+                .filter(Column::isPhysical)\n+                .collect(Collectors.toList()));\n \n     Configuration configuration = Configuration.fromMap(context.getCatalogTable().getOptions());\n     String dataId = configuration.get(DATA_ID);\n     Preconditions.checkArgument(\n         DATA_SETS.containsKey(dataId), \"data-id %s does not found in registered data set.\", dataId);\n \n-    return new BoundedTableSource(DATA_SETS.get(dataId), tableSchema);\n+    return new BoundedTableSource(DATA_SETS.get(dataId), resolvedSchema);\n   }\n \n   @Override\n@@ -97,16 +104,17 @@ public Set<ConfigOption<?>> optionalOptions() {\n   private static class BoundedTableSource implements ScanTableSource {\n \n     private final List<List<Row>> elementsPerCheckpoint;\n-    private final TableSchema tableSchema;\n+    private final ResolvedSchema resolvedSchema;\n \n-    private BoundedTableSource(List<List<Row>> elementsPerCheckpoint, TableSchema tableSchema) {\n+    private BoundedTableSource(\n+        List<List<Row>> elementsPerCheckpoint, ResolvedSchema resolvedSchema) {\n       this.elementsPerCheckpoint = elementsPerCheckpoint;\n-      this.tableSchema = tableSchema;\n+      this.resolvedSchema = resolvedSchema;\n     }\n \n     private BoundedTableSource(BoundedTableSource toCopy) {\n       this.elementsPerCheckpoint = toCopy.elementsPerCheckpoint;\n-      this.tableSchema = toCopy.tableSchema;\n+      this.resolvedSchema = toCopy.resolvedSchema;\n     }\n \n     @Override\n@@ -141,12 +149,18 @@ public DataStream<RowData> produceDataStream(\n           SourceFunction<Row> source =\n               new BoundedTestSource<>(elementsPerCheckpoint, checkpointEnabled);\n \n-          RowType rowType = (RowType) tableSchema.toRowDataType().getLogicalType();\n+          RowType rowType = (RowType) resolvedSchema.toSourceRowDataType().getLogicalType();\n           // Converter to convert the Row to RowData.\n           DataFormatConverters.RowConverter rowConverter =\n-              new DataFormatConverters.RowConverter(tableSchema.getFieldDataTypes());\n-\n-          return env.addSource(source, new RowTypeInfo(tableSchema.getFieldTypes()))\n+              new DataFormatConverters.RowConverter(\n+                  resolvedSchema.getColumnDataTypes().toArray(DataType[]::new));\n+\n+          return env.addSource(\n+                  source,\n+                  new RowTypeInfo(\n+                      resolvedSchema.getColumnDataTypes().stream()\n+                          .map(ExternalTypeInfo::of)\n+                          .toArray(TypeInformation[]::new)))\n               .map(rowConverter::toInternal, FlinkCompatibilityUtil.toTypeInfo(rowType));\n         }\n \n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceBounded.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceBounded.java\nindex 44bc31dd64ee..8c1e53e15f15 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceBounded.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceBounded.java\n@@ -21,15 +21,18 @@\n import static org.apache.iceberg.flink.SimpleDataUtil.SCHEMA;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import java.util.Arrays;\n import java.util.Collections;\n import java.util.List;\n import java.util.Map;\n+import java.util.Optional;\n+import java.util.stream.Collectors;\n import org.apache.flink.configuration.Configuration;\n import org.apache.flink.streaming.api.datastream.DataStream;\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n+import org.apache.flink.table.catalog.Column;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n import org.apache.flink.table.data.RowData;\n-import org.apache.flink.table.legacy.api.TableColumn;\n-import org.apache.flink.table.legacy.api.TableSchema;\n import org.apache.flink.types.Row;\n import org.apache.flink.util.CloseableIterator;\n import org.apache.iceberg.Schema;\n@@ -65,17 +68,23 @@ public void testValidation() {\n \n   @Override\n   protected List<Row> runWithProjection(String... projected) throws Exception {\n+    // Convert Iceberg schema to Flink schema\n     Schema icebergTableSchema =\n         CATALOG_EXTENSION.catalog().loadTable(TestFixtures.TABLE_IDENTIFIER).schema();\n-    TableSchema.Builder builder = TableSchema.builder();\n-    TableSchema schema = FlinkSchemaUtil.toSchema(FlinkSchemaUtil.convert(icebergTableSchema));\n-    for (String field : projected) {\n-      TableColumn column = schema.getTableColumn(field).get();\n-      builder.field(column.getName(), column.getType());\n-    }\n-    TableSchema flinkSchema = builder.build();\n-    Schema projectedSchema = FlinkSchemaUtil.convert(icebergTableSchema, flinkSchema);\n-    return run(projectedSchema, Lists.newArrayList(), Maps.newHashMap(), \"\", projected);\n+    ResolvedSchema fullFlinkSchema = FlinkSchemaUtil.toResolvedSchema(icebergTableSchema);\n+\n+    // Projection\n+    List<Column> projectedColumns =\n+        Arrays.stream(projected)\n+            .map(fullFlinkSchema::getColumn)\n+            .flatMap(Optional::stream)\n+            .collect(Collectors.toList());\n+\n+    // Convert back to Iceberg schema\n+    ResolvedSchema projectedFlinkSchema = ResolvedSchema.of(projectedColumns);\n+    Schema projectedIcebergSchema =\n+        FlinkSchemaUtil.convert(icebergTableSchema, projectedFlinkSchema);\n+    return run(projectedIcebergSchema, Lists.newArrayList(), Maps.newHashMap(), \"\", projected);\n   }\n \n   @Override\n@@ -125,7 +134,7 @@ protected List<Row> run(\n     }\n \n     sourceBuilder.filters(filters);\n-    sourceBuilder.properties(options);\n+    sourceBuilder.setAll(options);\n \n     DataStream<Row> stream =\n         sourceBuilder\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceBoundedRow.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceBoundedRow.java\nindex 02102a870db9..13087bc0a06a 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceBoundedRow.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceBoundedRow.java\n@@ -21,7 +21,8 @@\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.java.typeutils.RowTypeInfo;\n import org.apache.flink.streaming.api.datastream.DataStream;\n-import org.apache.flink.table.legacy.api.TableSchema;\n+import org.apache.flink.table.catalog.ResolvedSchema;\n+import org.apache.flink.table.runtime.typeutils.ExternalTypeInfo;\n import org.apache.flink.types.Row;\n import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Schema;\n@@ -41,8 +42,13 @@ protected RowDataConverter<Row> getConverter(Schema icebergSchema, Table table)\n \n   @Override\n   protected TypeInformation<Row> getTypeInfo(Schema icebergSchema) {\n-    TableSchema tableSchema = FlinkSchemaUtil.toSchema(icebergSchema);\n-    return new RowTypeInfo(tableSchema.getFieldTypes(), tableSchema.getFieldNames());\n+    ResolvedSchema resolvedSchema = FlinkSchemaUtil.toResolvedSchema(icebergSchema);\n+    TypeInformation<?>[] types =\n+        resolvedSchema.getColumnDataTypes().stream()\n+            .map(ExternalTypeInfo::of)\n+            .toArray(TypeInformation[]::new);\n+    String[] fieldNames = resolvedSchema.getColumnNames().toArray(String[]::new);\n+    return new RowTypeInfo(types, fieldNames);\n   }\n \n   @Override\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13044",
    "pr_id": 13044,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 20,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestHelperBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestChangelogIterator.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkFilters.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkSchemaUtil.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkTableUtil.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkV2Filters.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkValueConverter.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/SparkTestHelperBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkFilters.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkSchemaUtil.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkTableUtil.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkV2Filters.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestHelperBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestChangelogIterator.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkFilters.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkSchemaUtil.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkTableUtil.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkV2Filters.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkValueConverter.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/SparkTestHelperBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkFilters.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkSchemaUtil.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkTableUtil.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkV2Filters.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java"
    ],
    "base_commit": "d2124f2771e6e200ef50d98316bb2b01f46ed29a",
    "head_commit": "66f329dde964579f1f02d35176489b52c41dd057",
    "repo_url": "https://github.com/apache/iceberg/pull/13044",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13044",
    "dockerfile": "",
    "pr_merged_at": "2025-05-14T08:00:07.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestHelperBase.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestHelperBase.java\nindex 97484702cad6..022702d85378 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestHelperBase.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestHelperBase.java\n@@ -18,11 +18,12 @@\n  */\n package org.apache.iceberg.spark;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import java.util.List;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n import org.apache.spark.sql.Row;\n-import org.junit.Assert;\n \n public class SparkTestHelperBase {\n   protected static final Object ANY = new Object();\n@@ -55,12 +56,13 @@ private Object[] toJava(Row row) {\n \n   protected void assertEquals(\n       String context, List<Object[]> expectedRows, List<Object[]> actualRows) {\n-    Assert.assertEquals(\n-        context + \": number of results should match\", expectedRows.size(), actualRows.size());\n+    assertThat(actualRows)\n+        .as(\"%s: number of results should match\", context)\n+        .hasSameSizeAs(expectedRows);\n     for (int row = 0; row < expectedRows.size(); row += 1) {\n       Object[] expected = expectedRows.get(row);\n       Object[] actual = actualRows.get(row);\n-      Assert.assertEquals(\"Number of columns should match\", expected.length, actual.length);\n+      assertThat(actual).as(\"Number of columns should match\").hasSameSizeAs(expected);\n       for (int col = 0; col < actualRows.get(row).length; col += 1) {\n         String newContext = String.format(\"%s: row %d col %d\", context, row + 1, col + 1);\n         assertEquals(newContext, expected, actual);\n@@ -69,19 +71,19 @@ protected void assertEquals(\n   }\n \n   protected void assertEquals(String context, Object[] expectedRow, Object[] actualRow) {\n-    Assert.assertEquals(\"Number of columns should match\", expectedRow.length, actualRow.length);\n+    assertThat(actualRow).as(\"Number of columns should match\").hasSameSizeAs(expectedRow);\n     for (int col = 0; col < actualRow.length; col += 1) {\n       Object expectedValue = expectedRow[col];\n       Object actualValue = actualRow[col];\n       if (expectedValue != null && expectedValue.getClass().isArray()) {\n         String newContext = String.format(\"%s (nested col %d)\", context, col + 1);\n         if (expectedValue instanceof byte[]) {\n-          Assert.assertArrayEquals(newContext, (byte[]) expectedValue, (byte[]) actualValue);\n+          assertThat(actualValue).as(newContext).isEqualTo(expectedValue);\n         } else {\n           assertEquals(newContext, (Object[]) expectedValue, (Object[]) actualValue);\n         }\n       } else if (expectedValue != ANY) {\n-        Assert.assertEquals(context + \" contents should match\", expectedValue, actualValue);\n+        assertThat(actualValue).as(\"%s contents should match\", context).isEqualTo(expectedValue);\n       }\n     }\n   }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestChangelogIterator.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestChangelogIterator.java\nindex 0539598f147e..bd9832f7d674 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestChangelogIterator.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestChangelogIterator.java\n@@ -18,7 +18,8 @@\n  */\n package org.apache.iceberg.spark;\n \n-import static org.junit.Assert.assertThrows;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.Arrays;\n import java.util.Collections;\n@@ -33,8 +34,7 @@\n import org.apache.spark.sql.types.Metadata;\n import org.apache.spark.sql.types.StructField;\n import org.apache.spark.sql.types.StructType;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n public class TestChangelogIterator extends SparkTestHelperBase {\n   private static final String DELETE = ChangelogOperation.DELETE.name();\n@@ -78,7 +78,7 @@ public void testIterator() {\n         Arrays.asList(RowType.DELETED, RowType.INSERTED, RowType.CARRY_OVER, RowType.UPDATED),\n         0,\n         permutations);\n-    Assert.assertEquals(24, permutations.size());\n+    assertThat(permutations).hasSize(24);\n \n     for (Object[] permutation : permutations) {\n       validate(permutation);\n@@ -196,10 +196,10 @@ public void testUpdatedRowsWithDuplication() {\n     Iterator<Row> iterator =\n         ChangelogIterator.computeUpdates(rowsWithDuplication.iterator(), SCHEMA, IDENTIFIER_FIELDS);\n \n-    assertThrows(\n-        \"Cannot compute updates because there are multiple rows with the same identifier fields([id, name]). Please make sure the rows are unique.\",\n-        IllegalStateException.class,\n-        () -> Lists.newArrayList(iterator));\n+    assertThatThrownBy(() -> Lists.newArrayList(iterator))\n+        .isInstanceOf(IllegalStateException.class)\n+        .hasMessage(\n+            \"Cannot compute updates because there are multiple rows with the same identifier fields([id,name]). Please make sure the rows are unique.\");\n \n     // still allow extra insert rows\n     rowsWithDuplication =\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkFilters.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkFilters.java\nindex b1f8cf61a755..bc3f407d09fa 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkFilters.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkFilters.java\n@@ -18,6 +18,8 @@\n  */\n package org.apache.iceberg.spark;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import java.sql.Date;\n import java.sql.Timestamp;\n import java.time.Instant;\n@@ -40,8 +42,7 @@\n import org.apache.spark.sql.sources.LessThan;\n import org.apache.spark.sql.sources.LessThanOrEqual;\n import org.apache.spark.sql.sources.Not;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n public class TestSparkFilters {\n \n@@ -59,54 +60,68 @@ public void testQuotedAttributes() {\n           IsNull isNull = IsNull.apply(quoted);\n           Expression expectedIsNull = Expressions.isNull(unquoted);\n           Expression actualIsNull = SparkFilters.convert(isNull);\n-          Assert.assertEquals(\n-              \"IsNull must match\", expectedIsNull.toString(), actualIsNull.toString());\n+          assertThat(actualIsNull)\n+              .asString()\n+              .as(\"IsNull must match\")\n+              .isEqualTo(expectedIsNull.toString());\n \n           IsNotNull isNotNull = IsNotNull.apply(quoted);\n           Expression expectedIsNotNull = Expressions.notNull(unquoted);\n           Expression actualIsNotNull = SparkFilters.convert(isNotNull);\n-          Assert.assertEquals(\n-              \"IsNotNull must match\", expectedIsNotNull.toString(), actualIsNotNull.toString());\n+          assertThat(actualIsNotNull)\n+              .asString()\n+              .as(\"IsNotNull must match\")\n+              .isEqualTo(expectedIsNotNull.toString());\n \n           LessThan lt = LessThan.apply(quoted, 1);\n           Expression expectedLt = Expressions.lessThan(unquoted, 1);\n           Expression actualLt = SparkFilters.convert(lt);\n-          Assert.assertEquals(\"LessThan must match\", expectedLt.toString(), actualLt.toString());\n+          assertThat(actualLt)\n+              .asString()\n+              .as(\"LessThan must match\")\n+              .isEqualTo(expectedLt.toString());\n \n           LessThanOrEqual ltEq = LessThanOrEqual.apply(quoted, 1);\n           Expression expectedLtEq = Expressions.lessThanOrEqual(unquoted, 1);\n           Expression actualLtEq = SparkFilters.convert(ltEq);\n-          Assert.assertEquals(\n-              \"LessThanOrEqual must match\", expectedLtEq.toString(), actualLtEq.toString());\n+          assertThat(actualLtEq)\n+              .asString()\n+              .as(\"LessThanOrEqual must match\")\n+              .isEqualTo(expectedLtEq.toString());\n \n           GreaterThan gt = GreaterThan.apply(quoted, 1);\n           Expression expectedGt = Expressions.greaterThan(unquoted, 1);\n           Expression actualGt = SparkFilters.convert(gt);\n-          Assert.assertEquals(\"GreaterThan must match\", expectedGt.toString(), actualGt.toString());\n+          assertThat(actualGt)\n+              .asString()\n+              .as(\"GreaterThan must match\")\n+              .isEqualTo(expectedGt.toString());\n \n           GreaterThanOrEqual gtEq = GreaterThanOrEqual.apply(quoted, 1);\n           Expression expectedGtEq = Expressions.greaterThanOrEqual(unquoted, 1);\n           Expression actualGtEq = SparkFilters.convert(gtEq);\n-          Assert.assertEquals(\n-              \"GreaterThanOrEqual must match\", expectedGtEq.toString(), actualGtEq.toString());\n+          assertThat(actualGtEq)\n+              .asString()\n+              .as(\"GreaterThanOrEqual must match\")\n+              .isEqualTo(expectedGtEq.toString());\n \n           EqualTo eq = EqualTo.apply(quoted, 1);\n           Expression expectedEq = Expressions.equal(unquoted, 1);\n           Expression actualEq = SparkFilters.convert(eq);\n-          Assert.assertEquals(\"EqualTo must match\", expectedEq.toString(), actualEq.toString());\n+          assertThat(actualEq).asString().as(\"EqualTo must match\").isEqualTo(expectedEq.toString());\n \n           EqualNullSafe eqNullSafe = EqualNullSafe.apply(quoted, 1);\n           Expression expectedEqNullSafe = Expressions.equal(unquoted, 1);\n           Expression actualEqNullSafe = SparkFilters.convert(eqNullSafe);\n-          Assert.assertEquals(\n-              \"EqualNullSafe must match\",\n-              expectedEqNullSafe.toString(),\n-              actualEqNullSafe.toString());\n+          assertThat(actualEqNullSafe)\n+              .asString()\n+              .as(\"EqualNullSafe must match\")\n+              .isEqualTo(expectedEqNullSafe.toString());\n \n           In in = In.apply(quoted, new Integer[] {1});\n           Expression expectedIn = Expressions.in(unquoted, 1);\n           Expression actualIn = SparkFilters.convert(in);\n-          Assert.assertEquals(\"In must match\", expectedIn.toString(), actualIn.toString());\n+          assertThat(actualIn).asString().as(\"In must match\").isEqualTo(expectedIn.toString());\n         });\n   }\n \n@@ -120,14 +135,14 @@ public void testTimestampFilterConversion() {\n     Expression timestampExpression = SparkFilters.convert(GreaterThan.apply(\"x\", timestamp));\n     Expression rawExpression = Expressions.greaterThan(\"x\", epochMicros);\n \n-    Assert.assertEquals(\n-        \"Generated Timestamp expression should be correct\",\n-        rawExpression.toString(),\n-        timestampExpression.toString());\n-    Assert.assertEquals(\n-        \"Generated Instant expression should be correct\",\n-        rawExpression.toString(),\n-        instantExpression.toString());\n+    assertThat(timestampExpression)\n+        .asString()\n+        .as(\"Generated Timestamp expression should be correct\")\n+        .isEqualTo(rawExpression.toString());\n+    assertThat(instantExpression)\n+        .asString()\n+        .as(\"Generated Instant expression should be correct\")\n+        .isEqualTo(rawExpression.toString());\n   }\n \n   @Test\n@@ -139,10 +154,10 @@ public void testLocalDateTimeFilterConversion() {\n     Expression instantExpression = SparkFilters.convert(GreaterThan.apply(\"x\", ldt));\n     Expression rawExpression = Expressions.greaterThan(\"x\", epochMicros);\n \n-    Assert.assertEquals(\n-        \"Generated Instant expression should be correct\",\n-        rawExpression.toString(),\n-        instantExpression.toString());\n+    assertThat(instantExpression)\n+        .asString()\n+        .as(\"Generated Instant expression should be correct\")\n+        .isEqualTo(rawExpression.toString());\n   }\n \n   @Test\n@@ -155,15 +170,15 @@ public void testDateFilterConversion() {\n     Expression dateExpression = SparkFilters.convert(GreaterThan.apply(\"x\", date));\n     Expression rawExpression = Expressions.greaterThan(\"x\", epochDay);\n \n-    Assert.assertEquals(\n-        \"Generated localdate expression should be correct\",\n-        rawExpression.toString(),\n-        localDateExpression.toString());\n+    assertThat(localDateExpression)\n+        .asString()\n+        .as(\"Generated localdate expression should be correct\")\n+        .isEqualTo(rawExpression.toString());\n \n-    Assert.assertEquals(\n-        \"Generated date expression should be correct\",\n-        rawExpression.toString(),\n-        dateExpression.toString());\n+    assertThat(dateExpression)\n+        .asString()\n+        .as(\"Generated date expression should be correct\")\n+        .isEqualTo(rawExpression.toString());\n   }\n \n   @Test\n@@ -171,7 +186,7 @@ public void testNestedInInsideNot() {\n     Not filter =\n         Not.apply(And.apply(EqualTo.apply(\"col1\", 1), In.apply(\"col2\", new Integer[] {1, 2})));\n     Expression converted = SparkFilters.convert(filter);\n-    Assert.assertNull(\"Expression should not be converted\", converted);\n+    assertThat(converted).as(\"Expression should not be converted\").isNull();\n   }\n \n   @Test\n@@ -180,6 +195,6 @@ public void testNotIn() {\n     Expression actual = SparkFilters.convert(filter);\n     Expression expected =\n         Expressions.and(Expressions.notNull(\"col\"), Expressions.notIn(\"col\", 1, 2));\n-    Assert.assertEquals(\"Expressions should match\", expected.toString(), actual.toString());\n+    assertThat(actual).asString().as(\"Expressions should match\").isEqualTo(expected.toString());\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkSchemaUtil.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkSchemaUtil.java\nindex 259f7c3dd789..9b5b207a5b6b 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkSchemaUtil.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkSchemaUtil.java\n@@ -19,8 +19,8 @@\n package org.apache.iceberg.spark;\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n-import java.io.IOException;\n import java.util.List;\n import org.apache.iceberg.MetadataColumns;\n import org.apache.iceberg.Schema;\n@@ -28,8 +28,7 @@\n import org.apache.spark.sql.catalyst.expressions.AttributeReference;\n import org.apache.spark.sql.catalyst.expressions.MetadataAttribute;\n import org.apache.spark.sql.types.StructType;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n public class TestSparkSchemaUtil {\n   private static final Schema TEST_SCHEMA =\n@@ -44,24 +43,23 @@ public class TestSparkSchemaUtil {\n           MetadataColumns.ROW_POSITION);\n \n   @Test\n-  public void testEstimateSizeMaxValue() throws IOException {\n-    Assert.assertEquals(\n-        \"estimateSize returns Long max value\",\n-        Long.MAX_VALUE,\n-        SparkSchemaUtil.estimateSize(null, Long.MAX_VALUE));\n+  public void testEstimateSizeMaxValue() {\n+    assertThat(SparkSchemaUtil.estimateSize(null, Long.MAX_VALUE))\n+        .as(\"estimateSize returns Long max value\")\n+        .isEqualTo(Long.MAX_VALUE);\n   }\n \n   @Test\n-  public void testEstimateSizeWithOverflow() throws IOException {\n+  public void testEstimateSizeWithOverflow() {\n     long tableSize =\n         SparkSchemaUtil.estimateSize(SparkSchemaUtil.convert(TEST_SCHEMA), Long.MAX_VALUE - 1);\n-    Assert.assertEquals(\"estimateSize handles overflow\", Long.MAX_VALUE, tableSize);\n+    assertThat(tableSize).as(\"estimateSize handles overflow\").isEqualTo(Long.MAX_VALUE);\n   }\n \n   @Test\n-  public void testEstimateSize() throws IOException {\n+  public void testEstimateSize() {\n     long tableSize = SparkSchemaUtil.estimateSize(SparkSchemaUtil.convert(TEST_SCHEMA), 1);\n-    Assert.assertEquals(\"estimateSize matches with expected approximation\", 24, tableSize);\n+    assertThat(tableSize).as(\"estimateSize matches with expected approximation\").isEqualTo(24);\n   }\n \n   @Test\n@@ -71,13 +69,13 @@ public void testSchemaConversionWithMetaDataColumnSchema() {\n         scala.collection.JavaConverters.seqAsJavaList(structType.toAttributes());\n     for (AttributeReference attrRef : attrRefs) {\n       if (MetadataColumns.isMetadataColumn(attrRef.name())) {\n-        Assert.assertTrue(\n-            \"metadata columns should have __metadata_col in attribute metadata\",\n-            MetadataAttribute.unapply(attrRef).isDefined());\n+        assertThat(MetadataAttribute.unapply(attrRef).isDefined())\n+            .as(\"metadata columns should have __metadata_col in attribute metadata\")\n+            .isTrue();\n       } else {\n-        Assert.assertFalse(\n-            \"non metadata columns should not have __metadata_col in attribute metadata\",\n-            MetadataAttribute.unapply(attrRef).isDefined());\n+        assertThat(MetadataAttribute.unapply(attrRef).isDefined())\n+            .as(\"non metadata columns should not have __metadata_col in attribute metadata\")\n+            .isFalse();\n       }\n     }\n   }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkTableUtil.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkTableUtil.java\nindex 51c8d9643c11..93e4c8968715 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkTableUtil.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkTableUtil.java\n@@ -29,8 +29,7 @@\n import org.apache.iceberg.TestHelpers;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.spark.SparkTableUtil.SparkPartition;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n public class TestSparkTableUtil {\n   @Test\n@@ -69,13 +68,15 @@ public void testMetricsConfigKryoSerialization() throws Exception {\n     MetricsConfig config = MetricsConfig.fromProperties(metricsConfig);\n     MetricsConfig deserialized = KryoHelpers.roundTripSerialize(config);\n \n-    Assert.assertEquals(\n-        MetricsModes.Full.get().toString(), deserialized.columnMode(\"col1\").toString());\n-    Assert.assertEquals(\n-        MetricsModes.Truncate.withLength(16).toString(),\n-        deserialized.columnMode(\"col2\").toString());\n-    Assert.assertEquals(\n-        MetricsModes.Counts.get().toString(), deserialized.columnMode(\"col3\").toString());\n+    assertThat(deserialized.columnMode(\"col1\"))\n+        .asString()\n+        .isEqualTo(MetricsModes.Full.get().toString());\n+    assertThat(deserialized.columnMode(\"col2\"))\n+        .asString()\n+        .isEqualTo(MetricsModes.Truncate.withLength(16).toString());\n+    assertThat(deserialized.columnMode(\"col3\"))\n+        .asString()\n+        .isEqualTo(MetricsModes.Counts.get().toString());\n   }\n \n   @Test\n@@ -92,12 +93,14 @@ public void testMetricsConfigJavaSerialization() throws Exception {\n     MetricsConfig config = MetricsConfig.fromProperties(metricsConfig);\n     MetricsConfig deserialized = TestHelpers.roundTripSerialize(config);\n \n-    Assert.assertEquals(\n-        MetricsModes.Full.get().toString(), deserialized.columnMode(\"col1\").toString());\n-    Assert.assertEquals(\n-        MetricsModes.Truncate.withLength(16).toString(),\n-        deserialized.columnMode(\"col2\").toString());\n-    Assert.assertEquals(\n-        MetricsModes.Counts.get().toString(), deserialized.columnMode(\"col3\").toString());\n+    assertThat(deserialized.columnMode(\"col1\"))\n+        .asString()\n+        .isEqualTo(MetricsModes.Full.get().toString());\n+    assertThat(deserialized.columnMode(\"col2\"))\n+        .asString()\n+        .isEqualTo(MetricsModes.Truncate.withLength(16).toString());\n+    assertThat(deserialized.columnMode(\"col3\"))\n+        .asString()\n+        .isEqualTo(MetricsModes.Counts.get().toString());\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkV2Filters.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkV2Filters.java\nindex eae8a6ec02da..e0b590e5a6e8 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkV2Filters.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkV2Filters.java\n@@ -52,8 +52,7 @@\n import org.apache.spark.sql.types.DataTypes;\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.unsafe.types.UTF8String;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n public class TestSparkV2Filters {\n \n@@ -90,98 +89,130 @@ public void testV2Filters() {\n           Predicate isNull = new Predicate(\"IS_NULL\", attrOnly);\n           Expression expectedIsNull = Expressions.isNull(unquoted);\n           Expression actualIsNull = SparkV2Filters.convert(isNull);\n-          Assert.assertEquals(\n-              \"IsNull must match\", expectedIsNull.toString(), actualIsNull.toString());\n+          assertThat(actualIsNull)\n+              .asString()\n+              .as(\"IsNull must match\")\n+              .isEqualTo(expectedIsNull.toString());\n \n           Predicate isNotNull = new Predicate(\"IS_NOT_NULL\", attrOnly);\n           Expression expectedIsNotNull = Expressions.notNull(unquoted);\n           Expression actualIsNotNull = SparkV2Filters.convert(isNotNull);\n-          Assert.assertEquals(\n-              \"IsNotNull must match\", expectedIsNotNull.toString(), actualIsNotNull.toString());\n+          assertThat(actualIsNotNull)\n+              .asString()\n+              .as(\"IsNotNull must match\")\n+              .isEqualTo(expectedIsNotNull.toString());\n \n           Predicate lt1 = new Predicate(\"<\", attrAndValue);\n           Expression expectedLt1 = Expressions.lessThan(unquoted, 1);\n           Expression actualLt1 = SparkV2Filters.convert(lt1);\n-          Assert.assertEquals(\"LessThan must match\", expectedLt1.toString(), actualLt1.toString());\n+          assertThat(actualLt1)\n+              .asString()\n+              .as(\"LessThan must match\")\n+              .isEqualTo(expectedLt1.toString());\n \n           Predicate lt2 = new Predicate(\"<\", valueAndAttr);\n           Expression expectedLt2 = Expressions.greaterThan(unquoted, 1);\n           Expression actualLt2 = SparkV2Filters.convert(lt2);\n-          Assert.assertEquals(\"LessThan must match\", expectedLt2.toString(), actualLt2.toString());\n+          assertThat(actualLt2)\n+              .asString()\n+              .as(\"LessThan must match\")\n+              .isEqualTo(expectedLt2.toString());\n \n           Predicate ltEq1 = new Predicate(\"<=\", attrAndValue);\n           Expression expectedLtEq1 = Expressions.lessThanOrEqual(unquoted, 1);\n           Expression actualLtEq1 = SparkV2Filters.convert(ltEq1);\n-          Assert.assertEquals(\n-              \"LessThanOrEqual must match\", expectedLtEq1.toString(), actualLtEq1.toString());\n+          assertThat(actualLtEq1)\n+              .asString()\n+              .as(\"LessThanOrEqual must match\")\n+              .isEqualTo(expectedLtEq1.toString());\n \n           Predicate ltEq2 = new Predicate(\"<=\", valueAndAttr);\n           Expression expectedLtEq2 = Expressions.greaterThanOrEqual(unquoted, 1);\n           Expression actualLtEq2 = SparkV2Filters.convert(ltEq2);\n-          Assert.assertEquals(\n-              \"LessThanOrEqual must match\", expectedLtEq2.toString(), actualLtEq2.toString());\n+          assertThat(actualLtEq2)\n+              .asString()\n+              .as(\"LessThanOrEqual must match\")\n+              .isEqualTo(expectedLtEq2.toString());\n \n           Predicate gt1 = new Predicate(\">\", attrAndValue);\n           Expression expectedGt1 = Expressions.greaterThan(unquoted, 1);\n           Expression actualGt1 = SparkV2Filters.convert(gt1);\n-          Assert.assertEquals(\n-              \"GreaterThan must match\", expectedGt1.toString(), actualGt1.toString());\n+          assertThat(actualGt1)\n+              .asString()\n+              .as(\"GreaterThan must match\")\n+              .isEqualTo(expectedGt1.toString());\n \n           Predicate gt2 = new Predicate(\">\", valueAndAttr);\n           Expression expectedGt2 = Expressions.lessThan(unquoted, 1);\n           Expression actualGt2 = SparkV2Filters.convert(gt2);\n-          Assert.assertEquals(\n-              \"GreaterThan must match\", expectedGt2.toString(), actualGt2.toString());\n+          assertThat(actualGt2)\n+              .asString()\n+              .as(\"GreaterThan must match\")\n+              .isEqualTo(expectedGt2.toString());\n \n           Predicate gtEq1 = new Predicate(\">=\", attrAndValue);\n           Expression expectedGtEq1 = Expressions.greaterThanOrEqual(unquoted, 1);\n           Expression actualGtEq1 = SparkV2Filters.convert(gtEq1);\n-          Assert.assertEquals(\n-              \"GreaterThanOrEqual must match\", expectedGtEq1.toString(), actualGtEq1.toString());\n+          assertThat(actualGtEq1)\n+              .asString()\n+              .as(\"GreaterThanOrEqual must match\")\n+              .isEqualTo(expectedGtEq1.toString());\n \n           Predicate gtEq2 = new Predicate(\">=\", valueAndAttr);\n           Expression expectedGtEq2 = Expressions.lessThanOrEqual(unquoted, 1);\n           Expression actualGtEq2 = SparkV2Filters.convert(gtEq2);\n-          Assert.assertEquals(\n-              \"GreaterThanOrEqual must match\", expectedGtEq2.toString(), actualGtEq2.toString());\n+          assertThat(actualGtEq2)\n+              .asString()\n+              .as(\"GreaterThanOrEqual must match\")\n+              .isEqualTo(expectedGtEq2.toString());\n \n           Predicate eq1 = new Predicate(\"=\", attrAndValue);\n           Expression expectedEq1 = Expressions.equal(unquoted, 1);\n           Expression actualEq1 = SparkV2Filters.convert(eq1);\n-          Assert.assertEquals(\"EqualTo must match\", expectedEq1.toString(), actualEq1.toString());\n+          assertThat(actualEq1)\n+              .asString()\n+              .as(\"EqualTo must match\")\n+              .isEqualTo(expectedEq1.toString());\n \n           Predicate eq2 = new Predicate(\"=\", valueAndAttr);\n           Expression expectedEq2 = Expressions.equal(unquoted, 1);\n           Expression actualEq2 = SparkV2Filters.convert(eq2);\n-          Assert.assertEquals(\"EqualTo must match\", expectedEq2.toString(), actualEq2.toString());\n+          assertThat(actualEq2)\n+              .asString()\n+              .as(\"EqualTo must match\")\n+              .isEqualTo(expectedEq2.toString());\n \n           Predicate notEq1 = new Predicate(\"<>\", attrAndValue);\n           Expression expectedNotEq1 = Expressions.notEqual(unquoted, 1);\n           Expression actualNotEq1 = SparkV2Filters.convert(notEq1);\n-          Assert.assertEquals(\n-              \"NotEqualTo must match\", expectedNotEq1.toString(), actualNotEq1.toString());\n+          assertThat(actualNotEq1)\n+              .asString()\n+              .as(\"NotEqualTo must match\")\n+              .isEqualTo(expectedNotEq1.toString());\n \n           Predicate notEq2 = new Predicate(\"<>\", valueAndAttr);\n           Expression expectedNotEq2 = Expressions.notEqual(unquoted, 1);\n           Expression actualNotEq2 = SparkV2Filters.convert(notEq2);\n-          Assert.assertEquals(\n-              \"NotEqualTo must match\", expectedNotEq2.toString(), actualNotEq2.toString());\n+          assertThat(actualNotEq2)\n+              .asString()\n+              .as(\"NotEqualTo must match\")\n+              .isEqualTo(expectedNotEq2.toString());\n \n           Predicate eqNullSafe1 = new Predicate(\"<=>\", attrAndValue);\n           Expression expectedEqNullSafe1 = Expressions.equal(unquoted, 1);\n           Expression actualEqNullSafe1 = SparkV2Filters.convert(eqNullSafe1);\n-          Assert.assertEquals(\n-              \"EqualNullSafe must match\",\n-              expectedEqNullSafe1.toString(),\n-              actualEqNullSafe1.toString());\n+          assertThat(actualEqNullSafe1)\n+              .asString()\n+              .as(\"EqualNullSafe must match\")\n+              .isEqualTo(expectedEqNullSafe1.toString());\n \n           Predicate eqNullSafe2 = new Predicate(\"<=>\", valueAndAttr);\n           Expression expectedEqNullSafe2 = Expressions.equal(unquoted, 1);\n           Expression actualEqNullSafe2 = SparkV2Filters.convert(eqNullSafe2);\n-          Assert.assertEquals(\n-              \"EqualNullSafe must match\",\n-              expectedEqNullSafe2.toString(),\n-              actualEqNullSafe2.toString());\n+          assertThat(actualEqNullSafe2)\n+              .asString()\n+              .as(\"EqualNullSafe must match\")\n+              .isEqualTo(expectedEqNullSafe2.toString());\n \n           LiteralValue str =\n               new LiteralValue(UTF8String.fromString(\"iceberg\"), DataTypes.StringType);\n@@ -190,18 +221,20 @@ public void testV2Filters() {\n           Predicate startsWith = new Predicate(\"STARTS_WITH\", attrAndStr);\n           Expression expectedStartsWith = Expressions.startsWith(unquoted, \"iceberg\");\n           Expression actualStartsWith = SparkV2Filters.convert(startsWith);\n-          Assert.assertEquals(\n-              \"StartsWith must match\", expectedStartsWith.toString(), actualStartsWith.toString());\n+          assertThat(actualStartsWith)\n+              .asString()\n+              .as(\"StartsWith must match\")\n+              .isEqualTo(expectedStartsWith.toString());\n \n           Predicate in = new Predicate(\"IN\", attrAndValue);\n           Expression expectedIn = Expressions.in(unquoted, 1);\n           Expression actualIn = SparkV2Filters.convert(in);\n-          Assert.assertEquals(\"In must match\", expectedIn.toString(), actualIn.toString());\n+          assertThat(actualIn).asString().as(\"In must match\").isEqualTo(expectedIn.toString());\n \n           Predicate and = new And(lt1, eq1);\n           Expression expectedAnd = Expressions.and(expectedLt1, expectedEq1);\n           Expression actualAnd = SparkV2Filters.convert(and);\n-          Assert.assertEquals(\"And must match\", expectedAnd.toString(), actualAnd.toString());\n+          assertThat(actualAnd).asString().as(\"And must match\").isEqualTo(expectedAnd.toString());\n \n           org.apache.spark.sql.connector.expressions.Expression[] attrAndAttr =\n               new org.apache.spark.sql.connector.expressions.Expression[] {\n@@ -210,21 +243,21 @@ public void testV2Filters() {\n           Predicate invalid = new Predicate(\"<\", attrAndAttr);\n           Predicate andWithInvalidLeft = new And(invalid, eq1);\n           Expression convertedAnd = SparkV2Filters.convert(andWithInvalidLeft);\n-          Assert.assertEquals(\"And must match\", convertedAnd, null);\n+          assertThat(convertedAnd).as(\"And must match\").isNull();\n \n           Predicate or = new Or(lt1, eq1);\n           Expression expectedOr = Expressions.or(expectedLt1, expectedEq1);\n           Expression actualOr = SparkV2Filters.convert(or);\n-          Assert.assertEquals(\"Or must match\", expectedOr.toString(), actualOr.toString());\n+          assertThat(actualOr).asString().as(\"Or must match\").isEqualTo(expectedOr.toString());\n \n           Predicate orWithInvalidLeft = new Or(invalid, eq1);\n           Expression convertedOr = SparkV2Filters.convert(orWithInvalidLeft);\n-          Assert.assertEquals(\"Or must match\", convertedOr, null);\n+          assertThat(convertedOr).as(\"Or must match\").isNull();\n \n           Predicate not = new Not(lt1);\n           Expression expectedNot = Expressions.not(expectedLt1);\n           Expression actualNot = SparkV2Filters.convert(not);\n-          Assert.assertEquals(\"Not must match\", expectedNot.toString(), actualNot.toString());\n+          assertThat(actualNot).asString().as(\"Not must match\").isEqualTo(expectedNot.toString());\n         });\n   }\n \n@@ -380,10 +413,10 @@ public void testTimestampFilterConversion() {\n     Expression tsExpression = SparkV2Filters.convert(predicate);\n     Expression rawExpression = Expressions.greaterThan(\"x\", epochMicros);\n \n-    Assert.assertEquals(\n-        \"Generated Timestamp expression should be correct\",\n-        rawExpression.toString(),\n-        tsExpression.toString());\n+    assertThat(tsExpression)\n+        .asString()\n+        .as(\"Generated Timestamp expression should be correct\")\n+        .isEqualTo(rawExpression.toString());\n   }\n \n   @Test\n@@ -400,10 +433,10 @@ public void testDateFilterConversion() {\n     Expression dateExpression = SparkV2Filters.convert(predicate);\n     Expression rawExpression = Expressions.greaterThan(\"x\", epochDay);\n \n-    Assert.assertEquals(\n-        \"Generated date expression should be correct\",\n-        rawExpression.toString(),\n-        dateExpression.toString());\n+    assertThat(dateExpression)\n+        .asString()\n+        .as(\"Generated date expression should be correct\")\n+        .isEqualTo(rawExpression.toString());\n   }\n \n   @Test\n@@ -422,7 +455,7 @@ public void testNestedInInsideNot() {\n \n     Not filter = new Not(new And(equal, in));\n     Expression converted = SparkV2Filters.convert(filter);\n-    Assert.assertNull(\"Expression should not be converted\", converted);\n+    assertThat(converted).as(\"Expression should not be converted\").isNull();\n   }\n \n   @Test\n@@ -439,7 +472,7 @@ public void testNotIn() {\n     Expression actual = SparkV2Filters.convert(not);\n     Expression expected =\n         Expressions.and(Expressions.notNull(\"col\"), Expressions.notIn(\"col\", 1, 2));\n-    Assert.assertEquals(\"Expressions should match\", expected.toString(), actual.toString());\n+    assertThat(actual).asString().as(\"Expressions should match\").isEqualTo(expected.toString());\n   }\n \n   @Test\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkValueConverter.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkValueConverter.java\nindex 7f00c7edd8a9..c7a2e6c18fca 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkValueConverter.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkValueConverter.java\n@@ -18,14 +18,15 @@\n  */\n package org.apache.iceberg.spark;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.data.GenericRecord;\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.RowFactory;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n public class TestSparkValueConverter {\n   @Test\n@@ -86,9 +87,8 @@ private void assertCorrectNullConversion(Schema schema) {\n     Row sparkRow = RowFactory.create(1, null);\n     Record record = GenericRecord.create(schema);\n     record.set(0, 1);\n-    Assert.assertEquals(\n-        \"Round-trip conversion should produce original value\",\n-        record,\n-        SparkValueConverter.convert(schema, sparkRow));\n+    assertThat(SparkValueConverter.convert(schema, sparkRow))\n+        .as(\"Round-trip conversion should produce original value\")\n+        .isEqualTo(record);\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\nindex b3f9423c4703..81de0717621a 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\n@@ -19,11 +19,17 @@\n package org.apache.iceberg.spark.source;\n \n import static org.apache.avro.Schema.Type.UNION;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assertions.within;\n \n import java.io.IOException;\n+import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.data.GenericRecord;\n import org.apache.iceberg.data.Record;\n@@ -31,27 +37,23 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n-import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.types.Comparators;\n import org.apache.iceberg.types.Types;\n-import org.junit.Assert;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public abstract class TestReadProjection {\n-  final String format;\n-\n-  TestReadProjection(String format) {\n-    this.format = format;\n-  }\n+  @Parameter(index = 0)\n+  protected FileFormat format;\n \n   protected abstract Record writeAndRead(\n       String desc, Schema writeSchema, Schema readSchema, Record record) throws IOException;\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir protected Path temp;\n \n-  @Test\n+  @TestTemplate\n   public void testFullProjection() throws Exception {\n     Schema schema =\n         new Schema(\n@@ -64,15 +66,16 @@ public void testFullProjection() throws Exception {\n \n     Record projected = writeAndRead(\"full_projection\", schema, schema, record);\n \n-    Assert.assertEquals(\n-        \"Should contain the correct id value\", 34L, (long) projected.getField(\"id\"));\n+    assertThat((long) projected.getField(\"id\"))\n+        .as(\"Should contain the correct id value\")\n+        .isEqualTo(34L);\n \n     int cmp =\n         Comparators.charSequences().compare(\"test\", (CharSequence) projected.getField(\"data\"));\n-    Assert.assertEquals(\"Should contain the correct data value\", 0, cmp);\n+    assertThat(cmp).as(\"Should contain the correct data value\").isEqualTo(0);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReorderedFullProjection() throws Exception {\n     //    Assume.assumeTrue(\n     //        \"Spark's Parquet read support does not support reordered columns\",\n@@ -94,11 +97,14 @@ public void testReorderedFullProjection() throws Exception {\n \n     Record projected = writeAndRead(\"reordered_full_projection\", schema, reordered, record);\n \n-    Assert.assertEquals(\"Should contain the correct 0 value\", \"test\", projected.get(0).toString());\n-    Assert.assertEquals(\"Should contain the correct 1 value\", 34L, projected.get(1));\n+    assertThat(projected.get(0))\n+        .asString()\n+        .as(\"Should contain the correct 0 value\")\n+        .isEqualTo(\"test\");\n+    assertThat(projected.get(1)).as(\"Should contain the correct 1 value\").isEqualTo(34L);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReorderedProjection() throws Exception {\n     //    Assume.assumeTrue(\n     //        \"Spark's Parquet read support does not support reordered columns\",\n@@ -121,12 +127,14 @@ public void testReorderedProjection() throws Exception {\n \n     Record projected = writeAndRead(\"reordered_projection\", schema, reordered, record);\n \n-    Assert.assertNull(\"Should contain the correct 0 value\", projected.get(0));\n-    Assert.assertEquals(\"Should contain the correct 1 value\", \"test\", projected.get(1).toString());\n-    Assert.assertNull(\"Should contain the correct 2 value\", projected.get(2));\n+    assertThat(projected.get(0)).as(\"Should contain the correct 0 value\").isNull();\n+    assertThat(projected.get(1).toString())\n+        .as(\"Should contain the correct 1 value\")\n+        .isEqualTo(\"test\");\n+    assertThat(projected.get(2)).as(\"Should contain the correct 2 value\").isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   @SuppressWarnings(\"checkstyle:AssertThatThrownByWithMessageCheck\")\n   public void testEmptyProjection() throws Exception {\n     Schema schema =\n@@ -140,13 +148,13 @@ public void testEmptyProjection() throws Exception {\n \n     Record projected = writeAndRead(\"empty_projection\", schema, schema.select(), record);\n \n-    Assert.assertNotNull(\"Should read a non-null record\", projected);\n+    assertThat(projected).as(\"Should read a non-null record\").isNotNull();\n     // this is expected because there are no values\n     // no check on the underlying error msg as it might be missing based on the JDK version\n     assertThatThrownBy(() -> projected.get(0)).isInstanceOf(ArrayIndexOutOfBoundsException.class);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBasicProjection() throws Exception {\n     Schema writeSchema =\n         new Schema(\n@@ -160,21 +168,22 @@ public void testBasicProjection() throws Exception {\n     Schema idOnly = new Schema(Types.NestedField.required(0, \"id\", Types.LongType.get()));\n \n     Record projected = writeAndRead(\"basic_projection_id\", writeSchema, idOnly, record);\n-    Assert.assertNull(\"Should not project data\", projected.getField(\"data\"));\n-    Assert.assertEquals(\n-        \"Should contain the correct id value\", 34L, (long) projected.getField(\"id\"));\n+    assertThat(projected.getField(\"data\")).as(\"Should not project data\").isNull();\n+    assertThat((long) projected.getField(\"id\"))\n+        .as(\"Should contain the correct id value\")\n+        .isEqualTo(34L);\n \n     Schema dataOnly = new Schema(Types.NestedField.optional(1, \"data\", Types.StringType.get()));\n \n     projected = writeAndRead(\"basic_projection_data\", writeSchema, dataOnly, record);\n \n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n     int cmp =\n         Comparators.charSequences().compare(\"test\", (CharSequence) projected.getField(\"data\"));\n-    Assert.assertEquals(\"Should contain the correct data value\", 0, cmp);\n+    assertThat(cmp).as(\"Should contain the correct data value\").isEqualTo(0);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRename() throws Exception {\n     Schema writeSchema =\n         new Schema(\n@@ -192,14 +201,15 @@ public void testRename() throws Exception {\n \n     Record projected = writeAndRead(\"project_and_rename\", writeSchema, readSchema, record);\n \n-    Assert.assertEquals(\n-        \"Should contain the correct id value\", 34L, (long) projected.getField(\"id\"));\n+    assertThat((long) projected.getField(\"id\"))\n+        .as(\"Should contain the correct id value\")\n+        .isEqualTo(34L);\n     int cmp =\n         Comparators.charSequences().compare(\"test\", (CharSequence) projected.getField(\"renamed\"));\n-    Assert.assertEquals(\"Should contain the correct data/renamed value\", 0, cmp);\n+    assertThat(cmp).as(\"Should contain the correct data/renamed value\").isEqualTo(0);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNestedStructProjection() throws Exception {\n     Schema writeSchema =\n         new Schema(\n@@ -222,9 +232,10 @@ public void testNestedStructProjection() throws Exception {\n \n     Record projected = writeAndRead(\"id_only\", writeSchema, idOnly, record);\n     Record projectedLocation = (Record) projected.getField(\"location\");\n-    Assert.assertEquals(\n-        \"Should contain the correct id value\", 34L, (long) projected.getField(\"id\"));\n-    Assert.assertNull(\"Should not project location\", projectedLocation);\n+    assertThat((long) projected.getField(\"id\"))\n+        .as(\"Should contain the correct id value\")\n+        .isEqualTo(34L);\n+    assertThat(projectedLocation).as(\"Should not project location\").isNull();\n \n     Schema latOnly =\n         new Schema(\n@@ -235,14 +246,12 @@ public void testNestedStructProjection() throws Exception {\n \n     projected = writeAndRead(\"latitude_only\", writeSchema, latOnly, record);\n     projectedLocation = (Record) projected.getField(\"location\");\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n-    Assert.assertNotNull(\"Should project location\", projected.getField(\"location\"));\n-    Assert.assertNull(\"Should not project longitude\", projectedLocation.getField(\"long\"));\n-    Assert.assertEquals(\n-        \"Should project latitude\",\n-        52.995143f,\n-        (float) projectedLocation.getField(\"lat\"),\n-        0.000001f);\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+    assertThat(projected.getField(\"location\")).as(\"Should project location\").isNotNull();\n+    assertThat(projectedLocation.getField(\"long\")).as(\"Should not project longitude\").isNull();\n+    assertThat((float) projectedLocation.getField(\"lat\"))\n+        .as(\"Should project latitude\")\n+        .isCloseTo(52.995143f, within(0.000001f));\n \n     Schema longOnly =\n         new Schema(\n@@ -253,33 +262,28 @@ public void testNestedStructProjection() throws Exception {\n \n     projected = writeAndRead(\"longitude_only\", writeSchema, longOnly, record);\n     projectedLocation = (Record) projected.getField(\"location\");\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n-    Assert.assertNotNull(\"Should project location\", projected.getField(\"location\"));\n-    Assert.assertNull(\"Should not project latitutde\", projectedLocation.getField(\"lat\"));\n-    Assert.assertEquals(\n-        \"Should project longitude\",\n-        -1.539054f,\n-        (float) projectedLocation.getField(\"long\"),\n-        0.000001f);\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+    assertThat(projected.getField(\"location\")).as(\"Should project location\").isNotNull();\n+    assertThat(projectedLocation.getField(\"lat\")).as(\"Should not project latitude\").isNull();\n+    assertThat((float) projectedLocation.getField(\"long\"))\n+        .as(\"Should project longitude\")\n+        .isCloseTo(-1.539054f, within(0.000001f));\n \n     Schema locationOnly = writeSchema.select(\"location\");\n     projected = writeAndRead(\"location_only\", writeSchema, locationOnly, record);\n     projectedLocation = (Record) projected.getField(\"location\");\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n-    Assert.assertNotNull(\"Should project location\", projected.getField(\"location\"));\n-    Assert.assertEquals(\n-        \"Should project latitude\",\n-        52.995143f,\n-        (float) projectedLocation.getField(\"lat\"),\n-        0.000001f);\n-    Assert.assertEquals(\n-        \"Should project longitude\",\n-        -1.539054f,\n-        (float) projectedLocation.getField(\"long\"),\n-        0.000001f);\n+\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+    assertThat(projected.getField(\"location\")).as(\"Should project location\").isNotNull();\n+    assertThat((float) projectedLocation.getField(\"lat\"))\n+        .as(\"Should project latitude\")\n+        .isCloseTo(52.995143f, within(0.000001f));\n+    assertThat((float) projectedLocation.getField(\"long\"))\n+        .as(\"Should project longitude\")\n+        .isCloseTo(-1.539054f, within(0.000001f));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMapProjection() throws IOException {\n     Schema writeSchema =\n         new Schema(\n@@ -298,33 +302,31 @@ public void testMapProjection() throws IOException {\n     Schema idOnly = new Schema(Types.NestedField.required(0, \"id\", Types.LongType.get()));\n \n     Record projected = writeAndRead(\"id_only\", writeSchema, idOnly, record);\n-    Assert.assertEquals(\n-        \"Should contain the correct id value\", 34L, (long) projected.getField(\"id\"));\n-    Assert.assertNull(\"Should not project properties map\", projected.getField(\"properties\"));\n+    assertThat((long) projected.getField(\"id\"))\n+        .as(\"Should contain the correct id value\")\n+        .isEqualTo(34L);\n+    assertThat(projected.getField(\"properties\")).as(\"Should not project properties map\").isNull();\n \n     Schema keyOnly = writeSchema.select(\"properties.key\");\n     projected = writeAndRead(\"key_only\", writeSchema, keyOnly, record);\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n-    Assert.assertEquals(\n-        \"Should project entire map\",\n-        properties,\n-        toStringMap((Map) projected.getField(\"properties\")));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+    assertThat(toStringMap((Map) projected.getField(\"properties\")))\n+        .as(\"Should project entire map\")\n+        .isEqualTo(properties);\n \n     Schema valueOnly = writeSchema.select(\"properties.value\");\n     projected = writeAndRead(\"value_only\", writeSchema, valueOnly, record);\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n-    Assert.assertEquals(\n-        \"Should project entire map\",\n-        properties,\n-        toStringMap((Map) projected.getField(\"properties\")));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+    assertThat(toStringMap((Map) projected.getField(\"properties\")))\n+        .as(\"Should project entire map\")\n+        .isEqualTo(properties);\n \n     Schema mapOnly = writeSchema.select(\"properties\");\n     projected = writeAndRead(\"map_only\", writeSchema, mapOnly, record);\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n-    Assert.assertEquals(\n-        \"Should project entire map\",\n-        properties,\n-        toStringMap((Map) projected.getField(\"properties\")));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+    assertThat(toStringMap((Map) projected.getField(\"properties\")))\n+        .as(\"Should project entire map\")\n+        .isEqualTo(properties);\n   }\n \n   private Map<String, ?> toStringMap(Map<?, ?> map) {\n@@ -339,7 +341,7 @@ public void testMapProjection() throws IOException {\n     return stringMap;\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMapOfStructsProjection() throws IOException {\n     Schema writeSchema =\n         new Schema(\n@@ -368,51 +370,57 @@ public void testMapOfStructsProjection() throws IOException {\n     Schema idOnly = new Schema(Types.NestedField.required(0, \"id\", Types.LongType.get()));\n \n     Record projected = writeAndRead(\"id_only\", writeSchema, idOnly, record);\n-    Assert.assertEquals(\n-        \"Should contain the correct id value\", 34L, (long) projected.getField(\"id\"));\n-    Assert.assertNull(\"Should not project locations map\", projected.getField(\"locations\"));\n+    assertThat(34L)\n+        .as(\"Should contain the correct id value\")\n+        .isEqualTo((long) projected.getField(\"id\"));\n+    assertThat(projected.getField(\"locations\")).as(\"Should not project locations map\").isNull();\n \n     projected = writeAndRead(\"all_locations\", writeSchema, writeSchema.select(\"locations\"), record);\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n-    Assert.assertEquals(\n-        \"Should project locations map\",\n-        record.getField(\"locations\"),\n-        toStringMap((Map) projected.getField(\"locations\")));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+    assertThat(toStringMap((Map) projected.getField(\"locations\")))\n+        .as(\"Should project locations map\")\n+        .isEqualTo(record.getField(\"locations\"));\n \n     projected = writeAndRead(\"lat_only\", writeSchema, writeSchema.select(\"locations.lat\"), record);\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+\n     Map<String, ?> locations = toStringMap((Map) projected.getField(\"locations\"));\n-    Assert.assertNotNull(\"Should project locations map\", locations);\n-    Assert.assertEquals(\n-        \"Should contain L1 and L2\", Sets.newHashSet(\"L1\", \"L2\"), locations.keySet());\n+    assertThat(locations).isNotNull().containsKeys(\"L1\", \"L2\");\n+\n     Record projectedL1 = (Record) locations.get(\"L1\");\n-    Assert.assertNotNull(\"L1 should not be null\", projectedL1);\n-    Assert.assertEquals(\n-        \"L1 should contain lat\", 53.992811f, (float) projectedL1.getField(\"lat\"), 0.000001);\n-    Assert.assertNull(\"L1 should not contain long\", projectedL1.getField(\"long\"));\n+    assertThat(projectedL1).as(\"L1 should not be null\").isNotNull();\n+    assertThat((float) projectedL1.getField(\"lat\"))\n+        .as(\"L1 should contain lat\")\n+        .isCloseTo(53.992811f, within(0.000001f));\n+    assertThat(projectedL1.getField(\"long\")).as(\"L1 should not contain long\").isNull();\n+\n     Record projectedL2 = (Record) locations.get(\"L2\");\n-    Assert.assertNotNull(\"L2 should not be null\", projectedL2);\n-    Assert.assertEquals(\n-        \"L2 should contain lat\", 52.995143f, (float) projectedL2.getField(\"lat\"), 0.000001);\n-    Assert.assertNull(\"L2 should not contain long\", projectedL2.getField(\"long\"));\n+    assertThat(projectedL2).as(\"L2 should not be null\").isNotNull();\n+    assertThat((float) projectedL2.getField(\"lat\"))\n+        .as(\"L2 should contain lat\")\n+        .isCloseTo(52.995143f, within(0.000001f));\n+    assertThat(projectedL2.getField(\"long\")).as(\"L2 should not contain long\").isNull();\n \n     projected =\n         writeAndRead(\"long_only\", writeSchema, writeSchema.select(\"locations.long\"), record);\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+\n     locations = toStringMap((Map) projected.getField(\"locations\"));\n-    Assert.assertNotNull(\"Should project locations map\", locations);\n-    Assert.assertEquals(\n-        \"Should contain L1 and L2\", Sets.newHashSet(\"L1\", \"L2\"), locations.keySet());\n+    assertThat(locations).isNotNull().containsKeys(\"L1\", \"L2\");\n+\n     projectedL1 = (Record) locations.get(\"L1\");\n-    Assert.assertNotNull(\"L1 should not be null\", projectedL1);\n-    Assert.assertNull(\"L1 should not contain lat\", projectedL1.getField(\"lat\"));\n-    Assert.assertEquals(\n-        \"L1 should contain long\", -1.542616f, (float) projectedL1.getField(\"long\"), 0.000001);\n+    assertThat(projectedL1).as(\"L1 should not be null\").isNotNull();\n+    assertThat(projectedL1.getField(\"lat\")).as(\"L1 should not contain lat\").isNull();\n+    assertThat((float) projectedL1.getField(\"long\"))\n+        .as(\"L1 should contain long\")\n+        .isCloseTo(-1.542616f, within(0.000001f));\n+\n     projectedL2 = (Record) locations.get(\"L2\");\n-    Assert.assertNotNull(\"L2 should not be null\", projectedL2);\n-    Assert.assertNull(\"L2 should not contain lat\", projectedL2.getField(\"lat\"));\n-    Assert.assertEquals(\n-        \"L2 should contain long\", -1.539054f, (float) projectedL2.getField(\"long\"), 0.000001);\n+    assertThat(projectedL2).as(\"L2 should not be null\").isNotNull();\n+    assertThat(projectedL2.getField(\"lat\")).as(\"L2 should not contain lat\").isNull();\n+    assertThat((float) projectedL2.getField(\"long\"))\n+        .as(\"L2 should contain long\")\n+        .isCloseTo(-1.539054f, within(0.000001f));\n \n     Schema latitiudeRenamed =\n         new Schema(\n@@ -427,32 +435,28 @@ public void testMapOfStructsProjection() throws IOException {\n                         Types.NestedField.required(1, \"latitude\", Types.FloatType.get())))));\n \n     projected = writeAndRead(\"latitude_renamed\", writeSchema, latitiudeRenamed, record);\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n     locations = toStringMap((Map) projected.getField(\"locations\"));\n-    Assert.assertNotNull(\"Should project locations map\", locations);\n-    Assert.assertEquals(\n-        \"Should contain L1 and L2\", Sets.newHashSet(\"L1\", \"L2\"), locations.keySet());\n+    assertThat(locations).isNotNull().containsKeys(\"L1\", \"L2\");\n+\n     projectedL1 = (Record) locations.get(\"L1\");\n-    Assert.assertNotNull(\"L1 should not be null\", projectedL1);\n-    Assert.assertEquals(\n-        \"L1 should contain latitude\",\n-        53.992811f,\n-        (float) projectedL1.getField(\"latitude\"),\n-        0.000001);\n-    Assert.assertNull(\"L1 should not contain lat\", projectedL1.getField(\"lat\"));\n-    Assert.assertNull(\"L1 should not contain long\", projectedL1.getField(\"long\"));\n+    assertThat(projectedL1).as(\"L1 should not be null\").isNotNull();\n+    assertThat((float) projectedL1.getField(\"latitude\"))\n+        .as(\"L1 should contain latitude\")\n+        .isCloseTo(53.992811f, within(0.000001f));\n+    assertThat(projectedL1.getField(\"lat\")).as(\"L1 should not contain lat\").isNull();\n+    assertThat(projectedL1.getField(\"long\")).as(\"L1 should not contain long\").isNull();\n+\n     projectedL2 = (Record) locations.get(\"L2\");\n-    Assert.assertNotNull(\"L2 should not be null\", projectedL2);\n-    Assert.assertEquals(\n-        \"L2 should contain latitude\",\n-        52.995143f,\n-        (float) projectedL2.getField(\"latitude\"),\n-        0.000001);\n-    Assert.assertNull(\"L2 should not contain lat\", projectedL2.getField(\"lat\"));\n-    Assert.assertNull(\"L2 should not contain long\", projectedL2.getField(\"long\"));\n+    assertThat(projectedL2).as(\"L2 should not be null\").isNotNull();\n+    assertThat((float) projectedL2.getField(\"latitude\"))\n+        .as(\"L2 should contain latitude\")\n+        .isCloseTo(52.995143f, within(0.000001f));\n+    assertThat(projectedL2.getField(\"lat\")).as(\"L2 should not contain lat\").isNull();\n+    assertThat(projectedL2.getField(\"long\")).as(\"L2 should not contain long\").isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testListProjection() throws IOException {\n     Schema writeSchema =\n         new Schema(\n@@ -469,22 +473,23 @@ public void testListProjection() throws IOException {\n     Schema idOnly = new Schema(Types.NestedField.required(0, \"id\", Types.LongType.get()));\n \n     Record projected = writeAndRead(\"id_only\", writeSchema, idOnly, record);\n-    Assert.assertEquals(\n-        \"Should contain the correct id value\", 34L, (long) projected.getField(\"id\"));\n-    Assert.assertNull(\"Should not project values list\", projected.getField(\"values\"));\n+    assertThat((long) projected.getField(\"id\"))\n+        .as(\"Should contain the correct id value\")\n+        .isEqualTo(34L);\n+    assertThat(projected.getField(\"values\")).as(\"Should not project values list\").isNull();\n \n     Schema elementOnly = writeSchema.select(\"values.element\");\n     projected = writeAndRead(\"element_only\", writeSchema, elementOnly, record);\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n-    Assert.assertEquals(\"Should project entire list\", values, projected.getField(\"values\"));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+    assertThat(projected.getField(\"values\")).as(\"Should project entire list\").isEqualTo(values);\n \n     Schema listOnly = writeSchema.select(\"values\");\n     projected = writeAndRead(\"list_only\", writeSchema, listOnly, record);\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n-    Assert.assertEquals(\"Should project entire list\", values, projected.getField(\"values\"));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+    assertThat(projected.getField(\"values\")).as(\"Should project entire list\").isEqualTo(values);\n   }\n \n-  @Test\n+  @TestTemplate\n   @SuppressWarnings(\"unchecked\")\n   public void testListOfStructsProjection() throws IOException {\n     Schema writeSchema =\n@@ -512,38 +517,58 @@ public void testListOfStructsProjection() throws IOException {\n     Schema idOnly = new Schema(Types.NestedField.required(0, \"id\", Types.LongType.get()));\n \n     Record projected = writeAndRead(\"id_only\", writeSchema, idOnly, record);\n-    Assert.assertEquals(\n-        \"Should contain the correct id value\", 34L, (long) projected.getField(\"id\"));\n-    Assert.assertNull(\"Should not project points list\", projected.getField(\"points\"));\n+    assertThat((long) projected.getField(\"id\"))\n+        .as(\"Should contain the correct id value\")\n+        .isEqualTo(34L);\n+    assertThat(projected.getField(\"points\")).as(\"Should not project points list\").isNull();\n \n     projected = writeAndRead(\"all_points\", writeSchema, writeSchema.select(\"points\"), record);\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n-    Assert.assertEquals(\n-        \"Should project points list\", record.getField(\"points\"), projected.getField(\"points\"));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+    assertThat(projected.getField(\"points\"))\n+        .as(\"Should project points list\")\n+        .isEqualTo(record.getField(\"points\"));\n \n     projected = writeAndRead(\"x_only\", writeSchema, writeSchema.select(\"points.x\"), record);\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n-    Assert.assertNotNull(\"Should project points list\", projected.getField(\"points\"));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+    assertThat(projected.getField(\"points\")).as(\"Should project points list\").isNotNull();\n+\n     List<Record> points = (List<Record>) projected.getField(\"points\");\n-    Assert.assertEquals(\"Should read 2 points\", 2, points.size());\n-    Record projectedP1 = points.get(0);\n-    Assert.assertEquals(\"Should project x\", 1, (int) projectedP1.getField(\"x\"));\n-    Assert.assertNull(\"Should not project y\", projectedP1.getField(\"y\"));\n-    Record projectedP2 = points.get(1);\n-    Assert.assertEquals(\"Should project x\", 3, (int) projectedP2.getField(\"x\"));\n-    Assert.assertNull(\"Should not project y\", projectedP2.getField(\"y\"));\n+    assertThat(points).as(\"Should read 2 points\").hasSize(2);\n+    assertThat(points)\n+        .element(0)\n+        .satisfies(\n+            projectedP1 -> {\n+              assertThat((int) projectedP1.getField(\"x\")).isEqualTo(1);\n+              assertThat(projectedP1.getField(\"y\")).isNull();\n+            });\n+    assertThat(points)\n+        .element(1)\n+        .satisfies(\n+            projectedP2 -> {\n+              assertThat((int) projectedP2.getField(\"x\")).isEqualTo(3);\n+              assertThat(projectedP2.getField(\"y\")).isNull();\n+            });\n \n     projected = writeAndRead(\"y_only\", writeSchema, writeSchema.select(\"points.y\"), record);\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n-    Assert.assertNotNull(\"Should project points list\", projected.getField(\"points\"));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+    assertThat(projected.getField(\"points\")).as(\"Should project points list\").isNotNull();\n+\n     points = (List<Record>) projected.getField(\"points\");\n-    Assert.assertEquals(\"Should read 2 points\", 2, points.size());\n-    projectedP1 = points.get(0);\n-    Assert.assertNull(\"Should not project x\", projectedP1.getField(\"x\"));\n-    Assert.assertEquals(\"Should project y\", 2, (int) projectedP1.getField(\"y\"));\n-    projectedP2 = points.get(1);\n-    Assert.assertNull(\"Should not project x\", projectedP2.getField(\"x\"));\n-    Assert.assertNull(\"Should project null y\", projectedP2.getField(\"y\"));\n+    assertThat(points).as(\"Should read 2 points\").hasSize(2);\n+    assertThat(points)\n+        .element(0)\n+        .satisfies(\n+            projectedP1 -> {\n+              assertThat(projectedP1.getField(\"x\")).isNull();\n+              assertThat((int) projectedP1.getField(\"y\")).isEqualTo(2);\n+            });\n+    assertThat(points)\n+        .element(1)\n+        .satisfies(\n+            projectedP2 -> {\n+              assertThat(projectedP2.getField(\"x\")).isNull();\n+              assertThat(projectedP2.getField(\"y\")).isNull();\n+            });\n \n     Schema yRenamed =\n         new Schema(\n@@ -556,18 +581,27 @@ public void testListOfStructsProjection() throws IOException {\n                         Types.NestedField.optional(18, \"z\", Types.IntegerType.get())))));\n \n     projected = writeAndRead(\"y_renamed\", writeSchema, yRenamed, record);\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n-    Assert.assertNotNull(\"Should project points list\", projected.getField(\"points\"));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+    assertThat(projected.getField(\"points\")).as(\"Should project points list\").isNotNull();\n+\n     points = (List<Record>) projected.getField(\"points\");\n-    Assert.assertEquals(\"Should read 2 points\", 2, points.size());\n-    projectedP1 = points.get(0);\n-    Assert.assertNull(\"Should not project x\", projectedP1.getField(\"x\"));\n-    Assert.assertNull(\"Should not project y\", projectedP1.getField(\"y\"));\n-    Assert.assertEquals(\"Should project z\", 2, (int) projectedP1.getField(\"z\"));\n-    projectedP2 = points.get(1);\n-    Assert.assertNull(\"Should not project x\", projectedP2.getField(\"x\"));\n-    Assert.assertNull(\"Should not project y\", projectedP2.getField(\"y\"));\n-    Assert.assertNull(\"Should project null z\", projectedP2.getField(\"z\"));\n+    assertThat(points).as(\"Should read 2 points\").hasSize(2);\n+    assertThat(points)\n+        .element(0)\n+        .satisfies(\n+            projectedP1 -> {\n+              assertThat(projectedP1.getField(\"x\")).isNull();\n+              assertThat(projectedP1.getField(\"y\")).isNull();\n+              assertThat((int) projectedP1.getField(\"z\")).isEqualTo(2);\n+            });\n+    assertThat(points)\n+        .element(1)\n+        .satisfies(\n+            projectedP2 -> {\n+              assertThat(projectedP2.getField(\"x\")).isNull();\n+              assertThat(projectedP2.getField(\"y\")).isNull();\n+              assertThat(projectedP2.getField(\"z\")).isNull();\n+            });\n \n     Schema zAdded =\n         new Schema(\n@@ -582,18 +616,27 @@ public void testListOfStructsProjection() throws IOException {\n                         Types.NestedField.optional(20, \"z\", Types.IntegerType.get())))));\n \n     projected = writeAndRead(\"z_added\", writeSchema, zAdded, record);\n-    Assert.assertNull(\"Should not project id\", projected.getField(\"id\"));\n-    Assert.assertNotNull(\"Should project points list\", projected.getField(\"points\"));\n+    assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n+    assertThat(projected.getField(\"points\")).as(\"Should project points list\").isNotNull();\n+\n     points = (List<Record>) projected.getField(\"points\");\n-    Assert.assertEquals(\"Should read 2 points\", 2, points.size());\n-    projectedP1 = points.get(0);\n-    Assert.assertEquals(\"Should project x\", 1, (int) projectedP1.getField(\"x\"));\n-    Assert.assertEquals(\"Should project y\", 2, (int) projectedP1.getField(\"y\"));\n-    Assert.assertNull(\"Should contain null z\", projectedP1.getField(\"z\"));\n-    projectedP2 = points.get(1);\n-    Assert.assertEquals(\"Should project x\", 3, (int) projectedP2.getField(\"x\"));\n-    Assert.assertNull(\"Should project null y\", projectedP2.getField(\"y\"));\n-    Assert.assertNull(\"Should contain null z\", projectedP2.getField(\"z\"));\n+    assertThat(points).as(\"Should read 2 points\").hasSize(2);\n+    assertThat(points)\n+        .element(0)\n+        .satisfies(\n+            projectedP1 -> {\n+              assertThat((int) projectedP1.getField(\"x\")).isEqualTo(1);\n+              assertThat((int) projectedP1.getField(\"y\")).isEqualTo(2);\n+              assertThat(projectedP1.getField(\"z\")).isNull();\n+            });\n+    assertThat(points)\n+        .element(1)\n+        .satisfies(\n+            projectedP2 -> {\n+              assertThat((int) projectedP2.getField(\"x\")).isEqualTo(3);\n+              assertThat(projectedP2.getField(\"y\")).isNull();\n+              assertThat(projectedP2.getField(\"z\")).isNull();\n+            });\n   }\n \n   private static org.apache.avro.Schema fromOption(org.apache.avro.Schema schema) {\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java\nindex 9fc576dde5e8..3d74b9135991 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java\n@@ -24,19 +24,20 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n-import java.io.IOException;\n+import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n-import org.apache.iceberg.PlanningMode;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.spark.SparkReadOptions;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n@@ -46,21 +47,29 @@\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.RowFactory;\n import org.apache.spark.sql.SparkSession;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSnapshotSelection {\n \n-  @Parameterized.Parameters(name = \"planningMode = {0}\")\n+  @Parameters(name = \"properties = {0}\")\n   public static Object[] parameters() {\n-    return new Object[] {LOCAL, DISTRIBUTED};\n+    return new Object[][] {\n+      {\n+        ImmutableMap.of(\n+            TableProperties.DATA_PLANNING_MODE, LOCAL.modeName(),\n+            TableProperties.DELETE_PLANNING_MODE, LOCAL.modeName())\n+      },\n+      {\n+        ImmutableMap.of(\n+            TableProperties.DATA_PLANNING_MODE, DISTRIBUTED.modeName(),\n+            TableProperties.DELETE_PLANNING_MODE, DISTRIBUTED.modeName())\n+      }\n+    };\n   }\n \n   private static final Configuration CONF = new Configuration();\n@@ -68,34 +77,28 @@ public static Object[] parameters() {\n       new Schema(\n           optional(1, \"id\", Types.IntegerType.get()), optional(2, \"data\", Types.StringType.get()));\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n   private static SparkSession spark = null;\n \n-  private final Map<String, String> properties;\n-\n-  public TestSnapshotSelection(PlanningMode planningMode) {\n-    this.properties =\n-        ImmutableMap.of(\n-            TableProperties.DATA_PLANNING_MODE, planningMode.modeName(),\n-            TableProperties.DELETE_PLANNING_MODE, planningMode.modeName());\n-  }\n+  @Parameter(index = 0)\n+  private Map<String, String> properties;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestSnapshotSelection.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestSnapshotSelection.spark;\n     TestSnapshotSelection.spark = null;\n     currentSpark.stop();\n   }\n \n-  @Test\n-  public void testSnapshotSelectionById() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  @TestTemplate\n+  public void testSnapshotSelectionById() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -115,7 +118,7 @@ public void testSnapshotSelectionById() throws IOException {\n     Dataset<Row> secondDf = spark.createDataFrame(secondBatchRecords, SimpleRecord.class);\n     secondDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n \n-    Assert.assertEquals(\"Expected 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Expected 2 snapshots\").hasSize(2);\n \n     // verify records in the current snapshot\n     Dataset<Row> currentSnapshotResult = spark.read().format(\"iceberg\").load(tableLocation);\n@@ -124,8 +127,9 @@ public void testSnapshotSelectionById() throws IOException {\n     List<SimpleRecord> expectedRecords = Lists.newArrayList();\n     expectedRecords.addAll(firstBatchRecords);\n     expectedRecords.addAll(secondBatchRecords);\n-    Assert.assertEquals(\n-        \"Current snapshot rows should match\", expectedRecords, currentSnapshotRecords);\n+    assertThat(currentSnapshotRecords)\n+        .as(\"Current snapshot rows should match\")\n+        .isEqualTo(expectedRecords);\n \n     // verify records in the previous snapshot\n     Snapshot currentSnapshot = table.currentSnapshot();\n@@ -134,13 +138,14 @@ public void testSnapshotSelectionById() throws IOException {\n         spark.read().format(\"iceberg\").option(\"snapshot-id\", parentSnapshotId).load(tableLocation);\n     List<SimpleRecord> previousSnapshotRecords =\n         previousSnapshotResult.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    Assert.assertEquals(\n-        \"Previous snapshot rows should match\", firstBatchRecords, previousSnapshotRecords);\n+    assertThat(previousSnapshotRecords)\n+        .as(\"Previous snapshot rows should match\")\n+        .isEqualTo(firstBatchRecords);\n   }\n \n-  @Test\n-  public void testSnapshotSelectionByTimestamp() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  @TestTemplate\n+  public void testSnapshotSelectionByTimestamp() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -163,7 +168,7 @@ public void testSnapshotSelectionByTimestamp() throws IOException {\n     Dataset<Row> secondDf = spark.createDataFrame(secondBatchRecords, SimpleRecord.class);\n     secondDf.select(\"id\", \"data\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n \n-    Assert.assertEquals(\"Expected 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Expected 2 snapshots\").hasSize(2);\n \n     // verify records in the current snapshot\n     Dataset<Row> currentSnapshotResult = spark.read().format(\"iceberg\").load(tableLocation);\n@@ -172,8 +177,9 @@ public void testSnapshotSelectionByTimestamp() throws IOException {\n     List<SimpleRecord> expectedRecords = Lists.newArrayList();\n     expectedRecords.addAll(firstBatchRecords);\n     expectedRecords.addAll(secondBatchRecords);\n-    Assert.assertEquals(\n-        \"Current snapshot rows should match\", expectedRecords, currentSnapshotRecords);\n+    assertThat(currentSnapshotRecords)\n+        .as(\"Current snapshot rows should match\")\n+        .isEqualTo(expectedRecords);\n \n     // verify records in the previous snapshot\n     Dataset<Row> previousSnapshotResult =\n@@ -184,13 +190,14 @@ public void testSnapshotSelectionByTimestamp() throws IOException {\n             .load(tableLocation);\n     List<SimpleRecord> previousSnapshotRecords =\n         previousSnapshotResult.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    Assert.assertEquals(\n-        \"Previous snapshot rows should match\", firstBatchRecords, previousSnapshotRecords);\n+    assertThat(previousSnapshotRecords)\n+        .as(\"Previous snapshot rows should match\")\n+        .isEqualTo(firstBatchRecords);\n   }\n \n-  @Test\n-  public void testSnapshotSelectionByInvalidSnapshotId() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  @TestTemplate\n+  public void testSnapshotSelectionByInvalidSnapshotId() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -203,11 +210,11 @@ public void testSnapshotSelectionByInvalidSnapshotId() throws IOException {\n         .hasMessage(\"Cannot find snapshot with ID -10\");\n   }\n \n-  @Test\n-  public void testSnapshotSelectionByInvalidTimestamp() throws IOException {\n+  @TestTemplate\n+  public void testSnapshotSelectionByInvalidTimestamp() {\n     long timestamp = System.currentTimeMillis();\n \n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n     tables.create(SCHEMA, spec, properties, tableLocation);\n@@ -223,9 +230,9 @@ public void testSnapshotSelectionByInvalidTimestamp() throws IOException {\n         .hasMessageContaining(\"Cannot find a snapshot older than\");\n   }\n \n-  @Test\n-  public void testSnapshotSelectionBySnapshotIdAndTimestamp() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  @TestTemplate\n+  public void testSnapshotSelectionBySnapshotIdAndTimestamp() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -255,9 +262,9 @@ public void testSnapshotSelectionBySnapshotIdAndTimestamp() throws IOException {\n         .hasMessageContaining(\"tag\");\n   }\n \n-  @Test\n-  public void testSnapshotSelectionByTag() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  @TestTemplate\n+  public void testSnapshotSelectionByTag() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -286,13 +293,14 @@ public void testSnapshotSelectionByTag() throws IOException {\n         currentSnapshotResult.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n     List<SimpleRecord> expectedRecords = Lists.newArrayList();\n     expectedRecords.addAll(firstBatchRecords);\n-    Assert.assertEquals(\n-        \"Current snapshot rows should match\", expectedRecords, currentSnapshotRecords);\n+    assertThat(currentSnapshotRecords)\n+        .as(\"Current snapshot rows should match\")\n+        .isEqualTo(expectedRecords);\n   }\n \n-  @Test\n-  public void testSnapshotSelectionByBranch() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  @TestTemplate\n+  public void testSnapshotSelectionByBranch() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -321,13 +329,14 @@ public void testSnapshotSelectionByBranch() throws IOException {\n         currentSnapshotResult.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n     List<SimpleRecord> expectedRecords = Lists.newArrayList();\n     expectedRecords.addAll(firstBatchRecords);\n-    Assert.assertEquals(\n-        \"Current snapshot rows should match\", expectedRecords, currentSnapshotRecords);\n+    assertThat(currentSnapshotRecords)\n+        .as(\"Current snapshot rows should match\")\n+        .isEqualTo(expectedRecords);\n   }\n \n-  @Test\n-  public void testSnapshotSelectionByBranchAndTagFails() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  @TestTemplate\n+  public void testSnapshotSelectionByBranchAndTagFails() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -356,9 +365,9 @@ public void testSnapshotSelectionByBranchAndTagFails() throws IOException {\n         .hasMessageStartingWith(\"Can specify only one of snapshot-id\");\n   }\n \n-  @Test\n-  public void testSnapshotSelectionByTimestampAndBranchOrTagFails() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  @TestTemplate\n+  public void testSnapshotSelectionByTimestampAndBranchOrTagFails() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -399,9 +408,9 @@ public void testSnapshotSelectionByTimestampAndBranchOrTagFails() throws IOExcep\n         .hasMessageStartingWith(\"Can specify only one of snapshot-id\");\n   }\n \n-  @Test\n-  public void testSnapshotSelectionByBranchWithSchemaChange() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  @TestTemplate\n+  public void testSnapshotSelectionByBranchWithSchemaChange() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -422,8 +431,9 @@ public void testSnapshotSelectionByBranchWithSchemaChange() throws IOException {\n         branchSnapshotResult.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n     List<SimpleRecord> expectedRecords = Lists.newArrayList();\n     expectedRecords.addAll(firstBatchRecords);\n-    Assert.assertEquals(\n-        \"Current snapshot rows should match\", expectedRecords, branchSnapshotRecords);\n+    assertThat(branchSnapshotRecords)\n+        .as(\"Current snapshot rows should match\")\n+        .isEqualTo(expectedRecords);\n \n     // Deleting a column to indicate schema change\n     table.updateSchema().deleteColumn(\"data\").commit();\n@@ -455,9 +465,9 @@ public void testSnapshotSelectionByBranchWithSchemaChange() throws IOException {\n             new SimpleRecord(1, null), new SimpleRecord(2, null), new SimpleRecord(3, null));\n   }\n \n-  @Test\n-  public void testWritingToBranchAfterSchemaChange() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  @TestTemplate\n+  public void testWritingToBranchAfterSchemaChange() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -478,8 +488,9 @@ public void testWritingToBranchAfterSchemaChange() throws IOException {\n         branchSnapshotResult.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n     List<SimpleRecord> expectedRecords = Lists.newArrayList();\n     expectedRecords.addAll(firstBatchRecords);\n-    Assert.assertEquals(\n-        \"Current snapshot rows should match\", expectedRecords, branchSnapshotRecords);\n+    assertThat(branchSnapshotRecords)\n+        .as(\"Current snapshot rows should match\")\n+        .isEqualTo(expectedRecords);\n \n     // Deleting and add a new column of the same type to indicate schema change\n     table.updateSchema().deleteColumn(\"data\").addColumn(\"zip\", Types.IntegerType.get()).commit();\n@@ -528,9 +539,9 @@ public void testWritingToBranchAfterSchemaChange() throws IOException {\n         .containsAll(records);\n   }\n \n-  @Test\n-  public void testSnapshotSelectionByTagWithSchemaChange() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  @TestTemplate\n+  public void testSnapshotSelectionByTagWithSchemaChange() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -552,7 +563,9 @@ public void testSnapshotSelectionByTagWithSchemaChange() throws IOException {\n         spark.read().format(\"iceberg\").option(\"tag\", \"tag\").load(tableLocation);\n     List<SimpleRecord> tagSnapshotRecords =\n         tagSnapshotResult.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Current snapshot rows should match\", expectedRecords, tagSnapshotRecords);\n+    assertThat(tagSnapshotRecords)\n+        .as(\"Current snapshot rows should match\")\n+        .isEqualTo(expectedRecords);\n \n     // Deleting a column to indicate schema change\n     table.updateSchema().deleteColumn(\"data\").commit();\n@@ -565,7 +578,8 @@ public void testSnapshotSelectionByTagWithSchemaChange() throws IOException {\n             .orderBy(\"id\")\n             .as(Encoders.bean(SimpleRecord.class))\n             .collectAsList();\n-    Assert.assertEquals(\n-        \"Current snapshot rows should match\", expectedRecords, deletedColumnTagSnapshotRecords);\n+    assertThat(deletedColumnTagSnapshotRecords)\n+        .as(\"Current snapshot rows should match\")\n+        .isEqualTo(expectedRecords);\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java\nindex 63c18277aa92..2cb1a52b699f 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java\n@@ -22,12 +22,13 @@\n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n import static org.mockito.Mockito.doAnswer;\n import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.when;\n \n import java.io.File;\n-import java.io.IOException;\n+import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n import org.apache.hadoop.conf.Configuration;\n@@ -36,6 +37,9 @@\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.ManifestFile;\n import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.SnapshotRef;\n@@ -54,64 +58,61 @@\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SaveMode;\n import org.apache.spark.sql.SparkSession;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkDataWrite {\n   private static final Configuration CONF = new Configuration();\n-  private final FileFormat format;\n-  private final String branch;\n+\n+  @Parameter(index = 0)\n+  private FileFormat format;\n+\n+  @Parameter(index = 1)\n+  private String branch;\n+\n   private static SparkSession spark = null;\n   private static final Schema SCHEMA =\n       new Schema(\n           optional(1, \"id\", Types.IntegerType.get()), optional(2, \"data\", Types.StringType.get()));\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n-  @Parameterized.Parameters(name = \"format = {0}, branch = {1}\")\n-  public static Object[] parameters() {\n-    return new Object[] {\n-      new Object[] {\"parquet\", null},\n-      new Object[] {\"parquet\", \"main\"},\n-      new Object[] {\"parquet\", \"testBranch\"},\n-      new Object[] {\"avro\", null},\n-      new Object[] {\"orc\", \"testBranch\"}\n+  @Parameters(name = \"format = {0}, branch = {1}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+      new Object[] {FileFormat.PARQUET, null},\n+      new Object[] {FileFormat.PARQUET, \"main\"},\n+      new Object[] {FileFormat.PARQUET, \"testBranch\"},\n+      new Object[] {FileFormat.AVRO, null},\n+      new Object[] {FileFormat.ORC, \"testBranch\"}\n     };\n   }\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestSparkDataWrite.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n   }\n \n-  @Parameterized.AfterParam\n-  public static void clearSourceCache() {\n+  @AfterEach\n+  public void clearSourceCache() {\n     ManualSource.clearTables();\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestSparkDataWrite.spark;\n     TestSparkDataWrite.spark = null;\n     currentSpark.stop();\n   }\n \n-  public TestSparkDataWrite(String format, String branch) {\n-    this.format = FileFormat.fromString(format);\n-    this.branch = branch;\n-  }\n-\n-  @Test\n-  public void testBasicWrite() throws IOException {\n-    File parent = temp.newFolder(format.toString());\n+  @TestTemplate\n+  public void testBasicWrite() {\n+    File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n \n@@ -139,31 +140,30 @@ public void testBasicWrite() throws IOException {\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n     for (ManifestFile manifest :\n         SnapshotUtil.latestSnapshot(table, branch).allManifests(table.io())) {\n       for (DataFile file : ManifestFiles.read(manifest, table.io())) {\n         // TODO: avro not support split\n         if (!format.equals(FileFormat.AVRO)) {\n-          Assert.assertNotNull(\"Split offsets not present\", file.splitOffsets());\n+          assertThat(file.splitOffsets()).as(\"Split offsets not present\").isNotNull();\n         }\n-        Assert.assertEquals(\"Should have reported record count as 1\", 1, file.recordCount());\n+        assertThat(file.recordCount()).as(\"Should have reported record count as 1\").isEqualTo(1);\n         // TODO: append more metric info\n         if (format.equals(FileFormat.PARQUET)) {\n-          Assert.assertNotNull(\"Column sizes metric not present\", file.columnSizes());\n-          Assert.assertNotNull(\"Counts metric not present\", file.valueCounts());\n-          Assert.assertNotNull(\"Null value counts metric not present\", file.nullValueCounts());\n-          Assert.assertNotNull(\"Lower bounds metric not present\", file.lowerBounds());\n-          Assert.assertNotNull(\"Upper bounds metric not present\", file.upperBounds());\n+          assertThat(file.columnSizes()).as(\"Column sizes metric not present\").isNotNull();\n+          assertThat(file.valueCounts()).as(\"Counts metric not present\").isNotNull();\n+          assertThat(file.nullValueCounts()).as(\"Null value counts metric not present\").isNotNull();\n+          assertThat(file.lowerBounds()).as(\"Lower bounds metric not present\").isNotNull();\n+          assertThat(file.upperBounds()).as(\"Upper bounds metric not present\").isNotNull();\n         }\n       }\n     }\n   }\n \n-  @Test\n-  public void testAppend() throws IOException {\n-    File parent = temp.newFolder(format.toString());\n+  @TestTemplate\n+  public void testAppend() {\n+    File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n \n@@ -209,13 +209,12 @@ public void testAppend() throws IOException {\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n-  @Test\n-  public void testEmptyOverwrite() throws IOException {\n-    File parent = temp.newFolder(format.toString());\n+  @TestTemplate\n+  public void testEmptyOverwrite() {\n+    File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n \n@@ -255,13 +254,12 @@ public void testEmptyOverwrite() throws IOException {\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n-  @Test\n-  public void testOverwrite() throws IOException {\n-    File parent = temp.newFolder(format.toString());\n+  @TestTemplate\n+  public void testOverwrite() {\n+    File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n \n@@ -308,13 +306,12 @@ public void testOverwrite() throws IOException {\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n-  @Test\n-  public void testUnpartitionedOverwrite() throws IOException {\n-    File parent = temp.newFolder(format.toString());\n+  @TestTemplate\n+  public void testUnpartitionedOverwrite() {\n+    File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n \n@@ -351,13 +348,12 @@ public void testUnpartitionedOverwrite() throws IOException {\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n-  @Test\n-  public void testUnpartitionedCreateWithTargetFileSizeViaTableProperties() throws IOException {\n-    File parent = temp.newFolder(format.toString());\n+  @TestTemplate\n+  public void testUnpartitionedCreateWithTargetFileSizeViaTableProperties() {\n+    File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n \n@@ -391,8 +387,7 @@ public void testUnpartitionedCreateWithTargetFileSizeViaTableProperties() throws\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n \n     List<DataFile> files = Lists.newArrayList();\n     for (ManifestFile manifest :\n@@ -402,33 +397,37 @@ public void testUnpartitionedCreateWithTargetFileSizeViaTableProperties() throws\n       }\n     }\n \n-    Assert.assertEquals(\"Should have 4 DataFiles\", 4, files.size());\n-    Assert.assertTrue(\n-        \"All DataFiles contain 1000 rows\", files.stream().allMatch(d -> d.recordCount() == 1000));\n+    assertThat(files)\n+        .hasSize(4)\n+        .allSatisfy(\n+            dataFile ->\n+                assertThat(dataFile.recordCount())\n+                    .as(\"All DataFiles contain 1000 rows\")\n+                    .isEqualTo(1000));\n   }\n \n-  @Test\n-  public void testPartitionedCreateWithTargetFileSizeViaOption() throws IOException {\n+  @TestTemplate\n+  public void testPartitionedCreateWithTargetFileSizeViaOption() {\n     partitionedCreateWithTargetFileSizeViaOption(IcebergOptionsType.NONE);\n   }\n \n-  @Test\n-  public void testPartitionedFanoutCreateWithTargetFileSizeViaOption() throws IOException {\n+  @TestTemplate\n+  public void testPartitionedFanoutCreateWithTargetFileSizeViaOption() {\n     partitionedCreateWithTargetFileSizeViaOption(IcebergOptionsType.TABLE);\n   }\n \n-  @Test\n-  public void testPartitionedFanoutCreateWithTargetFileSizeViaOption2() throws IOException {\n+  @TestTemplate\n+  public void testPartitionedFanoutCreateWithTargetFileSizeViaOption2() {\n     partitionedCreateWithTargetFileSizeViaOption(IcebergOptionsType.JOB);\n   }\n \n-  @Test\n-  public void testWriteProjection() throws IOException {\n-    Assume.assumeTrue(\n-        \"Not supported in Spark 3; analysis requires all columns are present\",\n-        spark.version().startsWith(\"2\"));\n+  @TestTemplate\n+  public void testWriteProjection() {\n+    assumeThat(spark.version())\n+        .as(\"Not supported in Spark 3; analysis requires all columns are present\")\n+        .startsWith(\"2\");\n \n-    File parent = temp.newFolder(format.toString());\n+    File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n \n@@ -456,17 +455,16 @@ public void testWriteProjection() throws IOException {\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n-  @Test\n-  public void testWriteProjectionWithMiddle() throws IOException {\n-    Assume.assumeTrue(\n-        \"Not supported in Spark 3; analysis requires all columns are present\",\n-        spark.version().startsWith(\"2\"));\n+  @TestTemplate\n+  public void testWriteProjectionWithMiddle() {\n+    assumeThat(spark.version())\n+        .as(\"Not supported in Spark 3; analysis requires all columns are present\")\n+        .startsWith(\"2\");\n \n-    File parent = temp.newFolder(format.toString());\n+    File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n \n@@ -501,13 +499,12 @@ public void testWriteProjectionWithMiddle() throws IOException {\n \n     List<ThreeColumnRecord> actual =\n         result.orderBy(\"c1\").as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n-  @Test\n-  public void testViewsReturnRecentResults() throws IOException {\n-    File parent = temp.newFolder(format.toString());\n+  @TestTemplate\n+  public void testViewsReturnRecentResults() {\n+    File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n \n@@ -537,8 +534,7 @@ public void testViewsReturnRecentResults() throws IOException {\n     List<SimpleRecord> actual1 =\n         spark.table(\"tmp\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n     List<SimpleRecord> expected1 = Lists.newArrayList(new SimpleRecord(1, \"a\"));\n-    Assert.assertEquals(\"Number of rows should match\", expected1.size(), actual1.size());\n-    Assert.assertEquals(\"Result rows should match\", expected1, actual1);\n+    assertThat(actual1).hasSameSizeAs(expected1).isEqualTo(expected1);\n \n     df.select(\"id\", \"data\")\n         .write()\n@@ -551,13 +547,11 @@ public void testViewsReturnRecentResults() throws IOException {\n         spark.table(\"tmp\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n     List<SimpleRecord> expected2 =\n         Lists.newArrayList(new SimpleRecord(1, \"a\"), new SimpleRecord(1, \"a\"));\n-    Assert.assertEquals(\"Number of rows should match\", expected2.size(), actual2.size());\n-    Assert.assertEquals(\"Result rows should match\", expected2, actual2);\n+    assertThat(actual2).hasSameSizeAs(expected2).isEqualTo(expected2);\n   }\n \n-  public void partitionedCreateWithTargetFileSizeViaOption(IcebergOptionsType option)\n-      throws IOException {\n-    File parent = temp.newFolder(format.toString());\n+  public void partitionedCreateWithTargetFileSizeViaOption(IcebergOptionsType option) {\n+    File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n \n@@ -620,8 +614,7 @@ public void partitionedCreateWithTargetFileSizeViaOption(IcebergOptionsType opti\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n \n     List<DataFile> files = Lists.newArrayList();\n     for (ManifestFile manifest :\n@@ -631,14 +624,18 @@ public void partitionedCreateWithTargetFileSizeViaOption(IcebergOptionsType opti\n       }\n     }\n \n-    Assert.assertEquals(\"Should have 8 DataFiles\", 8, files.size());\n-    Assert.assertTrue(\n-        \"All DataFiles contain 1000 rows\", files.stream().allMatch(d -> d.recordCount() == 1000));\n+    assertThat(files)\n+        .hasSize(8)\n+        .allSatisfy(\n+            dataFile ->\n+                assertThat(dataFile.recordCount())\n+                    .as(\"All DataFiles contain 1000 rows\")\n+                    .isEqualTo(1000));\n   }\n \n-  @Test\n-  public void testCommitUnknownException() throws IOException {\n-    File parent = temp.newFolder(format.toString());\n+  @TestTemplate\n+  public void testCommitUnknownException() {\n+    File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"commitunknown\");\n     String targetLocation = locationWithBranch(location);\n \n@@ -706,10 +703,8 @@ public void testCommitUnknownException() throws IOException {\n     Dataset<Row> result = spark.read().format(\"iceberg\").load(targetLocation);\n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    Assert.assertEquals(\n-        \"Number of rows should match\", records.size() + records2.size(), actual.size());\n     assertThat(actual)\n-        .describedAs(\"Result rows should match\")\n+        .hasSize(records.size() + records2.size())\n         .containsExactlyInAnyOrder(\n             ImmutableList.<SimpleRecord>builder()\n                 .addAll(records)\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java\nindex 584a6b1c7008..becf6a064dcc 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadProjection.java\n@@ -23,6 +23,7 @@\n import static org.apache.iceberg.PlanningMode.LOCAL;\n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.File;\n import java.io.IOException;\n@@ -32,6 +33,9 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataFiles;\n import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.PlanningMode;\n import org.apache.iceberg.Schema;\n@@ -51,40 +55,33 @@\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SparkSession;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.BeforeClass;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-@RunWith(Parameterized.class)\n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkReadProjection extends TestReadProjection {\n \n   private static SparkSession spark = null;\n \n-  @Parameterized.Parameters(name = \"format = {0}, vectorized = {1}, planningMode = {2}\")\n+  @Parameters(name = \"format = {0}, vectorized = {1}, planningMode = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {\"parquet\", false, LOCAL},\n-      {\"parquet\", true, DISTRIBUTED},\n-      {\"avro\", false, LOCAL},\n-      {\"orc\", false, DISTRIBUTED},\n-      {\"orc\", true, LOCAL}\n+      {FileFormat.PARQUET, false, LOCAL},\n+      {FileFormat.PARQUET, true, DISTRIBUTED},\n+      {FileFormat.AVRO, false, LOCAL},\n+      {FileFormat.ORC, false, DISTRIBUTED},\n+      {FileFormat.ORC, true, LOCAL}\n     };\n   }\n \n-  private final FileFormat format;\n-  private final boolean vectorized;\n-  private final PlanningMode planningMode;\n+  @Parameter(index = 1)\n+  private boolean vectorized;\n \n-  public TestSparkReadProjection(String format, boolean vectorized, PlanningMode planningMode) {\n-    super(format);\n-    this.format = FileFormat.fromString(format);\n-    this.vectorized = vectorized;\n-    this.planningMode = planningMode;\n-  }\n+  @Parameter(index = 2)\n+  private PlanningMode planningMode;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestSparkReadProjection.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n     ImmutableMap<String, String> config =\n@@ -100,7 +97,7 @@ public static void startSpark() {\n         (key, value) -> spark.conf().set(\"spark.sql.catalog.spark_catalog.\" + key, value));\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestSparkReadProjection.spark;\n     TestSparkReadProjection.spark = null;\n@@ -110,10 +107,10 @@ public static void stopSpark() {\n   @Override\n   protected Record writeAndRead(String desc, Schema writeSchema, Schema readSchema, Record record)\n       throws IOException {\n-    File parent = temp.newFolder(desc);\n+    File parent = new File(temp.toFile(), desc);\n     File location = new File(parent, \"test\");\n     File dataFolder = new File(location, \"data\");\n-    Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n+    assertThat(dataFolder.mkdirs()).as(\"mkdirs should succeed\").isTrue();\n \n     File testFile = new File(dataFolder, format.addExtension(UUID.randomUUID().toString()));\n \n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java\nindex 4a386ee861d6..d14b1a52cf82 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java\n@@ -18,13 +18,14 @@\n  */\n package org.apache.iceberg.spark.source;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.connector.catalog.CatalogManager;\n import org.apache.spark.sql.connector.catalog.Identifier;\n import org.apache.spark.sql.connector.catalog.TableCatalog;\n-import org.junit.Assert;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n@@ -52,7 +53,7 @@ public void testTableEquality() throws NoSuchTableException {\n     SparkTable table2 = (SparkTable) catalog.loadTable(identifier);\n \n     // different instances pointing to the same table must be equivalent\n-    Assert.assertNotSame(\"References must be different\", table1, table2);\n-    Assert.assertEquals(\"Tables must be equivalent\", table1, table2);\n+    assertThat(table1).as(\"References must be different\").isNotSameAs(table2);\n+    assertThat(table1).as(\"Tables must be equivalent\").isEqualTo(table2);\n   }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/SparkTestHelperBase.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/SparkTestHelperBase.java\nindex e1b75ca55e34..9fc71125a92e 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/SparkTestHelperBase.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/SparkTestHelperBase.java\n@@ -57,7 +57,7 @@ private Object[] toJava(Row row) {\n   protected void assertEquals(\n       String context, List<Object[]> expectedRows, List<Object[]> actualRows) {\n     assertThat(actualRows)\n-        .as(context + \": number of results should match\")\n+        .as(\"%s: number of results should match\", context)\n         .hasSameSizeAs(expectedRows);\n     for (int row = 0; row < expectedRows.size(); row += 1) {\n       Object[] expected = expectedRows.get(row);\n@@ -80,9 +80,7 @@ protected void assertEquals(String context, Object[] expectedRow, Object[] actua\n           assertEquals(newContext, (Object[]) expectedValue, (Object[]) actualValue);\n         }\n       } else if (expectedValue != ANY) {\n-        assertThat(actualValue)\n-            .as(context + \" col \" + (col + 1) + \" contents should match\")\n-            .isEqualTo(expectedValue);\n+        assertThat(actualValue).as(\"%s contents should match\", context).isEqualTo(expectedValue);\n       }\n     }\n   }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkFilters.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkFilters.java\nindex a6205ae9ea3f..49c38b34d34a 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkFilters.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkFilters.java\n@@ -60,61 +60,68 @@ public void testQuotedAttributes() {\n           IsNull isNull = IsNull.apply(quoted);\n           Expression expectedIsNull = Expressions.isNull(unquoted);\n           Expression actualIsNull = SparkFilters.convert(isNull);\n-          assertThat(actualIsNull.toString())\n+          assertThat(actualIsNull)\n+              .asString()\n               .as(\"IsNull must match\")\n               .isEqualTo(expectedIsNull.toString());\n \n           IsNotNull isNotNull = IsNotNull.apply(quoted);\n           Expression expectedIsNotNull = Expressions.notNull(unquoted);\n           Expression actualIsNotNull = SparkFilters.convert(isNotNull);\n-          assertThat(actualIsNotNull.toString())\n+          assertThat(actualIsNotNull)\n+              .asString()\n               .as(\"IsNotNull must match\")\n               .isEqualTo(expectedIsNotNull.toString());\n \n           LessThan lt = LessThan.apply(quoted, 1);\n           Expression expectedLt = Expressions.lessThan(unquoted, 1);\n           Expression actualLt = SparkFilters.convert(lt);\n-          assertThat(actualLt.toString())\n+          assertThat(actualLt)\n+              .asString()\n               .as(\"LessThan must match\")\n               .isEqualTo(expectedLt.toString());\n \n           LessThanOrEqual ltEq = LessThanOrEqual.apply(quoted, 1);\n           Expression expectedLtEq = Expressions.lessThanOrEqual(unquoted, 1);\n           Expression actualLtEq = SparkFilters.convert(ltEq);\n-          assertThat(actualLtEq.toString())\n+          assertThat(actualLtEq)\n+              .asString()\n               .as(\"LessThanOrEqual must match\")\n               .isEqualTo(expectedLtEq.toString());\n \n           GreaterThan gt = GreaterThan.apply(quoted, 1);\n           Expression expectedGt = Expressions.greaterThan(unquoted, 1);\n           Expression actualGt = SparkFilters.convert(gt);\n-          assertThat(actualGt.toString())\n+          assertThat(actualGt)\n+              .asString()\n               .as(\"GreaterThan must match\")\n               .isEqualTo(expectedGt.toString());\n \n           GreaterThanOrEqual gtEq = GreaterThanOrEqual.apply(quoted, 1);\n           Expression expectedGtEq = Expressions.greaterThanOrEqual(unquoted, 1);\n           Expression actualGtEq = SparkFilters.convert(gtEq);\n-          assertThat(actualGtEq.toString())\n+          assertThat(actualGtEq)\n+              .asString()\n               .as(\"GreaterThanOrEqual must match\")\n               .isEqualTo(expectedGtEq.toString());\n \n           EqualTo eq = EqualTo.apply(quoted, 1);\n           Expression expectedEq = Expressions.equal(unquoted, 1);\n           Expression actualEq = SparkFilters.convert(eq);\n-          assertThat(actualEq.toString()).as(\"EqualTo must match\").isEqualTo(expectedEq.toString());\n+          assertThat(actualEq).asString().as(\"EqualTo must match\").isEqualTo(expectedEq.toString());\n \n           EqualNullSafe eqNullSafe = EqualNullSafe.apply(quoted, 1);\n           Expression expectedEqNullSafe = Expressions.equal(unquoted, 1);\n           Expression actualEqNullSafe = SparkFilters.convert(eqNullSafe);\n-          assertThat(actualEqNullSafe.toString())\n+          assertThat(actualEqNullSafe)\n+              .asString()\n               .as(\"EqualNullSafe must match\")\n               .isEqualTo(expectedEqNullSafe.toString());\n \n           In in = In.apply(quoted, new Integer[] {1});\n           Expression expectedIn = Expressions.in(unquoted, 1);\n           Expression actualIn = SparkFilters.convert(in);\n-          assertThat(actualIn.toString()).as(\"In must match\").isEqualTo(expectedIn.toString());\n+          assertThat(actualIn).asString().as(\"In must match\").isEqualTo(expectedIn.toString());\n         });\n   }\n \n@@ -128,11 +135,13 @@ public void testTimestampFilterConversion() {\n     Expression timestampExpression = SparkFilters.convert(GreaterThan.apply(\"x\", timestamp));\n     Expression rawExpression = Expressions.greaterThan(\"x\", epochMicros);\n \n-    assertThat(timestampExpression.toString())\n+    assertThat(timestampExpression)\n+        .asString()\n         .as(\"Generated Timestamp expression should be correct\")\n         .isEqualTo(rawExpression.toString());\n \n-    assertThat(instantExpression.toString())\n+    assertThat(instantExpression)\n+        .asString()\n         .as(\"Generated Instant expression should be correct\")\n         .isEqualTo(rawExpression.toString());\n   }\n@@ -146,7 +155,8 @@ public void testLocalDateTimeFilterConversion() {\n     Expression instantExpression = SparkFilters.convert(GreaterThan.apply(\"x\", ldt));\n     Expression rawExpression = Expressions.greaterThan(\"x\", epochMicros);\n \n-    assertThat(instantExpression.toString())\n+    assertThat(instantExpression)\n+        .asString()\n         .as(\"Generated Instant expression should be correct\")\n         .isEqualTo(rawExpression.toString());\n   }\n@@ -161,11 +171,13 @@ public void testDateFilterConversion() {\n     Expression dateExpression = SparkFilters.convert(GreaterThan.apply(\"x\", date));\n     Expression rawExpression = Expressions.greaterThan(\"x\", epochDay);\n \n-    assertThat(localDateExpression.toString())\n+    assertThat(localDateExpression)\n+        .asString()\n         .as(\"Generated localdate expression should be correct\")\n         .isEqualTo(rawExpression.toString());\n \n-    assertThat(dateExpression.toString())\n+    assertThat(dateExpression)\n+        .asString()\n         .as(\"Generated date expression should be correct\")\n         .isEqualTo(rawExpression.toString());\n   }\n@@ -184,6 +196,6 @@ public void testNotIn() {\n     Expression actual = SparkFilters.convert(filter);\n     Expression expected =\n         Expressions.and(Expressions.notNull(\"col\"), Expressions.notIn(\"col\", 1, 2));\n-    assertThat(actual.toString()).as(\"Expressions should match\").isEqualTo(expected.toString());\n+    assertThat(actual).asString().as(\"Expressions should match\").isEqualTo(expected.toString());\n   }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkSchemaUtil.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkSchemaUtil.java\nindex 4d4091bf9a9a..4045847d5a4a 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkSchemaUtil.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkSchemaUtil.java\n@@ -21,7 +21,6 @@\n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.assertj.core.api.Assertions.assertThat;\n \n-import java.io.IOException;\n import java.util.List;\n import org.apache.iceberg.MetadataColumns;\n import org.apache.iceberg.Schema;\n@@ -45,21 +44,21 @@ public class TestSparkSchemaUtil {\n           MetadataColumns.ROW_POSITION);\n \n   @Test\n-  public void testEstimateSizeMaxValue() throws IOException {\n+  public void testEstimateSizeMaxValue() {\n     assertThat(SparkSchemaUtil.estimateSize(null, Long.MAX_VALUE))\n         .as(\"estimateSize returns Long max value\")\n         .isEqualTo(Long.MAX_VALUE);\n   }\n \n   @Test\n-  public void testEstimateSizeWithOverflow() throws IOException {\n+  public void testEstimateSizeWithOverflow() {\n     long tableSize =\n         SparkSchemaUtil.estimateSize(SparkSchemaUtil.convert(TEST_SCHEMA), Long.MAX_VALUE - 1);\n     assertThat(tableSize).as(\"estimateSize handles overflow\").isEqualTo(Long.MAX_VALUE);\n   }\n \n   @Test\n-  public void testEstimateSize() throws IOException {\n+  public void testEstimateSize() {\n     long tableSize = SparkSchemaUtil.estimateSize(SparkSchemaUtil.convert(TEST_SCHEMA), 1);\n     assertThat(tableSize).as(\"estimateSize matches with expected approximation\").isEqualTo(24);\n   }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkTableUtil.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkTableUtil.java\nindex 772ae3a224ac..93e4c8968715 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkTableUtil.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkTableUtil.java\n@@ -68,11 +68,14 @@ public void testMetricsConfigKryoSerialization() throws Exception {\n     MetricsConfig config = MetricsConfig.fromProperties(metricsConfig);\n     MetricsConfig deserialized = KryoHelpers.roundTripSerialize(config);\n \n-    assertThat(deserialized.columnMode(\"col1\").toString())\n+    assertThat(deserialized.columnMode(\"col1\"))\n+        .asString()\n         .isEqualTo(MetricsModes.Full.get().toString());\n-    assertThat(deserialized.columnMode(\"col2\").toString())\n+    assertThat(deserialized.columnMode(\"col2\"))\n+        .asString()\n         .isEqualTo(MetricsModes.Truncate.withLength(16).toString());\n-    assertThat(deserialized.columnMode(\"col3\").toString())\n+    assertThat(deserialized.columnMode(\"col3\"))\n+        .asString()\n         .isEqualTo(MetricsModes.Counts.get().toString());\n   }\n \n@@ -90,11 +93,14 @@ public void testMetricsConfigJavaSerialization() throws Exception {\n     MetricsConfig config = MetricsConfig.fromProperties(metricsConfig);\n     MetricsConfig deserialized = TestHelpers.roundTripSerialize(config);\n \n-    assertThat(deserialized.columnMode(\"col1\").toString())\n+    assertThat(deserialized.columnMode(\"col1\"))\n+        .asString()\n         .isEqualTo(MetricsModes.Full.get().toString());\n-    assertThat(deserialized.columnMode(\"col2\").toString())\n+    assertThat(deserialized.columnMode(\"col2\"))\n+        .asString()\n         .isEqualTo(MetricsModes.Truncate.withLength(16).toString());\n-    assertThat(deserialized.columnMode(\"col3\").toString())\n+    assertThat(deserialized.columnMode(\"col3\"))\n+        .asString()\n         .isEqualTo(MetricsModes.Counts.get().toString());\n   }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkV2Filters.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkV2Filters.java\nindex 44fb64120ca0..e0b590e5a6e8 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkV2Filters.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkV2Filters.java\n@@ -89,112 +89,128 @@ public void testV2Filters() {\n           Predicate isNull = new Predicate(\"IS_NULL\", attrOnly);\n           Expression expectedIsNull = Expressions.isNull(unquoted);\n           Expression actualIsNull = SparkV2Filters.convert(isNull);\n-          assertThat(actualIsNull.toString())\n+          assertThat(actualIsNull)\n+              .asString()\n               .as(\"IsNull must match\")\n               .isEqualTo(expectedIsNull.toString());\n \n           Predicate isNotNull = new Predicate(\"IS_NOT_NULL\", attrOnly);\n           Expression expectedIsNotNull = Expressions.notNull(unquoted);\n           Expression actualIsNotNull = SparkV2Filters.convert(isNotNull);\n-          assertThat(actualIsNotNull.toString())\n+          assertThat(actualIsNotNull)\n+              .asString()\n               .as(\"IsNotNull must match\")\n               .isEqualTo(expectedIsNotNull.toString());\n \n           Predicate lt1 = new Predicate(\"<\", attrAndValue);\n           Expression expectedLt1 = Expressions.lessThan(unquoted, 1);\n           Expression actualLt1 = SparkV2Filters.convert(lt1);\n-          assertThat(actualLt1.toString())\n+          assertThat(actualLt1)\n+              .asString()\n               .as(\"LessThan must match\")\n               .isEqualTo(expectedLt1.toString());\n \n           Predicate lt2 = new Predicate(\"<\", valueAndAttr);\n           Expression expectedLt2 = Expressions.greaterThan(unquoted, 1);\n           Expression actualLt2 = SparkV2Filters.convert(lt2);\n-          assertThat(actualLt2.toString())\n+          assertThat(actualLt2)\n+              .asString()\n               .as(\"LessThan must match\")\n               .isEqualTo(expectedLt2.toString());\n \n           Predicate ltEq1 = new Predicate(\"<=\", attrAndValue);\n           Expression expectedLtEq1 = Expressions.lessThanOrEqual(unquoted, 1);\n           Expression actualLtEq1 = SparkV2Filters.convert(ltEq1);\n-          assertThat(actualLtEq1.toString())\n+          assertThat(actualLtEq1)\n+              .asString()\n               .as(\"LessThanOrEqual must match\")\n               .isEqualTo(expectedLtEq1.toString());\n \n           Predicate ltEq2 = new Predicate(\"<=\", valueAndAttr);\n           Expression expectedLtEq2 = Expressions.greaterThanOrEqual(unquoted, 1);\n           Expression actualLtEq2 = SparkV2Filters.convert(ltEq2);\n-          assertThat(actualLtEq2.toString())\n+          assertThat(actualLtEq2)\n+              .asString()\n               .as(\"LessThanOrEqual must match\")\n               .isEqualTo(expectedLtEq2.toString());\n \n           Predicate gt1 = new Predicate(\">\", attrAndValue);\n           Expression expectedGt1 = Expressions.greaterThan(unquoted, 1);\n           Expression actualGt1 = SparkV2Filters.convert(gt1);\n-          assertThat(actualGt1.toString())\n+          assertThat(actualGt1)\n+              .asString()\n               .as(\"GreaterThan must match\")\n               .isEqualTo(expectedGt1.toString());\n \n           Predicate gt2 = new Predicate(\">\", valueAndAttr);\n           Expression expectedGt2 = Expressions.lessThan(unquoted, 1);\n           Expression actualGt2 = SparkV2Filters.convert(gt2);\n-          assertThat(actualGt2.toString())\n+          assertThat(actualGt2)\n+              .asString()\n               .as(\"GreaterThan must match\")\n               .isEqualTo(expectedGt2.toString());\n \n           Predicate gtEq1 = new Predicate(\">=\", attrAndValue);\n           Expression expectedGtEq1 = Expressions.greaterThanOrEqual(unquoted, 1);\n           Expression actualGtEq1 = SparkV2Filters.convert(gtEq1);\n-          assertThat(actualGtEq1.toString())\n+          assertThat(actualGtEq1)\n+              .asString()\n               .as(\"GreaterThanOrEqual must match\")\n               .isEqualTo(expectedGtEq1.toString());\n \n           Predicate gtEq2 = new Predicate(\">=\", valueAndAttr);\n           Expression expectedGtEq2 = Expressions.lessThanOrEqual(unquoted, 1);\n           Expression actualGtEq2 = SparkV2Filters.convert(gtEq2);\n-          assertThat(actualGtEq2.toString())\n+          assertThat(actualGtEq2)\n+              .asString()\n               .as(\"GreaterThanOrEqual must match\")\n               .isEqualTo(expectedGtEq2.toString());\n \n           Predicate eq1 = new Predicate(\"=\", attrAndValue);\n           Expression expectedEq1 = Expressions.equal(unquoted, 1);\n           Expression actualEq1 = SparkV2Filters.convert(eq1);\n-          assertThat(actualEq1.toString())\n+          assertThat(actualEq1)\n+              .asString()\n               .as(\"EqualTo must match\")\n               .isEqualTo(expectedEq1.toString());\n \n           Predicate eq2 = new Predicate(\"=\", valueAndAttr);\n           Expression expectedEq2 = Expressions.equal(unquoted, 1);\n           Expression actualEq2 = SparkV2Filters.convert(eq2);\n-          assertThat(actualEq2.toString())\n+          assertThat(actualEq2)\n+              .asString()\n               .as(\"EqualTo must match\")\n               .isEqualTo(expectedEq2.toString());\n \n           Predicate notEq1 = new Predicate(\"<>\", attrAndValue);\n           Expression expectedNotEq1 = Expressions.notEqual(unquoted, 1);\n           Expression actualNotEq1 = SparkV2Filters.convert(notEq1);\n-          assertThat(actualNotEq1.toString())\n+          assertThat(actualNotEq1)\n+              .asString()\n               .as(\"NotEqualTo must match\")\n               .isEqualTo(expectedNotEq1.toString());\n \n           Predicate notEq2 = new Predicate(\"<>\", valueAndAttr);\n           Expression expectedNotEq2 = Expressions.notEqual(unquoted, 1);\n           Expression actualNotEq2 = SparkV2Filters.convert(notEq2);\n-          assertThat(actualNotEq2.toString())\n+          assertThat(actualNotEq2)\n+              .asString()\n               .as(\"NotEqualTo must match\")\n               .isEqualTo(expectedNotEq2.toString());\n \n           Predicate eqNullSafe1 = new Predicate(\"<=>\", attrAndValue);\n           Expression expectedEqNullSafe1 = Expressions.equal(unquoted, 1);\n           Expression actualEqNullSafe1 = SparkV2Filters.convert(eqNullSafe1);\n-          assertThat(actualEqNullSafe1.toString())\n+          assertThat(actualEqNullSafe1)\n+              .asString()\n               .as(\"EqualNullSafe must match\")\n               .isEqualTo(expectedEqNullSafe1.toString());\n \n           Predicate eqNullSafe2 = new Predicate(\"<=>\", valueAndAttr);\n           Expression expectedEqNullSafe2 = Expressions.equal(unquoted, 1);\n           Expression actualEqNullSafe2 = SparkV2Filters.convert(eqNullSafe2);\n-          assertThat(actualEqNullSafe2.toString())\n+          assertThat(actualEqNullSafe2)\n+              .asString()\n               .as(\"EqualNullSafe must match\")\n               .isEqualTo(expectedEqNullSafe2.toString());\n \n@@ -205,19 +221,20 @@ public void testV2Filters() {\n           Predicate startsWith = new Predicate(\"STARTS_WITH\", attrAndStr);\n           Expression expectedStartsWith = Expressions.startsWith(unquoted, \"iceberg\");\n           Expression actualStartsWith = SparkV2Filters.convert(startsWith);\n-          assertThat(actualStartsWith.toString())\n+          assertThat(actualStartsWith)\n+              .asString()\n               .as(\"StartsWith must match\")\n               .isEqualTo(expectedStartsWith.toString());\n \n           Predicate in = new Predicate(\"IN\", attrAndValue);\n           Expression expectedIn = Expressions.in(unquoted, 1);\n           Expression actualIn = SparkV2Filters.convert(in);\n-          assertThat(actualIn.toString()).as(\"In must match\").isEqualTo(expectedIn.toString());\n+          assertThat(actualIn).asString().as(\"In must match\").isEqualTo(expectedIn.toString());\n \n           Predicate and = new And(lt1, eq1);\n           Expression expectedAnd = Expressions.and(expectedLt1, expectedEq1);\n           Expression actualAnd = SparkV2Filters.convert(and);\n-          assertThat(actualAnd.toString()).as(\"And must match\").isEqualTo(expectedAnd.toString());\n+          assertThat(actualAnd).asString().as(\"And must match\").isEqualTo(expectedAnd.toString());\n \n           org.apache.spark.sql.connector.expressions.Expression[] attrAndAttr =\n               new org.apache.spark.sql.connector.expressions.Expression[] {\n@@ -231,7 +248,7 @@ public void testV2Filters() {\n           Predicate or = new Or(lt1, eq1);\n           Expression expectedOr = Expressions.or(expectedLt1, expectedEq1);\n           Expression actualOr = SparkV2Filters.convert(or);\n-          assertThat(actualOr.toString()).as(\"Or must match\").isEqualTo(expectedOr.toString());\n+          assertThat(actualOr).asString().as(\"Or must match\").isEqualTo(expectedOr.toString());\n \n           Predicate orWithInvalidLeft = new Or(invalid, eq1);\n           Expression convertedOr = SparkV2Filters.convert(orWithInvalidLeft);\n@@ -240,7 +257,7 @@ public void testV2Filters() {\n           Predicate not = new Not(lt1);\n           Expression expectedNot = Expressions.not(expectedLt1);\n           Expression actualNot = SparkV2Filters.convert(not);\n-          assertThat(actualNot.toString()).as(\"Not must match\").isEqualTo(expectedNot.toString());\n+          assertThat(actualNot).asString().as(\"Not must match\").isEqualTo(expectedNot.toString());\n         });\n   }\n \n@@ -396,7 +413,8 @@ public void testTimestampFilterConversion() {\n     Expression tsExpression = SparkV2Filters.convert(predicate);\n     Expression rawExpression = Expressions.greaterThan(\"x\", epochMicros);\n \n-    assertThat(tsExpression.toString())\n+    assertThat(tsExpression)\n+        .asString()\n         .as(\"Generated Timestamp expression should be correct\")\n         .isEqualTo(rawExpression.toString());\n   }\n@@ -415,7 +433,8 @@ public void testDateFilterConversion() {\n     Expression dateExpression = SparkV2Filters.convert(predicate);\n     Expression rawExpression = Expressions.greaterThan(\"x\", epochDay);\n \n-    assertThat(dateExpression.toString())\n+    assertThat(dateExpression)\n+        .asString()\n         .as(\"Generated date expression should be correct\")\n         .isEqualTo(rawExpression.toString());\n   }\n@@ -453,7 +472,7 @@ public void testNotIn() {\n     Expression actual = SparkV2Filters.convert(not);\n     Expression expected =\n         Expressions.and(Expressions.notNull(\"col\"), Expressions.notIn(\"col\", 1, 2));\n-    assertThat(actual.toString()).as(\"Expressions should match\").isEqualTo(expected.toString());\n+    assertThat(actual).asString().as(\"Expressions should match\").isEqualTo(expected.toString());\n   }\n \n   @Test\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\nindex 7dbe99d64e51..9cf8e435270c 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\n@@ -37,7 +37,6 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n-import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.types.Comparators;\n import org.apache.iceberg.types.Types;\n import org.junit.jupiter.api.TestTemplate;\n@@ -73,7 +72,6 @@ public void testFullProjection() throws Exception {\n \n     int cmp =\n         Comparators.charSequences().compare(\"test\", (CharSequence) projected.getField(\"data\"));\n-\n     assertThat(cmp).as(\"Should contain the correct data value\").isEqualTo(0);\n   }\n \n@@ -99,7 +97,8 @@ public void testReorderedFullProjection() throws Exception {\n \n     Record projected = writeAndRead(\"reordered_full_projection\", schema, reordered, record);\n \n-    assertThat(projected.get(0).toString())\n+    assertThat(projected.get(0))\n+        .asString()\n         .as(\"Should contain the correct 0 value\")\n         .isEqualTo(\"test\");\n     assertThat(projected.get(1)).as(\"Should contain the correct 1 value\").isEqualTo(34L);\n@@ -387,10 +386,7 @@ public void testMapOfStructsProjection() throws IOException {\n     assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n \n     Map<String, ?> locations = toStringMap((Map) projected.getField(\"locations\"));\n-    assertThat(locations).as(\"Should project locations map\").isNotNull();\n-    assertThat(locations.keySet())\n-        .as(\"Should contain L1 and L2\")\n-        .isEqualTo(Sets.newHashSet(\"L1\", \"L2\"));\n+    assertThat(locations).isNotNull().containsKeys(\"L1\", \"L2\");\n \n     Record projectedL1 = (Record) locations.get(\"L1\");\n     assertThat(projectedL1).as(\"L1 should not be null\").isNotNull();\n@@ -411,10 +407,7 @@ public void testMapOfStructsProjection() throws IOException {\n     assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n \n     locations = toStringMap((Map) projected.getField(\"locations\"));\n-    assertThat(locations).as(\"Should project locations map\").isNotNull();\n-    assertThat(locations.keySet())\n-        .as(\"Should contain L1 and L2\")\n-        .isEqualTo(Sets.newHashSet(\"L1\", \"L2\"));\n+    assertThat(locations).isNotNull().containsKeys(\"L1\", \"L2\");\n \n     projectedL1 = (Record) locations.get(\"L1\");\n     assertThat(projectedL1).as(\"L1 should not be null\").isNotNull();\n@@ -445,10 +438,7 @@ public void testMapOfStructsProjection() throws IOException {\n     projected = writeAndRead(\"latitude_renamed\", writeSchema, latitiudeRenamed, record);\n     assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n     locations = toStringMap((Map) projected.getField(\"locations\"));\n-    assertThat(locations).as(\"Should project locations map\").isNotNull();\n-    assertThat(locations.keySet())\n-        .as(\"Should contain L1 and L2\")\n-        .isEqualTo(Sets.newHashSet(\"L1\", \"L2\"));\n+    assertThat(locations).isNotNull().containsKeys(\"L1\", \"L2\");\n \n     projectedL1 = (Record) locations.get(\"L1\");\n     assertThat(projectedL1).as(\"L1 should not be null\").isNotNull();\n@@ -545,14 +535,20 @@ public void testListOfStructsProjection() throws IOException {\n \n     List<Record> points = (List<Record>) projected.getField(\"points\");\n     assertThat(points).as(\"Should read 2 points\").hasSize(2);\n-\n-    Record projectedP1 = points.get(0);\n-    assertThat((int) projectedP1.getField(\"x\")).as(\"Should project x\").isEqualTo(1);\n-    assertThat(projected.getField(\"y\")).as(\"Should not project y\").isNull();\n-\n-    Record projectedP2 = points.get(1);\n-    assertThat((int) projectedP2.getField(\"x\")).as(\"Should project x\").isEqualTo(3);\n-    assertThat(projected.getField(\"y\")).as(\"Should not project y\").isNull();\n+    assertThat(points)\n+        .element(0)\n+        .satisfies(\n+            projectedP1 -> {\n+              assertThat((int) projectedP1.getField(\"x\")).isEqualTo(1);\n+              assertThat(projectedP1.getField(\"y\")).isNull();\n+            });\n+    assertThat(points)\n+        .element(1)\n+        .satisfies(\n+            projectedP2 -> {\n+              assertThat((int) projectedP2.getField(\"x\")).isEqualTo(3);\n+              assertThat(projectedP2.getField(\"y\")).isNull();\n+            });\n \n     projected = writeAndRead(\"y_only\", writeSchema, writeSchema.select(\"points.y\"), record);\n     assertThat(projected.getField(\"id\")).as(\"Should not project id\").isNull();\n@@ -560,14 +556,20 @@ public void testListOfStructsProjection() throws IOException {\n \n     points = (List<Record>) projected.getField(\"points\");\n     assertThat(points).as(\"Should read 2 points\").hasSize(2);\n-\n-    projectedP1 = points.get(0);\n-    assertThat(projectedP1.getField(\"x\")).as(\"Should not project x\").isNull();\n-    assertThat((int) projectedP1.getField(\"y\")).as(\"Should project y\").isEqualTo(2);\n-\n-    projectedP2 = points.get(1);\n-    assertThat(projectedP2.getField(\"x\")).as(\"Should not project x\").isNull();\n-    assertThat(projectedP2.getField(\"y\")).as(\"Should not project y\").isNull();\n+    assertThat(points)\n+        .element(0)\n+        .satisfies(\n+            projectedP1 -> {\n+              assertThat(projectedP1.getField(\"x\")).isNull();\n+              assertThat((int) projectedP1.getField(\"y\")).isEqualTo(2);\n+            });\n+    assertThat(points)\n+        .element(1)\n+        .satisfies(\n+            projectedP2 -> {\n+              assertThat(projectedP2.getField(\"x\")).isNull();\n+              assertThat(projectedP2.getField(\"y\")).isNull();\n+            });\n \n     Schema yRenamed =\n         new Schema(\n@@ -585,16 +587,22 @@ public void testListOfStructsProjection() throws IOException {\n \n     points = (List<Record>) projected.getField(\"points\");\n     assertThat(points).as(\"Should read 2 points\").hasSize(2);\n-\n-    projectedP1 = points.get(0);\n-    assertThat(projectedP1.getField(\"x\")).as(\"Should not project x\").isNull();\n-    assertThat(projectedP1.getField(\"y\")).as(\"Should not project y\").isNull();\n-    assertThat((int) projectedP1.getField(\"z\")).as(\"Should project z\").isEqualTo(2);\n-\n-    projectedP2 = points.get(1);\n-    assertThat(projectedP2.getField(\"x\")).as(\"Should not project x\").isNull();\n-    assertThat(projectedP2.getField(\"y\")).as(\"Should not project y\").isNull();\n-    assertThat(projectedP2.getField(\"z\")).as(\"Should project null z\").isNull();\n+    assertThat(points)\n+        .element(0)\n+        .satisfies(\n+            projectedP1 -> {\n+              assertThat(projectedP1.getField(\"x\")).isNull();\n+              assertThat(projectedP1.getField(\"y\")).isNull();\n+              assertThat((int) projectedP1.getField(\"z\")).isEqualTo(2);\n+            });\n+    assertThat(points)\n+        .element(1)\n+        .satisfies(\n+            projectedP2 -> {\n+              assertThat(projectedP2.getField(\"x\")).isNull();\n+              assertThat(projectedP2.getField(\"y\")).isNull();\n+              assertThat(projectedP2.getField(\"z\")).isNull();\n+            });\n \n     Schema zAdded =\n         new Schema(\n@@ -614,16 +622,22 @@ public void testListOfStructsProjection() throws IOException {\n \n     points = (List<Record>) projected.getField(\"points\");\n     assertThat(points).as(\"Should read 2 points\").hasSize(2);\n-\n-    projectedP1 = points.get(0);\n-    assertThat((int) projectedP1.getField(\"x\")).as(\"Should project x\").isEqualTo(1);\n-    assertThat((int) projectedP1.getField(\"y\")).as(\"Should project y\").isEqualTo(2);\n-    assertThat(projectedP1.getField(\"z\")).as(\"Should contain null z\").isNull();\n-\n-    projectedP2 = points.get(1);\n-    assertThat((int) projectedP2.getField(\"x\")).as(\"Should project x\").isEqualTo(3);\n-    assertThat(projectedP2.getField(\"y\")).as(\"Should project null y\").isNull();\n-    assertThat(projectedP2.getField(\"z\")).as(\"Should contain null z\").isNull();\n+    assertThat(points)\n+        .element(0)\n+        .satisfies(\n+            projectedP1 -> {\n+              assertThat((int) projectedP1.getField(\"x\")).isEqualTo(1);\n+              assertThat((int) projectedP1.getField(\"y\")).isEqualTo(2);\n+              assertThat(projectedP1.getField(\"z\")).isNull();\n+            });\n+    assertThat(points)\n+        .element(1)\n+        .satisfies(\n+            projectedP2 -> {\n+              assertThat((int) projectedP2.getField(\"x\")).isEqualTo(3);\n+              assertThat(projectedP2.getField(\"y\")).isNull();\n+              assertThat(projectedP2.getField(\"z\")).isNull();\n+            });\n   }\n \n   private static org.apache.avro.Schema fromOption(org.apache.avro.Schema schema) {\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java\nindex a7334a580ca6..3d74b9135991 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSnapshotSelection.java\n@@ -24,7 +24,6 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n-import java.io.IOException;\n import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n@@ -98,7 +97,7 @@ public static void stopSpark() {\n   }\n \n   @TestTemplate\n-  public void testSnapshotSelectionById() throws IOException {\n+  public void testSnapshotSelectionById() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -145,7 +144,7 @@ public void testSnapshotSelectionById() throws IOException {\n   }\n \n   @TestTemplate\n-  public void testSnapshotSelectionByTimestamp() throws IOException {\n+  public void testSnapshotSelectionByTimestamp() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -197,7 +196,7 @@ public void testSnapshotSelectionByTimestamp() throws IOException {\n   }\n \n   @TestTemplate\n-  public void testSnapshotSelectionByInvalidSnapshotId() throws IOException {\n+  public void testSnapshotSelectionByInvalidSnapshotId() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -212,7 +211,7 @@ public void testSnapshotSelectionByInvalidSnapshotId() throws IOException {\n   }\n \n   @TestTemplate\n-  public void testSnapshotSelectionByInvalidTimestamp() throws IOException {\n+  public void testSnapshotSelectionByInvalidTimestamp() {\n     long timestamp = System.currentTimeMillis();\n \n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n@@ -232,7 +231,7 @@ public void testSnapshotSelectionByInvalidTimestamp() throws IOException {\n   }\n \n   @TestTemplate\n-  public void testSnapshotSelectionBySnapshotIdAndTimestamp() throws IOException {\n+  public void testSnapshotSelectionBySnapshotIdAndTimestamp() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -264,7 +263,7 @@ public void testSnapshotSelectionBySnapshotIdAndTimestamp() throws IOException {\n   }\n \n   @TestTemplate\n-  public void testSnapshotSelectionByTag() throws IOException {\n+  public void testSnapshotSelectionByTag() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -300,7 +299,7 @@ public void testSnapshotSelectionByTag() throws IOException {\n   }\n \n   @TestTemplate\n-  public void testSnapshotSelectionByBranch() throws IOException {\n+  public void testSnapshotSelectionByBranch() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -336,7 +335,7 @@ public void testSnapshotSelectionByBranch() throws IOException {\n   }\n \n   @TestTemplate\n-  public void testSnapshotSelectionByBranchAndTagFails() throws IOException {\n+  public void testSnapshotSelectionByBranchAndTagFails() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -367,7 +366,7 @@ public void testSnapshotSelectionByBranchAndTagFails() throws IOException {\n   }\n \n   @TestTemplate\n-  public void testSnapshotSelectionByTimestampAndBranchOrTagFails() throws IOException {\n+  public void testSnapshotSelectionByTimestampAndBranchOrTagFails() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -410,7 +409,7 @@ public void testSnapshotSelectionByTimestampAndBranchOrTagFails() throws IOExcep\n   }\n \n   @TestTemplate\n-  public void testSnapshotSelectionByBranchWithSchemaChange() throws IOException {\n+  public void testSnapshotSelectionByBranchWithSchemaChange() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -467,7 +466,7 @@ public void testSnapshotSelectionByBranchWithSchemaChange() throws IOException {\n   }\n \n   @TestTemplate\n-  public void testWritingToBranchAfterSchemaChange() throws IOException {\n+  public void testWritingToBranchAfterSchemaChange() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -541,7 +540,7 @@ public void testWritingToBranchAfterSchemaChange() throws IOException {\n   }\n \n   @TestTemplate\n-  public void testSnapshotSelectionByTagWithSchemaChange() throws IOException {\n+  public void testSnapshotSelectionByTagWithSchemaChange() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java\nindex fb2b312bed97..d2fde806f81a 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataWrite.java\n@@ -28,7 +28,6 @@\n import static org.mockito.Mockito.when;\n \n import java.io.File;\n-import java.io.IOException;\n import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n@@ -112,7 +111,7 @@ public static void stopSpark() {\n   }\n \n   @TestTemplate\n-  public void testBasicWrite() throws IOException {\n+  public void testBasicWrite() {\n     File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n@@ -141,8 +140,7 @@ public void testBasicWrite() throws IOException {\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-    assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n     for (ManifestFile manifest :\n         SnapshotUtil.latestSnapshot(table, branch).allManifests(table.io())) {\n       for (DataFile file : ManifestFiles.read(manifest, table.io())) {\n@@ -164,7 +162,7 @@ public void testBasicWrite() throws IOException {\n   }\n \n   @TestTemplate\n-  public void testAppend() throws IOException {\n+  public void testAppend() {\n     File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n@@ -211,12 +209,11 @@ public void testAppend() throws IOException {\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-    assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n   @TestTemplate\n-  public void testEmptyOverwrite() throws IOException {\n+  public void testEmptyOverwrite() {\n     File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n@@ -257,12 +254,11 @@ public void testEmptyOverwrite() throws IOException {\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-    assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n   @TestTemplate\n-  public void testOverwrite() throws IOException {\n+  public void testOverwrite() {\n     File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n@@ -310,12 +306,11 @@ public void testOverwrite() throws IOException {\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-    assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n   @TestTemplate\n-  public void testUnpartitionedOverwrite() throws IOException {\n+  public void testUnpartitionedOverwrite() {\n     File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n@@ -353,12 +348,11 @@ public void testUnpartitionedOverwrite() throws IOException {\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-    assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n   @TestTemplate\n-  public void testUnpartitionedCreateWithTargetFileSizeViaTableProperties() throws IOException {\n+  public void testUnpartitionedCreateWithTargetFileSizeViaTableProperties() {\n     File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n@@ -393,8 +387,7 @@ public void testUnpartitionedCreateWithTargetFileSizeViaTableProperties() throws\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-    assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n \n     List<DataFile> files = Lists.newArrayList();\n     for (ManifestFile manifest :\n@@ -404,29 +397,32 @@ public void testUnpartitionedCreateWithTargetFileSizeViaTableProperties() throws\n       }\n     }\n \n-    assertThat(files).as(\"Should have 4 DataFiles\").hasSize(4);\n-    assertThat(files.stream())\n-        .as(\"All DataFiles contain 1000 rows\")\n-        .allMatch(d -> d.recordCount() == 1000);\n+    assertThat(files)\n+        .hasSize(4)\n+        .allSatisfy(\n+            dataFile ->\n+                assertThat(dataFile.recordCount())\n+                    .as(\"All DataFiles contain 1000 rows\")\n+                    .isEqualTo(1000));\n   }\n \n   @TestTemplate\n-  public void testPartitionedCreateWithTargetFileSizeViaOption() throws IOException {\n+  public void testPartitionedCreateWithTargetFileSizeViaOption() {\n     partitionedCreateWithTargetFileSizeViaOption(IcebergOptionsType.NONE);\n   }\n \n   @TestTemplate\n-  public void testPartitionedFanoutCreateWithTargetFileSizeViaOption() throws IOException {\n+  public void testPartitionedFanoutCreateWithTargetFileSizeViaOption() {\n     partitionedCreateWithTargetFileSizeViaOption(IcebergOptionsType.TABLE);\n   }\n \n   @TestTemplate\n-  public void testPartitionedFanoutCreateWithTargetFileSizeViaOption2() throws IOException {\n+  public void testPartitionedFanoutCreateWithTargetFileSizeViaOption2() {\n     partitionedCreateWithTargetFileSizeViaOption(IcebergOptionsType.JOB);\n   }\n \n   @TestTemplate\n-  public void testWriteProjection() throws IOException {\n+  public void testWriteProjection() {\n     assumeThat(spark.version())\n         .as(\"Not supported in Spark 3; analysis requires all columns are present\")\n         .startsWith(\"2\");\n@@ -459,12 +455,11 @@ public void testWriteProjection() throws IOException {\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-    assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n   @TestTemplate\n-  public void testWriteProjectionWithMiddle() throws IOException {\n+  public void testWriteProjectionWithMiddle() {\n     assumeThat(spark.version())\n         .as(\"Not supported in Spark 3; analysis requires all columns are present\")\n         .startsWith(\"2\");\n@@ -504,12 +499,11 @@ public void testWriteProjectionWithMiddle() throws IOException {\n \n     List<ThreeColumnRecord> actual =\n         result.orderBy(\"c1\").as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n-    assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-    assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n   @TestTemplate\n-  public void testViewsReturnRecentResults() throws IOException {\n+  public void testViewsReturnRecentResults() {\n     File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n@@ -540,8 +534,7 @@ public void testViewsReturnRecentResults() throws IOException {\n     List<SimpleRecord> actual1 =\n         spark.table(\"tmp\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n     List<SimpleRecord> expected1 = Lists.newArrayList(new SimpleRecord(1, \"a\"));\n-    assertThat(actual1).as(\"Number of rows should match\").hasSameSizeAs(expected1);\n-    assertThat(actual1).as(\"Result rows should match\").isEqualTo(expected1);\n+    assertThat(actual1).hasSameSizeAs(expected1).isEqualTo(expected1);\n \n     df.select(\"id\", \"data\")\n         .write()\n@@ -554,12 +547,10 @@ public void testViewsReturnRecentResults() throws IOException {\n         spark.table(\"tmp\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n     List<SimpleRecord> expected2 =\n         Lists.newArrayList(new SimpleRecord(1, \"a\"), new SimpleRecord(1, \"a\"));\n-    assertThat(actual2).as(\"Number of rows should match\").hasSameSizeAs(expected2);\n-    assertThat(actual2).as(\"Result rows should match\").isEqualTo(expected2);\n+    assertThat(actual2).hasSameSizeAs(expected2).isEqualTo(expected2);\n   }\n \n-  public void partitionedCreateWithTargetFileSizeViaOption(IcebergOptionsType option)\n-      throws IOException {\n+  public void partitionedCreateWithTargetFileSizeViaOption(IcebergOptionsType option) {\n     File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"test\");\n     String targetLocation = locationWithBranch(location);\n@@ -623,8 +614,7 @@ public void partitionedCreateWithTargetFileSizeViaOption(IcebergOptionsType opti\n \n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-    assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n \n     List<DataFile> files = Lists.newArrayList();\n     for (ManifestFile manifest :\n@@ -633,14 +623,17 @@ public void partitionedCreateWithTargetFileSizeViaOption(IcebergOptionsType opti\n         files.add(file);\n       }\n     }\n-    assertThat(files).as(\"Should have 8 DataFiles\").hasSize(8);\n-    assertThat(files.stream())\n-        .as(\"All DataFiles contain 1000 rows\")\n-        .allMatch(d -> d.recordCount() == 1000);\n+    assertThat(files)\n+        .hasSize(8)\n+        .allSatisfy(\n+            dataFile ->\n+                assertThat(dataFile.recordCount())\n+                    .as(\"All DataFiles contain 1000 rows\")\n+                    .isEqualTo(1000));\n   }\n \n   @TestTemplate\n-  public void testCommitUnknownException() throws IOException {\n+  public void testCommitUnknownException() {\n     File parent = temp.resolve(format.toString()).toFile();\n     File location = new File(parent, \"commitunknown\");\n     String targetLocation = locationWithBranch(location);\n@@ -709,9 +702,8 @@ public void testCommitUnknownException() throws IOException {\n     Dataset<Row> result = spark.read().format(\"iceberg\").load(targetLocation);\n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    assertThat(actual).as(\"Number of rows should match\").hasSize(records.size() + records2.size());\n     assertThat(actual)\n-        .describedAs(\"Result rows should match\")\n+        .hasSize(records.size() + records2.size())\n         .containsExactlyInAnyOrder(\n             ImmutableList.<SimpleRecord>builder()\n                 .addAll(records)\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13040",
    "pr_id": 13040,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 23,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestBaseReader.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSpark.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestInternalRowWrapper.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAggregates.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataFile.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderWithBloomFilter.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStreamingOffset.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestWriteMetricsConfig.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSpark.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAggregates.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataFile.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestWriteMetricsConfig.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestBaseReader.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSpark.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestInternalRowWrapper.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAggregates.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataFile.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderWithBloomFilter.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStreamingOffset.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestWriteMetricsConfig.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSpark.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAggregates.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataFile.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestWriteMetricsConfig.java"
    ],
    "base_commit": "5d2230ead79da64a8c871a02eb1304a94aaece5c",
    "head_commit": "d7a744d1e4ebe1376e2012048b0e6b73d560e4ba",
    "repo_url": "https://github.com/apache/iceberg/pull/13040",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13040",
    "dockerfile": "",
    "pr_merged_at": "2025-05-13T17:03:15.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java\nindex aed41d19c1e7..d43d4dc05a43 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java\n@@ -64,7 +64,6 @@\n import org.apache.iceberg.spark.data.TestHelpers;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.Dataset;\n-import org.junit.Assert;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n@@ -653,7 +652,7 @@ public void testWithExpiringDanglingStageCommit() {\n         .hasSameSizeAs(deletedFiles);\n     // Take the diff\n     expectedDeletes.removeAll(deletedFiles);\n-    Assert.assertTrue(\"Exactly same files should be deleted\", expectedDeletes.isEmpty());\n+    assertThat(expectedDeletes).as(\"Exactly same files should be deleted\").isEmpty();\n   }\n \n   /**\n@@ -669,7 +668,7 @@ public void testWithCherryPickTableSnapshot() {\n     // `B` commit\n     Set<String> deletedAFiles = Sets.newHashSet();\n     table.newOverwrite().addFile(FILE_B).deleteFile(FILE_A).deleteWith(deletedAFiles::add).commit();\n-    Assert.assertTrue(\"No files should be physically deleted\", deletedAFiles.isEmpty());\n+    assertThat(deletedAFiles).as(\"No files should be physically deleted\").isEmpty();\n \n     // pick the snapshot 'B`\n     Snapshot snapshotB = table.currentSnapshot();\n@@ -704,7 +703,7 @@ public void testWithCherryPickTableSnapshot() {\n               i.addedDataFiles(table.io())\n                   .forEach(\n                       item -> {\n-                        Assert.assertFalse(deletedFiles.contains(item.location()));\n+                        assertThat(deletedFiles).doesNotContain(item.location());\n                       });\n             });\n \n@@ -753,7 +752,7 @@ public void testWithExpiringStagedThenCherrypick() {\n               i.addedDataFiles(table.io())\n                   .forEach(\n                       item -> {\n-                        Assert.assertFalse(deletedFiles.contains(item.location()));\n+                        assertThat(deletedFiles).doesNotContain(item.location());\n                       });\n             });\n     checkExpirationResults(0L, 0L, 0L, 1L, 1L, firstResult);\n@@ -773,7 +772,7 @@ public void testWithExpiringStagedThenCherrypick() {\n               i.addedDataFiles(table.io())\n                   .forEach(\n                       item -> {\n-                        Assert.assertFalse(deletedFiles.contains(item.location()));\n+                        assertThat(deletedFiles).doesNotContain(item.location());\n                       });\n             });\n     checkExpirationResults(0L, 0L, 0L, 0L, 2L, secondResult);\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestBaseReader.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestBaseReader.java\nindex d417acdeb20a..a6d7d4827c0d 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestBaseReader.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestBaseReader.java\n@@ -20,9 +20,11 @@\n \n import static org.apache.iceberg.FileFormat.PARQUET;\n import static org.apache.iceberg.Files.localOutput;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Path;\n import java.util.Iterator;\n import java.util.List;\n import java.util.Map;\n@@ -47,14 +49,12 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.spark.data.RandomData;\n import org.apache.iceberg.types.Types;\n-import org.junit.Assert;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n public class TestBaseReader {\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n   private Table table;\n \n@@ -129,15 +129,17 @@ public void testClosureOnDataExhaustion() throws IOException {\n     int countRecords = 0;\n     while (reader.next()) {\n       countRecords += 1;\n-      Assert.assertNotNull(\"Reader should return non-null value\", reader.get());\n+      assertThat(reader.get()).as(\"Reader should return non-null value\").isNotNull();\n     }\n \n-    Assert.assertEquals(\n-        \"Reader returned incorrect number of records\", totalTasks * recordPerTask, countRecords);\n+    assertThat(totalTasks * recordPerTask)\n+        .as(\"Reader returned incorrect number of records\")\n+        .isEqualTo(countRecords);\n     tasks.forEach(\n         t ->\n-            Assert.assertTrue(\n-                \"All iterators should be closed after read exhausion\", reader.isIteratorClosed(t)));\n+            assertThat(reader.isIteratorClosed(t))\n+                .as(\"All iterators should be closed after read exhausion\")\n+                .isTrue());\n   }\n \n   @Test\n@@ -145,28 +147,29 @@ public void testClosureDuringIteration() throws IOException {\n     Integer totalTasks = 2;\n     Integer recordPerTask = 1;\n     List<FileScanTask> tasks = createFileScanTasks(totalTasks, recordPerTask);\n-    Assert.assertEquals(2, tasks.size());\n+    assertThat(tasks).hasSize(2);\n     FileScanTask firstTask = tasks.get(0);\n     FileScanTask secondTask = tasks.get(1);\n \n     ClosureTrackingReader reader = new ClosureTrackingReader(table, tasks);\n \n     // Total of 2 elements\n-    Assert.assertTrue(reader.next());\n-    Assert.assertFalse(\n-        \"First iter should not be closed on its last element\", reader.isIteratorClosed(firstTask));\n-\n-    Assert.assertTrue(reader.next());\n-    Assert.assertTrue(\n-        \"First iter should be closed after moving to second iter\",\n-        reader.isIteratorClosed(firstTask));\n-    Assert.assertFalse(\n-        \"Second iter should not be closed on its last element\",\n-        reader.isIteratorClosed(secondTask));\n-\n-    Assert.assertFalse(reader.next());\n-    Assert.assertTrue(reader.isIteratorClosed(firstTask));\n-    Assert.assertTrue(reader.isIteratorClosed(secondTask));\n+    assertThat(reader.next()).isTrue();\n+    assertThat(reader.isIteratorClosed(firstTask))\n+        .as(\"First iter should not be closed on its last element\")\n+        .isFalse();\n+\n+    assertThat(reader.next()).isTrue();\n+    assertThat(reader.isIteratorClosed(firstTask))\n+        .as(\"First iter should be closed after moving to second iter\")\n+        .isTrue();\n+    assertThat(reader.isIteratorClosed(secondTask))\n+        .as(\"Second iter should not be closed on its last element\")\n+        .isFalse();\n+\n+    assertThat(reader.next()).isFalse();\n+    assertThat(reader.isIteratorClosed(firstTask)).isTrue();\n+    assertThat(reader.isIteratorClosed(secondTask)).isTrue();\n   }\n \n   @Test\n@@ -181,8 +184,9 @@ public void testClosureWithoutAnyRead() throws IOException {\n \n     tasks.forEach(\n         t ->\n-            Assert.assertFalse(\n-                \"Iterator should not be created eagerly for tasks\", reader.hasIterator(t)));\n+            assertThat(reader.hasIterator(t))\n+                .as(\"Iterator should not be created eagerly for tasks\")\n+                .isFalse());\n   }\n \n   @Test\n@@ -195,8 +199,8 @@ public void testExplicitClosure() throws IOException {\n \n     Integer halfDataSize = (totalTasks * recordPerTask) / 2;\n     for (int i = 0; i < halfDataSize; i++) {\n-      Assert.assertTrue(\"Reader should have some element\", reader.next());\n-      Assert.assertNotNull(\"Reader should return non-null value\", reader.get());\n+      assertThat(reader.next()).as(\"Reader should have some element\").isTrue();\n+      assertThat(reader.get()).as(\"Reader should return non-null value\").isNotNull();\n     }\n \n     reader.close();\n@@ -206,8 +210,9 @@ public void testExplicitClosure() throws IOException {\n     tasks.forEach(\n         t -> {\n           if (reader.hasIterator(t)) {\n-            Assert.assertTrue(\n-                \"Iterator should be closed after read exhausion\", reader.isIteratorClosed(t));\n+            assertThat(reader.isIteratorClosed(t))\n+                .as(\"Iterator should be closed after read exhausion\")\n+                .isTrue();\n           }\n         });\n   }\n@@ -222,20 +227,21 @@ public void testIdempotentExplicitClosure() throws IOException {\n \n     // Total 100 elements, only 5 iterators have been created\n     for (int i = 0; i < 45; i++) {\n-      Assert.assertTrue(\"eader should have some element\", reader.next());\n-      Assert.assertNotNull(\"Reader should return non-null value\", reader.get());\n+      assertThat(reader.next()).as(\"Reader should have some element\").isTrue();\n+      assertThat(reader.get()).as(\"Reader should return non-null value\").isNotNull();\n     }\n \n     for (int closeAttempt = 0; closeAttempt < 5; closeAttempt++) {\n       reader.close();\n       for (int i = 0; i < 5; i++) {\n-        Assert.assertTrue(\n-            \"Iterator should be closed after read exhausion\",\n-            reader.isIteratorClosed(tasks.get(i)));\n+        assertThat(reader.isIteratorClosed(tasks.get(i)))\n+            .as(\"Iterator should be closed after read exhausion\")\n+            .isTrue();\n       }\n       for (int i = 5; i < 10; i++) {\n-        Assert.assertFalse(\n-            \"Iterator should not be created eagerly for tasks\", reader.hasIterator(tasks.get(i)));\n+        assertThat(reader.hasIterator(tasks.get(i)))\n+            .as(\"Iterator should not be created eagerly for tasks\")\n+            .isFalse();\n       }\n     }\n   }\n@@ -243,10 +249,10 @@ public void testIdempotentExplicitClosure() throws IOException {\n   private List<FileScanTask> createFileScanTasks(Integer totalTasks, Integer recordPerTask)\n       throws IOException {\n     String desc = \"make_scan_tasks\";\n-    File parent = temp.newFolder(desc);\n+    File parent = temp.resolve(desc).toFile();\n     File location = new File(parent, \"test\");\n     File dataFolder = new File(location, \"data\");\n-    Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n+    assertThat(dataFolder.mkdirs()).as(\"mkdirs should succeed\").isTrue();\n \n     Schema schema = new Schema(Types.NestedField.required(0, \"id\", Types.LongType.get()));\n \n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java\nindex ba13d005bdc3..07b3722d6a5d 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java\n@@ -28,6 +28,7 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Path;\n import java.sql.Timestamp;\n import java.time.OffsetDateTime;\n import java.util.Arrays;\n@@ -39,6 +40,9 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataFiles;\n import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.PlanningMode;\n import org.apache.iceberg.Schema;\n@@ -51,7 +55,6 @@\n import org.apache.iceberg.io.FileAppender;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.spark.SparkReadOptions;\n import org.apache.iceberg.spark.data.GenericsHelpers;\n import org.apache.iceberg.transforms.Transforms;\n@@ -78,17 +81,14 @@\n import org.apache.spark.sql.types.LongType$;\n import org.apache.spark.sql.types.StringType$;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestFilteredScan {\n   private static final Configuration CONF = new Configuration();\n   private static final HadoopTables TABLES = new HadoopTables(CONF);\n@@ -116,7 +116,7 @@ public class TestFilteredScan {\n \n   private static SparkSession spark = null;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestFilteredScan.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n \n@@ -144,46 +144,45 @@ public static void startSpark() {\n     spark.udf().register(\"id_ident\", (UDF1<Long, Long>) id -> id, LongType$.MODULE$);\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestFilteredScan.spark;\n     TestFilteredScan.spark = null;\n     currentSpark.stop();\n   }\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n-  private final String format;\n-  private final boolean vectorized;\n-  private final PlanningMode planningMode;\n+  @Parameter(index = 0)\n+  private FileFormat fileFormat;\n \n-  @Parameterized.Parameters(name = \"format = {0}, vectorized = {1}, planningMode = {2}\")\n+  @Parameter(index = 1)\n+  private boolean vectorized;\n+\n+  @Parameter(index = 2)\n+  private PlanningMode planningMode;\n+\n+  @Parameters(name = \"format = {0}, vectorized = {1}, planningMode = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {\"parquet\", false, LOCAL},\n-      {\"parquet\", true, DISTRIBUTED},\n-      {\"avro\", false, LOCAL},\n-      {\"orc\", false, DISTRIBUTED},\n-      {\"orc\", true, LOCAL}\n+      {FileFormat.PARQUET, false, LOCAL},\n+      {FileFormat.PARQUET, true, DISTRIBUTED},\n+      {FileFormat.AVRO, false, LOCAL},\n+      {FileFormat.ORC, false, DISTRIBUTED},\n+      {FileFormat.ORC, true, LOCAL}\n     };\n   }\n \n-  public TestFilteredScan(String format, boolean vectorized, PlanningMode planningMode) {\n-    this.format = format;\n-    this.vectorized = vectorized;\n-    this.planningMode = planningMode;\n-  }\n-\n   private File parent = null;\n   private File unpartitioned = null;\n   private List<Record> records = null;\n \n-  @Before\n+  @BeforeEach\n   public void writeUnpartitionedTable() throws IOException {\n-    this.parent = temp.newFolder(\"TestFilteredScan\");\n+    this.parent = temp.resolve(\"TestFilteredScan\").toFile();\n     this.unpartitioned = new File(parent, \"unpartitioned\");\n     File dataFolder = new File(unpartitioned, \"data\");\n-    Assert.assertTrue(\"Mkdir should succeed\", dataFolder.mkdirs());\n+    assertThat(dataFolder.mkdirs()).as(\"Mkdir should succeed\").isTrue();\n \n     Table table =\n         TABLES.create(\n@@ -197,8 +196,6 @@ public void writeUnpartitionedTable() throws IOException {\n             unpartitioned.toString());\n     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned\n \n-    FileFormat fileFormat = FileFormat.fromString(format);\n-\n     File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n \n     this.records = testRecords(tableSchema);\n@@ -218,7 +215,7 @@ public void writeUnpartitionedTable() throws IOException {\n     table.newAppend().appendFile(file).commit();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedIDFilters() {\n     CaseInsensitiveStringMap options =\n         new CaseInsensitiveStringMap(ImmutableMap.of(\"path\", unpartitioned.toString()));\n@@ -230,7 +227,7 @@ public void testUnpartitionedIDFilters() {\n       Batch scan = builder.build().toBatch();\n \n       InputPartition[] partitions = scan.planInputPartitions();\n-      Assert.assertEquals(\"Should only create one task for a small file\", 1, partitions.length);\n+      assertThat(partitions).as(\"Should only create one task for a small file\").hasSize(1);\n \n       // validate row filtering\n       assertEqualsSafe(\n@@ -238,7 +235,7 @@ public void testUnpartitionedIDFilters() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedCaseInsensitiveIDFilters() {\n     CaseInsensitiveStringMap options =\n         new CaseInsensitiveStringMap(ImmutableMap.of(\"path\", unpartitioned.toString()));\n@@ -260,7 +257,7 @@ public void testUnpartitionedCaseInsensitiveIDFilters() {\n         Batch scan = builder.build().toBatch();\n \n         InputPartition[] tasks = scan.planInputPartitions();\n-        Assert.assertEquals(\"Should only create one task for a small file\", 1, tasks.length);\n+        assertThat(tasks).as(\"Should only create one task for a small file\").hasSize(1);\n \n         // validate row filtering\n         assertEqualsSafe(\n@@ -274,7 +271,7 @@ public void testUnpartitionedCaseInsensitiveIDFilters() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedTimestampFilter() {\n     CaseInsensitiveStringMap options =\n         new CaseInsensitiveStringMap(ImmutableMap.of(\"path\", unpartitioned.toString()));\n@@ -286,7 +283,7 @@ public void testUnpartitionedTimestampFilter() {\n     Batch scan = builder.build().toBatch();\n \n     InputPartition[] tasks = scan.planInputPartitions();\n-    Assert.assertEquals(\"Should only create one task for a small file\", 1, tasks.length);\n+    assertThat(tasks).as(\"Should only create one task for a small file\").hasSize(1);\n \n     assertEqualsSafe(\n         SCHEMA.asStruct(),\n@@ -297,7 +294,7 @@ public void testUnpartitionedTimestampFilter() {\n             \"ts < cast('2017-12-22 00:00:00+00:00' as timestamp)\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBucketPartitionedIDFilters() {\n     Table table = buildPartitionedTable(\"bucketed_by_id\", BUCKET_BY_ID, \"bucket4\", \"id\");\n     CaseInsensitiveStringMap options =\n@@ -305,8 +302,9 @@ public void testBucketPartitionedIDFilters() {\n \n     Batch unfiltered =\n         new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options).build().toBatch();\n-    Assert.assertEquals(\n-        \"Unfiltered table should created 4 read tasks\", 4, unfiltered.planInputPartitions().length);\n+    assertThat(unfiltered.planInputPartitions())\n+        .as(\"Unfiltered table should created 4 read tasks\")\n+        .hasSize(4);\n \n     for (int i = 0; i < 10; i += 1) {\n       SparkScanBuilder builder =\n@@ -318,7 +316,7 @@ public void testBucketPartitionedIDFilters() {\n       InputPartition[] tasks = scan.planInputPartitions();\n \n       // validate predicate push-down\n-      Assert.assertEquals(\"Should create one task for a single bucket\", 1, tasks.length);\n+      assertThat(tasks).as(\"Should only create one task for a single bucket\").hasSize(1);\n \n       // validate row filtering\n       assertEqualsSafe(\n@@ -327,7 +325,7 @@ public void testBucketPartitionedIDFilters() {\n   }\n \n   @SuppressWarnings(\"checkstyle:AvoidNestedBlocks\")\n-  @Test\n+  @TestTemplate\n   public void testDayPartitionedTimestampFilters() {\n     Table table = buildPartitionedTable(\"partitioned_by_day\", PARTITION_BY_DAY, \"ts_day\", \"ts\");\n     CaseInsensitiveStringMap options =\n@@ -335,8 +333,9 @@ public void testDayPartitionedTimestampFilters() {\n     Batch unfiltered =\n         new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options).build().toBatch();\n \n-    Assert.assertEquals(\n-        \"Unfiltered table should created 2 read tasks\", 2, unfiltered.planInputPartitions().length);\n+    assertThat(unfiltered.planInputPartitions())\n+        .as(\"Unfiltered table should created 2 read tasks\")\n+        .hasSize(2);\n \n     {\n       SparkScanBuilder builder =\n@@ -346,7 +345,7 @@ public void testDayPartitionedTimestampFilters() {\n       Batch scan = builder.build().toBatch();\n \n       InputPartition[] tasks = scan.planInputPartitions();\n-      Assert.assertEquals(\"Should create one task for 2017-12-21\", 1, tasks.length);\n+      assertThat(tasks).as(\"Should create one task for 2017-12-21\").hasSize(1);\n \n       assertEqualsSafe(\n           SCHEMA.asStruct(),\n@@ -367,7 +366,7 @@ public void testDayPartitionedTimestampFilters() {\n       Batch scan = builder.build().toBatch();\n \n       InputPartition[] tasks = scan.planInputPartitions();\n-      Assert.assertEquals(\"Should create one task for 2017-12-22\", 1, tasks.length);\n+      assertThat(tasks).as(\"Should create one task for 2017-12-22\").hasSize(1);\n \n       assertEqualsSafe(\n           SCHEMA.asStruct(),\n@@ -381,7 +380,7 @@ public void testDayPartitionedTimestampFilters() {\n   }\n \n   @SuppressWarnings(\"checkstyle:AvoidNestedBlocks\")\n-  @Test\n+  @TestTemplate\n   public void testHourPartitionedTimestampFilters() {\n     Table table = buildPartitionedTable(\"partitioned_by_hour\", PARTITION_BY_HOUR, \"ts_hour\", \"ts\");\n \n@@ -390,8 +389,9 @@ public void testHourPartitionedTimestampFilters() {\n     Batch unfiltered =\n         new SparkScanBuilder(spark, TABLES.load(options.get(\"path\")), options).build().toBatch();\n \n-    Assert.assertEquals(\n-        \"Unfiltered table should created 9 read tasks\", 9, unfiltered.planInputPartitions().length);\n+    assertThat(unfiltered.planInputPartitions())\n+        .as(\"Unfiltered table should created 9 read tasks\")\n+        .hasSize(9);\n \n     {\n       SparkScanBuilder builder =\n@@ -401,7 +401,7 @@ public void testHourPartitionedTimestampFilters() {\n       Batch scan = builder.build().toBatch();\n \n       InputPartition[] tasks = scan.planInputPartitions();\n-      Assert.assertEquals(\"Should create 4 tasks for 2017-12-21: 15, 17, 21, 22\", 4, tasks.length);\n+      assertThat(tasks).as(\"Should create 4 tasks for 2017-12-21: 15, 17, 21, 22\").hasSize(4);\n \n       assertEqualsSafe(\n           SCHEMA.asStruct(),\n@@ -422,7 +422,7 @@ public void testHourPartitionedTimestampFilters() {\n       Batch scan = builder.build().toBatch();\n \n       InputPartition[] tasks = scan.planInputPartitions();\n-      Assert.assertEquals(\"Should create 2 tasks for 2017-12-22: 6, 7\", 2, tasks.length);\n+      assertThat(tasks).as(\"Should create 2 tasks for 2017-12-22: 6, 7\").hasSize(2);\n \n       assertEqualsSafe(\n           SCHEMA.asStruct(),\n@@ -436,7 +436,7 @@ public void testHourPartitionedTimestampFilters() {\n   }\n \n   @SuppressWarnings(\"checkstyle:AvoidNestedBlocks\")\n-  @Test\n+  @TestTemplate\n   public void testFilterByNonProjectedColumn() {\n     {\n       Schema actualProjection = SCHEMA.select(\"id\", \"data\");\n@@ -477,7 +477,7 @@ public void testFilterByNonProjectedColumn() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedByDataStartsWithFilter() {\n     Table table =\n         buildPartitionedTable(\"partitioned_by_data\", PARTITION_BY_DATA, \"data_ident\", \"data\");\n@@ -490,10 +490,10 @@ public void testPartitionedByDataStartsWithFilter() {\n     pushFilters(builder, new StringStartsWith(\"data\", \"junc\"));\n     Batch scan = builder.build().toBatch();\n \n-    Assert.assertEquals(1, scan.planInputPartitions().length);\n+    assertThat(scan.planInputPartitions()).hasSize(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedByDataNotStartsWithFilter() {\n     Table table =\n         buildPartitionedTable(\"partitioned_by_data\", PARTITION_BY_DATA, \"data_ident\", \"data\");\n@@ -506,10 +506,10 @@ public void testPartitionedByDataNotStartsWithFilter() {\n     pushFilters(builder, new Not(new StringStartsWith(\"data\", \"junc\")));\n     Batch scan = builder.build().toBatch();\n \n-    Assert.assertEquals(9, scan.planInputPartitions().length);\n+    assertThat(scan.planInputPartitions()).hasSize(9);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedByIdStartsWith() {\n     Table table = buildPartitionedTable(\"partitioned_by_id\", PARTITION_BY_ID, \"id_ident\", \"id\");\n \n@@ -522,10 +522,10 @@ public void testPartitionedByIdStartsWith() {\n     pushFilters(builder, new StringStartsWith(\"data\", \"junc\"));\n     Batch scan = builder.build().toBatch();\n \n-    Assert.assertEquals(1, scan.planInputPartitions().length);\n+    assertThat(scan.planInputPartitions()).hasSize(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedByIdNotStartsWith() {\n     Table table = buildPartitionedTable(\"partitioned_by_id\", PARTITION_BY_ID, \"id_ident\", \"id\");\n \n@@ -538,10 +538,10 @@ public void testPartitionedByIdNotStartsWith() {\n     pushFilters(builder, new Not(new StringStartsWith(\"data\", \"junc\")));\n     Batch scan = builder.build().toBatch();\n \n-    Assert.assertEquals(9, scan.planInputPartitions().length);\n+    assertThat(scan.planInputPartitions()).hasSize(9);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedStartsWith() {\n     Dataset<Row> df =\n         spark\n@@ -553,11 +553,10 @@ public void testUnpartitionedStartsWith() {\n     List<String> matchedData =\n         df.select(\"data\").where(\"data LIKE 'jun%'\").as(Encoders.STRING()).collectAsList();\n \n-    Assert.assertEquals(1, matchedData.size());\n-    Assert.assertEquals(\"junction\", matchedData.get(0));\n+    assertThat(matchedData).singleElement().isEqualTo(\"junction\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedNotStartsWith() {\n     Dataset<Row> df =\n         spark\n@@ -575,8 +574,7 @@ public void testUnpartitionedNotStartsWith() {\n             .filter(d -> !d.startsWith(\"jun\"))\n             .collect(Collectors.toList());\n \n-    Assert.assertEquals(9, matchedData.size());\n-    Assert.assertEquals(Sets.newHashSet(expected), Sets.newHashSet(matchedData));\n+    assertThat(matchedData).hasSize(9).containsExactlyInAnyOrderElementsOf(expected);\n   }\n \n   private static Record projectFlat(Schema projection, Record record) {\n@@ -596,7 +594,7 @@ public static void assertEqualsUnsafe(\n     for (int i = 0; i < numRecords; i += 1) {\n       GenericsHelpers.assertEqualsUnsafe(struct, expected.get(i), actual.get(i));\n     }\n-    Assert.assertEquals(\"Number of results should match expected\", expected.size(), actual.size());\n+    assertThat(actual).as(\"Number of results should match expected\").hasSameSizeAs(expected);\n   }\n \n   public static void assertEqualsSafe(\n@@ -606,7 +604,7 @@ public static void assertEqualsSafe(\n     for (int i = 0; i < numRecords; i += 1) {\n       GenericsHelpers.assertEqualsSafe(struct, expected.get(i), actual.get(i));\n     }\n-    Assert.assertEquals(\"Number of results should match expected\", expected.size(), actual.size());\n+    assertThat(actual).as(\"Number of results should match expected\").hasSameSizeAs(expected);\n   }\n \n   private List<Record> expected(int... ordinals) {\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java\nindex 9f97753094a5..e0091058ec1d 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java\n@@ -20,10 +20,12 @@\n \n import static org.apache.iceberg.Files.localInput;\n import static org.apache.iceberg.Files.localOutput;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Path;\n import java.util.List;\n import java.util.UUID;\n import java.util.concurrent.TimeoutException;\n@@ -55,12 +57,10 @@\n import org.apache.spark.sql.execution.streaming.MemoryStream;\n import org.apache.spark.sql.streaming.StreamingQuery;\n import org.apache.spark.sql.streaming.StreamingQueryException;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n import scala.Option;\n import scala.collection.JavaConverters;\n \n@@ -88,16 +88,16 @@ public class TestForwardCompatibility {\n           .addField(\"identity\", 1, \"id_zero\")\n           .build();\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n   private static SparkSession spark = null;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestForwardCompatibility.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestForwardCompatibility.spark;\n     TestForwardCompatibility.spark = null;\n@@ -106,10 +106,10 @@ public static void stopSpark() {\n \n   @Test\n   public void testSparkWriteFailsUnknownTransform() throws IOException {\n-    File parent = temp.newFolder(\"avro\");\n+    File parent = temp.resolve(\"avro\").toFile();\n     File location = new File(parent, \"test\");\n     File dataFolder = new File(location, \"data\");\n-    dataFolder.mkdirs();\n+    assertThat(dataFolder.mkdirs()).isTrue();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     tables.create(SCHEMA, UNKNOWN_SPEC, location.toString());\n@@ -133,12 +133,12 @@ public void testSparkWriteFailsUnknownTransform() throws IOException {\n \n   @Test\n   public void testSparkStreamingWriteFailsUnknownTransform() throws IOException, TimeoutException {\n-    File parent = temp.newFolder(\"avro\");\n+    File parent = temp.resolve(\"avro\").toFile();\n     File location = new File(parent, \"test\");\n     File dataFolder = new File(location, \"data\");\n-    dataFolder.mkdirs();\n+    assertThat(dataFolder.mkdirs()).isTrue();\n     File checkpoint = new File(parent, \"checkpoint\");\n-    checkpoint.mkdirs();\n+    assertThat(checkpoint.mkdirs()).isTrue();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     tables.create(SCHEMA, UNKNOWN_SPEC, location.toString());\n@@ -165,10 +165,10 @@ public void testSparkStreamingWriteFailsUnknownTransform() throws IOException, T\n \n   @Test\n   public void testSparkCanReadUnknownTransform() throws IOException {\n-    File parent = temp.newFolder(\"avro\");\n+    File parent = temp.resolve(\"avro\").toFile();\n     File location = new File(parent, \"test\");\n     File dataFolder = new File(location, \"data\");\n-    dataFolder.mkdirs();\n+    assertThat(dataFolder.mkdirs()).isTrue();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     Table table = tables.create(SCHEMA, UNKNOWN_SPEC, location.toString());\n@@ -195,7 +195,7 @@ public void testSparkCanReadUnknownTransform() throws IOException {\n             .withPartitionPath(\"id_zero=0\")\n             .build();\n \n-    OutputFile manifestFile = localOutput(FileFormat.AVRO.addExtension(temp.newFile().toString()));\n+    OutputFile manifestFile = localOutput(FileFormat.AVRO.addExtension(temp.toFile().toString()));\n     ManifestWriter<DataFile> manifestWriter = ManifestFiles.write(FAKE_SPEC, manifestFile);\n     try {\n       manifestWriter.add(file);\n@@ -208,7 +208,7 @@ public void testSparkCanReadUnknownTransform() throws IOException {\n     Dataset<Row> df = spark.read().format(\"iceberg\").load(location.toString());\n \n     List<Row> rows = df.collectAsList();\n-    Assert.assertEquals(\"Should contain 100 rows\", 100, rows.size());\n+    assertThat(rows).as(\"Should contain 100 rows\").hasSize(100);\n \n     for (int i = 0; i < expected.size(); i += 1) {\n       TestHelpers.assertEqualsSafe(table.schema().asStruct(), expected.get(i), rows.get(i));\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSpark.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSpark.java\nindex 0154506f86b8..5843fe992549 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSpark.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSpark.java\n@@ -18,6 +18,7 @@\n  */\n package org.apache.iceberg.spark.source;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.math.BigDecimal;\n@@ -35,21 +36,20 @@\n import org.apache.spark.sql.types.DataTypes;\n import org.apache.spark.sql.types.DecimalType;\n import org.apache.spark.sql.types.VarcharType;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.BeforeClass;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Test;\n \n public class TestIcebergSpark {\n \n   private static SparkSession spark = null;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestIcebergSpark.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestIcebergSpark.spark;\n     TestIcebergSpark.spark = null;\n@@ -60,69 +60,84 @@ public static void stopSpark() {\n   public void testRegisterIntegerBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_int_16\", DataTypes.IntegerType, 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_int_16(1)\").collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        (int) Transforms.bucket(16).bind(Types.IntegerType.get()).apply(1),\n-        results.get(0).getInt(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.bucket(16).bind(Types.IntegerType.get()).apply(1)));\n   }\n \n   @Test\n   public void testRegisterShortBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_short_16\", DataTypes.ShortType, 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_short_16(1S)\").collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        (int) Transforms.bucket(16).bind(Types.IntegerType.get()).apply(1),\n-        results.get(0).getInt(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.bucket(16).bind(Types.IntegerType.get()).apply(1)));\n   }\n \n   @Test\n   public void testRegisterByteBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_byte_16\", DataTypes.ByteType, 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_byte_16(1Y)\").collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        (int) Transforms.bucket(16).bind(Types.IntegerType.get()).apply(1),\n-        results.get(0).getInt(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.bucket(16).bind(Types.IntegerType.get()).apply(1)));\n   }\n \n   @Test\n   public void testRegisterLongBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_long_16\", DataTypes.LongType, 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_long_16(1L)\").collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        (int) Transforms.bucket(16).bind(Types.LongType.get()).apply(1L), results.get(0).getInt(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.bucket(16).bind(Types.LongType.get()).apply(1L)));\n   }\n \n   @Test\n   public void testRegisterStringBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_string_16\", DataTypes.StringType, 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_string_16('hello')\").collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        (int) Transforms.bucket(16).bind(Types.StringType.get()).apply(\"hello\"),\n-        results.get(0).getInt(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.bucket(16).bind(Types.StringType.get()).apply(\"hello\")));\n   }\n \n   @Test\n   public void testRegisterCharBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_char_16\", new CharType(5), 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_char_16('hello')\").collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        (int) Transforms.bucket(16).bind(Types.StringType.get()).apply(\"hello\"),\n-        results.get(0).getInt(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.bucket(16).bind(Types.StringType.get()).apply(\"hello\")));\n   }\n \n   @Test\n   public void testRegisterVarCharBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_varchar_16\", new VarcharType(5), 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_varchar_16('hello')\").collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        (int) Transforms.bucket(16).bind(Types.StringType.get()).apply(\"hello\"),\n-        results.get(0).getInt(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.bucket(16).bind(Types.StringType.get()).apply(\"hello\")));\n   }\n \n   @Test\n@@ -130,13 +145,15 @@ public void testRegisterDateBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_date_16\", DataTypes.DateType, 16);\n     List<Row> results =\n         spark.sql(\"SELECT iceberg_bucket_date_16(DATE '2021-06-30')\").collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        (int)\n-            Transforms.bucket(16)\n-                .bind(Types.DateType.get())\n-                .apply(DateTimeUtils.fromJavaDate(Date.valueOf(\"2021-06-30\"))),\n-        results.get(0).getInt(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(\n+                        Transforms.bucket(16)\n+                            .bind(Types.DateType.get())\n+                            .apply(DateTimeUtils.fromJavaDate(Date.valueOf(\"2021-06-30\")))));\n   }\n \n   @Test\n@@ -147,37 +164,47 @@ public void testRegisterTimestampBucketUDF() {\n         spark\n             .sql(\"SELECT iceberg_bucket_timestamp_16(TIMESTAMP '2021-06-30 00:00:00.000')\")\n             .collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        (int)\n-            Transforms.bucket(16)\n-                .bind(Types.TimestampType.withZone())\n-                .apply(\n-                    DateTimeUtils.fromJavaTimestamp(Timestamp.valueOf(\"2021-06-30 00:00:00.000\"))),\n-        results.get(0).getInt(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(\n+                        Transforms.bucket(16)\n+                            .bind(Types.TimestampType.withZone())\n+                            .apply(\n+                                DateTimeUtils.fromJavaTimestamp(\n+                                    Timestamp.valueOf(\"2021-06-30 00:00:00.000\")))));\n   }\n \n   @Test\n   public void testRegisterBinaryBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_binary_16\", DataTypes.BinaryType, 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_binary_16(X'0020001F')\").collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        (int)\n-            Transforms.bucket(16)\n-                .bind(Types.BinaryType.get())\n-                .apply(ByteBuffer.wrap(new byte[] {0x00, 0x20, 0x00, 0x1F})),\n-        results.get(0).getInt(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(\n+                        Transforms.bucket(16)\n+                            .bind(Types.BinaryType.get())\n+                            .apply(ByteBuffer.wrap(new byte[] {0x00, 0x20, 0x00, 0x1F}))));\n   }\n \n   @Test\n   public void testRegisterDecimalBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_decimal_16\", new DecimalType(4, 2), 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_decimal_16(11.11)\").collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        (int) Transforms.bucket(16).bind(Types.DecimalType.of(4, 2)).apply(new BigDecimal(\"11.11\")),\n-        results.get(0).getInt(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(\n+                        Transforms.bucket(16)\n+                            .bind(Types.DecimalType.of(4, 2))\n+                            .apply(new BigDecimal(\"11.11\"))));\n   }\n \n   @Test\n@@ -214,37 +241,50 @@ public void testRegisterFloatBucketUDF() {\n   public void testRegisterIntegerTruncateUDF() {\n     IcebergSpark.registerTruncateUDF(spark, \"iceberg_truncate_int_4\", DataTypes.IntegerType, 4);\n     List<Row> results = spark.sql(\"SELECT iceberg_truncate_int_4(1)\").collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        Transforms.truncate(4).bind(Types.IntegerType.get()).apply(1), results.get(0).getInt(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.truncate(4).bind(Types.IntegerType.get()).apply(1)));\n   }\n \n   @Test\n   public void testRegisterLongTruncateUDF() {\n     IcebergSpark.registerTruncateUDF(spark, \"iceberg_truncate_long_4\", DataTypes.LongType, 4);\n     List<Row> results = spark.sql(\"SELECT iceberg_truncate_long_4(1L)\").collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        Transforms.truncate(4).bind(Types.LongType.get()).apply(1L), results.get(0).getLong(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getLong(0))\n+                    .isEqualTo(Transforms.truncate(4).bind(Types.LongType.get()).apply(1L)));\n   }\n \n   @Test\n   public void testRegisterDecimalTruncateUDF() {\n     IcebergSpark.registerTruncateUDF(spark, \"iceberg_truncate_decimal_4\", new DecimalType(4, 2), 4);\n     List<Row> results = spark.sql(\"SELECT iceberg_truncate_decimal_4(11.11)\").collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        Transforms.truncate(4).bind(Types.DecimalType.of(4, 2)).apply(new BigDecimal(\"11.11\")),\n-        results.get(0).getDecimal(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getDecimal(0))\n+                    .isEqualTo(\n+                        Transforms.truncate(4)\n+                            .bind(Types.DecimalType.of(4, 2))\n+                            .apply(new BigDecimal(\"11.11\"))));\n   }\n \n   @Test\n   public void testRegisterStringTruncateUDF() {\n     IcebergSpark.registerTruncateUDF(spark, \"iceberg_truncate_string_4\", DataTypes.StringType, 4);\n     List<Row> results = spark.sql(\"SELECT iceberg_truncate_string_4('hello')\").collectAsList();\n-    Assert.assertEquals(1, results.size());\n-    Assert.assertEquals(\n-        Transforms.truncate(4).bind(Types.StringType.get()).apply(\"hello\"),\n-        results.get(0).getString(0));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getString(0))\n+                    .isEqualTo(Transforms.truncate(4).bind(Types.StringType.get()).apply(\"hello\")));\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestInternalRowWrapper.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestInternalRowWrapper.java\nindex 1b4698fe5b7a..0c869aa8e7e0 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestInternalRowWrapper.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestInternalRowWrapper.java\n@@ -18,6 +18,8 @@\n  */\n package org.apache.iceberg.spark.source;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import java.util.Iterator;\n import org.apache.iceberg.RecordWrapperTest;\n import org.apache.iceberg.Schema;\n@@ -29,18 +31,17 @@\n import org.apache.iceberg.spark.data.RandomData;\n import org.apache.iceberg.util.StructLikeWrapper;\n import org.apache.spark.sql.catalyst.InternalRow;\n-import org.junit.Assert;\n-import org.junit.Ignore;\n+import org.junit.jupiter.api.Disabled;\n \n public class TestInternalRowWrapper extends RecordWrapperTest {\n \n-  @Ignore\n+  @Disabled\n   @Override\n   public void testTimestampWithoutZone() {\n     // Spark does not support timestamp without zone.\n   }\n \n-  @Ignore\n+  @Disabled\n   @Override\n   public void testTime() {\n     // Spark does not support time fields.\n@@ -62,8 +63,8 @@ protected void generateAndValidate(Schema schema, AssertMethod assertMethod) {\n     StructLikeWrapper actualWrapper = StructLikeWrapper.forType(schema.asStruct());\n     StructLikeWrapper expectedWrapper = StructLikeWrapper.forType(schema.asStruct());\n     for (int i = 0; i < numRecords; i++) {\n-      Assert.assertTrue(\"Should have more records\", actual.hasNext());\n-      Assert.assertTrue(\"Should have more InternalRow\", expected.hasNext());\n+      assertThat(actual).as(\"Should have more records\").hasNext();\n+      assertThat(expected).as(\"Should have more InternalRow\").hasNext();\n \n       StructLike recordStructLike = recordWrapper.wrap(actual.next());\n       StructLike rowStructLike = rowWrapper.wrap(expected.next());\n@@ -74,7 +75,7 @@ protected void generateAndValidate(Schema schema, AssertMethod assertMethod) {\n           expectedWrapper.set(rowStructLike));\n     }\n \n-    Assert.assertFalse(\"Shouldn't have more record\", actual.hasNext());\n-    Assert.assertFalse(\"Shouldn't have more InternalRow\", expected.hasNext());\n+    assertThat(actual).as(\"Shouldn't have more record\").isExhausted();\n+    assertThat(expected).as(\"Shouldn't have more InternalRow\").isExhausted();\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java\nindex 639d37c79336..9b81414e8e00 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java\n@@ -20,10 +20,12 @@\n \n import static org.apache.iceberg.PlanningMode.DISTRIBUTED;\n import static org.apache.iceberg.PlanningMode.LOCAL;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.File;\n import java.io.IOException;\n import java.net.URI;\n+import java.nio.file.Files;\n import java.sql.Timestamp;\n import java.time.Instant;\n import java.util.Arrays;\n@@ -39,6 +41,10 @@\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.PlanningMode;\n import org.apache.iceberg.Schema;\n@@ -63,41 +69,37 @@\n import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n import org.apache.spark.sql.types.DataTypes;\n import org.apache.spark.unsafe.types.UTF8String;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestPartitionPruning {\n \n   private static final Configuration CONF = new Configuration();\n   private static final HadoopTables TABLES = new HadoopTables(CONF);\n \n-  @Parameterized.Parameters(name = \"format = {0}, vectorized = {1}, planningMode = {2}\")\n+  @Parameters(name = \"format = {0}, vectorized = {1}, planningMode = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {\"parquet\", false, DISTRIBUTED},\n-      {\"parquet\", true, LOCAL},\n-      {\"avro\", false, DISTRIBUTED},\n-      {\"orc\", false, LOCAL},\n-      {\"orc\", true, DISTRIBUTED}\n+      {FileFormat.PARQUET, false, DISTRIBUTED},\n+      {FileFormat.PARQUET, true, LOCAL},\n+      {FileFormat.AVRO, false, DISTRIBUTED},\n+      {FileFormat.ORC, false, LOCAL},\n+      {FileFormat.ORC, true, DISTRIBUTED}\n     };\n   }\n \n-  private final String format;\n-  private final boolean vectorized;\n-  private final PlanningMode planningMode;\n+  @Parameter(index = 0)\n+  private FileFormat format;\n \n-  public TestPartitionPruning(String format, boolean vectorized, PlanningMode planningMode) {\n-    this.format = format;\n-    this.vectorized = vectorized;\n-    this.planningMode = planningMode;\n-  }\n+  @Parameter(index = 1)\n+  private boolean vectorized;\n+\n+  @Parameter(index = 2)\n+  private PlanningMode planningMode;\n \n   private static SparkSession spark = null;\n   private static JavaSparkContext sparkContext = null;\n@@ -109,7 +111,7 @@ public TestPartitionPruning(String format, boolean vectorized, PlanningMode plan\n   private static final Function<Object, Integer> HOUR_FUNC =\n       Transforms.hour().bind(Types.TimestampType.withoutZone());\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestPartitionPruning.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n     TestPartitionPruning.sparkContext = JavaSparkContext.fromSparkContext(spark.sparkContext());\n@@ -133,7 +135,7 @@ public static void startSpark() {\n             DataTypes.IntegerType);\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestPartitionPruning.spark;\n     TestPartitionPruning.spark = null;\n@@ -167,7 +169,7 @@ private static Instant getInstant(String timestampWithoutZone) {\n     return Instant.ofEpochMilli(TimeUnit.MICROSECONDS.toMillis(epochMicros));\n   }\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private java.nio.file.Path temp;\n \n   private final PartitionSpec spec =\n       PartitionSpec.builderFor(LOG_SCHEMA)\n@@ -178,7 +180,7 @@ private static Instant getInstant(String timestampWithoutZone) {\n           .hour(\"timestamp\")\n           .build();\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionPruningIdentityString() {\n     String filterCond = \"date >= '2020-02-03' AND level = 'DEBUG'\";\n     Predicate<Row> partCondition =\n@@ -191,7 +193,7 @@ public void testPartitionPruningIdentityString() {\n     runTest(filterCond, partCondition);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionPruningBucketingInteger() {\n     final int[] ids = new int[] {LOGS.get(3).getId(), LOGS.get(7).getId()};\n     String condForIds =\n@@ -208,7 +210,7 @@ public void testPartitionPruningBucketingInteger() {\n     runTest(filterCond, partCondition);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionPruningTruncatedString() {\n     String filterCond = \"message like 'info event%'\";\n     Predicate<Row> partCondition =\n@@ -220,7 +222,7 @@ public void testPartitionPruningTruncatedString() {\n     runTest(filterCond, partCondition);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionPruningTruncatedStringComparingValueShorterThanPartitionValue() {\n     String filterCond = \"message like 'inf%'\";\n     Predicate<Row> partCondition =\n@@ -232,7 +234,7 @@ public void testPartitionPruningTruncatedStringComparingValueShorterThanPartitio\n     runTest(filterCond, partCondition);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionPruningHourlyPartition() {\n     String filterCond;\n     if (spark.version().startsWith(\"2\")) {\n@@ -256,7 +258,7 @@ public void testPartitionPruningHourlyPartition() {\n \n   private void runTest(String filterCond, Predicate<Row> partCondition) {\n     File originTableLocation = createTempDir();\n-    Assert.assertTrue(\"Temp folder should exist\", originTableLocation.exists());\n+    assertThat(originTableLocation).as(\"Temp folder should exist\").exists();\n \n     Table table = createTable(originTableLocation);\n     Dataset<Row> logs = createTestDataset();\n@@ -267,7 +269,7 @@ private void runTest(String filterCond, Predicate<Row> partCondition) {\n             .filter(filterCond)\n             .orderBy(\"id\")\n             .collectAsList();\n-    Assert.assertFalse(\"Expected rows should be not empty\", expected.isEmpty());\n+    assertThat(expected).as(\"Expected rows should not be empty\").isNotEmpty();\n \n     // remove records which may be recorded during storing to table\n     CountOpenLocalFileSystem.resetRecordsInPathPrefix(originTableLocation.getAbsolutePath());\n@@ -282,16 +284,14 @@ private void runTest(String filterCond, Predicate<Row> partCondition) {\n             .filter(filterCond)\n             .orderBy(\"id\")\n             .collectAsList();\n-    Assert.assertFalse(\"Actual rows should not be empty\", actual.isEmpty());\n-\n-    Assert.assertEquals(\"Rows should match\", expected, actual);\n+    assertThat(actual).isNotEmpty().isEqualTo(expected);\n \n     assertAccessOnDataFiles(originTableLocation, table, partCondition);\n   }\n \n   private File createTempDir() {\n     try {\n-      return temp.newFolder();\n+      return Files.createTempDirectory(temp, \"junit\").toFile();\n     } catch (Exception e) {\n       throw new RuntimeException(e);\n     }\n@@ -301,7 +301,7 @@ private Table createTable(File originTableLocation) {\n     String trackedTableLocation = CountOpenLocalFileSystem.convertPath(originTableLocation);\n     Map<String, String> properties =\n         ImmutableMap.of(\n-            TableProperties.DEFAULT_FILE_FORMAT, format,\n+            TableProperties.DEFAULT_FILE_FORMAT, format.toString(),\n             TableProperties.DATA_PLANNING_MODE, planningMode.modeName(),\n             TableProperties.DELETE_PLANNING_MODE, planningMode.modeName());\n     return TABLES.create(LOG_SCHEMA, spec, properties, trackedTableLocation);\n@@ -366,29 +366,31 @@ private void assertAccessOnDataFiles(\n     Set<String> filesToNotRead = extractFilePathsNotIn(files, filesToRead);\n \n     // Just to be sure, they should be mutually exclusive.\n-    Assert.assertTrue(Sets.intersection(filesToRead, filesToNotRead).isEmpty());\n+    assertThat(filesToRead).doesNotContainAnyElementsOf(filesToNotRead);\n \n-    Assert.assertFalse(\"The query should prune some data files.\", filesToNotRead.isEmpty());\n+    assertThat(filesToNotRead).as(\"The query should prune some data files.\").isNotEmpty();\n \n     // We don't check \"all\" data files bound to the condition are being read, as data files can be\n     // pruned on\n     // other conditions like lower/upper bound of columns.\n-    Assert.assertFalse(\n-        \"Some of data files in partition range should be read. \"\n-            + \"Read files in query: \"\n-            + readFilesInQuery\n-            + \" / data files in partition range: \"\n-            + filesToRead,\n-        Sets.intersection(filesToRead, readFilesInQuery).isEmpty());\n+    assertThat(filesToRead)\n+        .as(\n+            \"Some of data files in partition range should be read. \"\n+                + \"Read files in query: \"\n+                + readFilesInQuery\n+                + \" / data files in partition range: \"\n+                + filesToRead)\n+        .containsAnyElementsOf(readFilesInQuery);\n \n     // Data files which aren't bound to the condition shouldn't be read.\n-    Assert.assertTrue(\n-        \"Data files outside of partition range should not be read. \"\n-            + \"Read files in query: \"\n-            + readFilesInQuery\n-            + \" / data files outside of partition range: \"\n-            + filesToNotRead,\n-        Sets.intersection(filesToNotRead, readFilesInQuery).isEmpty());\n+    assertThat(filesToNotRead)\n+        .as(\n+            \"Data files outside of partition range should not be read. \"\n+                + \"Read files in query: \"\n+                + readFilesInQuery\n+                + \" / data files outside of partition range: \"\n+                + filesToNotRead)\n+        .doesNotContainAnyElementsOf(readFilesInQuery);\n   }\n \n   private Set<String> extractFilePathsMatchingConditionOnPartition(\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java\nindex ad0984ef4220..934e57701b38 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java\n@@ -20,14 +20,21 @@\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.File;\n+import java.nio.file.Path;\n import java.util.List;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n import org.apache.avro.generic.GenericData;\n import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.Files;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -52,26 +59,22 @@\n import org.apache.spark.sql.types.Metadata;\n import org.apache.spark.sql.types.StructField;\n import org.apache.spark.sql.types.StructType;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestPartitionValues {\n-  @Parameterized.Parameters(name = \"format = {0}, vectorized = {1}\")\n+  @Parameters(name = \"format = {0}, vectorized = {1}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {\"parquet\", false},\n-      {\"parquet\", true},\n-      {\"avro\", false},\n-      {\"orc\", false},\n-      {\"orc\", true}\n+      {FileFormat.PARQUET, false},\n+      {FileFormat.PARQUET, true},\n+      {FileFormat.AVRO, false},\n+      {FileFormat.ORC, false},\n+      {FileFormat.ORC, true}\n     };\n   }\n \n@@ -102,39 +105,37 @@ public static Object[][] parameters() {\n \n   private static SparkSession spark = null;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestPartitionValues.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestPartitionValues.spark;\n     TestPartitionValues.spark = null;\n     currentSpark.stop();\n   }\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n-  private final String format;\n-  private final boolean vectorized;\n+  @Parameter(index = 0)\n+  private FileFormat format;\n \n-  public TestPartitionValues(String format, boolean vectorized) {\n-    this.format = format;\n-    this.vectorized = vectorized;\n-  }\n+  @Parameter(index = 1)\n+  private boolean vectorized;\n \n-  @Test\n-  public void testNullPartitionValue() throws Exception {\n+  @TestTemplate\n+  public void testNullPartitionValue() {\n     String desc = \"null_part\";\n-    File parent = temp.newFolder(desc);\n+    File parent = new File(temp.toFile(), desc);\n     File location = new File(parent, \"test\");\n     File dataFolder = new File(location, \"data\");\n-    Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n+    assertThat(dataFolder.mkdirs()).as(\"mkdirs should succeed\").isTrue();\n \n     HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n     Table table = tables.create(SIMPLE_SCHEMA, SPEC, location.toString());\n-    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n+    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format.toString()).commit();\n \n     List<SimpleRecord> expected =\n         Lists.newArrayList(\n@@ -161,21 +162,20 @@ public void testNullPartitionValue() throws Exception {\n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n \n-    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReorderedColumns() throws Exception {\n     String desc = \"reorder_columns\";\n-    File parent = temp.newFolder(desc);\n+    File parent = new File(temp.toFile(), desc);\n     File location = new File(parent, \"test\");\n     File dataFolder = new File(location, \"data\");\n-    Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n+    assertThat(dataFolder.mkdirs()).as(\"mkdirs should succeed\").isTrue();\n \n     HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n     Table table = tables.create(SIMPLE_SCHEMA, SPEC, location.toString());\n-    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n+    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format.toString()).commit();\n \n     List<SimpleRecord> expected =\n         Lists.newArrayList(\n@@ -200,21 +200,20 @@ public void testReorderedColumns() throws Exception {\n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n \n-    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReorderedColumnsNoNullability() throws Exception {\n     String desc = \"reorder_columns_no_nullability\";\n-    File parent = temp.newFolder(desc);\n+    File parent = new File(temp.toFile(), desc);\n     File location = new File(parent, \"test\");\n     File dataFolder = new File(location, \"data\");\n-    Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n+    assertThat(dataFolder.mkdirs()).as(\"mkdirs should succeed\").isTrue();\n \n     HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n     Table table = tables.create(SIMPLE_SCHEMA, SPEC, location.toString());\n-    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n+    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format.toString()).commit();\n \n     List<SimpleRecord> expected =\n         Lists.newArrayList(\n@@ -240,11 +239,10 @@ public void testReorderedColumnsNoNullability() throws Exception {\n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n \n-    Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-    Assert.assertEquals(\"Result rows should match\", expected, actual);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionValueTypes() throws Exception {\n     String[] columnNames =\n         new String[] {\n@@ -254,13 +252,13 @@ public void testPartitionValueTypes() throws Exception {\n     HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n \n     // create a table around the source data\n-    String sourceLocation = temp.newFolder(\"source_table\").toString();\n+    String sourceLocation = temp.resolve(\"source_table\").toString();\n     Table source = tables.create(SUPPORTED_PRIMITIVES, sourceLocation);\n \n     // write out an Avro data file with all of the data types for source data\n     List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n-    File avroData = temp.newFile(\"data.avro\");\n-    Assert.assertTrue(avroData.delete());\n+    File avroData = File.createTempFile(\"data\", \".avro\", temp.toFile());\n+    assertThat(avroData.delete()).isTrue();\n     try (FileAppender<GenericData.Record> appender =\n         Avro.write(Files.localOutput(avroData)).schema(source.schema()).build()) {\n       appender.addAll(expected);\n@@ -286,15 +284,15 @@ public void testPartitionValueTypes() throws Exception {\n     for (String column : columnNames) {\n       String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n \n-      File parent = temp.newFolder(desc);\n+      File parent = new File(temp.toFile(), desc);\n       File location = new File(parent, \"test\");\n       File dataFolder = new File(location, \"data\");\n-      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n+      assertThat(dataFolder.mkdirs()).as(\"mkdirs should succeed\").isTrue();\n \n       PartitionSpec spec = PartitionSpec.builderFor(SUPPORTED_PRIMITIVES).identity(column).build();\n \n       Table table = tables.create(SUPPORTED_PRIMITIVES, spec, location.toString());\n-      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n+      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format.toString()).commit();\n \n       sourceDF\n           .write()\n@@ -311,7 +309,7 @@ public void testPartitionValueTypes() throws Exception {\n               .load(location.toString())\n               .collectAsList();\n \n-      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+      assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n \n       for (int i = 0; i < expected.size(); i += 1) {\n         TestHelpers.assertEqualsSafe(\n@@ -320,7 +318,7 @@ public void testPartitionValueTypes() throws Exception {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNestedPartitionValues() throws Exception {\n     String[] columnNames =\n         new String[] {\n@@ -331,13 +329,13 @@ public void testNestedPartitionValues() throws Exception {\n     Schema nestedSchema = new Schema(optional(1, \"nested\", SUPPORTED_PRIMITIVES.asStruct()));\n \n     // create a table around the source data\n-    String sourceLocation = temp.newFolder(\"source_table\").toString();\n+    String sourceLocation = temp.resolve(\"source_table\").toString();\n     Table source = tables.create(nestedSchema, sourceLocation);\n \n     // write out an Avro data file with all of the data types for source data\n     List<GenericData.Record> expected = RandomData.generateList(source.schema(), 2, 128735L);\n-    File avroData = temp.newFile(\"data.avro\");\n-    Assert.assertTrue(avroData.delete());\n+    File avroData = File.createTempFile(\"data\", \".avro\", temp.toFile());\n+    assertThat(avroData.delete()).isTrue();\n     try (FileAppender<GenericData.Record> appender =\n         Avro.write(Files.localOutput(avroData)).schema(source.schema()).build()) {\n       appender.addAll(expected);\n@@ -363,16 +361,16 @@ public void testNestedPartitionValues() throws Exception {\n     for (String column : columnNames) {\n       String desc = \"partition_by_\" + SUPPORTED_PRIMITIVES.findType(column).toString();\n \n-      File parent = temp.newFolder(desc);\n+      File parent = new File(temp.toFile(), desc);\n       File location = new File(parent, \"test\");\n       File dataFolder = new File(location, \"data\");\n-      Assert.assertTrue(\"mkdirs should succeed\", dataFolder.mkdirs());\n+      assertThat(dataFolder.mkdirs()).as(\"mkdirs should succeed\").isTrue();\n \n       PartitionSpec spec =\n           PartitionSpec.builderFor(nestedSchema).identity(\"nested.\" + column).build();\n \n       Table table = tables.create(nestedSchema, spec, location.toString());\n-      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n+      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format.toString()).commit();\n \n       sourceDF\n           .write()\n@@ -389,7 +387,7 @@ public void testNestedPartitionValues() throws Exception {\n               .load(location.toString())\n               .collectAsList();\n \n-      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n+      assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n \n       for (int i = 0; i < expected.size(); i += 1) {\n         TestHelpers.assertEqualsSafe(nestedSchema.asStruct(), expected.get(i), actual.get(i));\n@@ -403,7 +401,7 @@ public void testNestedPartitionValues() throws Exception {\n    * thrown with the message like: Cannot cast org.apache.spark.unsafe.types.UTF8String to\n    * java.lang.CharSequence\n    */\n-  @Test\n+  @TestTemplate\n   public void testPartitionedByNestedString() throws Exception {\n     // schema and partition spec\n     Schema nestedSchema =\n@@ -417,7 +415,7 @@ public void testPartitionedByNestedString() throws Exception {\n \n     // create table\n     HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n-    String baseLocation = temp.newFolder(\"partition_by_nested_string\").toString();\n+    String baseLocation = temp.resolve(\"partition_by_nested_string\").toString();\n     tables.create(nestedSchema, spec, baseLocation);\n \n     // input data frame\n@@ -448,12 +446,12 @@ public void testPartitionedByNestedString() throws Exception {\n             .load(baseLocation)\n             .collectAsList();\n \n-    Assert.assertEquals(\"Number of rows should match\", rows.size(), actual.size());\n+    assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(rows);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadPartitionColumn() throws Exception {\n-    Assume.assumeTrue(\"Temporary skip ORC\", !\"orc\".equals(format));\n+    assumeThat(format).as(\"Temporary skip ORC\").isNotEqualTo(FileFormat.ORC);\n \n     Schema nestedSchema =\n         new Schema(\n@@ -469,9 +467,9 @@ public void testReadPartitionColumn() throws Exception {\n \n     // create table\n     HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n-    String baseLocation = temp.newFolder(\"partition_by_nested_string\").toString();\n+    String baseLocation = temp.resolve(\"partition_by_nested_string\").toString();\n     Table table = tables.create(nestedSchema, spec, baseLocation);\n-    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n+    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format.toString()).commit();\n \n     // write into iceberg\n     MapFunction<Long, ComplexRecord> func =\n@@ -495,10 +493,10 @@ public void testReadPartitionColumn() throws Exception {\n             .as(Encoders.STRING())\n             .collectAsList();\n \n-    Assert.assertEquals(\"Number of rows should match\", 10, actual.size());\n+    assertThat(actual).as(\"Number of rows should match\").hasSize(10);\n \n     List<String> inputRecords =\n         IntStream.range(0, 10).mapToObj(i -> \"name_\" + i).collect(Collectors.toList());\n-    Assert.assertEquals(\"Read object should be matched\", inputRecords, actual);\n+    assertThat(actual).as(\"Read object should be matched\").isEqualTo(inputRecords);\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAggregates.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAggregates.java\nindex e2d6f744f5a5..6cbe3914dcf8 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAggregates.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAggregates.java\n@@ -18,6 +18,8 @@\n  */\n package org.apache.iceberg.spark.source;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import java.util.Map;\n import org.apache.iceberg.expressions.Expression;\n import org.apache.iceberg.expressions.Expressions;\n@@ -29,8 +31,7 @@\n import org.apache.spark.sql.connector.expressions.aggregate.CountStar;\n import org.apache.spark.sql.connector.expressions.aggregate.Max;\n import org.apache.spark.sql.connector.expressions.aggregate.Min;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n public class TestSparkAggregates {\n \n@@ -50,27 +51,26 @@ public void testAggregates() {\n           Max max = new Max(namedReference);\n           Expression expectedMax = Expressions.max(unquoted);\n           Expression actualMax = SparkAggregates.convert(max);\n-          Assert.assertEquals(\"Max must match\", expectedMax.toString(), actualMax.toString());\n+          assertThat(actualMax).asString().isEqualTo(expectedMax.toString());\n \n           Min min = new Min(namedReference);\n           Expression expectedMin = Expressions.min(unquoted);\n           Expression actualMin = SparkAggregates.convert(min);\n-          Assert.assertEquals(\"Min must match\", expectedMin.toString(), actualMin.toString());\n+          assertThat(actualMin).asString().isEqualTo(expectedMin.toString());\n \n           Count count = new Count(namedReference, false);\n           Expression expectedCount = Expressions.count(unquoted);\n           Expression actualCount = SparkAggregates.convert(count);\n-          Assert.assertEquals(\"Count must match\", expectedCount.toString(), actualCount.toString());\n+          assertThat(actualCount).asString().isEqualTo(expectedCount.toString());\n \n           Count countDistinct = new Count(namedReference, true);\n           Expression convertedCountDistinct = SparkAggregates.convert(countDistinct);\n-          Assert.assertNull(\"Count Distinct is converted to null\", convertedCountDistinct);\n+          assertThat(convertedCountDistinct).as(\"Count Distinct is converted to null\").isNull();\n \n           CountStar countStar = new CountStar();\n           Expression expectedCountStar = Expressions.countStar();\n           Expression actualCountStar = SparkAggregates.convert(countStar);\n-          Assert.assertEquals(\n-              \"CountStar must match\", expectedCountStar.toString(), actualCountStar.toString());\n+          assertThat(actualCountStar).asString().isEqualTo(expectedCountStar.toString());\n         });\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataFile.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataFile.java\nindex 16fde3c95444..182b1ef8f5af 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataFile.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataFile.java\n@@ -70,12 +70,11 @@\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.catalyst.InternalRow;\n import org.apache.spark.sql.types.StructType;\n-import org.junit.AfterClass;\n-import org.junit.Before;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n public class TestSparkDataFile {\n \n@@ -119,13 +118,13 @@ public class TestSparkDataFile {\n   private static SparkSession spark;\n   private static JavaSparkContext sparkContext = null;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestSparkDataFile.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n     TestSparkDataFile.sparkContext = JavaSparkContext.fromSparkContext(spark.sparkContext());\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestSparkDataFile.spark;\n     TestSparkDataFile.spark = null;\n@@ -133,12 +132,11 @@ public static void stopSpark() {\n     currentSpark.stop();\n   }\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private File tableDir;\n   private String tableLocation = null;\n \n-  @Before\n+  @BeforeEach\n   public void setupTableLocation() throws Exception {\n-    File tableDir = temp.newFolder();\n     this.tableLocation = tableDir.toURI().toString();\n   }\n \n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderWithBloomFilter.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderWithBloomFilter.java\nindex e5831b76e424..baf7fa8f88a2 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderWithBloomFilter.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderWithBloomFilter.java\n@@ -24,10 +24,13 @@\n import static org.apache.iceberg.TableProperties.PARQUET_BLOOM_FILTER_COLUMN_ENABLED_PREFIX;\n import static org.apache.iceberg.TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES;\n import static org.apache.iceberg.TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES_DEFAULT;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.Closeable;\n+import java.io.File;\n import java.io.IOException;\n import java.math.BigDecimal;\n+import java.nio.file.Path;\n import java.time.LocalDate;\n import java.util.List;\n import java.util.Map;\n@@ -38,6 +41,9 @@\n import org.apache.iceberg.DataFiles;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.Files;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.StructLike;\n import org.apache.iceberg.Table;\n@@ -62,18 +68,15 @@\n import org.apache.iceberg.util.PropertyUtil;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.SparkSession;\n-import org.junit.After;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkReaderWithBloomFilter {\n \n   protected String tableName = null;\n@@ -84,13 +87,12 @@ public class TestSparkReaderWithBloomFilter {\n   private static TestHiveMetastore metastore = null;\n   protected static SparkSession spark = null;\n   protected static HiveCatalog catalog = null;\n-  protected final boolean vectorized;\n-  protected final boolean useBloomFilter;\n \n-  public TestSparkReaderWithBloomFilter(boolean vectorized, boolean useBloomFilter) {\n-    this.vectorized = vectorized;\n-    this.useBloomFilter = useBloomFilter;\n-  }\n+  @Parameter(index = 0)\n+  protected boolean vectorized;\n+\n+  @Parameter(index = 1)\n+  protected boolean useBloomFilter;\n \n   // Schema passed to create tables\n   public static final Schema SCHEMA =\n@@ -114,9 +116,9 @@ public TestSparkReaderWithBloomFilter(boolean vectorized, boolean useBloomFilter\n   private static final float FLOAT_BASE = 100000F;\n   private static final String BINARY_PREFIX = \"BINARYÊµãËØï_\";\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n-  @Before\n+  @BeforeEach\n   public void writeTestDataFile() throws IOException {\n     this.tableName = \"test\";\n     createTable(tableName, SCHEMA);\n@@ -151,22 +153,26 @@ public void writeTestDataFile() throws IOException {\n                   new BigDecimal(String.valueOf(99.99)))));\n     }\n \n-    this.dataFile = writeDataFile(Files.localOutput(temp.newFile()), Row.of(0), records);\n+    this.dataFile =\n+        writeDataFile(\n+            Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n+            Row.of(0),\n+            records);\n \n     table.newAppend().appendFile(dataFile).commit();\n   }\n \n-  @After\n+  @AfterEach\n   public void cleanup() throws IOException {\n     dropTable(\"test\");\n   }\n \n-  @Parameterized.Parameters(name = \"vectorized = {0}, useBloomFilter = {1}\")\n+  @Parameters(name = \"vectorized = {0}, useBloomFilter = {1}\")\n   public static Object[][] parameters() {\n     return new Object[][] {{false, false}, {true, false}, {false, true}, {true, true}};\n   }\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startMetastoreAndSpark() {\n     metastore = new TestHiveMetastore();\n     metastore.start();\n@@ -191,7 +197,7 @@ public static void startMetastoreAndSpark() {\n     }\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopMetastoreAndSpark() throws Exception {\n     catalog = null;\n     metastore.stop();\n@@ -334,7 +340,7 @@ private FileFormat defaultFormat(Map<String, String> properties) {\n     return FileFormat.fromString(formatString);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadWithFilter() {\n     Dataset<org.apache.spark.sql.Row> df =\n         spark\n@@ -349,9 +355,8 @@ public void testReadWithFilter() {\n \n     Record record = SparkValueConverter.convert(table.schema(), df.collectAsList().get(0));\n \n-    Assert.assertEquals(\"Table should contain 1 row\", 1, df.collectAsList().size());\n-\n-    Assert.assertEquals(\"Table should contain expected rows\", record.get(0), 30);\n+    assertThat(df.collectAsList()).as(\"Table should contain 1 row\").hasSize(1);\n+    assertThat(record.get(0)).as(\"Table should contain expected rows\").isEqualTo(30);\n \n     df =\n         spark\n@@ -366,8 +371,7 @@ public void testReadWithFilter() {\n \n     record = SparkValueConverter.convert(table.schema(), df.collectAsList().get(0));\n \n-    Assert.assertEquals(\"Table should contain 1 row\", 1, df.collectAsList().size());\n-\n-    Assert.assertEquals(\"Table should contain expected rows\", record.get(0), 250);\n+    assertThat(df.collectAsList()).as(\"Table should contain 1 row\").hasSize(1);\n+    assertThat(record.get(0)).as(\"Table should contain expected rows\").isEqualTo(250);\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStreamingOffset.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStreamingOffset.java\nindex 17370aaa22f2..d55e718ff2d3 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStreamingOffset.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStreamingOffset.java\n@@ -18,11 +18,12 @@\n  */\n package org.apache.iceberg.spark.source;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import com.fasterxml.jackson.databind.node.ObjectNode;\n import java.util.Arrays;\n import org.apache.iceberg.util.JsonUtil;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n public class TestStreamingOffset {\n \n@@ -35,10 +36,9 @@ public void testJsonConversion() {\n           new StreamingOffset(System.currentTimeMillis(), 3L, false),\n           new StreamingOffset(System.currentTimeMillis(), 4L, true)\n         };\n-    Assert.assertArrayEquals(\n-        \"StreamingOffsets should match\",\n-        expected,\n-        Arrays.stream(expected).map(elem -> StreamingOffset.fromJson(elem.json())).toArray());\n+    assertThat(Arrays.stream(expected).map(elem -> StreamingOffset.fromJson(elem.json())).toArray())\n+        .as(\"StreamingOffsets should match\")\n+        .isEqualTo(expected);\n   }\n \n   @Test\n@@ -51,6 +51,6 @@ public void testToJson() throws Exception {\n     actual.put(\"scan_all_files\", false);\n     String expectedJson = expected.json();\n     String actualJson = JsonUtil.mapper().writeValueAsString(actual);\n-    Assert.assertEquals(\"Json should match\", expectedJson, actualJson);\n+    assertThat(actualJson).isEqualTo(expectedJson);\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java\nindex 961d69b72127..9bdfc34dc2ce 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java\n@@ -19,10 +19,12 @@\n package org.apache.iceberg.spark.source;\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.io.File;\n import java.nio.file.Files;\n+import java.nio.file.Path;\n import java.nio.file.Paths;\n import java.util.List;\n import org.apache.hadoop.conf.Configuration;\n@@ -30,7 +32,6 @@\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.hadoop.HadoopTables;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.Dataset;\n@@ -43,12 +44,10 @@\n import org.apache.spark.sql.streaming.DataStreamWriter;\n import org.apache.spark.sql.streaming.StreamingQuery;\n import org.apache.spark.sql.streaming.StreamingQueryException;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n import scala.Option;\n import scala.collection.JavaConverters;\n \n@@ -60,9 +59,9 @@ public class TestStructuredStreaming {\n           optional(1, \"id\", Types.IntegerType.get()), optional(2, \"data\", Types.StringType.get()));\n   private static SparkSession spark = null;\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestStructuredStreaming.spark =\n         SparkSession.builder()\n@@ -71,7 +70,7 @@ public static void startSpark() {\n             .getOrCreate();\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestStructuredStreaming.spark;\n     TestStructuredStreaming.spark = null;\n@@ -80,7 +79,7 @@ public static void stopSpark() {\n \n   @Test\n   public void testStreamingWriteAppendMode() throws Exception {\n-    File parent = temp.newFolder(\"parquet\");\n+    File parent = temp.resolve(\"parquet\").toFile();\n     File location = new File(parent, \"test-table\");\n     File checkpoint = new File(parent, \"checkpoint\");\n \n@@ -119,7 +118,7 @@ public void testStreamingWriteAppendMode() throws Exception {\n \n       // remove the last commit to force Spark to reprocess batch #1\n       File lastCommitFile = new File(checkpoint + \"/commits/1\");\n-      Assert.assertTrue(\"The commit file must be deleted\", lastCommitFile.delete());\n+      assertThat(lastCommitFile.delete()).as(\"The commit file must be deleted\").isTrue();\n       Files.deleteIfExists(Paths.get(checkpoint + \"/commits/.1.crc\"));\n \n       // restart the query from the checkpoint\n@@ -130,9 +129,8 @@ public void testStreamingWriteAppendMode() throws Exception {\n       Dataset<Row> result = spark.read().format(\"iceberg\").load(location.toString());\n       List<SimpleRecord> actual =\n           result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-      Assert.assertEquals(\"Result rows should match\", expected, actual);\n-      Assert.assertEquals(\"Number of snapshots should match\", 2, Iterables.size(table.snapshots()));\n+      assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n+      assertThat(table.snapshots()).as(\"Number of snapshots should match\").hasSize(2);\n     } finally {\n       for (StreamingQuery query : spark.streams().active()) {\n         query.stop();\n@@ -142,7 +140,7 @@ public void testStreamingWriteAppendMode() throws Exception {\n \n   @Test\n   public void testStreamingWriteCompleteMode() throws Exception {\n-    File parent = temp.newFolder(\"parquet\");\n+    File parent = temp.resolve(\"parquet\").toFile();\n     File location = new File(parent, \"test-table\");\n     File checkpoint = new File(parent, \"checkpoint\");\n \n@@ -180,7 +178,7 @@ public void testStreamingWriteCompleteMode() throws Exception {\n \n       // remove the last commit to force Spark to reprocess batch #1\n       File lastCommitFile = new File(checkpoint + \"/commits/1\");\n-      Assert.assertTrue(\"The commit file must be deleted\", lastCommitFile.delete());\n+      assertThat(lastCommitFile.delete()).as(\"The commit file must be deleted\").isTrue();\n       Files.deleteIfExists(Paths.get(checkpoint + \"/commits/.1.crc\"));\n \n       // restart the query from the checkpoint\n@@ -191,9 +189,8 @@ public void testStreamingWriteCompleteMode() throws Exception {\n       Dataset<Row> result = spark.read().format(\"iceberg\").load(location.toString());\n       List<SimpleRecord> actual =\n           result.orderBy(\"data\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-      Assert.assertEquals(\"Result rows should match\", expected, actual);\n-      Assert.assertEquals(\"Number of snapshots should match\", 2, Iterables.size(table.snapshots()));\n+      assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n+      assertThat(table.snapshots()).as(\"Number of snapshots should match\").hasSize(2);\n     } finally {\n       for (StreamingQuery query : spark.streams().active()) {\n         query.stop();\n@@ -203,7 +200,7 @@ public void testStreamingWriteCompleteMode() throws Exception {\n \n   @Test\n   public void testStreamingWriteCompleteModeWithProjection() throws Exception {\n-    File parent = temp.newFolder(\"parquet\");\n+    File parent = temp.resolve(\"parquet\").toFile();\n     File location = new File(parent, \"test-table\");\n     File checkpoint = new File(parent, \"checkpoint\");\n \n@@ -241,7 +238,7 @@ public void testStreamingWriteCompleteModeWithProjection() throws Exception {\n \n       // remove the last commit to force Spark to reprocess batch #1\n       File lastCommitFile = new File(checkpoint + \"/commits/1\");\n-      Assert.assertTrue(\"The commit file must be deleted\", lastCommitFile.delete());\n+      assertThat(lastCommitFile.delete()).as(\"The commit file must be deleted\").isTrue();\n       Files.deleteIfExists(Paths.get(checkpoint + \"/commits/.1.crc\"));\n \n       // restart the query from the checkpoint\n@@ -252,9 +249,8 @@ public void testStreamingWriteCompleteModeWithProjection() throws Exception {\n       Dataset<Row> result = spark.read().format(\"iceberg\").load(location.toString());\n       List<SimpleRecord> actual =\n           result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-      Assert.assertEquals(\"Number of rows should match\", expected.size(), actual.size());\n-      Assert.assertEquals(\"Result rows should match\", expected, actual);\n-      Assert.assertEquals(\"Number of snapshots should match\", 2, Iterables.size(table.snapshots()));\n+      assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n+      assertThat(table.snapshots()).as(\"Number of snapshots should match\").hasSize(2);\n     } finally {\n       for (StreamingQuery query : spark.streams().active()) {\n         query.stop();\n@@ -264,7 +260,7 @@ public void testStreamingWriteCompleteModeWithProjection() throws Exception {\n \n   @Test\n   public void testStreamingWriteUpdateMode() throws Exception {\n-    File parent = temp.newFolder(\"parquet\");\n+    File parent = temp.resolve(\"parquet\").toFile();\n     File location = new File(parent, \"test-table\");\n     File checkpoint = new File(parent, \"checkpoint\");\n \n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestWriteMetricsConfig.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestWriteMetricsConfig.java\nindex 73827b309be2..ce298143f158 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestWriteMetricsConfig.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestWriteMetricsConfig.java\n@@ -21,10 +21,11 @@\n import static org.apache.iceberg.spark.SparkSchemaUtil.convert;\n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n-import java.io.IOException;\n import java.nio.ByteBuffer;\n+import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n import org.apache.hadoop.conf.Configuration;\n@@ -49,12 +50,10 @@\n import org.apache.spark.sql.SaveMode;\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.catalyst.InternalRow;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n public class TestWriteMetricsConfig {\n \n@@ -73,18 +72,18 @@ public class TestWriteMetricsConfig {\n                   required(4, \"id\", Types.IntegerType.get()),\n                   required(5, \"data\", Types.StringType.get()))));\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n   private static SparkSession spark = null;\n   private static JavaSparkContext sc = null;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestWriteMetricsConfig.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n     TestWriteMetricsConfig.sc = JavaSparkContext.fromSparkContext(spark.sparkContext());\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestWriteMetricsConfig.spark;\n     TestWriteMetricsConfig.spark = null;\n@@ -93,8 +92,8 @@ public static void stopSpark() {\n   }\n \n   @Test\n-  public void testFullMetricsCollectionForParquet() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  public void testFullMetricsCollectionForParquet() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -116,16 +115,16 @@ public void testFullMetricsCollectionForParquet() throws IOException {\n \n     for (FileScanTask task : table.newScan().includeColumnStats().planFiles()) {\n       DataFile file = task.file();\n-      Assert.assertEquals(2, file.nullValueCounts().size());\n-      Assert.assertEquals(2, file.valueCounts().size());\n-      Assert.assertEquals(2, file.lowerBounds().size());\n-      Assert.assertEquals(2, file.upperBounds().size());\n+      assertThat(file.nullValueCounts()).hasSize(2);\n+      assertThat(file.valueCounts()).hasSize(2);\n+      assertThat(file.lowerBounds()).hasSize(2);\n+      assertThat(file.upperBounds()).hasSize(2);\n     }\n   }\n \n   @Test\n-  public void testCountMetricsCollectionForParquet() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  public void testCountMetricsCollectionForParquet() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -147,16 +146,16 @@ public void testCountMetricsCollectionForParquet() throws IOException {\n \n     for (FileScanTask task : table.newScan().includeColumnStats().planFiles()) {\n       DataFile file = task.file();\n-      Assert.assertEquals(2, file.nullValueCounts().size());\n-      Assert.assertEquals(2, file.valueCounts().size());\n-      Assert.assertTrue(file.lowerBounds().isEmpty());\n-      Assert.assertTrue(file.upperBounds().isEmpty());\n+      assertThat(file.nullValueCounts()).hasSize(2);\n+      assertThat(file.valueCounts()).hasSize(2);\n+      assertThat(file.lowerBounds()).isEmpty();\n+      assertThat(file.upperBounds()).isEmpty();\n     }\n   }\n \n   @Test\n-  public void testNoMetricsCollectionForParquet() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  public void testNoMetricsCollectionForParquet() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -178,16 +177,16 @@ public void testNoMetricsCollectionForParquet() throws IOException {\n \n     for (FileScanTask task : table.newScan().includeColumnStats().planFiles()) {\n       DataFile file = task.file();\n-      Assert.assertTrue(file.nullValueCounts().isEmpty());\n-      Assert.assertTrue(file.valueCounts().isEmpty());\n-      Assert.assertTrue(file.lowerBounds().isEmpty());\n-      Assert.assertTrue(file.upperBounds().isEmpty());\n+      assertThat(file.nullValueCounts()).isEmpty();\n+      assertThat(file.valueCounts()).isEmpty();\n+      assertThat(file.lowerBounds()).isEmpty();\n+      assertThat(file.upperBounds()).isEmpty();\n     }\n   }\n \n   @Test\n-  public void testCustomMetricCollectionForParquet() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  public void testCustomMetricCollectionForParquet() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -212,18 +211,16 @@ public void testCustomMetricCollectionForParquet() throws IOException {\n     Types.NestedField id = schema.findField(\"id\");\n     for (FileScanTask task : table.newScan().includeColumnStats().planFiles()) {\n       DataFile file = task.file();\n-      Assert.assertEquals(2, file.nullValueCounts().size());\n-      Assert.assertEquals(2, file.valueCounts().size());\n-      Assert.assertEquals(1, file.lowerBounds().size());\n-      Assert.assertTrue(file.lowerBounds().containsKey(id.fieldId()));\n-      Assert.assertEquals(1, file.upperBounds().size());\n-      Assert.assertTrue(file.upperBounds().containsKey(id.fieldId()));\n+      assertThat(file.nullValueCounts()).hasSize(2);\n+      assertThat(file.valueCounts()).hasSize(2);\n+      assertThat(file.lowerBounds()).hasSize(1).containsKey(id.fieldId());\n+      assertThat(file.upperBounds()).hasSize(1).containsKey(id.fieldId());\n     }\n   }\n \n   @Test\n-  public void testBadCustomMetricCollectionForParquet() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  public void testBadCustomMetricCollectionForParquet() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -238,8 +235,8 @@ public void testBadCustomMetricCollectionForParquet() throws IOException {\n   }\n \n   @Test\n-  public void testCustomMetricCollectionForNestedParquet() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+  public void testCustomMetricCollectionForNestedParquet() {\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.builderFor(COMPLEX_SCHEMA).identity(\"strCol\").build();\n@@ -270,28 +267,26 @@ public void testCustomMetricCollectionForNestedParquet() throws IOException {\n       DataFile file = task.file();\n \n       Map<Integer, Long> nullValueCounts = file.nullValueCounts();\n-      Assert.assertEquals(3, nullValueCounts.size());\n-      Assert.assertTrue(nullValueCounts.containsKey(longCol.fieldId()));\n-      Assert.assertTrue(nullValueCounts.containsKey(recordId.fieldId()));\n-      Assert.assertTrue(nullValueCounts.containsKey(recordData.fieldId()));\n+      assertThat(nullValueCounts)\n+          .hasSize(3)\n+          .containsKeys(longCol.fieldId(), recordId.fieldId(), recordData.fieldId());\n \n       Map<Integer, Long> valueCounts = file.valueCounts();\n-      Assert.assertEquals(3, valueCounts.size());\n-      Assert.assertTrue(valueCounts.containsKey(longCol.fieldId()));\n-      Assert.assertTrue(valueCounts.containsKey(recordId.fieldId()));\n-      Assert.assertTrue(valueCounts.containsKey(recordData.fieldId()));\n+      assertThat(valueCounts)\n+          .hasSize(3)\n+          .containsKeys(longCol.fieldId(), recordId.fieldId(), recordData.fieldId());\n \n       Map<Integer, ByteBuffer> lowerBounds = file.lowerBounds();\n-      Assert.assertEquals(2, lowerBounds.size());\n-      Assert.assertTrue(lowerBounds.containsKey(recordId.fieldId()));\n+      assertThat(lowerBounds).hasSize(2).containsKey(recordId.fieldId());\n+\n       ByteBuffer recordDataLowerBound = lowerBounds.get(recordData.fieldId());\n-      Assert.assertEquals(2, ByteBuffers.toByteArray(recordDataLowerBound).length);\n+      assertThat(ByteBuffers.toByteArray(recordDataLowerBound)).hasSize(2);\n \n       Map<Integer, ByteBuffer> upperBounds = file.upperBounds();\n-      Assert.assertEquals(2, upperBounds.size());\n-      Assert.assertTrue(upperBounds.containsKey(recordId.fieldId()));\n+      assertThat(upperBounds).hasSize(2).containsKey(recordId.fieldId());\n+\n       ByteBuffer recordDataUpperBound = upperBounds.get(recordData.fieldId());\n-      Assert.assertEquals(2, ByteBuffers.toByteArray(recordDataUpperBound).length);\n+      assertThat(ByteBuffers.toByteArray(recordDataUpperBound)).hasSize(2);\n     }\n   }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java\nindex 348173596e46..8c85f360b501 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestFilteredScan.java\n@@ -50,7 +50,6 @@\n import org.apache.iceberg.io.FileAppender;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.spark.SparkReadOptions;\n import org.apache.iceberg.spark.SparkWriteOptions;\n import org.apache.iceberg.spark.data.GenericsHelpers;\n@@ -123,7 +122,7 @@ public static void stopSpark() {\n   @TempDir private Path temp;\n \n   @Parameter(index = 0)\n-  private String format;\n+  private FileFormat fileFormat;\n \n   @Parameter(index = 1)\n   private boolean vectorized;\n@@ -134,11 +133,11 @@ public static void stopSpark() {\n   @Parameters(name = \"format = {0}, vectorized = {1}, planningMode = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {\"parquet\", false, LOCAL},\n-      {\"parquet\", true, DISTRIBUTED},\n-      {\"avro\", false, LOCAL},\n-      {\"orc\", false, DISTRIBUTED},\n-      {\"orc\", true, LOCAL}\n+      {FileFormat.PARQUET, false, LOCAL},\n+      {FileFormat.PARQUET, true, DISTRIBUTED},\n+      {FileFormat.AVRO, false, LOCAL},\n+      {FileFormat.ORC, false, DISTRIBUTED},\n+      {FileFormat.ORC, true, LOCAL}\n     };\n   }\n \n@@ -165,8 +164,6 @@ public void writeUnpartitionedTable() throws IOException {\n             unpartitioned.toString());\n     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned\n \n-    FileFormat fileFormat = FileFormat.fromString(format);\n-\n     File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n \n     this.records = testRecords(tableSchema);\n@@ -522,8 +519,7 @@ public void testUnpartitionedStartsWith() {\n     List<String> matchedData =\n         df.select(\"data\").where(\"data LIKE 'jun%'\").as(Encoders.STRING()).collectAsList();\n \n-    assertThat(matchedData).hasSize(1);\n-    assertThat(matchedData.get(0)).isEqualTo(\"junction\");\n+    assertThat(matchedData).singleElement().isEqualTo(\"junction\");\n   }\n \n   @TestTemplate\n@@ -544,8 +540,7 @@ public void testUnpartitionedNotStartsWith() {\n             .filter(d -> !d.startsWith(\"jun\"))\n             .collect(Collectors.toList());\n \n-    assertThat(matchedData).hasSize(9);\n-    assertThat(Sets.newHashSet(matchedData)).isEqualTo(Sets.newHashSet(expected));\n+    assertThat(matchedData).hasSize(9).containsExactlyInAnyOrderElementsOf(expected);\n   }\n \n   private static Record projectFlat(Schema projection, Record record) {\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java\nindex 84c99a575c8d..cbce824bfa21 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestForwardCompatibility.java\n@@ -108,7 +108,7 @@ public void testSparkWriteFailsUnknownTransform() throws IOException {\n     File parent = temp.resolve(\"avro\").toFile();\n     File location = new File(parent, \"test\");\n     File dataFolder = new File(location, \"data\");\n-    dataFolder.mkdirs();\n+    assertThat(dataFolder.mkdirs()).isTrue();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     tables.create(SCHEMA, UNKNOWN_SPEC, location.toString());\n@@ -135,9 +135,9 @@ public void testSparkStreamingWriteFailsUnknownTransform() throws IOException, T\n     File parent = temp.resolve(\"avro\").toFile();\n     File location = new File(parent, \"test\");\n     File dataFolder = new File(location, \"data\");\n-    dataFolder.mkdirs();\n+    assertThat(dataFolder.mkdirs()).isTrue();\n     File checkpoint = new File(parent, \"checkpoint\");\n-    checkpoint.mkdirs();\n+    assertThat(checkpoint.mkdirs()).isTrue();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     tables.create(SCHEMA, UNKNOWN_SPEC, location.toString());\n@@ -167,7 +167,7 @@ public void testSparkCanReadUnknownTransform() throws IOException {\n     File parent = temp.resolve(\"avro\").toFile();\n     File location = new File(parent, \"test\");\n     File dataFolder = new File(location, \"data\");\n-    dataFolder.mkdirs();\n+    assertThat(dataFolder.mkdirs()).isTrue();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     Table table = tables.create(SCHEMA, UNKNOWN_SPEC, location.toString());\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSpark.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSpark.java\nindex 7eff93d204e4..5843fe992549 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSpark.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSpark.java\n@@ -60,64 +60,84 @@ public static void stopSpark() {\n   public void testRegisterIntegerBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_int_16\", DataTypes.IntegerType, 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_int_16(1)\").collectAsList();\n-\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getInt(0))\n-        .isEqualTo(Transforms.bucket(16).bind(Types.IntegerType.get()).apply(1));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.bucket(16).bind(Types.IntegerType.get()).apply(1)));\n   }\n \n   @Test\n   public void testRegisterShortBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_short_16\", DataTypes.ShortType, 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_short_16(1S)\").collectAsList();\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getInt(0))\n-        .isEqualTo(Transforms.bucket(16).bind(Types.IntegerType.get()).apply(1));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.bucket(16).bind(Types.IntegerType.get()).apply(1)));\n   }\n \n   @Test\n   public void testRegisterByteBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_byte_16\", DataTypes.ByteType, 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_byte_16(1Y)\").collectAsList();\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getInt(0))\n-        .isEqualTo(Transforms.bucket(16).bind(Types.IntegerType.get()).apply(1));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.bucket(16).bind(Types.IntegerType.get()).apply(1)));\n   }\n \n   @Test\n   public void testRegisterLongBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_long_16\", DataTypes.LongType, 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_long_16(1L)\").collectAsList();\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getInt(0))\n-        .isEqualTo(Transforms.bucket(16).bind(Types.LongType.get()).apply(1L));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.bucket(16).bind(Types.LongType.get()).apply(1L)));\n   }\n \n   @Test\n   public void testRegisterStringBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_string_16\", DataTypes.StringType, 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_string_16('hello')\").collectAsList();\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getInt(0))\n-        .isEqualTo(Transforms.bucket(16).bind(Types.StringType.get()).apply(\"hello\"));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.bucket(16).bind(Types.StringType.get()).apply(\"hello\")));\n   }\n \n   @Test\n   public void testRegisterCharBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_char_16\", new CharType(5), 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_char_16('hello')\").collectAsList();\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getInt(0))\n-        .isEqualTo(Transforms.bucket(16).bind(Types.StringType.get()).apply(\"hello\"));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.bucket(16).bind(Types.StringType.get()).apply(\"hello\")));\n   }\n \n   @Test\n   public void testRegisterVarCharBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_varchar_16\", new VarcharType(5), 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_varchar_16('hello')\").collectAsList();\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getInt(0))\n-        .isEqualTo(Transforms.bucket(16).bind(Types.StringType.get()).apply(\"hello\"));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.bucket(16).bind(Types.StringType.get()).apply(\"hello\")));\n   }\n \n   @Test\n@@ -125,12 +145,15 @@ public void testRegisterDateBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_date_16\", DataTypes.DateType, 16);\n     List<Row> results =\n         spark.sql(\"SELECT iceberg_bucket_date_16(DATE '2021-06-30')\").collectAsList();\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getInt(0))\n-        .isEqualTo(\n-            Transforms.bucket(16)\n-                .bind(Types.DateType.get())\n-                .apply(DateTimeUtils.fromJavaDate(Date.valueOf(\"2021-06-30\"))));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(\n+                        Transforms.bucket(16)\n+                            .bind(Types.DateType.get())\n+                            .apply(DateTimeUtils.fromJavaDate(Date.valueOf(\"2021-06-30\")))));\n   }\n \n   @Test\n@@ -141,35 +164,47 @@ public void testRegisterTimestampBucketUDF() {\n         spark\n             .sql(\"SELECT iceberg_bucket_timestamp_16(TIMESTAMP '2021-06-30 00:00:00.000')\")\n             .collectAsList();\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getInt(0))\n-        .isEqualTo(\n-            Transforms.bucket(16)\n-                .bind(Types.TimestampType.withZone())\n-                .apply(\n-                    DateTimeUtils.fromJavaTimestamp(Timestamp.valueOf(\"2021-06-30 00:00:00.000\"))));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(\n+                        Transforms.bucket(16)\n+                            .bind(Types.TimestampType.withZone())\n+                            .apply(\n+                                DateTimeUtils.fromJavaTimestamp(\n+                                    Timestamp.valueOf(\"2021-06-30 00:00:00.000\")))));\n   }\n \n   @Test\n   public void testRegisterBinaryBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_binary_16\", DataTypes.BinaryType, 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_binary_16(X'0020001F')\").collectAsList();\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getInt(0))\n-        .isEqualTo(\n-            Transforms.bucket(16)\n-                .bind(Types.BinaryType.get())\n-                .apply(ByteBuffer.wrap(new byte[] {0x00, 0x20, 0x00, 0x1F})));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(\n+                        Transforms.bucket(16)\n+                            .bind(Types.BinaryType.get())\n+                            .apply(ByteBuffer.wrap(new byte[] {0x00, 0x20, 0x00, 0x1F}))));\n   }\n \n   @Test\n   public void testRegisterDecimalBucketUDF() {\n     IcebergSpark.registerBucketUDF(spark, \"iceberg_bucket_decimal_16\", new DecimalType(4, 2), 16);\n     List<Row> results = spark.sql(\"SELECT iceberg_bucket_decimal_16(11.11)\").collectAsList();\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getInt(0))\n-        .isEqualTo(\n-            Transforms.bucket(16).bind(Types.DecimalType.of(4, 2)).apply(new BigDecimal(\"11.11\")));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(\n+                        Transforms.bucket(16)\n+                            .bind(Types.DecimalType.of(4, 2))\n+                            .apply(new BigDecimal(\"11.11\"))));\n   }\n \n   @Test\n@@ -206,36 +241,50 @@ public void testRegisterFloatBucketUDF() {\n   public void testRegisterIntegerTruncateUDF() {\n     IcebergSpark.registerTruncateUDF(spark, \"iceberg_truncate_int_4\", DataTypes.IntegerType, 4);\n     List<Row> results = spark.sql(\"SELECT iceberg_truncate_int_4(1)\").collectAsList();\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getInt(0))\n-        .isEqualTo(Transforms.truncate(4).bind(Types.IntegerType.get()).apply(1));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getInt(0))\n+                    .isEqualTo(Transforms.truncate(4).bind(Types.IntegerType.get()).apply(1)));\n   }\n \n   @Test\n   public void testRegisterLongTruncateUDF() {\n     IcebergSpark.registerTruncateUDF(spark, \"iceberg_truncate_long_4\", DataTypes.LongType, 4);\n     List<Row> results = spark.sql(\"SELECT iceberg_truncate_long_4(1L)\").collectAsList();\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getLong(0))\n-        .isEqualTo(Transforms.truncate(4).bind(Types.LongType.get()).apply(1L));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getLong(0))\n+                    .isEqualTo(Transforms.truncate(4).bind(Types.LongType.get()).apply(1L)));\n   }\n \n   @Test\n   public void testRegisterDecimalTruncateUDF() {\n     IcebergSpark.registerTruncateUDF(spark, \"iceberg_truncate_decimal_4\", new DecimalType(4, 2), 4);\n     List<Row> results = spark.sql(\"SELECT iceberg_truncate_decimal_4(11.11)\").collectAsList();\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getDecimal(0))\n-        .isEqualTo(\n-            Transforms.truncate(4).bind(Types.DecimalType.of(4, 2)).apply(new BigDecimal(\"11.11\")));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getDecimal(0))\n+                    .isEqualTo(\n+                        Transforms.truncate(4)\n+                            .bind(Types.DecimalType.of(4, 2))\n+                            .apply(new BigDecimal(\"11.11\"))));\n   }\n \n   @Test\n   public void testRegisterStringTruncateUDF() {\n     IcebergSpark.registerTruncateUDF(spark, \"iceberg_truncate_string_4\", DataTypes.StringType, 4);\n     List<Row> results = spark.sql(\"SELECT iceberg_truncate_string_4('hello')\").collectAsList();\n-    assertThat(results).hasSize(1);\n-    assertThat(results.get(0).getString(0))\n-        .isEqualTo(Transforms.truncate(4).bind(Types.StringType.get()).apply(\"hello\"));\n+    assertThat(results)\n+        .singleElement()\n+        .satisfies(\n+            row ->\n+                assertThat(row.getString(0))\n+                    .isEqualTo(Transforms.truncate(4).bind(Types.StringType.get()).apply(\"hello\")));\n   }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java\nindex 9464f687b0eb..9b81414e8e00 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionPruning.java\n@@ -41,6 +41,7 @@\n import org.apache.hadoop.fs.FSDataInputStream;\n import org.apache.hadoop.fs.Path;\n import org.apache.hadoop.fs.RawLocalFileSystem;\n+import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.Parameter;\n import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n@@ -83,16 +84,16 @@ public class TestPartitionPruning {\n   @Parameters(name = \"format = {0}, vectorized = {1}, planningMode = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {\"parquet\", false, DISTRIBUTED},\n-      {\"parquet\", true, LOCAL},\n-      {\"avro\", false, DISTRIBUTED},\n-      {\"orc\", false, LOCAL},\n-      {\"orc\", true, DISTRIBUTED}\n+      {FileFormat.PARQUET, false, DISTRIBUTED},\n+      {FileFormat.PARQUET, true, LOCAL},\n+      {FileFormat.AVRO, false, DISTRIBUTED},\n+      {FileFormat.ORC, false, LOCAL},\n+      {FileFormat.ORC, true, DISTRIBUTED}\n     };\n   }\n \n   @Parameter(index = 0)\n-  private String format;\n+  private FileFormat format;\n \n   @Parameter(index = 1)\n   private boolean vectorized;\n@@ -283,9 +284,7 @@ private void runTest(String filterCond, Predicate<Row> partCondition) {\n             .filter(filterCond)\n             .orderBy(\"id\")\n             .collectAsList();\n-    assertThat(actual).as(\"Actual rows should not be empty\").isNotEmpty();\n-\n-    assertThat(actual).as(\"Rows should match\").isEqualTo(expected);\n+    assertThat(actual).isNotEmpty().isEqualTo(expected);\n \n     assertAccessOnDataFiles(originTableLocation, table, partCondition);\n   }\n@@ -302,7 +301,7 @@ private Table createTable(File originTableLocation) {\n     String trackedTableLocation = CountOpenLocalFileSystem.convertPath(originTableLocation);\n     Map<String, String> properties =\n         ImmutableMap.of(\n-            TableProperties.DEFAULT_FILE_FORMAT, format,\n+            TableProperties.DEFAULT_FILE_FORMAT, format.toString(),\n             TableProperties.DATA_PLANNING_MODE, planningMode.modeName(),\n             TableProperties.DELETE_PLANNING_MODE, planningMode.modeName());\n     return TABLES.create(LOG_SCHEMA, spec, properties, trackedTableLocation);\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java\nindex 5c218f21c47e..0153996eb5e6 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPartitionValues.java\n@@ -18,7 +18,6 @@\n  */\n package org.apache.iceberg.spark.source;\n \n-import static org.apache.iceberg.spark.data.TestVectorizedOrcDataReader.temp;\n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n import static org.assertj.core.api.Assertions.assertThat;\n@@ -31,6 +30,7 @@\n import java.util.stream.IntStream;\n import org.apache.avro.generic.GenericData;\n import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.Files;\n import org.apache.iceberg.Parameter;\n import org.apache.iceberg.ParameterizedTestExtension;\n@@ -70,11 +70,11 @@ public class TestPartitionValues {\n   @Parameters(name = \"format = {0}, vectorized = {1}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {\"parquet\", false},\n-      {\"parquet\", true},\n-      {\"avro\", false},\n-      {\"orc\", false},\n-      {\"orc\", true}\n+      {FileFormat.PARQUET, false},\n+      {FileFormat.PARQUET, true},\n+      {FileFormat.AVRO, false},\n+      {FileFormat.ORC, false},\n+      {FileFormat.ORC, true}\n     };\n   }\n \n@@ -120,7 +120,7 @@ public static void stopSpark() {\n   @TempDir private Path temp;\n \n   @Parameter(index = 0)\n-  private String format;\n+  private FileFormat format;\n \n   @Parameter(index = 1)\n   private boolean vectorized;\n@@ -135,7 +135,7 @@ public void testNullPartitionValue() throws Exception {\n \n     HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n     Table table = tables.create(SIMPLE_SCHEMA, SPEC, location.toString());\n-    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n+    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format.toString()).commit();\n \n     List<SimpleRecord> expected =\n         Lists.newArrayList(\n@@ -162,8 +162,7 @@ public void testNullPartitionValue() throws Exception {\n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n \n-    assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-    assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n   @TestTemplate\n@@ -176,7 +175,7 @@ public void testReorderedColumns() throws Exception {\n \n     HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n     Table table = tables.create(SIMPLE_SCHEMA, SPEC, location.toString());\n-    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n+    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format.toString()).commit();\n \n     List<SimpleRecord> expected =\n         Lists.newArrayList(\n@@ -201,8 +200,7 @@ public void testReorderedColumns() throws Exception {\n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n \n-    assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-    assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n   @TestTemplate\n@@ -215,7 +213,7 @@ public void testReorderedColumnsNoNullability() throws Exception {\n \n     HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n     Table table = tables.create(SIMPLE_SCHEMA, SPEC, location.toString());\n-    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n+    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format.toString()).commit();\n \n     List<SimpleRecord> expected =\n         Lists.newArrayList(\n@@ -241,8 +239,7 @@ public void testReorderedColumnsNoNullability() throws Exception {\n     List<SimpleRecord> actual =\n         result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n \n-    assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-    assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+    assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n   }\n \n   @TestTemplate\n@@ -295,7 +292,7 @@ public void testPartitionValueTypes() throws Exception {\n       PartitionSpec spec = PartitionSpec.builderFor(SUPPORTED_PRIMITIVES).identity(column).build();\n \n       Table table = tables.create(SUPPORTED_PRIMITIVES, spec, location.toString());\n-      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n+      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format.toString()).commit();\n \n       // disable distribution/ordering and fanout writers to preserve the original ordering\n       sourceDF\n@@ -375,7 +372,7 @@ public void testNestedPartitionValues() throws Exception {\n           PartitionSpec.builderFor(nestedSchema).identity(\"nested.\" + column).build();\n \n       Table table = tables.create(nestedSchema, spec, location.toString());\n-      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n+      table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format.toString()).commit();\n \n       // disable distribution/ordering and fanout writers to preserve the original ordering\n       sourceDF\n@@ -458,7 +455,7 @@ public void testPartitionedByNestedString() throws Exception {\n \n   @TestTemplate\n   public void testReadPartitionColumn() throws Exception {\n-    assumeThat(format).as(\"Temporary skip ORC\").isNotEqualTo(\"orc\");\n+    assumeThat(format).as(\"Temporary skip ORC\").isNotEqualTo(FileFormat.ORC);\n \n     Schema nestedSchema =\n         new Schema(\n@@ -476,7 +473,7 @@ public void testReadPartitionColumn() throws Exception {\n     HadoopTables tables = new HadoopTables(spark.sessionState().newHadoopConf());\n     String baseLocation = temp.resolve(\"partition_by_nested_string\").toString();\n     Table table = tables.create(nestedSchema, spec, baseLocation);\n-    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format).commit();\n+    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format.toString()).commit();\n \n     // write into iceberg\n     MapFunction<Long, ComplexRecord> func =\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAggregates.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAggregates.java\nindex 06b68b77e680..6cbe3914dcf8 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAggregates.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkAggregates.java\n@@ -51,23 +51,17 @@ public void testAggregates() {\n           Max max = new Max(namedReference);\n           Expression expectedMax = Expressions.max(unquoted);\n           Expression actualMax = SparkAggregates.convert(max);\n-          assertThat(String.valueOf(actualMax))\n-              .as(\"Max must match\")\n-              .isEqualTo(expectedMax.toString());\n+          assertThat(actualMax).asString().isEqualTo(expectedMax.toString());\n \n           Min min = new Min(namedReference);\n           Expression expectedMin = Expressions.min(unquoted);\n           Expression actualMin = SparkAggregates.convert(min);\n-          assertThat(String.valueOf(actualMin))\n-              .as(\"Min must match\")\n-              .isEqualTo(expectedMin.toString());\n+          assertThat(actualMin).asString().isEqualTo(expectedMin.toString());\n \n           Count count = new Count(namedReference, false);\n           Expression expectedCount = Expressions.count(unquoted);\n           Expression actualCount = SparkAggregates.convert(count);\n-          assertThat(String.valueOf(actualCount))\n-              .as(\"Count must match\")\n-              .isEqualTo(expectedCount.toString());\n+          assertThat(actualCount).asString().isEqualTo(expectedCount.toString());\n \n           Count countDistinct = new Count(namedReference, true);\n           Expression convertedCountDistinct = SparkAggregates.convert(countDistinct);\n@@ -76,9 +70,7 @@ public void testAggregates() {\n           CountStar countStar = new CountStar();\n           Expression expectedCountStar = Expressions.countStar();\n           Expression actualCountStar = SparkAggregates.convert(countStar);\n-          assertThat(String.valueOf(actualCountStar))\n-              .as(\"CountStar must match\")\n-              .isEqualTo(expectedCountStar.toString());\n+          assertThat(actualCountStar).asString().isEqualTo(expectedCountStar.toString());\n         });\n   }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataFile.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataFile.java\nindex 182b1ef8f5af..b7f372164976 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataFile.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDataFile.java\n@@ -136,7 +136,7 @@ public static void stopSpark() {\n   private String tableLocation = null;\n \n   @BeforeEach\n-  public void setupTableLocation() throws Exception {\n+  public void setupTableLocation() {\n     this.tableLocation = tableDir.toURI().toString();\n   }\n \n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java\nindex c84a65cbe951..99130403a5d1 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java\n@@ -130,8 +130,7 @@ public void testStreamingWriteAppendMode() throws Exception {\n       List<SimpleRecord> actual =\n           result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n \n-      assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-      assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+      assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n       assertThat(table.snapshots()).as(\"Number of snapshots should match\").hasSize(2);\n     } finally {\n       for (StreamingQuery query : spark.streams().active()) {\n@@ -192,8 +191,7 @@ public void testStreamingWriteCompleteMode() throws Exception {\n       List<SimpleRecord> actual =\n           result.orderBy(\"data\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n \n-      assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-      assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+      assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n       assertThat(table.snapshots()).as(\"Number of snapshots should match\").hasSize(2);\n     } finally {\n       for (StreamingQuery query : spark.streams().active()) {\n@@ -254,8 +252,7 @@ public void testStreamingWriteCompleteModeWithProjection() throws Exception {\n       List<SimpleRecord> actual =\n           result.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n \n-      assertThat(actual).as(\"Number of rows should match\").hasSameSizeAs(expected);\n-      assertThat(actual).as(\"Result rows should match\").isEqualTo(expected);\n+      assertThat(actual).hasSameSizeAs(expected).isEqualTo(expected);\n       assertThat(table.snapshots()).as(\"Number of snapshots should match\").hasSize(2);\n     } finally {\n       for (StreamingQuery query : spark.streams().active()) {\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestWriteMetricsConfig.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestWriteMetricsConfig.java\nindex 841268a6be0e..ce298143f158 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestWriteMetricsConfig.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestWriteMetricsConfig.java\n@@ -24,7 +24,6 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n-import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.nio.file.Path;\n import java.util.List;\n@@ -93,7 +92,7 @@ public static void stopSpark() {\n   }\n \n   @Test\n-  public void testFullMetricsCollectionForParquet() throws IOException {\n+  public void testFullMetricsCollectionForParquet() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -116,7 +115,6 @@ public void testFullMetricsCollectionForParquet() throws IOException {\n \n     for (FileScanTask task : table.newScan().includeColumnStats().planFiles()) {\n       DataFile file = task.file();\n-\n       assertThat(file.nullValueCounts()).hasSize(2);\n       assertThat(file.valueCounts()).hasSize(2);\n       assertThat(file.lowerBounds()).hasSize(2);\n@@ -125,7 +123,7 @@ public void testFullMetricsCollectionForParquet() throws IOException {\n   }\n \n   @Test\n-  public void testCountMetricsCollectionForParquet() throws IOException {\n+  public void testCountMetricsCollectionForParquet() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -156,7 +154,7 @@ public void testCountMetricsCollectionForParquet() throws IOException {\n   }\n \n   @Test\n-  public void testNoMetricsCollectionForParquet() throws IOException {\n+  public void testNoMetricsCollectionForParquet() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -187,7 +185,7 @@ public void testNoMetricsCollectionForParquet() throws IOException {\n   }\n \n   @Test\n-  public void testCustomMetricCollectionForParquet() throws IOException {\n+  public void testCustomMetricCollectionForParquet() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -221,7 +219,7 @@ public void testCustomMetricCollectionForParquet() throws IOException {\n   }\n \n   @Test\n-  public void testBadCustomMetricCollectionForParquet() throws IOException {\n+  public void testBadCustomMetricCollectionForParquet() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -237,7 +235,7 @@ public void testBadCustomMetricCollectionForParquet() throws IOException {\n   }\n \n   @Test\n-  public void testCustomMetricCollectionForNestedParquet() throws IOException {\n+  public void testCustomMetricCollectionForNestedParquet() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -271,16 +269,12 @@ public void testCustomMetricCollectionForNestedParquet() throws IOException {\n       Map<Integer, Long> nullValueCounts = file.nullValueCounts();\n       assertThat(nullValueCounts)\n           .hasSize(3)\n-          .containsKey(longCol.fieldId())\n-          .containsKey(recordId.fieldId())\n-          .containsKey(recordData.fieldId());\n+          .containsKeys(longCol.fieldId(), recordId.fieldId(), recordData.fieldId());\n \n       Map<Integer, Long> valueCounts = file.valueCounts();\n       assertThat(valueCounts)\n           .hasSize(3)\n-          .containsKey(longCol.fieldId())\n-          .containsKey(recordId.fieldId())\n-          .containsKey(recordData.fieldId());\n+          .containsKeys(longCol.fieldId(), recordId.fieldId(), recordData.fieldId());\n \n       Map<Integer, ByteBuffer> lowerBounds = file.lowerBounds();\n       assertThat(lowerBounds).hasSize(2).containsKey(recordId.fieldId());\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13038",
    "pr_id": 13038,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 17,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/GenericsHelpers.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestOrcWrite.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroReader.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroWriter.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkAvroEnums.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReadMetadataColumns.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetWriter.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/GenericsHelpers.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestOrcWrite.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetWriter.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/GenericsHelpers.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestOrcWrite.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroReader.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroWriter.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkAvroEnums.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReadMetadataColumns.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetWriter.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/GenericsHelpers.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestOrcWrite.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetWriter.java"
    ],
    "base_commit": "6c29507008f3cdda206709dc2be16cb40c1c6173",
    "head_commit": "cc294fbf079e5d66ec8d8eae743b6c0b0052b40d",
    "repo_url": "https://github.com/apache/iceberg/pull/13038",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13038",
    "dockerfile": "",
    "pr_merged_at": "2025-05-13T12:42:14.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/GenericsHelpers.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/GenericsHelpers.java\nindex 1bad42f450fb..e7f6389cb55c 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/GenericsHelpers.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/GenericsHelpers.java\n@@ -48,7 +48,6 @@\n import org.apache.spark.sql.catalyst.util.MapData;\n import org.apache.spark.sql.types.Decimal;\n import org.apache.spark.unsafe.types.UTF8String;\n-import org.junit.Assert;\n import scala.collection.Seq;\n \n public class GenericsHelpers {\n@@ -84,8 +83,9 @@ private static void assertEqualsSafe(\n   private static void assertEqualsSafe(Types.MapType map, Map<?, ?> expected, Map<?, ?> actual) {\n     Type keyType = map.keyType();\n     Type valueType = map.valueType();\n-    Assert.assertEquals(\n-        \"Should have the same number of keys\", expected.keySet().size(), actual.keySet().size());\n+    assertThat(actual.keySet())\n+        .as(\"Should have the same number of keys\")\n+        .hasSameSizeAs(expected.keySet());\n \n     for (Object expectedKey : expected.keySet()) {\n       Object matchingKey = null;\n@@ -99,7 +99,7 @@ private static void assertEqualsSafe(Types.MapType map, Map<?, ?> expected, Map<\n         }\n       }\n \n-      Assert.assertNotNull(\"Should have a matching key\", matchingKey);\n+      assertThat(matchingKey).as(\"Should have a matching key\").isNotNull();\n       assertEqualsSafe(valueType, expected.get(expectedKey), actual.get(matchingKey));\n     }\n   }\n@@ -116,13 +116,11 @@ private static void assertEqualsSafe(Type type, Object expected, Object actual)\n       case LONG:\n       case FLOAT:\n       case DOUBLE:\n-        Assert.assertEquals(\"Primitive value should be equal to expected\", expected, actual);\n+        assertThat(actual).as(\"Primitive value should be equal to expected\").isEqualTo(expected);\n         break;\n       case DATE:\n         assertThat(expected).as(\"Should expect a LocalDate\").isInstanceOf(LocalDate.class);\n-        assertThat(actual).as(\"Should be a Date\").isInstanceOf(Date.class);\n-        Assert.assertEquals(\n-            \"ISO-8601 date should be equal\", expected.toString(), actual.toString());\n+        assertThat(actual).isInstanceOf(Date.class).asString().isEqualTo(expected);\n         break;\n       case TIMESTAMP:\n         Types.TimestampType timestampType = (Types.TimestampType) type;\n@@ -137,7 +135,7 @@ private static void assertEqualsSafe(Type type, Object expected, Object actual)\n           assertThat(expected)\n               .as(\"Should expect an OffsetDateTime\")\n               .isInstanceOf(OffsetDateTime.class);\n-          Assert.assertEquals(\"Timestamp should be equal\", expected, actualTs);\n+          assertThat(actualTs).as(\"Timestamp should be equal\").isEqualTo(expected);\n         } else {\n           // Timestamp\n           assertThat(actual).as(\"Should be a LocalDateTime\").isInstanceOf(LocalDateTime.class);\n@@ -146,33 +144,33 @@ private static void assertEqualsSafe(Type type, Object expected, Object actual)\n           assertThat(expected)\n               .as(\"Should expect an LocalDateTime\")\n               .isInstanceOf(LocalDateTime.class);\n-          Assert.assertEquals(\"Timestamp should be equal\", expected, ts);\n+          assertThat(actual).as(\"Timestamp should be equal\").isEqualTo(expected);\n         }\n         break;\n       case STRING:\n-        assertThat(actual).as(\"Should be a String\").isInstanceOf(String.class);\n-        Assert.assertEquals(\"Strings should be equal\", String.valueOf(expected), actual);\n+        assertThat(actual)\n+            .isInstanceOf(String.class)\n+            .asString()\n+            .isEqualTo(String.valueOf(expected));\n         break;\n       case UUID:\n         assertThat(expected).as(\"Should expect a UUID\").isInstanceOf(UUID.class);\n-        assertThat(actual).as(\"Should be a String\").isInstanceOf(String.class);\n-        Assert.assertEquals(\"UUID string representation should match\", expected.toString(), actual);\n+        assertThat(actual)\n+            .isInstanceOf(String.class)\n+            .asString()\n+            .isEqualTo(String.valueOf(expected));\n         break;\n       case FIXED:\n         assertThat(expected).as(\"Should expect a byte[]\").isInstanceOf(byte[].class);\n-        assertThat(actual).as(\"Should be a byte[]\").isInstanceOf(byte[].class);\n-        Assert.assertArrayEquals(\"Bytes should match\", (byte[]) expected, (byte[]) actual);\n+        assertThat(actual).isInstanceOf(byte[].class).isEqualTo(expected);\n         break;\n       case BINARY:\n         assertThat(expected).as(\"Should expect a ByteBuffer\").isInstanceOf(ByteBuffer.class);\n-        assertThat(actual).as(\"Should be a byte[]\").isInstanceOf(byte[].class);\n-        Assert.assertArrayEquals(\n-            \"Bytes should match\", ((ByteBuffer) expected).array(), (byte[]) actual);\n+        assertThat(actual).isInstanceOf(byte[].class).isEqualTo(((ByteBuffer) expected).array());\n         break;\n       case DECIMAL:\n         assertThat(expected).as(\"Should expect a BigDecimal\").isInstanceOf(BigDecimal.class);\n-        assertThat(actual).as(\"Should be a BigDecimal\").isInstanceOf(BigDecimal.class);\n-        Assert.assertEquals(\"BigDecimals should be equal\", expected, actual);\n+        assertThat(actual).isInstanceOf(BigDecimal.class).isEqualTo(expected);\n         break;\n       case STRUCT:\n         assertThat(expected).as(\"Should expect a Record\").isInstanceOf(Record.class);\n@@ -252,12 +250,14 @@ private static void assertEqualsUnsafe(Type type, Object expected, Object actual\n       case LONG:\n       case FLOAT:\n       case DOUBLE:\n-        Assert.assertEquals(\"Primitive value should be equal to expected\", expected, actual);\n+        assertThat(actual).as(\"Primitive value should be equal to expected\").isEqualTo(expected);\n         break;\n       case DATE:\n         assertThat(expected).as(\"Should expect a LocalDate\").isInstanceOf(LocalDate.class);\n         int expectedDays = (int) ChronoUnit.DAYS.between(EPOCH_DAY, (LocalDate) expected);\n-        Assert.assertEquals(\"Primitive value should be equal to expected\", expectedDays, actual);\n+        assertThat(actual)\n+            .as(\"Primitive value should be equal to expected\")\n+            .isEqualTo(expectedDays);\n         break;\n       case TIMESTAMP:\n         Types.TimestampType timestampType = (Types.TimestampType) type;\n@@ -266,44 +266,47 @@ private static void assertEqualsUnsafe(Type type, Object expected, Object actual\n               .as(\"Should expect an OffsetDateTime\")\n               .isInstanceOf(OffsetDateTime.class);\n           long expectedMicros = ChronoUnit.MICROS.between(EPOCH, (OffsetDateTime) expected);\n-          Assert.assertEquals(\n-              \"Primitive value should be equal to expected\", expectedMicros, actual);\n+          assertThat(actual)\n+              .as(\"Primitive value should be equal to expected\")\n+              .isEqualTo(expectedMicros);\n         } else {\n           assertThat(expected)\n               .as(\"Should expect an LocalDateTime\")\n               .isInstanceOf(LocalDateTime.class);\n           long expectedMicros =\n               ChronoUnit.MICROS.between(EPOCH, ((LocalDateTime) expected).atZone(ZoneId.of(\"UTC\")));\n-          Assert.assertEquals(\n-              \"Primitive value should be equal to expected\", expectedMicros, actual);\n+          assertThat(actual)\n+              .as(\"Primitive value should be equal to expected\")\n+              .isEqualTo(expectedMicros);\n         }\n         break;\n       case STRING:\n-        assertThat(actual).as(\"Should be a UTF8String\").isInstanceOf(UTF8String.class);\n-        Assert.assertEquals(\"Strings should be equal\", expected, actual.toString());\n+        assertThat(actual)\n+            .isInstanceOf(UTF8String.class)\n+            .asString()\n+            .isEqualTo(String.valueOf(expected));\n         break;\n       case UUID:\n         assertThat(expected).as(\"Should expect a UUID\").isInstanceOf(UUID.class);\n-        assertThat(actual).as(\"Should be a UTF8String\").isInstanceOf(UTF8String.class);\n-        Assert.assertEquals(\n-            \"UUID string representation should match\", expected.toString(), actual.toString());\n+        assertThat(actual)\n+            .isInstanceOf(UTF8String.class)\n+            .asString()\n+            .isEqualTo(String.valueOf(expected));\n         break;\n       case FIXED:\n         assertThat(expected).as(\"Should expect a byte[]\").isInstanceOf(byte[].class);\n-        assertThat(actual).as(\"Should be a byte[]\").isInstanceOf(byte[].class);\n-        Assert.assertArrayEquals(\"Bytes should match\", (byte[]) expected, (byte[]) actual);\n+        assertThat(actual).isInstanceOf(byte[].class).isEqualTo(expected);\n         break;\n       case BINARY:\n         assertThat(expected).as(\"Should expect a ByteBuffer\").isInstanceOf(ByteBuffer.class);\n-        assertThat(actual).as(\"Should be a byte[]\").isInstanceOf(byte[].class);\n-        Assert.assertArrayEquals(\n-            \"Bytes should match\", ((ByteBuffer) expected).array(), (byte[]) actual);\n+        assertThat(actual).isInstanceOf(byte[].class).isEqualTo(((ByteBuffer) expected).array());\n         break;\n       case DECIMAL:\n         assertThat(expected).as(\"Should expect a BigDecimal\").isInstanceOf(BigDecimal.class);\n         assertThat(actual).as(\"Should be a Decimal\").isInstanceOf(Decimal.class);\n-        Assert.assertEquals(\n-            \"BigDecimals should be equal\", expected, ((Decimal) actual).toJavaBigDecimal());\n+        assertThat(((Decimal) actual).toJavaBigDecimal())\n+            .as(\"BigDecimals should be equal\")\n+            .isEqualTo(expected);\n         break;\n       case STRUCT:\n         assertThat(expected).as(\"Should expect a Record\").isInstanceOf(Record.class);\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java\nindex d8c0de32bf00..120d6eeb1730 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java\n@@ -83,7 +83,6 @@\n import org.apache.spark.sql.types.TimestampType$;\n import org.apache.spark.sql.vectorized.ColumnarBatch;\n import org.apache.spark.unsafe.types.UTF8String;\n-import org.junit.Assert;\n import scala.collection.Seq;\n \n public class TestHelpers {\n@@ -169,7 +168,7 @@ private static void assertEqualsSafe(Types.MapType map, Map<?, ?> expected, Map<\n         }\n       }\n \n-      Assert.assertNotNull(\"Should have a matching key\", matchingKey);\n+      assertThat(matchingKey).as(\"Should have a matching key\").isNotNull();\n       assertEqualsSafe(valueType, expected.get(expectedKey), actual.get(matchingKey));\n     }\n   }\n@@ -189,14 +188,16 @@ private static void assertEqualsSafe(Type type, Object expected, Object actual)\n       case LONG:\n       case FLOAT:\n       case DOUBLE:\n-        Assert.assertEquals(\"Primitive value should be equal to expected\", expected, actual);\n+        assertThat(actual).as(\"Primitive value should be equal to expected\").isEqualTo(expected);\n         break;\n       case DATE:\n         assertThat(expected).as(\"Should be an int\").isInstanceOf(Integer.class);\n         assertThat(actual).as(\"Should be a Date\").isInstanceOf(Date.class);\n-        int daysFromEpoch = (Integer) expected;\n-        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, daysFromEpoch);\n-        Assert.assertEquals(\"ISO-8601 date should be equal\", date.toString(), actual.toString());\n+        LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (Integer) expected);\n+        assertThat(actual)\n+            .as(\"ISO-8601 date should be equal\")\n+            .asString()\n+            .isEqualTo(String.valueOf(date));\n         break;\n       case TIMESTAMP:\n         Types.TimestampType timestampType = (Types.TimestampType) type;\n@@ -208,7 +209,7 @@ private static void assertEqualsSafe(Type type, Object expected, Object actual)\n           Timestamp ts = (Timestamp) actual;\n           // milliseconds from nanos has already been added by getTime\n           long tsMicros = (ts.getTime() * 1000) + ((ts.getNanos() / 1000) % 1000);\n-          Assert.assertEquals(\"Timestamp micros should be equal\", expected, tsMicros);\n+          assertThat(tsMicros).as(\"Timestamp micros should be equal\").isEqualTo(expected);\n         } else {\n           assertThat(actual).as(\"Should be a LocalDateTime\").isInstanceOf(LocalDateTime.class);\n \n@@ -216,17 +217,18 @@ private static void assertEqualsSafe(Type type, Object expected, Object actual)\n           Instant instant = ts.toInstant(ZoneOffset.UTC);\n           // milliseconds from nanos has already been added by getTime\n           long tsMicros = (instant.toEpochMilli() * 1000) + ((ts.getNano() / 1000) % 1000);\n-          Assert.assertEquals(\"Timestamp micros should be equal\", expected, tsMicros);\n+          assertThat(tsMicros).as(\"Timestamp micros should be equal\").isEqualTo(expected);\n         }\n         break;\n       case STRING:\n-        assertThat(actual).as(\"Should be a String\").isInstanceOf(String.class);\n-        Assert.assertEquals(\"Strings should be equal\", String.valueOf(expected), actual);\n+        assertThat(actual).isInstanceOf(String.class).isEqualTo(String.valueOf(expected));\n         break;\n       case UUID:\n         assertThat(expected).as(\"Should expect a UUID\").isInstanceOf(UUID.class);\n-        assertThat(actual).as(\"Should be a String\").isInstanceOf(String.class);\n-        Assert.assertEquals(\"UUID string representation should match\", expected.toString(), actual);\n+        assertThat(actual)\n+            .isInstanceOf(String.class)\n+            .asString()\n+            .isEqualTo(String.valueOf(expected));\n         break;\n       case FIXED:\n         // generated data is written using Avro or Parquet/Avro so generated rows use\n@@ -242,19 +244,15 @@ private static void assertEqualsSafe(Type type, Object expected, Object actual)\n               \"Invalid expected value, not byte[] or Fixed: \" + expected);\n         }\n \n-        assertThat(actual).as(\"Should be a byte[]\").isInstanceOf(byte[].class);\n-        assertThat(actual).as(\"Bytes should match\").isEqualTo(expectedBytes);\n+        assertThat(actual).isInstanceOf(byte[].class).isEqualTo(expectedBytes);\n         break;\n       case BINARY:\n         assertThat(expected).as(\"Should expect a ByteBuffer\").isInstanceOf(ByteBuffer.class);\n-        assertThat(actual).as(\"Should be a byte[]\").isInstanceOf(byte[].class);\n-        Assert.assertArrayEquals(\n-            \"Bytes should match\", ((ByteBuffer) expected).array(), (byte[]) actual);\n+        assertThat(actual).isInstanceOf(byte[].class).isEqualTo(((ByteBuffer) expected).array());\n         break;\n       case DECIMAL:\n         assertThat(expected).as(\"Should expect a BigDecimal\").isInstanceOf(BigDecimal.class);\n-        assertThat(actual).as(\"Should be a BigDecimal\").isInstanceOf(BigDecimal.class);\n-        Assert.assertEquals(\"BigDecimals should be equal\", expected, actual);\n+        assertThat(actual).isInstanceOf(BigDecimal.class).isEqualTo(expected);\n         break;\n       case STRUCT:\n         assertThat(expected).as(\"Should expect a Record\").isInstanceOf(Record.class);\n@@ -340,20 +338,19 @@ private static void assertEqualsUnsafe(Type type, Object expected, Object actual\n       case LONG:\n         assertThat(actual).as(\"Should be a long\").isInstanceOf(Long.class);\n         if (expected instanceof Integer) {\n-          Assert.assertEquals(\"Values didn't match\", ((Number) expected).longValue(), actual);\n+          assertThat(actual).as(\"Values didn't match\").isEqualTo(((Number) expected).longValue());\n         } else {\n-          Assert.assertEquals(\"Primitive value should be equal to expected\", expected, actual);\n+          assertThat(actual).as(\"Primitive value should be equal to expected\").isEqualTo(expected);\n         }\n         break;\n       case DOUBLE:\n         assertThat(actual).as(\"Should be a double\").isInstanceOf(Double.class);\n         if (expected instanceof Float) {\n-          Assert.assertEquals(\n-              \"Values didn't match\",\n-              Double.doubleToLongBits(((Number) expected).doubleValue()),\n-              Double.doubleToLongBits((double) actual));\n+          assertThat(Double.doubleToLongBits((double) actual))\n+              .as(\"Values didn't match\")\n+              .isEqualTo(Double.doubleToLongBits(((Number) expected).doubleValue()));\n         } else {\n-          Assert.assertEquals(\"Primitive value should be equal to expected\", expected, actual);\n+          assertThat(actual).as(\"Primitive value should be equal to expected\").isEqualTo(expected);\n         }\n         break;\n       case INTEGER:\n@@ -361,17 +358,17 @@ private static void assertEqualsUnsafe(Type type, Object expected, Object actual\n       case BOOLEAN:\n       case DATE:\n       case TIMESTAMP:\n-        Assert.assertEquals(\"Primitive value should be equal to expected\", expected, actual);\n+        assertThat(actual).as(\"Primitive value should be equal to expected\").isEqualTo(expected);\n         break;\n       case STRING:\n-        assertThat(actual).as(\"Should be a UTF8String\").isInstanceOf(UTF8String.class);\n-        Assert.assertEquals(\"Strings should be equal\", expected, actual.toString());\n+        assertThat(actual).isInstanceOf(UTF8String.class).asString().isEqualTo(expected);\n         break;\n       case UUID:\n         assertThat(expected).as(\"Should expect a UUID\").isInstanceOf(UUID.class);\n-        assertThat(actual).as(\"Should be a UTF8String\").isInstanceOf(UTF8String.class);\n-        Assert.assertEquals(\n-            \"UUID string representation should match\", expected.toString(), actual.toString());\n+        assertThat(actual)\n+            .isInstanceOf(UTF8String.class)\n+            .asString()\n+            .isEqualTo(String.valueOf(expected));\n         break;\n       case FIXED:\n         // generated data is written using Avro or Parquet/Avro so generated rows use\n@@ -392,15 +389,14 @@ private static void assertEqualsUnsafe(Type type, Object expected, Object actual\n         break;\n       case BINARY:\n         assertThat(expected).as(\"Should expect a ByteBuffer\").isInstanceOf(ByteBuffer.class);\n-        assertThat(actual).as(\"Should be a byte[]\").isInstanceOf(byte[].class);\n-        Assert.assertArrayEquals(\n-            \"Bytes should match\", ((ByteBuffer) expected).array(), (byte[]) actual);\n+        assertThat(actual).isInstanceOf(byte[].class).isEqualTo(((ByteBuffer) expected).array());\n         break;\n       case DECIMAL:\n         assertThat(expected).as(\"Should expect a BigDecimal\").isInstanceOf(BigDecimal.class);\n         assertThat(actual).as(\"Should be a Decimal\").isInstanceOf(Decimal.class);\n-        Assert.assertEquals(\n-            \"BigDecimals should be equal\", expected, ((Decimal) actual).toJavaBigDecimal());\n+        assertThat(((Decimal) actual).toJavaBigDecimal())\n+            .as(\"BigDecimals should be equal\")\n+            .isEqualTo(expected);\n         break;\n       case STRUCT:\n         assertThat(expected).as(\"Should expect a Record\").isInstanceOf(Record.class);\n@@ -436,7 +432,7 @@ private static void assertEqualsUnsafe(Type type, Object expected, Object actual\n   public static void assertEquals(\n       String prefix, Types.StructType type, InternalRow expected, Row actual) {\n     if (expected == null || actual == null) {\n-      Assert.assertEquals(prefix, expected, actual);\n+      assertThat(actual).as(prefix).isEqualTo(expected);\n     } else {\n       List<Types.NestedField> fields = type.fields();\n       for (int c = 0; c < fields.size(); ++c) {\n@@ -452,10 +448,9 @@ public static void assertEquals(\n           case DECIMAL:\n           case DATE:\n           case TIMESTAMP:\n-            Assert.assertEquals(\n-                prefix + \".\" + fieldName + \" - \" + childType,\n-                getValue(expected, c, childType),\n-                getPrimitiveValue(actual, c, childType));\n+            assertThat(getPrimitiveValue(actual, c, childType))\n+                .as(prefix + \".\" + fieldName + \" - \" + childType)\n+                .isEqualTo(getValue(expected, c, childType));\n             break;\n           case UUID:\n           case FIXED:\n@@ -499,9 +494,9 @@ public static void assertEquals(\n   private static void assertEqualsLists(\n       String prefix, Types.ListType type, ArrayData expected, List actual) {\n     if (expected == null || actual == null) {\n-      Assert.assertEquals(prefix, expected, actual);\n+      assertThat(actual).as(prefix).isEqualTo(expected);\n     } else {\n-      Assert.assertEquals(prefix + \" length\", expected.numElements(), actual.size());\n+      assertThat(actual).as(prefix + \"length\").hasSize(expected.numElements());\n       Type childType = type.elementType();\n       for (int e = 0; e < expected.numElements(); ++e) {\n         switch (childType.typeId()) {\n@@ -514,10 +509,10 @@ private static void assertEqualsLists(\n           case DECIMAL:\n           case DATE:\n           case TIMESTAMP:\n-            Assert.assertEquals(\n-                prefix + \".elem \" + e + \" - \" + childType,\n-                getValue(expected, e, childType),\n-                actual.get(e));\n+            assertThat(actual)\n+                .as(prefix + \".elem \" + e + \" - \" + childType)\n+                .element(e)\n+                .isEqualTo(getValue(expected, e, childType));\n             break;\n           case UUID:\n           case FIXED:\n@@ -561,21 +556,20 @@ private static void assertEqualsLists(\n   private static void assertEqualsMaps(\n       String prefix, Types.MapType type, MapData expected, Map<?, ?> actual) {\n     if (expected == null || actual == null) {\n-      Assert.assertEquals(prefix, expected, actual);\n+      assertThat(actual).as(prefix).isEqualTo(expected);\n     } else {\n       Type keyType = type.keyType();\n       Type valueType = type.valueType();\n       ArrayData expectedKeyArray = expected.keyArray();\n       ArrayData expectedValueArray = expected.valueArray();\n-      Assert.assertEquals(prefix + \" length\", expected.numElements(), actual.size());\n+      assertThat(actual).as(prefix + \" length\").hasSize(expectedKeyArray.numElements());\n       for (int e = 0; e < expected.numElements(); ++e) {\n         Object expectedKey = getValue(expectedKeyArray, e, keyType);\n         Object actualValue = actual.get(expectedKey);\n         if (actualValue == null) {\n-          Assert.assertEquals(\n-              prefix + \".key=\" + expectedKey + \" has null\",\n-              true,\n-              expected.valueArray().isNullAt(e));\n+          assertThat(expected.valueArray().isNullAt(e))\n+              .as(prefix + \".key=\" + expectedKey + \" has null\")\n+              .isTrue();\n         } else {\n           switch (valueType.typeId()) {\n             case BOOLEAN:\n@@ -587,10 +581,9 @@ private static void assertEqualsMaps(\n             case DECIMAL:\n             case DATE:\n             case TIMESTAMP:\n-              Assert.assertEquals(\n-                  prefix + \".key=\" + expectedKey + \" - \" + valueType,\n-                  getValue(expectedValueArray, e, valueType),\n-                  actual.get(expectedKey));\n+              assertThat(actual.get(expectedKey))\n+                  .as(prefix + \".key=\" + expectedKey + \" - \" + valueType)\n+                  .isEqualTo(getValue(expectedValueArray, e, valueType));\n               break;\n             case UUID:\n             case FIXED:\n@@ -720,11 +713,7 @@ private static List toList(Seq<?> val) {\n   }\n \n   private static void assertEqualBytes(String context, byte[] expected, byte[] actual) {\n-    if (expected == null || actual == null) {\n-      Assert.assertEquals(context, expected, actual);\n-    } else {\n-      Assert.assertArrayEquals(context, expected, actual);\n-    }\n+    assertThat(actual).as(context).isEqualTo(expected);\n   }\n \n   static void assertEquals(Schema schema, Object expected, Object actual) {\n@@ -764,13 +753,15 @@ private static void assertEquals(String context, DataType type, Object expected,\n     } else if (type instanceof BinaryType) {\n       assertEqualBytes(context, (byte[]) expected, (byte[]) actual);\n     } else {\n-      Assert.assertEquals(\"Value should match expected: \" + context, expected, actual);\n+      assertThat(actual).as(\"Value should match expected: \" + context).isEqualTo(expected);\n     }\n   }\n \n   private static void assertEquals(\n       String context, StructType struct, InternalRow expected, InternalRow actual) {\n-    Assert.assertEquals(\"Should have correct number of fields\", struct.size(), actual.numFields());\n+    assertThat(actual.numFields())\n+        .as(\"Should have correct number of fields\")\n+        .isEqualTo(struct.size());\n     for (int i = 0; i < actual.numFields(); i += 1) {\n       StructField field = struct.fields()[i];\n       DataType type = field.dataType();\n@@ -790,8 +781,9 @@ private static void assertEquals(\n \n   private static void assertEquals(\n       String context, ArrayType array, ArrayData expected, ArrayData actual) {\n-    Assert.assertEquals(\n-        \"Should have the same number of elements\", expected.numElements(), actual.numElements());\n+    assertThat(actual.numElements())\n+        .as(\"Should have the same number of elements\")\n+        .isEqualTo(expected.numElements());\n     DataType type = array.elementType();\n     for (int i = 0; i < actual.numElements(); i += 1) {\n       assertEquals(\n@@ -803,8 +795,9 @@ private static void assertEquals(\n   }\n \n   private static void assertEquals(String context, MapType map, MapData expected, MapData actual) {\n-    Assert.assertEquals(\n-        \"Should have the same number of elements\", expected.numElements(), actual.numElements());\n+    assertThat(actual.numElements())\n+        .as(\"Should have the same number of elements\")\n+        .isEqualTo(expected.numElements());\n \n     DataType keyType = map.keyType();\n     ArrayData expectedKeys = expected.keyArray();\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestOrcWrite.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestOrcWrite.java\nindex 1e51a088390e..e149e57e8144 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestOrcWrite.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestOrcWrite.java\n@@ -19,22 +19,22 @@\n package org.apache.iceberg.spark.data;\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Path;\n import org.apache.iceberg.Files;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.io.FileAppender;\n import org.apache.iceberg.orc.ORC;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.catalyst.InternalRow;\n-import org.junit.Assert;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n public class TestOrcWrite {\n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n   private static final Schema SCHEMA =\n       new Schema(\n@@ -42,8 +42,8 @@ public class TestOrcWrite {\n \n   @Test\n   public void splitOffsets() throws IOException {\n-    File testFile = temp.newFile();\n-    Assert.assertTrue(\"Delete should succeed\", testFile.delete());\n+    File testFile = File.createTempFile(\"junit\", null, temp.toFile());\n+    assertThat(testFile.delete()).as(\"Delete should succeed\").isTrue();\n \n     Iterable<InternalRow> rows = RandomData.generateSpark(SCHEMA, 1, 0L);\n     FileAppender<InternalRow> writer =\n@@ -54,6 +54,6 @@ public void splitOffsets() throws IOException {\n \n     writer.addAll(rows);\n     writer.close();\n-    Assert.assertNotNull(\"Split offsets not present\", writer.splitOffsets());\n+    assertThat(writer.splitOffsets()).as(\"Split offsets not present\").isNotEmpty();\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroReader.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroReader.java\nindex 657eebe00af7..3f9b4bb587ba 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroReader.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroReader.java\n@@ -20,9 +20,11 @@\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Path;\n import java.util.Iterator;\n import org.apache.avro.generic.GenericData.Record;\n import org.apache.iceberg.Files;\n@@ -34,14 +36,12 @@\n import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.types.Types;\n import org.apache.parquet.schema.MessageType;\n-import org.junit.Assert;\n-import org.junit.Ignore;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.Disabled;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n public class TestParquetAvroReader {\n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n   private static final Schema COMPLEX_SCHEMA =\n       new Schema(\n@@ -87,7 +87,7 @@ public class TestParquetAvroReader {\n           optional(2, \"slide\", Types.StringType.get()),\n           required(25, \"foo\", Types.DecimalType.of(7, 5)));\n \n-  @Ignore\n+  @Disabled\n   public void testStructSchema() throws IOException {\n     Schema structSchema =\n         new Schema(\n@@ -145,7 +145,7 @@ public void testStructSchema() throws IOException {\n     double stddev = Math.sqrt((((double) sumSq) / trials) - (mean * mean));\n   }\n \n-  @Ignore\n+  @Disabled\n   public void testWithOldReadPath() throws IOException {\n     File testFile = writeTestData(COMPLEX_SCHEMA, 500_000, 1985);\n     // RandomData uses the root record name \"test\", which must match for records to be equal\n@@ -194,8 +194,8 @@ public void testWithOldReadPath() throws IOException {\n   public void testCorrectness() throws IOException {\n     Iterable<Record> records = RandomData.generate(COMPLEX_SCHEMA, 50_000, 34139);\n \n-    File testFile = temp.newFile();\n-    Assert.assertTrue(\"Delete should succeed\", testFile.delete());\n+    File testFile = File.createTempFile(\"junit\", null, temp.toFile());\n+    assertThat(testFile.delete()).as(\"Delete should succeed\").isTrue();\n \n     try (FileAppender<Record> writer =\n         Parquet.write(Files.localOutput(testFile)).schema(COMPLEX_SCHEMA).build()) {\n@@ -217,15 +217,15 @@ public void testCorrectness() throws IOException {\n       Iterator<Record> iter = records.iterator();\n       for (Record actual : reader) {\n         Record expected = iter.next();\n-        Assert.assertEquals(\"Record \" + recordNum + \" should match expected\", expected, actual);\n+        assertThat(actual).as(\"Record \" + recordNum + \" should match expected\").isEqualTo(expected);\n         recordNum += 1;\n       }\n     }\n   }\n \n   private File writeTestData(Schema schema, int numRecords, int seed) throws IOException {\n-    File testFile = temp.newFile();\n-    Assert.assertTrue(\"Delete should succeed\", testFile.delete());\n+    File testFile = File.createTempFile(\"junit\", null, temp.toFile());\n+    assertThat(testFile.delete()).as(\"Delete should succeed\").isTrue();\n \n     try (FileAppender<Record> writer =\n         Parquet.write(Files.localOutput(testFile)).schema(schema).build()) {\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroWriter.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroWriter.java\nindex 15c6268da478..83f8f7f168b1 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroWriter.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestParquetAvroWriter.java\n@@ -20,9 +20,11 @@\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Path;\n import java.util.Iterator;\n import org.apache.avro.generic.GenericData.Record;\n import org.apache.iceberg.Files;\n@@ -35,13 +37,11 @@\n import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.types.Types;\n import org.apache.parquet.schema.MessageType;\n-import org.junit.Assert;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n public class TestParquetAvroWriter {\n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n   private static final Schema COMPLEX_SCHEMA =\n       new Schema(\n@@ -90,8 +90,8 @@ public class TestParquetAvroWriter {\n   public void testCorrectness() throws IOException {\n     Iterable<Record> records = RandomData.generate(COMPLEX_SCHEMA, 50_000, 34139);\n \n-    File testFile = temp.newFile();\n-    Assert.assertTrue(\"Delete should succeed\", testFile.delete());\n+    File testFile = File.createTempFile(\"junit\", null, temp.toFile());\n+    assertThat(testFile.delete()).as(\"Delete should succeed\").isTrue();\n \n     try (FileAppender<Record> writer =\n         Parquet.write(Files.localOutput(testFile))\n@@ -115,7 +115,7 @@ public void testCorrectness() throws IOException {\n       Iterator<Record> iter = records.iterator();\n       for (Record actual : reader) {\n         Record expected = iter.next();\n-        Assert.assertEquals(\"Record \" + recordNum + \" should match expected\", expected, actual);\n+        assertThat(actual).as(\"Record \" + recordNum + \" should match expected\").isEqualTo(expected);\n         recordNum += 1;\n       }\n     }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkAvroEnums.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkAvroEnums.java\nindex 1f4e798a4ae7..0dc8b48b2317 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkAvroEnums.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkAvroEnums.java\n@@ -18,8 +18,11 @@\n  */\n package org.apache.iceberg.spark.data;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Path;\n import java.util.List;\n import org.apache.avro.SchemaBuilder;\n import org.apache.avro.file.DataFileWriter;\n@@ -34,14 +37,12 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.spark.sql.catalyst.InternalRow;\n-import org.junit.Assert;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n public class TestSparkAvroEnums {\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n   @Test\n   public void writeAndValidateEnums() throws IOException {\n@@ -64,8 +65,8 @@ public void writeAndValidateEnums() throws IOException {\n     Record enumRecord3 = new GenericData.Record(avroSchema); // null enum\n     List<Record> expected = ImmutableList.of(enumRecord1, enumRecord2, enumRecord3);\n \n-    File testFile = temp.newFile();\n-    Assert.assertTrue(\"Delete should succeed\", testFile.delete());\n+    File testFile = File.createTempFile(\"junit\", null, temp.toFile());\n+    assertThat(testFile.delete()).as(\"Delete should succeed\").isTrue();\n \n     try (DataFileWriter<Record> writer = new DataFileWriter<>(new GenericDatumWriter<>())) {\n       writer.create(avroSchema, testFile);\n@@ -90,7 +91,7 @@ public void writeAndValidateEnums() throws IOException {\n           expected.get(i).get(\"enumCol\") == null ? null : expected.get(i).get(\"enumCol\").toString();\n       String sparkString =\n           rows.get(i).getUTF8String(0) == null ? null : rows.get(i).getUTF8String(0).toString();\n-      Assert.assertEquals(expectedEnumString, sparkString);\n+      assertThat(sparkString).isEqualTo(expectedEnumString);\n     }\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java\nindex b31ea8fd277d..dbe386e89980 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java\n@@ -18,14 +18,15 @@\n  */\n package org.apache.iceberg.spark.data;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import java.time.ZoneId;\n import java.util.TimeZone;\n import org.apache.iceberg.expressions.Literal;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.catalyst.util.DateTimeUtils;\n import org.apache.spark.sql.catalyst.util.TimestampFormatter;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n public class TestSparkDateTimes {\n   @Test\n@@ -46,8 +47,10 @@ public void testSparkDate() {\n \n   public void checkSparkDate(String dateString) {\n     Literal<Integer> date = Literal.of(dateString).to(Types.DateType.get());\n-    String sparkDate = DateTimeUtils.toJavaDate(date.value()).toString();\n-    Assert.assertEquals(\"Should be the same date (\" + date.value() + \")\", dateString, sparkDate);\n+    assertThat(DateTimeUtils.toJavaDate(date.value()))\n+        .as(\"Should be the same date (\" + date.value() + \")\")\n+        .asString()\n+        .isEqualTo(dateString);\n   }\n \n   @Test\n@@ -68,7 +71,8 @@ public void checkSparkTimestamp(String timestampString, String sparkRepr) {\n     ZoneId zoneId = DateTimeUtils.getZoneId(\"UTC\");\n     TimestampFormatter formatter = TimestampFormatter.getFractionFormatter(zoneId);\n     String sparkTimestamp = formatter.format(ts.value());\n-    Assert.assertEquals(\n-        \"Should be the same timestamp (\" + ts.value() + \")\", sparkRepr, sparkTimestamp);\n+    assertThat(sparkTimestamp)\n+        .as(\"Should be the same timestamp (\" + ts.value() + \")\")\n+        .isEqualTo(sparkRepr);\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReadMetadataColumns.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReadMetadataColumns.java\nindex 3c9037adc393..9d725250d3d2 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReadMetadataColumns.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkOrcReadMetadataColumns.java\n@@ -19,9 +19,12 @@\n package org.apache.iceberg.spark.data;\n \n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.File;\n import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.Collection;\n import java.util.Iterator;\n import java.util.List;\n import java.util.stream.Collectors;\n@@ -29,6 +32,9 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.iceberg.Files;\n import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.exceptions.RuntimeIOException;\n import org.apache.iceberg.expressions.Expression;\n@@ -50,15 +56,12 @@\n import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n import org.apache.spark.sql.vectorized.ColumnarBatch;\n import org.apache.spark.unsafe.types.UTF8String;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkOrcReadMetadataColumns {\n   private static final Schema DATA_SCHEMA =\n       new Schema(\n@@ -95,24 +98,20 @@ public class TestSparkOrcReadMetadataColumns {\n     }\n   }\n \n-  @Parameterized.Parameters(name = \"vectorized = {0}\")\n-  public static Object[] parameters() {\n-    return new Object[] {false, true};\n+  @Parameters(name = \"vectorized = {0}\")\n+  public static Collection<Boolean> parameters() {\n+    return Arrays.asList(false, true);\n   }\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private java.nio.file.Path temp;\n \n-  private boolean vectorized;\n+  @Parameter private boolean vectorized;\n   private File testFile;\n \n-  public TestSparkOrcReadMetadataColumns(boolean vectorized) {\n-    this.vectorized = vectorized;\n-  }\n-\n-  @Before\n+  @BeforeEach\n   public void writeFile() throws IOException {\n-    testFile = temp.newFile();\n-    Assert.assertTrue(\"Delete should succeed\", testFile.delete());\n+    testFile = File.createTempFile(\"junit\", null, temp.toFile());\n+    assertThat(testFile.delete()).as(\"Delete should succeed\").isTrue();\n \n     try (FileAppender<InternalRow> writer =\n         ORC.write(Files.localOutput(testFile))\n@@ -127,18 +126,18 @@ public void writeFile() throws IOException {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadRowNumbers() throws IOException {\n     readAndValidate(null, null, null, EXPECTED_ROWS);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadRowNumbersWithFilter() throws IOException {\n     readAndValidate(\n         Expressions.greaterThanOrEqual(\"id\", 500), null, null, EXPECTED_ROWS.subList(500, 1000));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadRowNumbersWithSplits() throws IOException {\n     Reader reader;\n     try {\n@@ -201,10 +200,10 @@ private void readAndValidate(\n       final Iterator<InternalRow> actualRows = reader.iterator();\n       final Iterator<InternalRow> expectedRows = expected.iterator();\n       while (expectedRows.hasNext()) {\n-        Assert.assertTrue(\"Should have expected number of rows\", actualRows.hasNext());\n+        assertThat(actualRows).as(\"Should have expected number of rows\").hasNext();\n         TestHelpers.assertEquals(PROJECTION_SCHEMA, expectedRows.next(), actualRows.next());\n       }\n-      Assert.assertFalse(\"Should not have extra rows\", actualRows.hasNext());\n+      assertThat(actualRows).as(\"Should not have extra rows\").isExhausted();\n     } finally {\n       if (reader != null) {\n         reader.close();\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java\nindex 01b633da0f9e..044ea3d93c0b 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetReadMetadataColumns.java\n@@ -19,6 +19,8 @@\n package org.apache.iceberg.spark.data;\n \n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n import static org.mockito.Mockito.mock;\n import static org.mockito.Mockito.when;\n \n@@ -31,6 +33,9 @@\n import org.apache.hadoop.fs.Path;\n import org.apache.iceberg.Files;\n import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.data.DeleteFilter;\n import org.apache.iceberg.deletes.PositionDeleteIndex;\n@@ -57,16 +62,12 @@\n import org.apache.spark.sql.types.StructType;\n import org.apache.spark.sql.vectorized.ColumnarBatch;\n import org.apache.spark.unsafe.types.UTF8String;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkParquetReadMetadataColumns {\n   private static final Schema DATA_SCHEMA =\n       new Schema(\n@@ -114,28 +115,24 @@ public class TestSparkParquetReadMetadataColumns {\n     }\n   }\n \n-  @Parameterized.Parameters(name = \"vectorized = {0}\")\n+  @Parameters(name = \"vectorized = {0}\")\n   public static Object[][] parameters() {\n     return new Object[][] {new Object[] {false}, new Object[] {true}};\n   }\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir protected java.nio.file.Path temp;\n \n-  private final boolean vectorized;\n+  @Parameter private boolean vectorized;\n   private File testFile;\n \n-  public TestSparkParquetReadMetadataColumns(boolean vectorized) {\n-    this.vectorized = vectorized;\n-  }\n-\n-  @Before\n+  @BeforeEach\n   public void writeFile() throws IOException {\n     List<Path> fileSplits = Lists.newArrayList();\n     StructType struct = SparkSchemaUtil.convert(DATA_SCHEMA);\n     Configuration conf = new Configuration();\n \n-    testFile = temp.newFile();\n-    Assert.assertTrue(\"Delete should succeed\", testFile.delete());\n+    testFile = File.createTempFile(\"junit\", null, temp.toFile());\n+    assertThat(testFile.delete()).as(\"Delete should succeed\").isTrue();\n     ParquetFileWriter parquetFileWriter =\n         new ParquetFileWriter(\n             conf,\n@@ -144,8 +141,8 @@ public void writeFile() throws IOException {\n \n     parquetFileWriter.start();\n     for (int i = 0; i < NUM_ROW_GROUPS; i += 1) {\n-      File split = temp.newFile();\n-      Assert.assertTrue(\"Delete should succeed\", split.delete());\n+      File split = File.createTempFile(\"junit\", null, temp.toFile());\n+      assertThat(split.delete()).as(\"Delete should succeed\").isTrue();\n       fileSplits.add(new Path(split.getAbsolutePath()));\n       try (FileAppender<InternalRow> writer =\n           Parquet.write(Files.localOutput(split))\n@@ -164,14 +161,14 @@ public void writeFile() throws IOException {\n             .getKeyValueMetaData());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadRowNumbers() throws IOException {\n     readAndValidate(null, null, null, EXPECTED_ROWS);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadRowNumbersWithDelete() throws IOException {\n-    Assume.assumeTrue(vectorized);\n+    assumeThat(vectorized).isTrue();\n \n     List<InternalRow> expectedRowsAfterDelete = Lists.newArrayList();\n     EXPECTED_ROWS.forEach(row -> expectedRowsAfterDelete.add(row.copy()));\n@@ -229,7 +226,7 @@ public boolean isEmpty() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadRowNumbersWithFilter() throws IOException {\n     // current iceberg supports row group filter.\n     for (int i = 1; i < 5; i += 1) {\n@@ -243,7 +240,7 @@ public void testReadRowNumbersWithFilter() throws IOException {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadRowNumbersWithSplits() throws IOException {\n     ParquetFileReader fileReader =\n         new ParquetFileReader(\n@@ -295,11 +292,11 @@ private void validate(List<InternalRow> expected, Parquet.ReadBuilder builder)\n       final Iterator<InternalRow> actualRows = reader.iterator();\n \n       for (InternalRow internalRow : expected) {\n-        Assert.assertTrue(\"Should have expected number of rows\", actualRows.hasNext());\n+        assertThat(actualRows).as(\"Should have expected number of rows\").hasNext();\n         TestHelpers.assertEquals(PROJECTION_SCHEMA, internalRow, actualRows.next());\n       }\n \n-      Assert.assertFalse(\"Should not have extra rows\", actualRows.hasNext());\n+      assertThat(actualRows).as(\"Should not have extra rows\").isExhausted();\n     }\n   }\n \n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetWriter.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetWriter.java\nindex 467d8a27a27c..5da66eea4fdf 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetWriter.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetWriter.java\n@@ -18,27 +18,39 @@\n  */\n package org.apache.iceberg.spark.data;\n \n+import static org.apache.iceberg.TableProperties.PARQUET_BLOOM_FILTER_COLUMN_ENABLED_PREFIX;\n+import static org.apache.iceberg.TableProperties.PARQUET_BLOOM_FILTER_COLUMN_FPP_PREFIX;\n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.File;\n import java.io.IOException;\n+import java.lang.reflect.Field;\n+import java.nio.file.Path;\n import java.util.Iterator;\n import org.apache.iceberg.Files;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.io.FileAppender;\n import org.apache.iceberg.parquet.Parquet;\n+import org.apache.iceberg.parquet.ParquetSchemaUtil;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.iceberg.types.Types;\n+import org.apache.parquet.column.ColumnDescriptor;\n+import org.apache.parquet.column.ParquetProperties;\n+import org.apache.parquet.schema.MessageType;\n import org.apache.spark.sql.catalyst.InternalRow;\n-import org.junit.Assert;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n public class TestSparkParquetWriter {\n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n+\n+  private static final Schema SCHEMA =\n+      new Schema(\n+          Types.NestedField.required(1, \"id\", Types.IntegerType.get()),\n+          Types.NestedField.required(2, \"id_long\", Types.LongType.get()));\n \n   private static final Schema COMPLEX_SCHEMA =\n       new Schema(\n@@ -88,8 +100,8 @@ public void testCorrectness() throws IOException {\n     int numRows = 50_000;\n     Iterable<InternalRow> records = RandomData.generateSpark(COMPLEX_SCHEMA, numRows, 19981);\n \n-    File testFile = temp.newFile();\n-    Assert.assertTrue(\"Delete should succeed\", testFile.delete());\n+    File testFile = File.createTempFile(\"junit\", null, temp.toFile());\n+    assertThat(testFile.delete()).as(\"Delete should succeed\").isTrue();\n \n     try (FileAppender<InternalRow> writer =\n         Parquet.write(Files.localOutput(testFile))\n@@ -110,10 +122,33 @@ public void testCorrectness() throws IOException {\n       Iterator<InternalRow> expected = records.iterator();\n       Iterator<InternalRow> rows = reader.iterator();\n       for (int i = 0; i < numRows; i += 1) {\n-        Assert.assertTrue(\"Should have expected number of rows\", rows.hasNext());\n+        assertThat(rows).as(\"Should have expected number of rows\").hasNext();\n         TestHelpers.assertEquals(COMPLEX_SCHEMA, expected.next(), rows.next());\n       }\n-      Assert.assertFalse(\"Should not have extra rows\", rows.hasNext());\n+      assertThat(rows).as(\"Should not have extra rows\").isExhausted();\n+    }\n+  }\n+\n+  @Test\n+  public void testFpp() throws IOException, NoSuchFieldException, IllegalAccessException {\n+    File testFile = File.createTempFile(\"junit\", null, temp.toFile());\n+    try (FileAppender<InternalRow> writer =\n+        Parquet.write(Files.localOutput(testFile))\n+            .schema(SCHEMA)\n+            .set(PARQUET_BLOOM_FILTER_COLUMN_ENABLED_PREFIX + \"id\", \"true\")\n+            .set(PARQUET_BLOOM_FILTER_COLUMN_FPP_PREFIX + \"id\", \"0.05\")\n+            .createWriterFunc(\n+                msgType ->\n+                    SparkParquetWriters.buildWriter(SparkSchemaUtil.convert(SCHEMA), msgType))\n+            .build()) {\n+      // Using reflection to access the private 'props' field in ParquetWriter\n+      Field propsField = writer.getClass().getDeclaredField(\"props\");\n+      propsField.setAccessible(true);\n+      ParquetProperties props = (ParquetProperties) propsField.get(writer);\n+      MessageType parquetSchema = ParquetSchemaUtil.convert(SCHEMA, \"test\");\n+      ColumnDescriptor descriptor = parquetSchema.getColumnDescription(new String[] {\"id\"});\n+      double fpp = props.getBloomFilterFPP(descriptor).getAsDouble();\n+      assertThat(fpp).isEqualTo(0.05);\n     }\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java\nindex 91ace0f8059f..394755d3378b 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java\n@@ -28,8 +28,8 @@\n import org.apache.iceberg.relocated.com.google.common.base.Function;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.spark.data.RandomData;\n-import org.junit.Ignore;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Disabled;\n+import org.junit.jupiter.api.Test;\n \n public class TestParquetDictionaryFallbackToPlainEncodingVectorizedReads\n     extends TestParquetVectorizedReads {\n@@ -64,11 +64,11 @@ FileAppender<GenericData.Record> getParquetWriter(Schema schema, OutputFile outp\n \n   @Test\n   @Override\n-  @Ignore // Fallback encoding not triggered when data is mostly null\n+  @Disabled // Fallback encoding not triggered when data is mostly null\n   public void testMostlyNullsForOptionalFields() {}\n \n   @Test\n   @Override\n-  @Ignore // Ignored since this code path is already tested in TestParquetVectorizedReads\n+  @Disabled // Ignored since this code path is already tested in TestParquetVectorizedReads\n   public void testVectorizedReadsWithNewContainers() throws IOException {}\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java\nindex 3858f8a2dde3..2c3eda065593 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java\n@@ -20,7 +20,9 @@\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.IOException;\n import java.util.Iterator;\n@@ -48,9 +50,7 @@\n import org.apache.parquet.schema.MessageType;\n import org.apache.parquet.schema.Type;\n import org.apache.spark.sql.vectorized.ColumnarBatch;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n public class TestParquetVectorizedReads extends AvroDataTest {\n   private static final int NUM_ROWS = 200_000;\n@@ -104,12 +104,12 @@ private void writeAndValidate(\n       Function<GenericData.Record, GenericData.Record> transform)\n       throws IOException {\n     // Write test data\n-    Assume.assumeTrue(\n-        \"Parquet Avro cannot write non-string map keys\",\n-        null\n-            == TypeUtil.find(\n+    assumeThat(\n+            TypeUtil.find(\n                 writeSchema,\n-                type -> type.isMapType() && type.asMapType().keyType() != Types.StringType.get()));\n+                type -> type.isMapType() && type.asMapType().keyType() != Types.StringType.get()))\n+        .as(\"Parquet Avro cannot write non-string map keys\")\n+        .isNull();\n \n     Iterable<GenericData.Record> expected =\n         generateData(writeSchema, numRecords, seed, nullPercentage, transform);\n@@ -181,7 +181,7 @@ void assertRecordsMatch(\n         numRowsRead += batch.numRows();\n         TestHelpers.assertEqualsBatch(schema.asStruct(), expectedIter, batch);\n       }\n-      Assert.assertEquals(expectedSize, numRowsRead);\n+      assertThat(numRowsRead).isEqualTo(expectedSize);\n     }\n   }\n \n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/GenericsHelpers.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/GenericsHelpers.java\nindex bd1eb7bc81ee..aed9f2c2af94 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/GenericsHelpers.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/GenericsHelpers.java\n@@ -133,9 +133,10 @@ private static void assertEqualsSafe(Type type, Object expected, Object actual)\n         break;\n       case DATE:\n         assertThat(expected).as(\"Should expect a LocalDate\").isInstanceOf(LocalDate.class);\n-        assertThat(actual).as(\"Should be a Date\").isInstanceOf(Date.class);\n-        assertThat(actual.toString())\n+        assertThat(actual)\n+            .isInstanceOf(Date.class)\n             .as(\"ISO-8601 date should be equal\")\n+            .asString()\n             .isEqualTo(String.valueOf(expected));\n         break;\n       case TIMESTAMP:\n@@ -165,32 +166,29 @@ private static void assertEqualsSafe(Type type, Object expected, Object actual)\n         }\n         break;\n       case STRING:\n-        assertThat(actual).as(\"Should be a String\").isInstanceOf(String.class);\n-        assertThat(actual.toString())\n-            .as(\"Strings should be equal\")\n+        assertThat(actual)\n+            .isInstanceOf(String.class)\n+            .asString()\n             .isEqualTo(String.valueOf(expected));\n         break;\n       case UUID:\n         assertThat(expected).as(\"Should expect a UUID\").isInstanceOf(UUID.class);\n-        assertThat(actual).as(\"Should be a String\").isInstanceOf(String.class);\n-        assertThat(actual.toString())\n-            .as(\"UUID string representation should match\")\n+        assertThat(actual)\n+            .isInstanceOf(String.class)\n+            .asString()\n             .isEqualTo(String.valueOf(expected));\n         break;\n       case FIXED:\n         assertThat(expected).as(\"Should expect a byte[]\").isInstanceOf(byte[].class);\n-        assertThat(actual).as(\"Should be a byte[]\").isInstanceOf(byte[].class);\n-        assertThat(actual).as(\"Bytes should match\").isEqualTo(expected);\n+        assertThat(actual).isInstanceOf(byte[].class).isEqualTo(expected);\n         break;\n       case BINARY:\n         assertThat(expected).as(\"Should expect a ByteBuffer\").isInstanceOf(ByteBuffer.class);\n-        assertThat(actual).as(\"Should be a byte[]\").isInstanceOf(byte[].class);\n-        assertThat(actual).as(\"Bytes should match\").isEqualTo(((ByteBuffer) expected).array());\n+        assertThat(actual).isInstanceOf(byte[].class).isEqualTo(((ByteBuffer) expected).array());\n         break;\n       case DECIMAL:\n         assertThat(expected).as(\"Should expect a BigDecimal\").isInstanceOf(BigDecimal.class);\n-        assertThat(actual).as(\"Should be a BigDecimal\").isInstanceOf(BigDecimal.class);\n-        assertThat(actual).as(\"BigDecimals should be equal\").isEqualTo(expected);\n+        assertThat(actual).isInstanceOf(BigDecimal.class).isEqualTo(expected);\n         break;\n       case STRUCT:\n         assertThat(expected).as(\"Should expect a Record\").isInstanceOf(Record.class);\n@@ -352,27 +350,25 @@ private static void assertEqualsUnsafe(Type type, Object expected, Object actual\n         }\n         break;\n       case STRING:\n-        assertThat(actual).as(\"Should be a UTF8String\").isInstanceOf(UTF8String.class);\n-        assertThat(actual.toString())\n-            .as(\"Strings should be equal\")\n+        assertThat(actual)\n+            .isInstanceOf(UTF8String.class)\n+            .asString()\n             .isEqualTo(String.valueOf(expected));\n         break;\n       case UUID:\n         assertThat(expected).as(\"Should expect a UUID\").isInstanceOf(UUID.class);\n-        assertThat(actual).as(\"Should be a UTF8String\").isInstanceOf(UTF8String.class);\n-        assertThat(actual.toString())\n-            .as(\"UUID string representation should match\")\n+        assertThat(actual)\n+            .isInstanceOf(UTF8String.class)\n+            .asString()\n             .isEqualTo(String.valueOf(expected));\n         break;\n       case FIXED:\n         assertThat(expected).as(\"Should expect a byte[]\").isInstanceOf(byte[].class);\n-        assertThat(actual).as(\"Should be a byte[]\").isInstanceOf(byte[].class);\n-        assertThat(actual).as(\"Bytes should match\").isEqualTo(expected);\n+        assertThat(actual).isInstanceOf(byte[].class).isEqualTo(expected);\n         break;\n       case BINARY:\n         assertThat(expected).as(\"Should expect a ByteBuffer\").isInstanceOf(ByteBuffer.class);\n-        assertThat(actual).as(\"Should be a byte[]\").isInstanceOf(byte[].class);\n-        assertThat(actual).as(\"Bytes should match\").isEqualTo(((ByteBuffer) expected).array());\n+        assertThat(actual).isInstanceOf(byte[].class).isEqualTo(((ByteBuffer) expected).array());\n         break;\n       case DECIMAL:\n         assertThat(expected).as(\"Should expect a BigDecimal\").isInstanceOf(BigDecimal.class);\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java\nindex 6111e1b0c38b..7baad225eb3c 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java\n@@ -207,8 +207,9 @@ private static void assertEqualsSafe(Type type, Object expected, Object actual)\n         assertThat(expected).as(\"Should be an int\").isInstanceOf(Integer.class);\n         assertThat(actual).as(\"Should be a Date\").isInstanceOf(Date.class);\n         LocalDate date = ChronoUnit.DAYS.addTo(EPOCH_DAY, (Integer) expected);\n-        assertThat(actual.toString())\n+        assertThat(actual)\n             .as(\"ISO-8601 date should be equal\")\n+            .asString()\n             .isEqualTo(String.valueOf(date));\n         break;\n       case TIMESTAMP:\n@@ -233,14 +234,13 @@ private static void assertEqualsSafe(Type type, Object expected, Object actual)\n         }\n         break;\n       case STRING:\n-        assertThat(actual).as(\"Should be a String\").isInstanceOf(String.class);\n-        assertThat(actual).as(\"Strings should be equal\").isEqualTo(String.valueOf(expected));\n+        assertThat(actual).isInstanceOf(String.class).isEqualTo(String.valueOf(expected));\n         break;\n       case UUID:\n         assertThat(expected).as(\"Should expect a UUID\").isInstanceOf(UUID.class);\n-        assertThat(actual).as(\"Should be a String\").isInstanceOf(String.class);\n-        assertThat(actual.toString())\n-            .as(\"UUID string representation should match\")\n+        assertThat(actual)\n+            .isInstanceOf(String.class)\n+            .asString()\n             .isEqualTo(String.valueOf(expected));\n         break;\n       case FIXED:\n@@ -257,18 +257,15 @@ private static void assertEqualsSafe(Type type, Object expected, Object actual)\n               \"Invalid expected value, not byte[] or Fixed: \" + expected);\n         }\n \n-        assertThat(actual).as(\"Should be a byte[]\").isInstanceOf(byte[].class);\n-        assertThat(actual).as(\"Bytes should match\").isEqualTo(expectedBytes);\n+        assertThat(actual).isInstanceOf(byte[].class).isEqualTo(expectedBytes);\n         break;\n       case BINARY:\n         assertThat(expected).as(\"Should expect a ByteBuffer\").isInstanceOf(ByteBuffer.class);\n-        assertThat(actual).as(\"Should be a byte[]\").isInstanceOf(byte[].class);\n-        assertThat(actual).as(\"Bytes should match\").isEqualTo(((ByteBuffer) expected).array());\n+        assertThat(actual).isInstanceOf(byte[].class).isEqualTo(((ByteBuffer) expected).array());\n         break;\n       case DECIMAL:\n         assertThat(expected).as(\"Should expect a BigDecimal\").isInstanceOf(BigDecimal.class);\n-        assertThat(actual).as(\"Should be a BigDecimal\").isInstanceOf(BigDecimal.class);\n-        assertThat(actual).as(\"BigDecimals should be equal\").isEqualTo(expected);\n+        assertThat(actual).isInstanceOf(BigDecimal.class).isEqualTo(expected);\n         break;\n       case STRUCT:\n         assertThat(expected).as(\"Should expect a Record\").isInstanceOf(Record.class);\n@@ -377,14 +374,13 @@ private static void assertEqualsUnsafe(Type type, Object expected, Object actual\n         assertThat(actual).as(\"Primitive value should be equal to expected\").isEqualTo(expected);\n         break;\n       case STRING:\n-        assertThat(actual).as(\"Should be a UTF8String\").isInstanceOf(UTF8String.class);\n-        assertThat(actual.toString()).as(\"Strings should be equal\").isEqualTo(expected);\n+        assertThat(actual).isInstanceOf(UTF8String.class).asString().isEqualTo(expected);\n         break;\n       case UUID:\n         assertThat(expected).as(\"Should expect a UUID\").isInstanceOf(UUID.class);\n-        assertThat(actual).as(\"Should be a UTF8String\").isInstanceOf(UTF8String.class);\n-        assertThat(actual.toString())\n-            .as(\"UUID string representation should match\")\n+        assertThat(actual)\n+            .isInstanceOf(UTF8String.class)\n+            .asString()\n             .isEqualTo(String.valueOf(expected));\n         break;\n       case FIXED:\n@@ -406,8 +402,7 @@ private static void assertEqualsUnsafe(Type type, Object expected, Object actual\n         break;\n       case BINARY:\n         assertThat(expected).as(\"Should expect a ByteBuffer\").isInstanceOf(ByteBuffer.class);\n-        assertThat(actual).as(\"Should be a byte[]\").isInstanceOf(byte[].class);\n-        assertThat(actual).as(\"Bytes should match\").isEqualTo(((ByteBuffer) expected).array());\n+        assertThat(actual).isInstanceOf(byte[].class).isEqualTo(((ByteBuffer) expected).array());\n         break;\n       case DECIMAL:\n         assertThat(expected).as(\"Should expect a BigDecimal\").isInstanceOf(BigDecimal.class);\n@@ -514,7 +509,7 @@ private static void assertEqualsLists(\n     if (expected == null || actual == null) {\n       assertThat(actual).as(prefix).isEqualTo(expected);\n     } else {\n-      assertThat(actual.size()).as(prefix + \"length\").isEqualTo(expected.numElements());\n+      assertThat(actual).as(prefix + \"length\").hasSize(expected.numElements());\n       Type childType = type.elementType();\n       for (int e = 0; e < expected.numElements(); ++e) {\n         switch (childType.typeId()) {\n@@ -527,8 +522,9 @@ private static void assertEqualsLists(\n           case DECIMAL:\n           case DATE:\n           case TIMESTAMP:\n-            assertThat(actual.get(e))\n+            assertThat(actual)\n                 .as(prefix + \".elem \" + e + \" - \" + childType)\n+                .element(e)\n                 .isEqualTo(getValue(expected, e, childType));\n             break;\n           case UUID:\n@@ -579,14 +575,14 @@ private static void assertEqualsMaps(\n       Type valueType = type.valueType();\n       ArrayData expectedKeyArray = expected.keyArray();\n       ArrayData expectedValueArray = expected.valueArray();\n-      assertThat(actual.size()).as(prefix + \" length\").isEqualTo(expected.numElements());\n+      assertThat(actual).as(prefix + \" length\").hasSize(expectedKeyArray.numElements());\n       for (int e = 0; e < expected.numElements(); ++e) {\n         Object expectedKey = getValue(expectedKeyArray, e, keyType);\n         Object actualValue = actual.get(expectedKey);\n         if (actualValue == null) {\n-          assertThat(true)\n+          assertThat(expected.valueArray().isNullAt(e))\n               .as(prefix + \".key=\" + expectedKey + \" has null\")\n-              .isEqualTo(expected.valueArray().isNullAt(e));\n+              .isTrue();\n         } else {\n           switch (valueType.typeId()) {\n             case BOOLEAN:\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestOrcWrite.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestOrcWrite.java\nindex cbaad6543076..e149e57e8144 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestOrcWrite.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestOrcWrite.java\n@@ -54,6 +54,6 @@ public void splitOffsets() throws IOException {\n \n     writer.addAll(rows);\n     writer.close();\n-    assertThat(writer.splitOffsets()).as(\"Split offsets not present\").isNotNull();\n+    assertThat(writer.splitOffsets()).as(\"Split offsets not present\").isNotEmpty();\n   }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java\nindex 6a06f9d5836d..dbe386e89980 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkDateTimes.java\n@@ -47,9 +47,9 @@ public void testSparkDate() {\n \n   public void checkSparkDate(String dateString) {\n     Literal<Integer> date = Literal.of(dateString).to(Types.DateType.get());\n-    String sparkDate = DateTimeUtils.toJavaDate(date.value()).toString();\n-    assertThat(sparkDate)\n+    assertThat(DateTimeUtils.toJavaDate(date.value()))\n         .as(\"Should be the same date (\" + date.value() + \")\")\n+        .asString()\n         .isEqualTo(dateString);\n   }\n \n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetWriter.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetWriter.java\nindex 73800d3cf3e0..5da66eea4fdf 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetWriter.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestSparkParquetWriter.java\n@@ -47,7 +47,7 @@\n public class TestSparkParquetWriter {\n   @TempDir private Path temp;\n \n-  public static final Schema SCHEMA =\n+  private static final Schema SCHEMA =\n       new Schema(\n           Types.NestedField.required(1, \"id\", Types.IntegerType.get()),\n           Types.NestedField.required(2, \"id_long\", Types.LongType.get()));\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13031",
    "pr_id": 13031,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 8,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestChangelogReader.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkMetadataColumns.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestChangelogReader.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestChangelogReader.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkMetadataColumns.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestChangelogReader.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java"
    ],
    "base_commit": "8976bc5fd9b39f4d079e6e5d66749dee8d4c423f",
    "head_commit": "cb35151ed206348f641dc7e6b70b3a4890399a56",
    "repo_url": "https://github.com/apache/iceberg/pull/13031",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13031",
    "dockerfile": "",
    "pr_merged_at": "2025-05-12T10:33:14.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestBase.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestBase.java\ndeleted file mode 100644\nindex 3e8953fb950c..000000000000\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestBase.java\n+++ /dev/null\n@@ -1,287 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.iceberg.spark;\n-\n-import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.METASTOREURIS;\n-\n-import java.io.IOException;\n-import java.io.UncheckedIOException;\n-import java.net.URI;\n-import java.nio.file.Files;\n-import java.nio.file.Path;\n-import java.nio.file.Paths;\n-import java.util.List;\n-import java.util.Map;\n-import java.util.TimeZone;\n-import java.util.concurrent.TimeoutException;\n-import java.util.concurrent.atomic.AtomicReference;\n-import org.apache.hadoop.hive.conf.HiveConf;\n-import org.apache.iceberg.CatalogUtil;\n-import org.apache.iceberg.ContentFile;\n-import org.apache.iceberg.catalog.Namespace;\n-import org.apache.iceberg.exceptions.AlreadyExistsException;\n-import org.apache.iceberg.hive.HiveCatalog;\n-import org.apache.iceberg.hive.TestHiveMetastore;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n-import org.apache.spark.api.java.JavaSparkContext;\n-import org.apache.spark.sql.Dataset;\n-import org.apache.spark.sql.Encoders;\n-import org.apache.spark.sql.Row;\n-import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n-import org.apache.spark.sql.execution.QueryExecution;\n-import org.apache.spark.sql.execution.SparkPlan;\n-import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec;\n-import org.apache.spark.sql.internal.SQLConf;\n-import org.apache.spark.sql.util.QueryExecutionListener;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.BeforeClass;\n-\n-public abstract class SparkTestBase extends SparkTestHelperBase {\n-\n-  protected static TestHiveMetastore metastore = null;\n-  protected static HiveConf hiveConf = null;\n-  protected static SparkSession spark = null;\n-  protected static JavaSparkContext sparkContext = null;\n-  protected static HiveCatalog catalog = null;\n-\n-  @BeforeClass\n-  public static void startMetastoreAndSpark() {\n-    SparkTestBase.metastore = new TestHiveMetastore();\n-    metastore.start();\n-    SparkTestBase.hiveConf = metastore.hiveConf();\n-\n-    SparkTestBase.spark =\n-        SparkSession.builder()\n-            .master(\"local[2]\")\n-            .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n-            .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n-            .config(\"spark.sql.legacy.respectNullabilityInTextDatasetConversion\", \"true\")\n-            .enableHiveSupport()\n-            .getOrCreate();\n-\n-    SparkTestBase.sparkContext = JavaSparkContext.fromSparkContext(spark.sparkContext());\n-\n-    SparkTestBase.catalog =\n-        (HiveCatalog)\n-            CatalogUtil.loadCatalog(\n-                HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n-\n-    try {\n-      catalog.createNamespace(Namespace.of(\"default\"));\n-    } catch (AlreadyExistsException ignored) {\n-      // the default namespace already exists. ignore the create error\n-    }\n-  }\n-\n-  @AfterClass\n-  public static void stopMetastoreAndSpark() throws Exception {\n-    SparkTestBase.catalog = null;\n-    if (metastore != null) {\n-      metastore.stop();\n-      SparkTestBase.metastore = null;\n-    }\n-    if (spark != null) {\n-      spark.stop();\n-      SparkTestBase.spark = null;\n-      SparkTestBase.sparkContext = null;\n-    }\n-  }\n-\n-  protected long waitUntilAfter(long timestampMillis) {\n-    long current = System.currentTimeMillis();\n-    while (current <= timestampMillis) {\n-      current = System.currentTimeMillis();\n-    }\n-    return current;\n-  }\n-\n-  protected List<Object[]> sql(String query, Object... args) {\n-    List<Row> rows = spark.sql(String.format(query, args)).collectAsList();\n-    if (rows.size() < 1) {\n-      return ImmutableList.of();\n-    }\n-\n-    return rowsToJava(rows);\n-  }\n-\n-  protected Object scalarSql(String query, Object... args) {\n-    List<Object[]> rows = sql(query, args);\n-    Assert.assertEquals(\"Scalar SQL should return one row\", 1, rows.size());\n-    Object[] row = Iterables.getOnlyElement(rows);\n-    Assert.assertEquals(\"Scalar SQL should return one value\", 1, row.length);\n-    return row[0];\n-  }\n-\n-  protected Object[] row(Object... values) {\n-    return values;\n-  }\n-\n-  protected static String dbPath(String dbName) {\n-    return metastore.getDatabasePath(dbName);\n-  }\n-\n-  protected void withUnavailableFiles(Iterable<? extends ContentFile<?>> files, Action action) {\n-    Iterable<String> fileLocations = Iterables.transform(files, ContentFile::location);\n-    withUnavailableLocations(fileLocations, action);\n-  }\n-\n-  private void move(String location, String newLocation) {\n-    Path path = Paths.get(URI.create(location));\n-    Path tempPath = Paths.get(URI.create(newLocation));\n-\n-    try {\n-      Files.move(path, tempPath);\n-    } catch (IOException e) {\n-      throw new UncheckedIOException(\"Failed to move: \" + location, e);\n-    }\n-  }\n-\n-  protected void withUnavailableLocations(Iterable<String> locations, Action action) {\n-    for (String location : locations) {\n-      move(location, location + \"_temp\");\n-    }\n-\n-    try {\n-      action.invoke();\n-    } finally {\n-      for (String location : locations) {\n-        move(location + \"_temp\", location);\n-      }\n-    }\n-  }\n-\n-  protected void withDefaultTimeZone(String zoneId, Action action) {\n-    TimeZone currentZone = TimeZone.getDefault();\n-    try {\n-      TimeZone.setDefault(TimeZone.getTimeZone(zoneId));\n-      action.invoke();\n-    } finally {\n-      TimeZone.setDefault(currentZone);\n-    }\n-  }\n-\n-  protected void withSQLConf(Map<String, String> conf, Action action) {\n-    SQLConf sqlConf = SQLConf.get();\n-\n-    Map<String, String> currentConfValues = Maps.newHashMap();\n-    conf.keySet()\n-        .forEach(\n-            confKey -> {\n-              if (sqlConf.contains(confKey)) {\n-                String currentConfValue = sqlConf.getConfString(confKey);\n-                currentConfValues.put(confKey, currentConfValue);\n-              }\n-            });\n-\n-    conf.forEach(\n-        (confKey, confValue) -> {\n-          if (SQLConf.isStaticConfigKey(confKey)) {\n-            throw new RuntimeException(\"Cannot modify the value of a static config: \" + confKey);\n-          }\n-          sqlConf.setConfString(confKey, confValue);\n-        });\n-\n-    try {\n-      action.invoke();\n-    } finally {\n-      conf.forEach(\n-          (confKey, confValue) -> {\n-            if (currentConfValues.containsKey(confKey)) {\n-              sqlConf.setConfString(confKey, currentConfValues.get(confKey));\n-            } else {\n-              sqlConf.unsetConf(confKey);\n-            }\n-          });\n-    }\n-  }\n-\n-  protected Dataset<Row> jsonToDF(String schema, String... records) {\n-    Dataset<String> jsonDF = spark.createDataset(ImmutableList.copyOf(records), Encoders.STRING());\n-    return spark.read().schema(schema).json(jsonDF);\n-  }\n-\n-  protected void append(String table, String... jsonRecords) {\n-    try {\n-      String schema = spark.table(table).schema().toDDL();\n-      Dataset<Row> df = jsonToDF(schema, jsonRecords);\n-      df.coalesce(1).writeTo(table).append();\n-    } catch (NoSuchTableException e) {\n-      throw new RuntimeException(\"Failed to write data\", e);\n-    }\n-  }\n-\n-  protected String tablePropsAsString(Map<String, String> tableProps) {\n-    StringBuilder stringBuilder = new StringBuilder();\n-\n-    for (Map.Entry<String, String> property : tableProps.entrySet()) {\n-      if (stringBuilder.length() > 0) {\n-        stringBuilder.append(\", \");\n-      }\n-      stringBuilder.append(String.format(\"'%s' '%s'\", property.getKey(), property.getValue()));\n-    }\n-\n-    return stringBuilder.toString();\n-  }\n-\n-  protected SparkPlan executeAndKeepPlan(String query, Object... args) {\n-    return executeAndKeepPlan(() -> sql(query, args));\n-  }\n-\n-  protected SparkPlan executeAndKeepPlan(Action action) {\n-    AtomicReference<SparkPlan> executedPlanRef = new AtomicReference<>();\n-\n-    QueryExecutionListener listener =\n-        new QueryExecutionListener() {\n-          @Override\n-          public void onSuccess(String funcName, QueryExecution qe, long durationNs) {\n-            executedPlanRef.set(qe.executedPlan());\n-          }\n-\n-          @Override\n-          public void onFailure(String funcName, QueryExecution qe, Exception exception) {}\n-        };\n-\n-    spark.listenerManager().register(listener);\n-\n-    action.invoke();\n-\n-    try {\n-      spark.sparkContext().listenerBus().waitUntilEmpty();\n-    } catch (TimeoutException e) {\n-      throw new RuntimeException(\"Timeout while waiting for processing events\", e);\n-    }\n-\n-    SparkPlan executedPlan = executedPlanRef.get();\n-    if (executedPlan instanceof AdaptiveSparkPlanExec) {\n-      return ((AdaptiveSparkPlanExec) executedPlan).executedPlan();\n-    } else {\n-      return executedPlan;\n-    }\n-  }\n-\n-  @FunctionalInterface\n-  protected interface Action {\n-    void invoke();\n-  }\n-}\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestChangelogReader.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestChangelogReader.java\nindex fc17547fad41..803603a9e3d9 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestChangelogReader.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestChangelogReader.java\n@@ -20,8 +20,11 @@\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n+import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Path;\n import java.util.List;\n import java.util.stream.Collectors;\n import org.apache.iceberg.ChangelogOperation;\n@@ -41,17 +44,15 @@\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.catalyst.InternalRow;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-\n-public class TestChangelogReader extends SparkTestBase {\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+public class TestChangelogReader extends TestBase {\n   private static final Schema SCHEMA =\n       new Schema(\n           required(1, \"id\", Types.IntegerType.get()), optional(2, \"data\", Types.StringType.get()));\n@@ -64,9 +65,9 @@ public class TestChangelogReader extends SparkTestBase {\n   private DataFile dataFile1;\n   private DataFile dataFile2;\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n-  @Before\n+  @BeforeEach\n   public void before() throws IOException {\n     table = catalog.createTable(TableIdentifier.of(\"default\", \"test\"), SCHEMA, SPEC);\n     // create some data\n@@ -85,7 +86,7 @@ public void before() throws IOException {\n     dataFile2 = writeDataFile(records2);\n   }\n \n-  @After\n+  @AfterEach\n   public void after() {\n     catalog.dropTable(TableIdentifier.of(\"default\", \"test\"));\n   }\n@@ -176,7 +177,7 @@ public void testDataFileRewrite() throws IOException {\n       reader.close();\n     }\n \n-    Assert.assertEquals(\"Should have no rows\", 0, rows.size());\n+    assertThat(rows).as(\"Should have no rows\").isEmpty();\n   }\n \n   @Test\n@@ -254,6 +255,9 @@ private Object[] toJava(InternalRow row) {\n   private DataFile writeDataFile(List<Record> records) throws IOException {\n     // records all use IDs that are in bucket id_bucket=0\n     return FileHelpers.writeDataFile(\n-        table, Files.localOutput(temp.newFile()), TestHelpers.Row.of(0), records);\n+        table,\n+        Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n+        TestHelpers.Row.of(0),\n+        records);\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java\nindex 4ee77345dbe5..bf3bcacbfbe9 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java\n@@ -20,14 +20,20 @@\n \n import static org.apache.iceberg.PlanningMode.DISTRIBUTED;\n import static org.apache.iceberg.PlanningMode.LOCAL;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.File;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n import java.util.Arrays;\n import java.util.List;\n import java.util.Map;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n-import org.apache.iceberg.PlanningMode;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n@@ -37,48 +43,75 @@\n import org.apache.iceberg.spark.SparkReadOptions;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.iceberg.spark.SparkTableUtil;\n-import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.TableIdentifier;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n-public class TestIdentityPartitionData extends SparkTestBase {\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestIdentityPartitionData extends TestBase {\n   private static final Configuration CONF = new Configuration();\n   private static final HadoopTables TABLES = new HadoopTables(CONF);\n \n-  @Parameterized.Parameters(name = \"format = {0}, vectorized = {1}, planningMode = {2}\")\n+  @Parameters(name = \"format = {0}, vectorized = {1}, properties = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {\"parquet\", false, LOCAL},\n-      {\"parquet\", true, DISTRIBUTED},\n-      {\"avro\", false, LOCAL},\n-      {\"orc\", false, DISTRIBUTED},\n-      {\"orc\", true, LOCAL},\n+      {\n+        FileFormat.PARQUET,\n+        false,\n+        ImmutableMap.of(\n+            TableProperties.DEFAULT_FILE_FORMAT, \"parquet\",\n+            TableProperties.DATA_PLANNING_MODE, LOCAL.modeName(),\n+            TableProperties.DELETE_PLANNING_MODE, LOCAL.modeName())\n+      },\n+      {\n+        FileFormat.PARQUET,\n+        true,\n+        ImmutableMap.of(\n+            TableProperties.DEFAULT_FILE_FORMAT, \"parquet\",\n+            TableProperties.DATA_PLANNING_MODE, DISTRIBUTED.modeName(),\n+            TableProperties.DELETE_PLANNING_MODE, DISTRIBUTED.modeName())\n+      },\n+      {\n+        FileFormat.AVRO,\n+        false,\n+        ImmutableMap.of(\n+            TableProperties.DEFAULT_FILE_FORMAT, \"avro\",\n+            TableProperties.DATA_PLANNING_MODE, LOCAL.modeName(),\n+            TableProperties.DELETE_PLANNING_MODE, LOCAL.modeName())\n+      },\n+      {\n+        FileFormat.ORC,\n+        false,\n+        ImmutableMap.of(\n+            TableProperties.DEFAULT_FILE_FORMAT, \"orc\",\n+            TableProperties.DATA_PLANNING_MODE, DISTRIBUTED.modeName(),\n+            TableProperties.DELETE_PLANNING_MODE, DISTRIBUTED.modeName())\n+      },\n+      {\n+        FileFormat.ORC,\n+        true,\n+        ImmutableMap.of(\n+            TableProperties.DEFAULT_FILE_FORMAT, \"orc\",\n+            TableProperties.DATA_PLANNING_MODE, LOCAL.modeName(),\n+            TableProperties.DELETE_PLANNING_MODE, LOCAL.modeName())\n+      },\n     };\n   }\n \n-  private final String format;\n-  private final boolean vectorized;\n-  private final Map<String, String> properties;\n+  @Parameter(index = 0)\n+  private FileFormat format;\n \n-  public TestIdentityPartitionData(String format, boolean vectorized, PlanningMode planningMode) {\n-    this.format = format;\n-    this.vectorized = vectorized;\n-    this.properties =\n-        ImmutableMap.of(\n-            TableProperties.DEFAULT_FILE_FORMAT, format,\n-            TableProperties.DATA_PLANNING_MODE, planningMode.modeName(),\n-            TableProperties.DELETE_PLANNING_MODE, planningMode.modeName());\n-  }\n+  @Parameter(index = 1)\n+  private boolean vectorized;\n+\n+  @Parameter(index = 2)\n+  private Map<String, String> properties;\n \n   private static final Schema LOG_SCHEMA =\n       new Schema(\n@@ -100,7 +133,7 @@ public TestIdentityPartitionData(String format, boolean vectorized, PlanningMode\n           LogMessage.warn(\"2020-02-04\", \"warn event 1\"),\n           LogMessage.debug(\"2020-02-04\", \"debug event 5\"));\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n   private final PartitionSpec spec =\n       PartitionSpec.builderFor(LOG_SCHEMA).identity(\"date\").identity(\"level\").build();\n@@ -113,10 +146,10 @@ public TestIdentityPartitionData(String format, boolean vectorized, PlanningMode\n    * also fail.\n    */\n   private void setupParquet() throws Exception {\n-    File location = temp.newFolder(\"logs\");\n-    File hiveLocation = temp.newFolder(\"hive\");\n+    File location = Files.createTempDirectory(temp, \"logs\").toFile();\n+    File hiveLocation = Files.createTempDirectory(temp, \"hive\").toFile();\n     String hiveTable = \"hivetable\";\n-    Assert.assertTrue(\"Temp folder should exist\", location.exists());\n+    assertThat(location).as(\"Temp folder should exist\").exists();\n \n     this.logs =\n         spark.createDataFrame(LOGS, LogMessage.class).select(\"id\", \"date\", \"level\", \"message\");\n@@ -139,13 +172,13 @@ private void setupParquet() throws Exception {\n         spark, new TableIdentifier(hiveTable), table, location.toString());\n   }\n \n-  @Before\n+  @BeforeEach\n   public void setupTable() throws Exception {\n-    if (format.equals(\"parquet\")) {\n+    if (format.equals(FileFormat.PARQUET)) {\n       setupParquet();\n     } else {\n-      File location = temp.newFolder(\"logs\");\n-      Assert.assertTrue(\"Temp folder should exist\", location.exists());\n+      File location = Files.createTempDirectory(temp, \"logs\").toFile();\n+      assertThat(location).as(\"Temp folder should exist\").exists();\n \n       this.table = TABLES.create(LOG_SCHEMA, spec, properties, location.toString());\n       this.logs =\n@@ -159,7 +192,7 @@ public void setupTable() throws Exception {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFullProjection() {\n     List<Row> expected = logs.orderBy(\"id\").collectAsList();\n     List<Row> actual =\n@@ -171,10 +204,10 @@ public void testFullProjection() {\n             .orderBy(\"id\")\n             .select(\"id\", \"date\", \"level\", \"message\")\n             .collectAsList();\n-    Assert.assertEquals(\"Rows should match\", expected, actual);\n+    assertThat(actual).as(\"Rows should match\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testProjections() {\n     String[][] cases =\n         new String[][] {\n@@ -210,8 +243,9 @@ public void testProjections() {\n               .select(\"id\", ordering)\n               .orderBy(\"id\")\n               .collectAsList();\n-      Assert.assertEquals(\n-          \"Rows should match for ordering: \" + Arrays.toString(ordering), expected, actual);\n+      assertThat(actual)\n+          .as(\"Rows should match for ordering: \" + Arrays.toString(ordering))\n+          .isEqualTo(expected);\n     }\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkMetadataColumns.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkMetadataColumns.java\nindex 1fd017025e58..4fd018149365 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkMetadataColumns.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkMetadataColumns.java\n@@ -26,9 +26,13 @@\n import static org.apache.iceberg.TableProperties.PARQUET_VECTORIZATION_ENABLED;\n import static org.apache.spark.sql.functions.expr;\n import static org.apache.spark.sql.functions.lit;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.Collectors;\n@@ -36,6 +40,9 @@\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.HasTableOperations;\n import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -49,26 +56,22 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n-import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Encoders;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.types.StructType;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Before;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n-public class TestSparkMetadataColumns extends SparkTestBase {\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkMetadataColumns extends TestBase {\n \n   private static final String TABLE_NAME = \"test_table\";\n   private static final Schema SCHEMA =\n@@ -84,37 +87,41 @@ public class TestSparkMetadataColumns extends SparkTestBase {\n           .addField(\"zero\", 1, \"id_zero\")\n           .build();\n \n-  @Parameterized.Parameters(name = \"fileFormat = {0}, vectorized = {1}, formatVersion = {2}\")\n+  @Parameters(name = \"fileFormat = {0}, vectorized = {1}, formatVersion = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n       {FileFormat.PARQUET, false, 1},\n       {FileFormat.PARQUET, true, 1},\n       {FileFormat.PARQUET, false, 2},\n       {FileFormat.PARQUET, true, 2},\n+      {FileFormat.PARQUET, false, 3},\n+      {FileFormat.PARQUET, true, 3},\n       {FileFormat.AVRO, false, 1},\n       {FileFormat.AVRO, false, 2},\n+      {FileFormat.AVRO, false, 3},\n       {FileFormat.ORC, false, 1},\n       {FileFormat.ORC, true, 1},\n       {FileFormat.ORC, false, 2},\n       {FileFormat.ORC, true, 2},\n+      {FileFormat.ORC, false, 3},\n+      {FileFormat.ORC, true, 3},\n     };\n   }\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n-  private final FileFormat fileFormat;\n-  private final boolean vectorized;\n-  private final int formatVersion;\n+  @Parameter(index = 0)\n+  private FileFormat fileFormat;\n \n-  private Table table = null;\n+  @Parameter(index = 1)\n+  private boolean vectorized;\n \n-  public TestSparkMetadataColumns(FileFormat fileFormat, boolean vectorized, int formatVersion) {\n-    this.fileFormat = fileFormat;\n-    this.vectorized = vectorized;\n-    this.formatVersion = formatVersion;\n-  }\n+  @Parameter(index = 2)\n+  private int formatVersion;\n+\n+  private Table table = null;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void setupSpark() {\n     ImmutableMap<String, String> config =\n         ImmutableMap.of(\n@@ -128,20 +135,21 @@ public static void setupSpark() {\n         (key, value) -> spark.conf().set(\"spark.sql.catalog.spark_catalog.\" + key, value));\n   }\n \n-  @Before\n+  @BeforeEach\n   public void setupTable() throws IOException {\n     createAndInitTable();\n   }\n \n-  @After\n+  @AfterEach\n   public void dropTable() {\n     TestTables.clearTables();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSpecAndPartitionMetadataColumns() {\n     // TODO: support metadata structs in vectorized ORC reads\n-    Assume.assumeFalse(fileFormat == FileFormat.ORC && vectorized);\n+    assumeThat(fileFormat).isNotEqualTo(FileFormat.ORC);\n+    assumeThat(vectorized).isFalse();\n \n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a1', 'b1')\", TABLE_NAME);\n \n@@ -172,7 +180,7 @@ public void testSpecAndPartitionMetadataColumns() {\n         sql(\"SELECT _spec_id, _partition FROM %s ORDER BY _spec_id\", TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionMetadataColumnWithManyColumns() {\n     List<Types.NestedField> fields =\n         Lists.newArrayList(Types.NestedField.required(0, \"id\", Types.LongType.get()));\n@@ -204,7 +212,7 @@ public void testPartitionMetadataColumnWithManyColumns() {\n         .mode(\"append\")\n         .save(TABLE_NAME);\n \n-    Assert.assertEquals(2, spark.table(TABLE_NAME).select(\"*\", \"_partition\").count());\n+    assertThat(spark.table(TABLE_NAME).select(\"*\", \"_partition\").count()).isEqualTo(2);\n     List<Object[]> expected =\n         ImmutableList.of(row(row(0L), 0L, \"0\", \"0\", \"0\"), row(row(1L), 1L, \"1\", \"1\", \"1\"));\n     assertEquals(\n@@ -213,9 +221,9 @@ public void testPartitionMetadataColumnWithManyColumns() {\n         sql(\"SELECT _partition, id, c999, c1000, c1001 FROM %s ORDER BY id\", TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPositionMetadataColumnWithMultipleRowGroups() throws NoSuchTableException {\n-    Assume.assumeTrue(fileFormat == FileFormat.PARQUET);\n+    assumeThat(fileFormat).isEqualTo(FileFormat.PARQUET);\n \n     table.updateProperties().set(PARQUET_ROW_GROUP_SIZE_BYTES, \"100\").commit();\n \n@@ -231,15 +239,15 @@ public void testPositionMetadataColumnWithMultipleRowGroups() throws NoSuchTable\n             .withColumn(\"data\", lit(\"ABCDEF\"));\n     df.coalesce(1).writeTo(TABLE_NAME).append();\n \n-    Assert.assertEquals(200, spark.table(TABLE_NAME).count());\n+    assertThat(spark.table(TABLE_NAME).count()).isEqualTo(200);\n \n     List<Object[]> expectedRows = ids.stream().map(this::row).collect(Collectors.toList());\n     assertEquals(\"Rows must match\", expectedRows, sql(\"SELECT _pos FROM %s\", TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPositionMetadataColumnWithMultipleBatches() throws NoSuchTableException {\n-    Assume.assumeTrue(fileFormat == FileFormat.PARQUET);\n+    assumeThat(fileFormat).isEqualTo(FileFormat.PARQUET);\n \n     table.updateProperties().set(PARQUET_BATCH_SIZE, \"1000\").commit();\n \n@@ -255,13 +263,13 @@ public void testPositionMetadataColumnWithMultipleBatches() throws NoSuchTableEx\n             .withColumn(\"data\", lit(\"ABCDEF\"));\n     df.coalesce(1).writeTo(TABLE_NAME).append();\n \n-    Assert.assertEquals(7500, spark.table(TABLE_NAME).count());\n+    assertThat(spark.table(TABLE_NAME).count()).isEqualTo(7500);\n \n     List<Object[]> expectedRows = ids.stream().map(this::row).collect(Collectors.toList());\n     assertEquals(\"Rows must match\", expectedRows, sql(\"SELECT _pos FROM %s\", TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionMetadataColumnWithUnknownTransforms() {\n     // replace the table spec to include an unknown transform\n     TableOperations ops = ((HasTableOperations) table).operations();\n@@ -273,7 +281,7 @@ public void testPartitionMetadataColumnWithUnknownTransforms() {\n         .hasMessage(\"Cannot build table partition type, unknown transforms: [zero]\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testConflictingColumns() {\n     table\n         .updateSchema()\n@@ -325,6 +333,12 @@ private void createAndInitTable() throws IOException {\n             !vectorized, \"File format %s does not support vectorized reads\", fileFormat);\n     }\n \n-    this.table = TestTables.create(temp.newFolder(), TABLE_NAME, SCHEMA, SPEC, properties);\n+    this.table =\n+        TestTables.create(\n+            Files.createTempDirectory(temp, \"junit\").toFile(),\n+            TABLE_NAME,\n+            SCHEMA,\n+            SPEC,\n+            properties);\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java\nindex ac674e2e62e8..8b1e3fbfc77c 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java\n@@ -19,9 +19,11 @@\n package org.apache.iceberg.spark.source;\n \n import static org.apache.iceberg.Files.localOutput;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Path;\n import java.time.LocalDateTime;\n import java.util.List;\n import java.util.UUID;\n@@ -31,6 +33,9 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataFiles;\n import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -41,25 +46,22 @@\n import org.apache.iceberg.io.FileAppender;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.spark.SparkReadOptions;\n-import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.spark.data.GenericsHelpers;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SaveMode;\n import org.apache.spark.sql.SparkSession;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n-public class TestTimestampWithoutZone extends SparkTestBase {\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestTimestampWithoutZone extends TestBase {\n   private static final Configuration CONF = new Configuration();\n   private static final HadoopTables TABLES = new HadoopTables(CONF);\n \n@@ -71,53 +73,49 @@ public class TestTimestampWithoutZone extends SparkTestBase {\n \n   private static SparkSession spark = null;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestTimestampWithoutZone.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestTimestampWithoutZone.spark;\n     TestTimestampWithoutZone.spark = null;\n     currentSpark.stop();\n   }\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n-  private final String format;\n-  private final boolean vectorized;\n+  @Parameter(index = 0)\n+  private FileFormat fileFormat;\n \n-  @Parameterized.Parameters(name = \"format = {0}, vectorized = {1}\")\n+  @Parameter(index = 1)\n+  private boolean vectorized;\n+\n+  @Parameters(name = \"format = {0}, vectorized = {1}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {\"parquet\", false},\n-      {\"parquet\", true},\n-      {\"avro\", false}\n+      {FileFormat.PARQUET, false},\n+      {FileFormat.PARQUET, true},\n+      {FileFormat.AVRO, false}\n     };\n   }\n \n-  public TestTimestampWithoutZone(String format, boolean vectorized) {\n-    this.format = format;\n-    this.vectorized = vectorized;\n-  }\n-\n   private File parent = null;\n   private File unpartitioned = null;\n   private List<Record> records = null;\n \n-  @Before\n+  @BeforeEach\n   public void writeUnpartitionedTable() throws IOException {\n-    this.parent = temp.newFolder(\"TestTimestampWithoutZone\");\n+    this.parent = temp.resolve(\"TestTimestampWithoutZone\").toFile();\n     this.unpartitioned = new File(parent, \"unpartitioned\");\n     File dataFolder = new File(unpartitioned, \"data\");\n-    Assert.assertTrue(\"Mkdir should succeed\", dataFolder.mkdirs());\n+    assertThat(dataFolder.mkdirs()).as(\"Mkdir should succeed\").isTrue();\n \n     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());\n     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned\n \n-    FileFormat fileFormat = FileFormat.fromString(format);\n-\n     File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n \n     // create records using the table's schema\n@@ -138,12 +136,12 @@ public void writeUnpartitionedTable() throws IOException {\n     table.newAppend().appendFile(file).commit();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedTimestampWithoutZone() {\n     assertEqualsSafe(SCHEMA.asStruct(), records, read(unpartitioned.toString(), vectorized));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedTimestampWithoutZoneProjection() {\n     Schema projection = SCHEMA.select(\"id\", \"ts\");\n     assertEqualsSafe(\n@@ -152,7 +150,7 @@ public void testUnpartitionedTimestampWithoutZoneProjection() {\n         read(unpartitioned.toString(), vectorized, \"id\", \"ts\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedTimestampWithoutZoneAppend() {\n     spark\n         .read()\n@@ -182,7 +180,7 @@ private static Record projectFlat(Schema projection, Record record) {\n \n   public static void assertEqualsSafe(\n       Types.StructType struct, List<Record> expected, List<Row> actual) {\n-    Assert.assertEquals(\"Number of results should match expected\", expected.size(), actual.size());\n+    assertThat(actual).as(\"Number of results should match expected\").hasSameSizeAs(expected);\n     for (int i = 0; i < expected.size(); i += 1) {\n       GenericsHelpers.assertEqualsSafe(struct, expected.get(i), actual.get(i));\n     }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestChangelogReader.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestChangelogReader.java\nindex 52d6ff8c9c8b..803603a9e3d9 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestChangelogReader.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestChangelogReader.java\n@@ -177,7 +177,7 @@ public void testDataFileRewrite() throws IOException {\n       reader.close();\n     }\n \n-    assertThat(rows).as(\"Should have no rows\").hasSize(0);\n+    assertThat(rows).as(\"Should have no rows\").isEmpty();\n   }\n \n   @Test\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java\nindex 35a675029c1c..bf3bcacbfbe9 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIdentityPartitionData.java\n@@ -29,6 +29,7 @@\n import java.util.List;\n import java.util.Map;\n import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.Parameter;\n import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n@@ -61,7 +62,7 @@ public class TestIdentityPartitionData extends TestBase {\n   public static Object[][] parameters() {\n     return new Object[][] {\n       {\n-        \"parquet\",\n+        FileFormat.PARQUET,\n         false,\n         ImmutableMap.of(\n             TableProperties.DEFAULT_FILE_FORMAT, \"parquet\",\n@@ -69,7 +70,7 @@ public static Object[][] parameters() {\n             TableProperties.DELETE_PLANNING_MODE, LOCAL.modeName())\n       },\n       {\n-        \"parquet\",\n+        FileFormat.PARQUET,\n         true,\n         ImmutableMap.of(\n             TableProperties.DEFAULT_FILE_FORMAT, \"parquet\",\n@@ -77,7 +78,7 @@ public static Object[][] parameters() {\n             TableProperties.DELETE_PLANNING_MODE, DISTRIBUTED.modeName())\n       },\n       {\n-        \"avro\",\n+        FileFormat.AVRO,\n         false,\n         ImmutableMap.of(\n             TableProperties.DEFAULT_FILE_FORMAT, \"avro\",\n@@ -85,7 +86,7 @@ public static Object[][] parameters() {\n             TableProperties.DELETE_PLANNING_MODE, LOCAL.modeName())\n       },\n       {\n-        \"orc\",\n+        FileFormat.ORC,\n         false,\n         ImmutableMap.of(\n             TableProperties.DEFAULT_FILE_FORMAT, \"orc\",\n@@ -93,7 +94,7 @@ public static Object[][] parameters() {\n             TableProperties.DELETE_PLANNING_MODE, DISTRIBUTED.modeName())\n       },\n       {\n-        \"orc\",\n+        FileFormat.ORC,\n         true,\n         ImmutableMap.of(\n             TableProperties.DEFAULT_FILE_FORMAT, \"orc\",\n@@ -104,7 +105,7 @@ public static Object[][] parameters() {\n   }\n \n   @Parameter(index = 0)\n-  private String format;\n+  private FileFormat format;\n \n   @Parameter(index = 1)\n   private boolean vectorized;\n@@ -173,7 +174,7 @@ private void setupParquet() throws Exception {\n \n   @BeforeEach\n   public void setupTable() throws Exception {\n-    if (format.equals(\"parquet\")) {\n+    if (format.equals(FileFormat.PARQUET)) {\n       setupParquet();\n     } else {\n       File location = Files.createTempDirectory(temp, \"logs\").toFile();\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java\nindex 306444b9f29f..8b1e3fbfc77c 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestTimestampWithoutZone.java\n@@ -88,7 +88,7 @@ public static void stopSpark() {\n   @TempDir private Path temp;\n \n   @Parameter(index = 0)\n-  private String format;\n+  private FileFormat fileFormat;\n \n   @Parameter(index = 1)\n   private boolean vectorized;\n@@ -96,9 +96,9 @@ public static void stopSpark() {\n   @Parameters(name = \"format = {0}, vectorized = {1}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {\"parquet\", false},\n-      {\"parquet\", true},\n-      {\"avro\", false}\n+      {FileFormat.PARQUET, false},\n+      {FileFormat.PARQUET, true},\n+      {FileFormat.AVRO, false}\n     };\n   }\n \n@@ -116,8 +116,6 @@ public void writeUnpartitionedTable() throws IOException {\n     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), unpartitioned.toString());\n     Schema tableSchema = table.schema(); // use the table schema because ids are reassigned\n \n-    FileFormat fileFormat = FileFormat.fromString(format);\n-\n     File testFile = new File(dataFolder, fileFormat.addExtension(UUID.randomUUID().toString()));\n \n     // create records using the table's schema\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13021",
    "pr_id": 13021,
    "issue_id": 12937,
    "repo": "apache/iceberg",
    "problem_statement": "Remove JUnit4 dependency from Flink\n### Feature Request / Improvement\n\nI started removing JUnit4 from Flink via the below diff\n```\ndiff --git a/flink/v2.0/build.gradle b/flink/v2.0/build.gradle\nindex c0de408e98..ef4635e133 100644\n--- a/flink/v2.0/build.gradle\n+++ b/flink/v2.0/build.gradle\n@@ -68,7 +68,9 @@ project(\":iceberg-flink:iceberg-flink-${flinkMajorVersion}\") {\n\n     implementation libs.datasketches\n\n-    testImplementation libs.flink20.connector.test.utils\n+    testImplementation(libs.flink20.connector.test.utils) {\n+      exclude group: 'junit'\n+    }\n     testImplementation libs.flink20.core\n     testImplementation libs.flink20.runtime\n     testImplementation(libs.flink20.test.utilsjunit) {\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\nindex b02b3337dc..362d004cfe 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\n@@ -21,6 +21,7 @@ package org.apache.iceberg.flink.sink;\n import static org.apache.iceberg.flink.TestFixtures.DATABASE;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n\n import java.io.IOException;\n import java.util.Collections;\n@@ -55,7 +56,6 @@ import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n-import org.junit.Assume;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n@@ -421,8 +421,9 @@ public class TestIcebergSink extends TestFlinkIcebergSinkBase {\n\n   @TestTemplate\n   void testErrorOnNullForRequiredField() throws Exception {\n-    Assume.assumeFalse(\n-        \"ORC file format supports null values even for required fields.\", format == FileFormat.ORC);\n+    assumeThat(format)\n+        .as(\"ORC file format supports null values even for required fields.\")\n+        .isEqualTo(FileFormat.ORC);\n```\n\nHowever, there is still `TestIcebergSourceFailover` that uses `MiniClusterWithClientResource` which requires JUnit4. This would be good to convert so that we can eventually remove all JUnit4 dependencies across the codebase.\nWe would also need to backport the fix to Flink 1.20 & 1.19.\n\n### Query engine\n\nNone\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [x] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 394,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceFailover.java"
    ],
    "pr_changed_test_files": [
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceFailover.java"
    ],
    "base_commit": "cbbc8728e2bb5ee1f32d587dd99bb44384f8b799",
    "head_commit": "031334c4ae64c8f74bc286f1318483a45ac944e9",
    "repo_url": "https://github.com/apache/iceberg/pull/13021",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13021",
    "dockerfile": "",
    "pr_merged_at": "2025-05-23T08:58:27.000Z",
    "patch": "",
    "test_patch": "diff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceFailover.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceFailover.java\nindex ca48f408e4f9..310c8be2eff6 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceFailover.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceFailover.java\n@@ -44,9 +44,8 @@\n import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.test.junit5.InjectClusterClient;\n+import org.apache.flink.test.junit5.InjectMiniCluster;\n import org.apache.flink.test.junit5.MiniClusterExtension;\n-import org.apache.flink.test.util.MiniClusterWithClientResource;\n-import org.apache.flink.util.function.ThrowingConsumer;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -62,6 +61,7 @@\n import org.apache.iceberg.flink.sink.FlinkSink;\n import org.apache.iceberg.flink.source.assigner.SimpleSplitAssignerFactory;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Disabled;\n import org.junit.jupiter.api.Test;\n@@ -113,6 +113,18 @@ protected void setupTable() {\n             .createTable(TestFixtures.SINK_TABLE_IDENTIFIER, TestFixtures.SCHEMA);\n   }\n \n+  @BeforeEach\n+  protected void startMiniCluster(@InjectMiniCluster MiniCluster miniCluster) throws Exception {\n+    if (!miniCluster.isRunning()) {\n+      miniCluster.start();\n+    }\n+  }\n+\n+  @AfterEach\n+  protected void stopMiniCluster(@InjectMiniCluster MiniCluster miniCluster) throws Exception {\n+    miniCluster.close();\n+  }\n+\n   protected IcebergSource.Builder<RowData> sourceBuilder() {\n     Configuration config = new Configuration();\n     return IcebergSource.forRowData()\n@@ -183,15 +195,15 @@ public void testBoundedWithSavepoint(@InjectClusterClient ClusterClient<?> clust\n   }\n \n   @Test\n-  public void testBoundedWithTaskManagerFailover() throws Exception {\n-    runTestWithNewMiniCluster(\n-        miniCluster -> testBoundedIcebergSource(FailoverType.TM, miniCluster));\n+  public void testBoundedWithTaskManagerFailover(@InjectMiniCluster MiniCluster miniCluster)\n+      throws Exception {\n+    testBoundedIcebergSource(FailoverType.TM, miniCluster);\n   }\n \n   @Test\n-  public void testBoundedWithJobManagerFailover() throws Exception {\n-    runTestWithNewMiniCluster(\n-        miniCluster -> testBoundedIcebergSource(FailoverType.JM, miniCluster));\n+  public void testBoundedWithJobManagerFailover(@InjectMiniCluster MiniCluster miniCluster)\n+      throws Exception {\n+    testBoundedIcebergSource(FailoverType.JM, miniCluster);\n   }\n \n   private void testBoundedIcebergSource(FailoverType failoverType, MiniCluster miniCluster)\n@@ -224,15 +236,15 @@ private void testBoundedIcebergSource(FailoverType failoverType, MiniCluster min\n   }\n \n   @Test\n-  public void testContinuousWithTaskManagerFailover() throws Exception {\n-    runTestWithNewMiniCluster(\n-        miniCluster -> testContinuousIcebergSource(FailoverType.TM, miniCluster));\n+  public void testContinuousWithTaskManagerFailover(@InjectMiniCluster MiniCluster miniCluster)\n+      throws Exception {\n+    testContinuousIcebergSource(FailoverType.TM, miniCluster);\n   }\n \n   @Test\n-  public void testContinuousWithJobManagerFailover() throws Exception {\n-    runTestWithNewMiniCluster(\n-        miniCluster -> testContinuousIcebergSource(FailoverType.JM, miniCluster));\n+  public void testContinuousWithJobManagerFailover(@InjectMiniCluster MiniCluster miniCluster)\n+      throws Exception {\n+    testContinuousIcebergSource(FailoverType.JM, miniCluster);\n   }\n \n   private void testContinuousIcebergSource(FailoverType failoverType, MiniCluster miniCluster)\n@@ -314,20 +326,6 @@ private void createBoundedStreams(StreamExecutionEnvironment env, int failAfter)\n   // test utilities copied from Flink's FileSourceTextLinesITCase\n   // ------------------------------------------------------------------------\n \n-  private static void runTestWithNewMiniCluster(ThrowingConsumer<MiniCluster, Exception> testMethod)\n-      throws Exception {\n-    MiniClusterWithClientResource miniCluster = null;\n-    try {\n-      miniCluster = new MiniClusterWithClientResource(MINI_CLUSTER_RESOURCE_CONFIG);\n-      miniCluster.before();\n-      testMethod.accept(miniCluster.getMiniCluster());\n-    } finally {\n-      if (miniCluster != null) {\n-        miniCluster.after();\n-      }\n-    }\n-  }\n-\n   private enum FailoverType {\n     NONE,\n     TM,\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13017",
    "pr_id": 13017,
    "issue_id": 12009,
    "repo": "apache/iceberg",
    "problem_statement": "Flaky Spark tests due to initializationError\n### Apache Iceberg version\n\nmain (development)\n\n### Query engine\n\nSpark\n\n### Please describe the bug üêû\n\nhttps://github.com/apache/iceberg/actions/runs/12857234112/job/35845760586\n```\nTestRewritePositionDeleteFilesAction > initializationError FAILED\n    java.io.IOException: Failed to bind to 0.0.0.0/0.0.0.0:44503\n        at org.eclipse.jetty.server.ServerConnector.openAcceptChannel(ServerConnector.java:344)\n        at org.eclipse.jetty.server.ServerConnector.open(ServerConnector.java:304)\n        at org.eclipse.jetty.server.Server.lambda$doStart$0(Server.java:402)\n        at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.accept(ForEachOps.java:184)\n        at java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:197)\n        at java.base/java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:179)\n        at java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:1024)\n        at java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:509)\n        at java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:499)\n        at java.base/java.util.stream.ForEachOps$ForEachOp.evaluateSequential(ForEachOps.java:151)\n        at java.base/java.util.stream.ForEachOps$ForEachOp$OfRef.evaluateSequential(ForEachOps.java:174)\n        at java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n        at java.base/java.util.stream.ReferencePipeline.forEach(ReferencePipeline.java:596)\n        at org.eclipse.jetty.server.Server.doStart(Server.java:398)\n        at org.eclipse.jetty.util.component.AbstractLifeCycle.start(AbstractLifeCycle.java:93)\n        at org.apache.iceberg.rest.RESTCatalogServer.start(RESTCatalogServer.java:116)\n        at org.apache.iceberg.rest.RESTServerExtension.beforeAll(RESTServerExtension.java:62)\n        at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.lambda$invokeBeforeAllCallbacks$13(ClassBasedTestDescriptor.java:396)\n        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n        at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.invokeBeforeAllCallbacks(ClassBasedTestDescriptor.java:396)\n        at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:212)\n        at org.junit.jupiter.engine.descriptor.ClassBasedTestDescriptor.before(ClassBasedTestDescriptor.java:85)\n        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:153)\n        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:146)\n        at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:144)\n        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n        at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:143)\n        at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:100)\n        at java.base/java.util.ArrayList.forEach(ArrayList.java:1596)\n        at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.invokeAll(SameThreadHierarchicalTestExecutorService.java:41)\n        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$6(NodeTestTask.java:160)\n        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$8(NodeTestTask.java:146)\n        at org.junit.platform.engine.support.hierarchical.Node.around(Node.java:137)\n        at org.junit.platform.engine.support.hierarchical.NodeTestTask.lambda$executeRecursively$9(NodeTestTask.java:144)\n        at org.junit.platform.engine.support.hierarchical.ThrowableCollector.execute(ThrowableCollector.java:73)\n        at org.junit.platform.engine.support.hierarchical.NodeTestTask.executeRecursively(NodeTestTask.java:143)\n        at org.junit.platform.engine.support.hierarchical.NodeTestTask.execute(NodeTestTask.java:100)\n        at org.junit.platform.engine.support.hierarchical.SameThreadHierarchicalTestExecutorService.submit(SameThreadHierarchicalTestExecutorService.java:35)\n        at org.junit.platform.engine.support.hierarchical.HierarchicalTestExecutor.execute(HierarchicalTestExecutor.java:57)\n        at org.junit.platform.engine.support.hierarchical.HierarchicalTestEngine.execute(HierarchicalTestEngine.java:54)\n        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:107)\n        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:88)\n        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.lambda$execute$0(EngineExecutionOrchestrator.java:54)\n        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.withInterceptedStreams(EngineExecutionOrchestrator.java:67)\n        at org.junit.platform.launcher.core.EngineExecutionOrchestrator.execute(EngineExecutionOrchestrator.java:52)\n        at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:114)\n        at org.junit.platform.launcher.core.DefaultLauncher.execute(DefaultLauncher.java:86)\n        at org.junit.platform.launcher.core.DefaultLauncherSession$DelegatingLauncher.execute(DefaultLauncherSession.java:86)\n        at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.processAllTestClasses(JUnitPlatformTestClassProcessor.java:124)\n        at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor$CollectAllTestClassesExecutor.access$000(JUnitPlatformTestClassProcessor.java:99)\n        at org.gradle.api.internal.tasks.testing.junitplatform.JUnitPlatformTestClassProcessor.stop(JUnitPlatformTestClassProcessor.java:94)\n        at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.stop(SuiteTestClassProcessor.java:63)\n        at java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n        at java.base/java.lang.reflect.Method.invoke(Method.java:580)\n        at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)\n        at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)\n        at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)\n        at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:92)\n        at jdk.proxy1/jdk.proxy1.$Proxy4.stop(Unknown Source)\n        at org.gradle.api.internal.tasks.testing.worker.TestWorker$3.run(TestWorker.java:200)\n        at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:132)\n        at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:103)\n        at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:63)\n        at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)\n        at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:121)\n        at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71)\n        at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)\n        at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)\n\n        Caused by:\n        java.net.BindException: Address already in use\n            at java.base/sun.nio.ch.Net.bind0(Native Method)\n            at java.base/sun.nio.ch.Net.bind(Net.java:565)\n            at java.base/sun.nio.ch.ServerSocketChannelImpl.netBind(ServerSocketChannelImpl.java:346)\n            at java.base/sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:303)\n            at org.eclipse.jetty.server.ServerConnector.openAcceptChannel(ServerConnector.java:339)\n            ... 70 more\n\n```\n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [x] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 994,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "open-api/src/testFixtures/java/org/apache/iceberg/rest/RESTServerExtension.java"
    ],
    "pr_changed_test_files": [
      "open-api/src/testFixtures/java/org/apache/iceberg/rest/RESTServerExtension.java"
    ],
    "base_commit": "d04be9b6df0e4bc59bceb6a80c2b828d5318d270",
    "head_commit": "cb4ca7ae15a8a2b45a061fec99a05db0088ac531",
    "repo_url": "https://github.com/apache/iceberg/pull/13017",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13017",
    "dockerfile": "",
    "pr_merged_at": "2025-05-13T10:19:54.000Z",
    "patch": "",
    "test_patch": "diff --git a/open-api/src/testFixtures/java/org/apache/iceberg/rest/RESTServerExtension.java b/open-api/src/testFixtures/java/org/apache/iceberg/rest/RESTServerExtension.java\nindex 19236bec64c3..9b79f7e73e4e 100644\n--- a/open-api/src/testFixtures/java/org/apache/iceberg/rest/RESTServerExtension.java\n+++ b/open-api/src/testFixtures/java/org/apache/iceberg/rest/RESTServerExtension.java\n@@ -18,6 +18,7 @@\n  */\n package org.apache.iceberg.rest;\n \n+import java.net.BindException;\n import java.util.Map;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.junit.jupiter.api.extension.AfterAllCallback;\n@@ -32,17 +33,18 @@ public class RESTServerExtension implements BeforeAllCallback, AfterAllCallback\n   private RESTCatalogServer localServer;\n   private RESTCatalog client;\n   private final Map<String, String> config;\n+  private final boolean findFreePort;\n \n   public RESTServerExtension() {\n     config = Maps.newHashMap();\n+    findFreePort = false;\n   }\n \n   public RESTServerExtension(Map<String, String> config) {\n     Map<String, String> conf = Maps.newHashMap(config);\n-    if (conf.containsKey(RESTCatalogServer.REST_PORT)\n-        && conf.get(RESTCatalogServer.REST_PORT).equals(FREE_PORT)) {\n-      conf.put(RESTCatalogServer.REST_PORT, String.valueOf(RCKUtils.findFreePort()));\n-    }\n+    findFreePort =\n+        conf.containsKey(RESTCatalogServer.REST_PORT)\n+            && conf.get(RESTCatalogServer.REST_PORT).equals(FREE_PORT);\n     this.config = conf;\n   }\n \n@@ -58,8 +60,24 @@ public RESTCatalog client() {\n   public void beforeAll(ExtensionContext extensionContext) throws Exception {\n     if (Boolean.parseBoolean(\n         extensionContext.getConfigurationParameter(RCKUtils.RCK_LOCAL).orElse(\"true\"))) {\n-      this.localServer = new RESTCatalogServer(config);\n-      this.localServer.start(false);\n+      int maxAttempts = 10;\n+      for (int i = 0; i < maxAttempts; i++) {\n+        try {\n+          if (findFreePort) {\n+            config.put(RESTCatalogServer.REST_PORT, String.valueOf(RCKUtils.findFreePort()));\n+          }\n+          this.localServer = new RESTCatalogServer(config);\n+          this.localServer.start(false);\n+          break;\n+        } catch (BindException e) {\n+          if (!findFreePort || i == maxAttempts - 1) {\n+            throw new RuntimeException(\"Failed to start REST server\", e);\n+          }\n+        } catch (Exception e) {\n+          throw new RuntimeException(\"Failed to start REST server\", e);\n+        }\n+      }\n+\n       this.client = RCKUtils.initCatalogClient(config);\n     }\n   }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13015",
    "pr_id": 13015,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 15,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestBaseWithCatalog.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkDistributionAndOrderingUtil.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkExecutorCache.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2Coercion.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogCacheExpiration.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPlanningUtil.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadMetrics.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkScan.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkDistributionAndOrderingUtil.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogCacheExpiration.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPlanningUtil.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadMetrics.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkScan.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestBaseWithCatalog.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkDistributionAndOrderingUtil.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkExecutorCache.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2Coercion.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogCacheExpiration.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPlanningUtil.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadMetrics.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkScan.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkDistributionAndOrderingUtil.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogCacheExpiration.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPlanningUtil.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadMetrics.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkScan.java"
    ],
    "base_commit": "9c8c431f830ed1f413abc355a316b73271385ccf",
    "head_commit": "c4c6f47373f0696f9f698453dc0b3bd8eccfce9c",
    "repo_url": "https://github.com/apache/iceberg/pull/13015",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13015",
    "dockerfile": "",
    "pr_merged_at": "2025-05-12T08:15:27.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestBaseWithCatalog.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestBaseWithCatalog.java\ndeleted file mode 100644\nindex 1c5d9b711b49..000000000000\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkTestBaseWithCatalog.java\n+++ /dev/null\n@@ -1,182 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.iceberg.spark;\n-\n-import static org.apache.iceberg.CatalogProperties.CATALOG_IMPL;\n-import static org.apache.iceberg.CatalogUtil.ICEBERG_CATALOG_TYPE;\n-import static org.apache.iceberg.CatalogUtil.ICEBERG_CATALOG_TYPE_HADOOP;\n-import static org.apache.iceberg.CatalogUtil.ICEBERG_CATALOG_TYPE_HIVE;\n-import static org.apache.iceberg.CatalogUtil.ICEBERG_CATALOG_TYPE_REST;\n-\n-import java.io.File;\n-import java.io.IOException;\n-import java.util.Map;\n-import org.apache.hadoop.fs.FileSystem;\n-import org.apache.hadoop.fs.Path;\n-import org.apache.iceberg.CatalogProperties;\n-import org.apache.iceberg.PlanningMode;\n-import org.apache.iceberg.TableProperties;\n-import org.apache.iceberg.catalog.Catalog;\n-import org.apache.iceberg.catalog.Namespace;\n-import org.apache.iceberg.catalog.SupportsNamespaces;\n-import org.apache.iceberg.catalog.TableIdentifier;\n-import org.apache.iceberg.hadoop.HadoopCatalog;\n-import org.apache.iceberg.inmemory.InMemoryCatalog;\n-import org.apache.iceberg.rest.RESTCatalog;\n-import org.apache.iceberg.rest.RESTCatalogServer;\n-import org.apache.iceberg.rest.RESTServerRule;\n-import org.apache.iceberg.util.PropertyUtil;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.BeforeClass;\n-import org.junit.ClassRule;\n-import org.junit.Rule;\n-import org.junit.rules.TemporaryFolder;\n-\n-public abstract class SparkTestBaseWithCatalog extends SparkTestBase {\n-  protected static File warehouse = null;\n-\n-  @ClassRule\n-  public static final RESTServerRule REST_SERVER_RULE =\n-      new RESTServerRule(\n-          Map.of(\n-              RESTCatalogServer.REST_PORT,\n-              RESTServerRule.FREE_PORT,\n-              // In-memory sqlite database by default is private to the connection that created it.\n-              // If more than 1 jdbc connection backed by in-memory sqlite is created behind one\n-              // JdbcCatalog, then different jdbc connections could provide different views of table\n-              // status even belonging to the same catalog. Reference:\n-              // https://www.sqlite.org/inmemorydb.html\n-              CatalogProperties.CLIENT_POOL_SIZE,\n-              \"1\"));\n-\n-  protected static RESTCatalog restCatalog;\n-\n-  @BeforeClass\n-  public static void createWarehouse() throws IOException {\n-    SparkTestBaseWithCatalog.warehouse = File.createTempFile(\"warehouse\", null);\n-    Assert.assertTrue(warehouse.delete());\n-    restCatalog = REST_SERVER_RULE.client();\n-  }\n-\n-  @AfterClass\n-  public static void dropWarehouse() throws IOException {\n-    if (warehouse != null && warehouse.exists()) {\n-      Path warehousePath = new Path(warehouse.getAbsolutePath());\n-      FileSystem fs = warehousePath.getFileSystem(hiveConf);\n-      Assert.assertTrue(\"Failed to delete \" + warehousePath, fs.delete(warehousePath, true));\n-    }\n-  }\n-\n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n-\n-  protected final String catalogName;\n-  protected final Map<String, String> catalogConfig;\n-  protected Catalog validationCatalog;\n-  protected SupportsNamespaces validationNamespaceCatalog;\n-  protected final TableIdentifier tableIdent = TableIdentifier.of(Namespace.of(\"default\"), \"table\");\n-  protected final String tableName;\n-\n-  public SparkTestBaseWithCatalog() {\n-    this(SparkCatalogConfig.HADOOP);\n-  }\n-\n-  public SparkTestBaseWithCatalog(SparkCatalogConfig config) {\n-    this(config.catalogName(), config.implementation(), config.properties());\n-  }\n-\n-  public SparkTestBaseWithCatalog(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    this.catalogName = catalogName;\n-    this.catalogConfig = config;\n-    configureValidationCatalog();\n-\n-    spark.conf().set(\"spark.sql.catalog.\" + catalogName, implementation);\n-    config.forEach(\n-        (key, value) -> spark.conf().set(\"spark.sql.catalog.\" + catalogName + \".\" + key, value));\n-\n-    if (\"hadoop\".equalsIgnoreCase(config.get(\"type\"))) {\n-      spark.conf().set(\"spark.sql.catalog.\" + catalogName + \".warehouse\", \"file:\" + warehouse);\n-    }\n-\n-    this.tableName =\n-        (catalogName.equals(\"spark_catalog\") ? \"\" : catalogName + \".\") + \"default.table\";\n-\n-    sql(\"CREATE NAMESPACE IF NOT EXISTS default\");\n-  }\n-\n-  protected String tableName(String name) {\n-    return (catalogName.equals(\"spark_catalog\") ? \"\" : catalogName + \".\") + \"default.\" + name;\n-  }\n-\n-  protected String commitTarget() {\n-    return tableName;\n-  }\n-\n-  protected String selectTarget() {\n-    return tableName;\n-  }\n-\n-  protected boolean cachingCatalogEnabled() {\n-    return PropertyUtil.propertyAsBoolean(\n-        catalogConfig, CatalogProperties.CACHE_ENABLED, CatalogProperties.CACHE_ENABLED_DEFAULT);\n-  }\n-\n-  protected void configurePlanningMode(PlanningMode planningMode) {\n-    configurePlanningMode(tableName, planningMode);\n-  }\n-\n-  protected void configurePlanningMode(String table, PlanningMode planningMode) {\n-    sql(\n-        \"ALTER TABLE %s SET TBLPROPERTIES ('%s' '%s', '%s' '%s')\",\n-        table,\n-        TableProperties.DATA_PLANNING_MODE,\n-        planningMode.modeName(),\n-        TableProperties.DELETE_PLANNING_MODE,\n-        planningMode.modeName());\n-  }\n-\n-  private void configureValidationCatalog() {\n-    if (catalogConfig.containsKey(ICEBERG_CATALOG_TYPE)) {\n-      switch (catalogConfig.get(ICEBERG_CATALOG_TYPE)) {\n-        case ICEBERG_CATALOG_TYPE_HADOOP:\n-          this.validationCatalog =\n-              new HadoopCatalog(spark.sessionState().newHadoopConf(), \"file:\" + warehouse);\n-          break;\n-        case ICEBERG_CATALOG_TYPE_REST:\n-          this.validationCatalog = restCatalog;\n-          break;\n-        case ICEBERG_CATALOG_TYPE_HIVE:\n-          this.validationCatalog = catalog;\n-          break;\n-        default:\n-          throw new IllegalArgumentException(\"Unknown catalog type\");\n-      }\n-    } else if (catalogConfig.containsKey(CATALOG_IMPL)) {\n-      switch (catalogConfig.get(CATALOG_IMPL)) {\n-        case \"org.apache.iceberg.inmemory.InMemoryCatalog\":\n-          this.validationCatalog = new InMemoryCatalog();\n-          break;\n-        default:\n-          throw new IllegalArgumentException(\"Unknown catalog impl\");\n-      }\n-    }\n-    this.validationNamespaceCatalog = (SupportsNamespaces) validationCatalog;\n-  }\n-}\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkDistributionAndOrderingUtil.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkDistributionAndOrderingUtil.java\nindex 79374edc3f16..0f3078d7bfa7 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkDistributionAndOrderingUtil.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkDistributionAndOrderingUtil.java\n@@ -29,8 +29,10 @@\n import static org.apache.spark.sql.connector.write.RowLevelOperation.Command.DELETE;\n import static org.apache.spark.sql.connector.write.RowLevelOperation.Command.MERGE;\n import static org.apache.spark.sql.connector.write.RowLevelOperation.Command.UPDATE;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.spark.sql.connector.distributions.Distribution;\n@@ -40,11 +42,12 @@\n import org.apache.spark.sql.connector.expressions.SortDirection;\n import org.apache.spark.sql.connector.expressions.SortOrder;\n import org.apache.spark.sql.connector.write.RowLevelOperation.Command;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSparkDistributionAndOrderingUtil extends SparkTestBaseWithCatalog {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkDistributionAndOrderingUtil extends TestBaseWithCatalog {\n \n   private static final Distribution UNSPECIFIED_DISTRIBUTION = Distributions.unspecified();\n   private static final Distribution FILE_CLUSTERED_DISTRIBUTION =\n@@ -100,7 +103,7 @@ public class TestSparkDistributionAndOrderingUtil extends SparkTestBaseWithCatal\n             Expressions.column(MetadataColumns.ROW_POSITION.name()), SortDirection.ASCENDING)\n       };\n \n-  @After\n+  @AfterEach\n   public void dropTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n@@ -141,7 +144,7 @@ public void dropTable() {\n   // write mode is HASH -> CLUSTER BY date + LOCALLY ORDER BY date, id\n   // write mode is RANGE -> ORDER BY date, id\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultWriteUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -150,7 +153,7 @@ public void testDefaultWriteUnpartitionedUnsortedTable() {\n     checkWriteDistributionAndOrdering(table, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashWriteUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -161,7 +164,7 @@ public void testHashWriteUnpartitionedUnsortedTable() {\n     checkWriteDistributionAndOrdering(table, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeWriteUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -172,7 +175,7 @@ public void testRangeWriteUnpartitionedUnsortedTable() {\n     checkWriteDistributionAndOrdering(table, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultWriteUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -191,7 +194,7 @@ public void testDefaultWriteUnpartitionedSortedTable() {\n     checkWriteDistributionAndOrdering(table, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashWriteUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -210,7 +213,7 @@ public void testHashWriteUnpartitionedSortedTable() {\n     checkWriteDistributionAndOrdering(table, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeWriteUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -231,7 +234,7 @@ public void testRangeWriteUnpartitionedSortedTable() {\n     checkWriteDistributionAndOrdering(table, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultWritePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -254,7 +257,7 @@ public void testDefaultWritePartitionedUnsortedTable() {\n     checkWriteDistributionAndOrdering(table, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultWritePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -273,7 +276,7 @@ public void testDefaultWritePartitionedUnsortedTableFanout() {\n     checkWriteDistributionAndOrdering(table, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashWritePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -298,7 +301,7 @@ public void testHashWritePartitionedUnsortedTable() {\n     checkWriteDistributionAndOrdering(table, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashWritePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -321,7 +324,7 @@ public void testHashWritePartitionedUnsortedTableFanout() {\n     checkWriteDistributionAndOrdering(table, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeWritePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -344,7 +347,7 @@ public void testRangeWritePartitionedUnsortedTable() {\n     checkWriteDistributionAndOrdering(table, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeWritePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -371,7 +374,7 @@ public void testRangeWritePartitionedUnsortedTableFanout() {\n     checkWriteDistributionAndOrdering(table, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultWritePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -394,7 +397,7 @@ public void testDefaultWritePartitionedSortedTable() {\n     checkWriteDistributionAndOrdering(table, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashWritePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -422,7 +425,7 @@ public void testHashWritePartitionedSortedTable() {\n     checkWriteDistributionAndOrdering(table, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeWritePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -445,7 +448,7 @@ public void testRangeWritePartitionedSortedTable() {\n     checkWriteDistributionAndOrdering(table, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeWritePartitionedSortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -506,7 +509,7 @@ public void testRangeWritePartitionedSortedTableFanout() {\n   // delete mode is HASH -> CLUSTER BY date + LOCALLY ORDER BY date, id\n   // delete mode is RANGE -> ORDER BY date, id\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteDeleteUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -516,7 +519,7 @@ public void testDefaultCopyOnWriteDeleteUnpartitionedUnsortedTable() {\n         table, DELETE, FILE_CLUSTERED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteDeleteUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -528,7 +531,7 @@ public void testNoneCopyOnWriteDeleteUnpartitionedUnsortedTable() {\n         table, DELETE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteDeleteUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -540,7 +543,7 @@ public void testHashCopyOnWriteDeleteUnpartitionedUnsortedTable() {\n         table, DELETE, FILE_CLUSTERED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteDeleteUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -553,7 +556,7 @@ public void testRangeCopyOnWriteDeleteUnpartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, DELETE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteDeleteUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -571,7 +574,7 @@ public void testDefaultCopyOnWriteDeleteUnpartitionedSortedTable() {\n         table, DELETE, FILE_CLUSTERED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteDeleteUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -591,7 +594,7 @@ public void testNoneCopyOnWriteDeleteUnpartitionedSortedTable() {\n         table, DELETE, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteDeleteUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -611,7 +614,7 @@ public void testHashCopyOnWriteDeleteUnpartitionedSortedTable() {\n         table, DELETE, FILE_CLUSTERED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteDeleteUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -632,7 +635,7 @@ public void testRangeCopyOnWriteDeleteUnpartitionedSortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, DELETE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteDeletePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -655,7 +658,7 @@ public void testDefaultCopyOnWriteDeletePartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, DELETE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteDeletePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -674,7 +677,7 @@ public void testDefaultCopyOnWriteDeletePartitionedUnsortedTableFanout() {\n     checkCopyOnWriteDistributionAndOrdering(table, DELETE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteDeletePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -696,7 +699,7 @@ public void testNoneCopyOnWriteDeletePartitionedUnsortedTable() {\n         table, DELETE, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteDeletePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -716,7 +719,7 @@ public void testNoneCopyOnWriteDeletePartitionedUnsortedTableFanout() {\n         table, DELETE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteDeletePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -741,7 +744,7 @@ public void testHashCopyOnWriteDeletePartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, DELETE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteDeletePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -764,7 +767,7 @@ public void testHashCopyOnWriteDeletePartitionedUnsortedTableFanout() {\n     checkCopyOnWriteDistributionAndOrdering(table, DELETE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteDeletePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -787,7 +790,7 @@ public void testRangeCopyOnWriteDeletePartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, DELETE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteDeletePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -813,7 +816,7 @@ public void testRangeCopyOnWriteDeletePartitionedUnsortedTableFanout() {\n     checkCopyOnWriteDistributionAndOrdering(table, DELETE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteDeletePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -837,7 +840,7 @@ public void testDefaultCopyOnWriteDeletePartitionedSortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, DELETE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteDeletePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -861,7 +864,7 @@ public void testNoneCopyOnWriteDeletePartitionedSortedTable() {\n         table, DELETE, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteDeletePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -889,7 +892,7 @@ public void testHashCopyOnWriteDeletePartitionedSortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, DELETE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteDeletePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -950,7 +953,7 @@ public void testRangeCopyOnWriteDeletePartitionedSortedTable() {\n   // update mode is HASH -> CLUSTER BY date + LOCALLY ORDER BY date, id\n   // update mode is RANGE -> ORDER BY date, id\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteUpdateUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -960,7 +963,7 @@ public void testDefaultCopyOnWriteUpdateUnpartitionedUnsortedTable() {\n         table, UPDATE, FILE_CLUSTERED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteUpdateUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -972,7 +975,7 @@ public void testNoneCopyOnWriteUpdateUnpartitionedUnsortedTable() {\n         table, UPDATE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteUpdateUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -984,7 +987,7 @@ public void testHashCopyOnWriteUpdateUnpartitionedUnsortedTable() {\n         table, UPDATE, FILE_CLUSTERED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteUpdateUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -997,7 +1000,7 @@ public void testRangeCopyOnWriteUpdateUnpartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, UPDATE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteUpdateUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1015,7 +1018,7 @@ public void testDefaultCopyOnWriteUpdateUnpartitionedSortedTable() {\n         table, UPDATE, FILE_CLUSTERED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteUpdateUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1035,7 +1038,7 @@ public void testNoneCopyOnWriteUpdateUnpartitionedSortedTable() {\n         table, UPDATE, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteUpdateUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1055,7 +1058,7 @@ public void testHashCopyOnWriteUpdateUnpartitionedSortedTable() {\n         table, UPDATE, FILE_CLUSTERED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteUpdateUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1076,7 +1079,7 @@ public void testRangeCopyOnWriteUpdateUnpartitionedSortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, UPDATE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteUpdatePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1099,7 +1102,7 @@ public void testDefaultCopyOnWriteUpdatePartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, UPDATE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteUpdatePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1118,7 +1121,7 @@ public void testDefaultCopyOnWriteUpdatePartitionedUnsortedTableFanout() {\n     checkCopyOnWriteDistributionAndOrdering(table, UPDATE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteUpdatePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1140,7 +1143,7 @@ public void testNoneCopyOnWriteUpdatePartitionedUnsortedTable() {\n         table, UPDATE, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteUpdatePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1160,7 +1163,7 @@ public void testNoneCopyOnWriteUpdatePartitionedUnsortedTableFanout() {\n         table, UPDATE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteUpdatePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1185,7 +1188,7 @@ public void testHashCopyOnWriteUpdatePartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, UPDATE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteUpdatePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1208,7 +1211,7 @@ public void testHashCopyOnWriteUpdatePartitionedUnsortedTableFanout() {\n     checkCopyOnWriteDistributionAndOrdering(table, UPDATE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteUpdatePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1231,7 +1234,7 @@ public void testRangeCopyOnWriteUpdatePartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, UPDATE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteUpdatePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1257,7 +1260,7 @@ public void testRangeCopyOnWriteUpdatePartitionedUnsortedTableFanout() {\n     checkCopyOnWriteDistributionAndOrdering(table, UPDATE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteUpdatePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1281,7 +1284,7 @@ public void testDefaultCopyOnWriteUpdatePartitionedSortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, UPDATE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteUpdatePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1305,7 +1308,7 @@ public void testNoneCopyOnWriteUpdatePartitionedSortedTable() {\n         table, UPDATE, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteUpdatePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1333,7 +1336,7 @@ public void testHashCopyOnWriteUpdatePartitionedSortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, UPDATE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteUpdatePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1394,7 +1397,7 @@ public void testRangeCopyOnWriteUpdatePartitionedSortedTable() {\n   // merge mode is HASH -> CLUSTER BY date + LOCALLY ORDER BY date, id\n   // merge mode is RANGE -> ORDERED BY date, id\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteMergeUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1403,7 +1406,7 @@ public void testDefaultCopyOnWriteMergeUnpartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteMergeUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1414,7 +1417,7 @@ public void testNoneCopyOnWriteMergeUnpartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteMergeUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1425,7 +1428,7 @@ public void testHashCopyOnWriteMergeUnpartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteMergeUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1436,7 +1439,7 @@ public void testRangeCopyOnWriteMergeUnpartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteMergeUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1455,7 +1458,7 @@ public void testDefaultCopyOnWriteMergeUnpartitionedSortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteMergeUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1475,7 +1478,7 @@ public void testNoneCopyOnWriteMergeUnpartitionedSortedTable() {\n         table, MERGE, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteMergeUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1495,7 +1498,7 @@ public void testHashCopyOnWriteMergeUnpartitionedSortedTable() {\n         table, MERGE, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteMergeUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1516,7 +1519,7 @@ public void testRangeCopyOnWriteMergeUnpartitionedSortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteMergePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1539,7 +1542,7 @@ public void testDefaultCopyOnWriteMergePartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteMergePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1558,7 +1561,7 @@ public void testDefaultCopyOnWriteMergePartitionedUnsortedTableFanout() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteMergePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1580,7 +1583,7 @@ public void testNoneCopyOnWriteMergePartitionedUnsortedTable() {\n         table, MERGE, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteMergePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1599,7 +1602,7 @@ public void testNoneCopyOnWriteMergePartitionedUnsortedTableFanout() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteMergePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1624,7 +1627,7 @@ public void testHashCopyOnWriteMergePartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteMergePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1647,7 +1650,7 @@ public void testHashCopyOnWriteMergePartitionedUnsortedTableFanout() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteMergePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1670,7 +1673,7 @@ public void testRangeCopyOnWriteMergePartitionedUnsortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteMergePartitionedUnsortedTableFanout() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1696,7 +1699,7 @@ public void testRangeCopyOnWriteMergePartitionedUnsortedTableFanout() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultCopyOnWriteMergePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1720,7 +1723,7 @@ public void testDefaultCopyOnWriteMergePartitionedSortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoneCopyOnWriteMergePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1744,7 +1747,7 @@ public void testNoneCopyOnWriteMergePartitionedSortedTable() {\n         table, MERGE, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashCopyOnWriteMergePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1772,7 +1775,7 @@ public void testHashCopyOnWriteMergePartitionedSortedTable() {\n     checkCopyOnWriteDistributionAndOrdering(table, MERGE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeCopyOnWriteMergePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1832,7 +1835,7 @@ public void testRangeCopyOnWriteMergePartitionedSortedTable() {\n   //                         LOCALLY ORDERED BY _spec_id, _partition, _file, _pos\n   // delete mode is RANGE (fanout) -> RANGE DISTRIBUTE BY _spec_id, _partition + empty ordering\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultPositionDeltaDeleteUnpartitionedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1850,7 +1853,7 @@ public void testDefaultPositionDeltaDeleteUnpartitionedTable() {\n         table, DELETE, SPEC_ID_PARTITION_FILE_CLUSTERED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNonePositionDeltaDeleteUnpartitionedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1867,7 +1870,7 @@ public void testNonePositionDeltaDeleteUnpartitionedTable() {\n         table, DELETE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashPositionDeltaDeleteUnpartitionedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1887,7 +1890,7 @@ public void testHashPositionDeltaDeleteUnpartitionedTable() {\n         table, DELETE, SPEC_ID_PARTITION_FILE_CLUSTERED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangePositionDeltaDeleteUnpartitionedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -1905,7 +1908,7 @@ public void testRangePositionDeltaDeleteUnpartitionedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, DELETE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultPositionDeltaDeletePartitionedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1927,7 +1930,7 @@ public void testDefaultPositionDeltaDeletePartitionedTable() {\n         table, DELETE, SPEC_ID_PARTITION_CLUSTERED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNonePositionDeltaDeletePartitionedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1948,7 +1951,7 @@ public void testNonePositionDeltaDeletePartitionedTable() {\n         table, DELETE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashPositionDeltaDeletePartitionedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -1972,7 +1975,7 @@ public void testHashPositionDeltaDeletePartitionedTable() {\n         table, DELETE, SPEC_ID_PARTITION_CLUSTERED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangePositionDeltaDeletePartitionedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -2057,7 +2060,7 @@ public void testRangePositionDeltaDeletePartitionedTable() {\n   // update mode is RANGE -> RANGE DISTRIBUTE BY _spec_id, _partition, date, id +\n   //                         LOCALLY ORDERED BY _spec_id, _partition, _file, _pos, date, id\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultPositionDeltaUpdateUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2075,7 +2078,7 @@ public void testDefaultPositionDeltaUpdateUnpartitionedUnsortedTable() {\n         table, UPDATE, SPEC_ID_PARTITION_FILE_CLUSTERED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNonePositionDeltaUpdateUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2092,7 +2095,7 @@ public void testNonePositionDeltaUpdateUnpartitionedUnsortedTable() {\n         table, UPDATE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashPositionDeltaUpdateUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2112,7 +2115,7 @@ public void testHashPositionDeltaUpdateUnpartitionedUnsortedTable() {\n         table, UPDATE, SPEC_ID_PARTITION_FILE_CLUSTERED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangePositionDeltaUpdateUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2130,7 +2133,7 @@ public void testRangePositionDeltaUpdateUnpartitionedUnsortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, UPDATE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultPositionDeltaUpdateUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2156,7 +2159,7 @@ public void testDefaultPositionDeltaUpdateUnpartitionedSortedTable() {\n         table, UPDATE, SPEC_ID_PARTITION_FILE_CLUSTERED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNonePositionDeltaUpdateUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2184,7 +2187,7 @@ public void testNonePositionDeltaUpdateUnpartitionedSortedTable() {\n         table, UPDATE, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashPositionDeltaUpdateUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2212,7 +2215,7 @@ public void testHashPositionDeltaUpdateUnpartitionedSortedTable() {\n         table, UPDATE, SPEC_ID_PARTITION_FILE_CLUSTERED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangePositionDeltaUpdateUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2253,7 +2256,7 @@ public void testRangePositionDeltaUpdateUnpartitionedSortedTable() {\n         table, UPDATE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultPositionDeltaUpdatePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -2294,7 +2297,7 @@ public void testDefaultPositionDeltaUpdatePartitionedUnsortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, UPDATE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNonePositionDeltaUpdatePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -2329,7 +2332,7 @@ public void testNonePositionDeltaUpdatePartitionedUnsortedTable() {\n         table, UPDATE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashPositionDeltaUpdatePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -2372,7 +2375,7 @@ public void testHashPositionDeltaUpdatePartitionedUnsortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, UPDATE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangePositionDeltaUpdatePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -2417,7 +2420,7 @@ public void testRangePositionDeltaUpdatePartitionedUnsortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, UPDATE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultPositionDeltaUpdatePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -2457,7 +2460,7 @@ public void testDefaultPositionDeltaUpdatePartitionedSortedTable() {\n         table, UPDATE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNonePositionDeltaUpdatePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -2490,7 +2493,7 @@ public void testNonePositionDeltaUpdatePartitionedSortedTable() {\n         table, UPDATE, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashPositionDeltaUpdatePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -2532,7 +2535,7 @@ public void testHashPositionDeltaUpdatePartitionedSortedTable() {\n         table, UPDATE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangePositionDeltaUpdatePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -2641,7 +2644,7 @@ public void testRangePositionDeltaUpdatePartitionedSortedTable() {\n   // merge mode is RANGE -> RANGE DISTRIBUTE BY _spec_id, _partition, date, id\n   //                        LOCALLY ORDERED BY _spec_id, _partition, _file, _pos, date, id\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultPositionDeltaMergeUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2663,7 +2666,7 @@ public void testDefaultPositionDeltaMergeUnpartitionedUnsortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, MERGE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNonePositionDeltaMergeUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2680,7 +2683,7 @@ public void testNonePositionDeltaMergeUnpartitionedUnsortedTable() {\n         table, MERGE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashPositionDeltaMergeUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2704,7 +2707,7 @@ public void testHashPositionDeltaMergeUnpartitionedUnsortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, MERGE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangePositionDeltaMergeUnpartitionedUnsortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2731,7 +2734,7 @@ public void testRangePositionDeltaMergeUnpartitionedUnsortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, MERGE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultPositionDeltaMergeUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2764,7 +2767,7 @@ public void testDefaultPositionDeltaMergeUnpartitionedSortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, MERGE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNonePositionDeltaMergeUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2792,7 +2795,7 @@ public void testNonePositionDeltaMergeUnpartitionedSortedTable() {\n         table, MERGE, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashPositionDeltaMergeUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2827,7 +2830,7 @@ public void testHashPositionDeltaMergeUnpartitionedSortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, MERGE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangePositionDeltaMergeUnpartitionedSortedTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -2867,7 +2870,7 @@ public void testRangePositionDeltaMergeUnpartitionedSortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, MERGE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultPositionDeltaMergePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -2907,7 +2910,7 @@ public void testDefaultPositionDeltaMergePartitionedUnsortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, MERGE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNonePositionDeltaMergePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -2942,7 +2945,7 @@ public void testNonePositionDeltaMergePartitionedUnsortedTable() {\n         table, MERGE, UNSPECIFIED_DISTRIBUTION, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashPositionDeltaMergePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -2984,7 +2987,7 @@ public void testHashPositionDeltaMergePartitionedUnsortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, MERGE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangePositionDeltaMergePartitionedUnsortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -3028,7 +3031,7 @@ public void testRangePositionDeltaMergePartitionedUnsortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, MERGE, expectedDistribution, EMPTY_ORDERING);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNonePositionDeltaMergePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -3060,7 +3063,7 @@ public void testNonePositionDeltaMergePartitionedSortedTable() {\n         table, MERGE, UNSPECIFIED_DISTRIBUTION, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultPositionDeltaMergePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -3099,7 +3102,7 @@ public void testDefaultPositionDeltaMergePartitionedSortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, MERGE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashPositionDeltaMergePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -3140,7 +3143,7 @@ public void testHashPositionDeltaMergePartitionedSortedTable() {\n     checkPositionDeltaDistributionAndOrdering(table, MERGE, expectedDistribution, expectedOrdering);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangePositionDeltaMergePartitionedSortedTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -3189,10 +3192,10 @@ private void checkWriteDistributionAndOrdering(\n     SparkWriteRequirements requirements = writeConf.writeRequirements();\n \n     Distribution distribution = requirements.distribution();\n-    Assert.assertEquals(\"Distribution must match\", expectedDistribution, distribution);\n+    assertThat(distribution).as(\"Distribution must match\").isEqualTo(expectedDistribution);\n \n     SortOrder[] ordering = requirements.ordering();\n-    Assert.assertArrayEquals(\"Ordering must match\", expectedOrdering, ordering);\n+    assertThat(ordering).as(\"Ordering must match\").isEqualTo(expectedOrdering);\n   }\n \n   private void checkCopyOnWriteDistributionAndOrdering(\n@@ -3205,10 +3208,10 @@ private void checkCopyOnWriteDistributionAndOrdering(\n     SparkWriteRequirements requirements = writeConf.copyOnWriteRequirements(command);\n \n     Distribution distribution = requirements.distribution();\n-    Assert.assertEquals(\"Distribution must match\", expectedDistribution, distribution);\n+    assertThat(distribution).as(\"Distribution must match\").isEqualTo(expectedDistribution);\n \n     SortOrder[] ordering = requirements.ordering();\n-    Assert.assertArrayEquals(\"Ordering must match\", expectedOrdering, ordering);\n+    assertThat(ordering).as(\"Ordering must match\").isEqualTo(expectedOrdering);\n   }\n \n   private void checkPositionDeltaDistributionAndOrdering(\n@@ -3221,10 +3224,10 @@ private void checkPositionDeltaDistributionAndOrdering(\n     SparkWriteRequirements requirements = writeConf.positionDeltaRequirements(command);\n \n     Distribution distribution = requirements.distribution();\n-    Assert.assertEquals(\"Distribution must match\", expectedDistribution, distribution);\n+    assertThat(distribution).as(\"Distribution must match\").isEqualTo(expectedDistribution);\n \n     SortOrder[] ordering = requirements.ordering();\n-    Assert.assertArrayEquals(\"Ordering must match\", expectedOrdering, ordering);\n+    assertThat(ordering).as(\"Ordering must match\").isEqualTo(expectedOrdering);\n   }\n \n   private void enableFanoutWriters(Table table) {\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkExecutorCache.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkExecutorCache.java\nindex 0d523b659cc1..18a6ae927bad 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkExecutorCache.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkExecutorCache.java\n@@ -44,6 +44,8 @@\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.Files;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -74,18 +76,16 @@\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.storage.memory.MemoryStore;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.Test;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-import org.junit.runners.Parameterized.Parameters;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-@RunWith(Parameterized.class)\n-public class TestSparkExecutorCache extends SparkTestBaseWithCatalog {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkExecutorCache extends TestBaseWithCatalog {\n \n   @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n-  public static Object[][] parameters() {\n+  protected static Object[][] parameters() {\n     return new Object[][] {\n       {\n         \"testhive\",\n@@ -109,32 +109,27 @@ public static Object[][] parameters() {\n   private String targetTableName;\n   private TableIdentifier targetTableIdent;\n \n-  public TestSparkExecutorCache(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Before\n+  @BeforeEach\n   public void configureTargetTableName() {\n     String name = \"target_exec_cache_\" + JOB_COUNTER.incrementAndGet();\n     this.targetTableName = tableName(name);\n     this.targetTableIdent = TableIdentifier.of(Namespace.of(\"default\"), name);\n   }\n \n-  @After\n+  @AfterEach\n   public void releaseResources() {\n     sql(\"DROP TABLE IF EXISTS %s\", targetTableName);\n     sql(\"DROP TABLE IF EXISTS %s\", UPDATES_VIEW_NAME);\n     INPUT_FILES.clear();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCacheValueWeightOverflow() {\n     CacheValue cacheValue = new CacheValue(\"v\", Integer.MAX_VALUE + 1L);\n     assertThat(cacheValue.weight()).isEqualTo(Integer.MAX_VALUE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCacheEnabledConfig() {\n     withSQLConf(\n         ImmutableMap.of(SparkSQLProperties.EXECUTOR_CACHE_ENABLED, \"true\"),\n@@ -151,7 +146,7 @@ public void testCacheEnabledConfig() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimeoutConfig() {\n     withSQLConf(\n         ImmutableMap.of(SparkSQLProperties.EXECUTOR_CACHE_TIMEOUT, \"10s\"),\n@@ -168,7 +163,7 @@ public void testTimeoutConfig() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMaxEntrySizeConfig() {\n     withSQLConf(\n         ImmutableMap.of(SparkSQLProperties.EXECUTOR_CACHE_MAX_ENTRY_SIZE, \"128\"),\n@@ -178,7 +173,7 @@ public void testMaxEntrySizeConfig() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMaxTotalSizeConfig() {\n     withSQLConf(\n         ImmutableMap.of(SparkSQLProperties.EXECUTOR_CACHE_MAX_TOTAL_SIZE, \"512\"),\n@@ -188,7 +183,7 @@ public void testMaxTotalSizeConfig() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testConcurrentAccess() throws InterruptedException {\n     SparkExecutorCache cache = SparkExecutorCache.getOrCreate();\n \n@@ -253,12 +248,12 @@ public void testConcurrentAccess() throws InterruptedException {\n     assertThat(liveKeys).noneMatch(key -> key.startsWith(table1) || key.startsWith(table2));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteDelete() throws Exception {\n     checkDelete(COPY_ON_WRITE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadDelete() throws Exception {\n     checkDelete(MERGE_ON_READ);\n   }\n@@ -344,13 +339,13 @@ private DeleteFile writeEqDeletes(Table table, String col, Object... values) thr\n       deletes.add(delete.copy(col, value));\n     }\n \n-    OutputFile out = Files.localOutput(temp.newFile(\"eq-deletes-\" + UUID.randomUUID()));\n+    OutputFile out = Files.localOutput(new File(temp.toFile(), \"eq-deletes-\" + UUID.randomUUID()));\n     return FileHelpers.writeDeleteFile(table, out, null, deletes, deleteSchema);\n   }\n \n   private Pair<DeleteFile, CharSequenceSet> writePosDeletes(\n       Table table, List<Pair<CharSequence, Long>> deletes) throws IOException {\n-    OutputFile out = Files.localOutput(temp.newFile(\"pos-deletes-\" + UUID.randomUUID()));\n+    OutputFile out = Files.localOutput(new File(temp.toFile(), \"pos-deletes-\" + UUID.randomUUID()));\n     return FileHelpers.writeDeleteFile(table, out, null, deletes);\n   }\n \n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\nindex 47a0e87b9398..72dd85bed390 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\n@@ -24,10 +24,11 @@\n \n import java.math.BigDecimal;\n import java.util.List;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.spark.Spark3Util;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.AnalysisException;\n@@ -38,23 +39,24 @@\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n import org.apache.spark.sql.internal.SQLConf;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n-\n-public class TestDataFrameWriterV2 extends SparkTestBaseWithCatalog {\n-  @Before\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestDataFrameWriterV2 extends TestBaseWithCatalog {\n+  @BeforeEach\n   public void createTable() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeSchemaFailsWithoutWriterOption() throws Exception {\n     sql(\n         \"ALTER TABLE %s SET TBLPROPERTIES ('%s'='true')\",\n@@ -86,7 +88,7 @@ public void testMergeSchemaFailsWithoutWriterOption() throws Exception {\n         .hasMessage(\"Field new_col not found in source schema\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeSchemaWithoutAcceptAnySchema() throws Exception {\n     Dataset<Row> twoColDF =\n         jsonToDF(\n@@ -113,7 +115,7 @@ public void testMergeSchemaWithoutAcceptAnySchema() throws Exception {\n             \"Cannot write to 'testhadoop.default.table', too many data columns\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeSchemaSparkProperty() throws Exception {\n     sql(\n         \"ALTER TABLE %s SET TBLPROPERTIES ('%s'='true')\",\n@@ -147,7 +149,7 @@ public void testMergeSchemaSparkProperty() throws Exception {\n         sql(\"select * from %s order by id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeSchemaIcebergProperty() throws Exception {\n     sql(\n         \"ALTER TABLE %s SET TBLPROPERTIES ('%s'='true')\",\n@@ -181,7 +183,7 @@ public void testMergeSchemaIcebergProperty() throws Exception {\n         sql(\"select * from %s order by id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWriteWithCaseSensitiveOption() throws NoSuchTableException, ParseException {\n     SparkSession sparkSession = spark.cloneSession();\n     sparkSession\n@@ -205,16 +207,16 @@ public void testWriteWithCaseSensitiveOption() throws NoSuchTableException, Pars\n     List<Types.NestedField> fields =\n         Spark3Util.loadIcebergTable(sparkSession, tableName).schema().asStruct().fields();\n     // Additional columns should not be created\n-    Assert.assertEquals(2, fields.size());\n+    assertThat(fields).hasSize(2);\n \n     // enable spark.sql.caseSensitive\n     sparkSession.sql(String.format(\"SET %s=true\", SQLConf.CASE_SENSITIVE().key()));\n     ds.writeTo(tableName).option(\"merge-schema\", \"true\").option(\"check-ordering\", \"false\").append();\n     fields = Spark3Util.loadIcebergTable(sparkSession, tableName).schema().asStruct().fields();\n-    Assert.assertEquals(4, fields.size());\n+    assertThat(fields).hasSize(4);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeSchemaIgnoreCastingLongToInt() throws Exception {\n     sql(\n         \"ALTER TABLE %s SET TBLPROPERTIES ('%s'='true')\",\n@@ -254,7 +256,7 @@ public void testMergeSchemaIgnoreCastingLongToInt() throws Exception {\n     assertThat(idField.type().typeId()).isEqualTo(Type.TypeID.LONG);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeSchemaIgnoreCastingDoubleToFloat() throws Exception {\n     removeTables();\n     sql(\"CREATE TABLE %s (id double, data string) USING iceberg\", tableName);\n@@ -296,7 +298,7 @@ public void testMergeSchemaIgnoreCastingDoubleToFloat() throws Exception {\n     assertThat(idField.type().typeId()).isEqualTo(Type.TypeID.DOUBLE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeSchemaIgnoreCastingDecimalToDecimalWithNarrowerPrecision() throws Exception {\n     removeTables();\n     sql(\"CREATE TABLE %s (id decimal(6,2), data string) USING iceberg\", tableName);\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2Coercion.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2Coercion.java\nindex efb6352ce8ba..f51a06853a69 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2Coercion.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2Coercion.java\n@@ -19,38 +19,50 @@\n package org.apache.iceberg.spark.source;\n \n import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.spark.SparkCatalogConfig;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n-import org.junit.Test;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-@RunWith(Parameterized.class)\n-public class TestDataFrameWriterV2Coercion extends SparkTestBaseWithCatalog {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestDataFrameWriterV2Coercion extends TestBaseWithCatalog {\n \n-  private final FileFormat format;\n-  private final String dataType;\n-\n-  public TestDataFrameWriterV2Coercion(FileFormat format, String dataType) {\n-    this.format = format;\n-    this.dataType = dataType;\n-  }\n-\n-  @Parameterized.Parameters(name = \"format = {0}, dataType = {1}\")\n+  @Parameters(\n+      name = \"catalogName = {0}, implementation = {1}, config = {2}, format = {3}, dataType = {4}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      new Object[] {FileFormat.AVRO, \"byte\"},\n-      new Object[] {FileFormat.ORC, \"byte\"},\n-      new Object[] {FileFormat.PARQUET, \"byte\"},\n-      new Object[] {FileFormat.AVRO, \"short\"},\n-      new Object[] {FileFormat.ORC, \"short\"},\n-      new Object[] {FileFormat.PARQUET, \"short\"}\n+      parameter(FileFormat.AVRO, \"byte\"),\n+      parameter(FileFormat.ORC, \"byte\"),\n+      parameter(FileFormat.PARQUET, \"byte\"),\n+      parameter(FileFormat.AVRO, \"short\"),\n+      parameter(FileFormat.ORC, \"short\"),\n+      parameter(FileFormat.PARQUET, \"short\")\n+    };\n+  }\n+\n+  private static Object[] parameter(FileFormat fileFormat, String dataType) {\n+    return new Object[] {\n+      SparkCatalogConfig.HADOOP.catalogName(),\n+      SparkCatalogConfig.HADOOP.implementation(),\n+      SparkCatalogConfig.HADOOP.properties(),\n+      fileFormat,\n+      dataType\n     };\n   }\n \n-  @Test\n+  @Parameter(index = 3)\n+  private FileFormat format;\n+\n+  @Parameter(index = 4)\n+  private String dataType;\n+\n+  @TestTemplate\n   public void testByteAndShortCoercion() {\n \n     Dataset<Row> df =\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogCacheExpiration.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogCacheExpiration.java\nindex f94d9fde1989..f16db1972e7b 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogCacheExpiration.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogCacheExpiration.java\n@@ -23,19 +23,21 @@\n import java.util.Map;\n import org.apache.iceberg.CachingCatalog;\n import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.spark.SparkCatalog;\n import org.apache.iceberg.spark.SparkSessionCatalog;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.spark.sql.connector.catalog.TableCatalog;\n-import org.junit.BeforeClass;\n-import org.junit.Test;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSparkCatalogCacheExpiration extends SparkTestBaseWithCatalog {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkCatalogCacheExpiration extends TestBaseWithCatalog {\n \n-  private static final String SESSION_CATALOG_NAME = \"spark_catalog\";\n-  private static final String SESSION_CATALOG_IMPL = SparkSessionCatalog.class.getName();\n   private static final Map<String, String> SESSION_CATALOG_CONFIG =\n       ImmutableMap.of(\n           \"type\",\n@@ -47,6 +49,13 @@ public class TestSparkCatalogCacheExpiration extends SparkTestBaseWithCatalog {\n           CatalogProperties.CACHE_EXPIRATION_INTERVAL_MS,\n           \"3000\");\n \n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+      {\"spark_catalog\", SparkSessionCatalog.class.getName(), SESSION_CATALOG_CONFIG},\n+    };\n+  }\n+\n   private static String asSqlConfCatalogKeyFor(String catalog, String configKey) {\n     // configKey is empty when the catalog's class is being defined\n     if (configKey.isEmpty()) {\n@@ -58,7 +67,7 @@ private static String asSqlConfCatalogKeyFor(String catalog, String configKey) {\n \n   // Add more catalogs to the spark session, so we only need to start spark one time for multiple\n   // different catalog configuration tests.\n-  @BeforeClass\n+  @BeforeAll\n   public static void beforeClass() {\n     // Catalog - expiration_disabled: Catalog with caching on and expiration disabled.\n     ImmutableMap.of(\n@@ -87,11 +96,7 @@ public static void beforeClass() {\n             (k, v) -> spark.conf().set(asSqlConfCatalogKeyFor(\"cache_disabled_implicitly\", k), v));\n   }\n \n-  public TestSparkCatalogCacheExpiration() {\n-    super(SESSION_CATALOG_NAME, SESSION_CATALOG_IMPL, SESSION_CATALOG_CONFIG);\n-  }\n-\n-  @Test\n+  @TestTemplate\n   public void testSparkSessionCatalogWithExpirationEnabled() {\n     SparkSessionCatalog<?> sparkCatalog = sparkSessionCatalog();\n     assertThat(sparkCatalog)\n@@ -112,7 +117,7 @@ public void testSparkSessionCatalogWithExpirationEnabled() {\n             });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCacheEnabledAndExpirationDisabled() {\n     SparkCatalog sparkCatalog = getSparkCatalog(\"expiration_disabled\");\n     assertThat(sparkCatalog).extracting(\"cacheEnabled\").isEqualTo(true);\n@@ -126,7 +131,7 @@ public void testCacheEnabledAndExpirationDisabled() {\n             });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCacheDisabledImplicitly() {\n     SparkCatalog sparkCatalog = getSparkCatalog(\"cache_disabled_implicitly\");\n     assertThat(sparkCatalog).extracting(\"cacheEnabled\").isEqualTo(false);\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPlanningUtil.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPlanningUtil.java\nindex 76f47c8c35d7..e4602532f029 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPlanningUtil.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPlanningUtil.java\n@@ -28,6 +28,7 @@\n import org.apache.iceberg.DataTask;\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.MockFileScanTask;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.ScanTask;\n import org.apache.iceberg.ScanTaskGroup;\n@@ -36,12 +37,14 @@\n import org.apache.iceberg.TestHelpers.Row;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.iceberg.types.Types;\n-import org.junit.Test;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n import org.mockito.Mockito;\n \n-public class TestSparkPlanningUtil extends SparkTestBaseWithCatalog {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkPlanningUtil extends TestBaseWithCatalog {\n \n   private static final Schema SCHEMA =\n       new Schema(\n@@ -55,7 +58,7 @@ public class TestSparkPlanningUtil extends SparkTestBaseWithCatalog {\n   private static final List<String> EXECUTOR_LOCATIONS =\n       ImmutableList.of(\"host1_exec1\", \"host1_exec2\", \"host1_exec3\", \"host2_exec1\", \"host2_exec2\");\n \n-  @Test\n+  @TestTemplate\n   public void testFileScanTaskWithoutDeletes() {\n     List<ScanTask> tasks =\n         ImmutableList.of(\n@@ -68,11 +71,10 @@ public void testFileScanTaskWithoutDeletes() {\n     String[][] locations = SparkPlanningUtil.assignExecutors(taskGroups, EXECUTOR_LOCATIONS);\n \n     // should not assign executors if there are no deletes\n-    assertThat(locations.length).isEqualTo(1);\n-    assertThat(locations[0]).isEmpty();\n+    assertThat(locations).hasDimensions(1, 0);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFileScanTaskWithDeletes() {\n     StructLike partition1 = Row.of(\"k2\", null);\n     StructLike partition2 = Row.of(\"k1\");\n@@ -94,7 +96,7 @@ public void testFileScanTaskWithDeletes() {\n     assertThat(locations[0].length).isGreaterThanOrEqualTo(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFileScanTaskWithUnpartitionedDeletes() {\n     List<ScanTask> tasks1 =\n         ImmutableList.of(\n@@ -137,12 +139,10 @@ public void testFileScanTaskWithUnpartitionedDeletes() {\n     String[][] locations = SparkPlanningUtil.assignExecutors(taskGroups, EXECUTOR_LOCATIONS);\n \n     // should not assign executors if the table is unpartitioned\n-    assertThat(locations.length).isEqualTo(2);\n-    assertThat(locations[0]).isEmpty();\n-    assertThat(locations[1]).isEmpty();\n+    assertThat(locations).hasDimensions(2, 0);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDataTasks() {\n     List<ScanTask> tasks =\n         ImmutableList.of(\n@@ -155,11 +155,10 @@ public void testDataTasks() {\n     String[][] locations = SparkPlanningUtil.assignExecutors(taskGroups, EXECUTOR_LOCATIONS);\n \n     // should not assign executors for data tasks\n-    assertThat(locations.length).isEqualTo(1);\n-    assertThat(locations[0]).isEmpty();\n+    assertThat(locations).hasDimensions(1, 0);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnknownTasks() {\n     List<ScanTask> tasks = ImmutableList.of(new UnknownScanTask(), new UnknownScanTask());\n     ScanTaskGroup<ScanTask> taskGroup = new BaseScanTaskGroup<>(tasks);\n@@ -168,8 +167,7 @@ public void testUnknownTasks() {\n     String[][] locations = SparkPlanningUtil.assignExecutors(taskGroups, EXECUTOR_LOCATIONS);\n \n     // should not assign executors for unknown tasks\n-    assertThat(locations.length).isEqualTo(1);\n-    assertThat(locations[0]).isEmpty();\n+    assertThat(locations).hasDimensions(1, 0);\n   }\n \n   private static DataFile mockDataFile(StructLike partition) {\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadMetrics.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadMetrics.java\nindex d5d51c27a0f6..57b2c7146b79 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadMetrics.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadMetrics.java\n@@ -23,24 +23,27 @@\n \n import java.util.List;\n import java.util.Map;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.execution.SparkPlan;\n import org.apache.spark.sql.execution.metric.SQLMetric;\n-import org.junit.After;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n import scala.collection.JavaConverters;\n \n-public class TestSparkReadMetrics extends SparkTestBaseWithCatalog {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkReadMetrics extends TestBaseWithCatalog {\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadMetricsForV1Table() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (id BIGINT) USING iceberg TBLPROPERTIES ('format-version'='1')\",\n@@ -57,33 +60,65 @@ public void testReadMetricsForV1Table() throws NoSuchTableException {\n     Map<String, SQLMetric> metricsMap =\n         JavaConverters.mapAsJavaMapConverter(sparkPlans.get(0).metrics()).asJava();\n     // Common\n-    assertThat(metricsMap.get(\"totalPlanningDuration\").value()).isNotEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalPlanningDuration\", sqlMetric -> assertThat(sqlMetric.value()).isNotEqualTo(0));\n \n     // data manifests\n-    assertThat(metricsMap.get(\"totalDataManifest\").value()).isEqualTo(2);\n-    assertThat(metricsMap.get(\"scannedDataManifests\").value()).isEqualTo(2);\n-    assertThat(metricsMap.get(\"skippedDataManifests\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDataManifest\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(2));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"scannedDataManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(2));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDataManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n \n     // data files\n-    assertThat(metricsMap.get(\"resultDataFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"skippedDataFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"totalDataFileSize\").value()).isNotEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"resultDataFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDataFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDataFileSize\", sqlMetric -> assertThat(sqlMetric.value()).isNotEqualTo(0));\n \n     // delete manifests\n-    assertThat(metricsMap.get(\"totalDeleteManifests\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"scannedDeleteManifests\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"skippedDeleteManifests\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"scannedDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n \n     // delete files\n-    assertThat(metricsMap.get(\"totalDeleteFileSize\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"resultDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"equalityDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"indexedDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"positionalDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"skippedDeleteFiles\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDeleteFileSize\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"resultDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"equalityDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"indexedDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"positionalDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadMetricsForV2Table() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (id BIGINT) USING iceberg TBLPROPERTIES ('format-version'='2')\",\n@@ -101,33 +136,65 @@ public void testReadMetricsForV2Table() throws NoSuchTableException {\n         JavaConverters.mapAsJavaMapConverter(sparkPlans.get(0).metrics()).asJava();\n \n     // Common\n-    assertThat(metricsMap.get(\"totalPlanningDuration\").value()).isNotEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalPlanningDuration\", sqlMetric -> assertThat(sqlMetric.value()).isNotEqualTo(0));\n \n     // data manifests\n-    assertThat(metricsMap.get(\"totalDataManifest\").value()).isEqualTo(2);\n-    assertThat(metricsMap.get(\"scannedDataManifests\").value()).isEqualTo(2);\n-    assertThat(metricsMap.get(\"skippedDataManifests\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDataManifest\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(2));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"scannedDataManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(2));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDataManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n \n     // data files\n-    assertThat(metricsMap.get(\"resultDataFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"skippedDataFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"totalDataFileSize\").value()).isNotEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"resultDataFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDataFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDataFileSize\", sqlMetric -> assertThat(sqlMetric.value()).isNotEqualTo(0));\n \n     // delete manifests\n-    assertThat(metricsMap.get(\"totalDeleteManifests\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"scannedDeleteManifests\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"skippedDeleteManifests\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"scannedDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n \n     // delete files\n-    assertThat(metricsMap.get(\"totalDeleteFileSize\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"resultDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"equalityDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"indexedDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"positionalDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"skippedDeleteFiles\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDeleteFileSize\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"resultDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"equalityDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"indexedDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"positionalDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteMetrics() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (id BIGINT)\"\n@@ -152,29 +219,61 @@ public void testDeleteMetrics() throws NoSuchTableException {\n         JavaConverters.mapAsJavaMapConverter(sparkPlans.get(0).metrics()).asJava();\n \n     // Common\n-    assertThat(metricsMap.get(\"totalPlanningDuration\").value()).isNotEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalPlanningDuration\", sqlMetric -> assertThat(sqlMetric.value()).isNotEqualTo(0));\n \n     // data manifests\n-    assertThat(metricsMap.get(\"totalDataManifest\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"scannedDataManifests\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"skippedDataManifests\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDataManifest\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"scannedDataManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDataManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n \n     // data files\n-    assertThat(metricsMap.get(\"resultDataFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"skippedDataFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"totalDataFileSize\").value()).isNotEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"resultDataFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDataFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDataFileSize\", sqlMetric -> assertThat(sqlMetric.value()).isNotEqualTo(0));\n \n     // delete manifests\n-    assertThat(metricsMap.get(\"totalDeleteManifests\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"scannedDeleteManifests\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"skippedDeleteManifests\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"scannedDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n \n     // delete files\n-    assertThat(metricsMap.get(\"totalDeleteFileSize\").value()).isNotEqualTo(0);\n-    assertThat(metricsMap.get(\"resultDeleteFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"equalityDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"indexedDeleteFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"positionalDeleteFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"skippedDeleteFiles\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDeleteFileSize\", sqlMetric -> assertThat(sqlMetric.value()).isNotEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"resultDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"equalityDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"indexedDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"positionalDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkScan.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkScan.java\nindex 399619c8a75d..a248d4e12827 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkScan.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkScan.java\n@@ -33,6 +33,9 @@\n import java.util.Map;\n import org.apache.iceberg.GenericBlobMetadata;\n import org.apache.iceberg.GenericStatisticsFile;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n@@ -40,8 +43,9 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkCatalogConfig;\n import org.apache.iceberg.spark.SparkSQLProperties;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.iceberg.spark.functions.BucketFunction;\n import org.apache.iceberg.spark.functions.DaysFunction;\n import org.apache.iceberg.spark.functions.HoursFunction;\n@@ -70,40 +74,54 @@\n import org.apache.spark.sql.internal.SQLConf;\n import org.apache.spark.sql.types.DataTypes;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-@RunWith(Parameterized.class)\n-public class TestSparkScan extends SparkTestBaseWithCatalog {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkScan extends TestBaseWithCatalog {\n \n   private static final String DUMMY_BLOB_TYPE = \"sum-data-size-bytes-v1\";\n \n-  private final String format;\n-\n-  @Parameterized.Parameters(name = \"format = {0}\")\n-  public static Object[] parameters() {\n-    return new Object[] {\"parquet\", \"avro\", \"orc\"};\n-  }\n-\n-  public TestSparkScan(String format) {\n-    this.format = format;\n+  @Parameter(index = 3)\n+  private String format;\n+\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}, format = {3}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties(),\n+        \"parquet\"\n+      },\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties(),\n+        \"avro\"\n+      },\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties(),\n+        \"orc\"\n+      }\n+    };\n   }\n \n-  @Before\n+  @BeforeEach\n   public void useCatalog() {\n     sql(\"USE %s\", catalogName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testEstimatedRowCount() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, date DATE) USING iceberg TBLPROPERTIES('%s' = '%s')\",\n@@ -123,10 +141,10 @@ public void testEstimatedRowCount() throws NoSuchTableException {\n     SparkScan scan = (SparkScan) scanBuilder.build();\n     Statistics stats = scan.estimateStatistics();\n \n-    Assert.assertEquals(10000L, stats.numRows().getAsLong());\n+    assertThat(stats.numRows().getAsLong()).isEqualTo(10000L);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTableWithoutColStats() throws NoSuchTableException {\n     sql(\"CREATE TABLE %s (id int, data string) USING iceberg\", tableName);\n \n@@ -162,7 +180,7 @@ public void testTableWithoutColStats() throws NoSuchTableException {\n         reportColStatsEnabled, () -> checkColStatisticsReported(scan, 4L, Maps.newHashMap()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTableWithoutApacheDatasketchColStat() throws NoSuchTableException {\n     sql(\"CREATE TABLE %s (id int, data string) USING iceberg\", tableName);\n \n@@ -215,7 +233,7 @@ public void testTableWithoutApacheDatasketchColStat() throws NoSuchTableExceptio\n         reportColStatsEnabled, () -> checkColStatisticsReported(scan, 4L, Maps.newHashMap()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTableWithOneColStats() throws NoSuchTableException {\n     sql(\"CREATE TABLE %s (id int, data string) USING iceberg\", tableName);\n \n@@ -296,7 +314,7 @@ public void testTableWithOneColStats() throws NoSuchTableException {\n     withSQLConf(reportColStatsEnabled, () -> checkColStatisticsReported(scan, 6L, expectedNDV));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTableWithOneApacheDatasketchColStatAndOneDifferentColStat()\n       throws NoSuchTableException {\n     sql(\"CREATE TABLE %s (id int, data string) USING iceberg\", tableName);\n@@ -357,7 +375,7 @@ public void testTableWithOneApacheDatasketchColStatAndOneDifferentColStat()\n     withSQLConf(reportColStatsEnabled, () -> checkColStatisticsReported(scan, 4L, expectedOneNDV));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTableWithTwoColStats() throws NoSuchTableException {\n     sql(\"CREATE TABLE %s (id int, data string) USING iceberg\", tableName);\n \n@@ -418,7 +436,7 @@ public void testTableWithTwoColStats() throws NoSuchTableException {\n     withSQLConf(reportColStatsEnabled, () -> checkColStatisticsReported(scan, 4L, expectedTwoNDVs));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedYears() throws Exception {\n     createUnpartitionedTable(spark, tableName);\n \n@@ -444,10 +462,10 @@ public void testUnpartitionedYears() throws Exception {\n     scan = builder.build().toBatch();\n \n     // notEq can't be answered using column bounds because they are not exact\n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedYears() throws Exception {\n     createPartitionedTable(spark, tableName, \"years(ts)\");\n \n@@ -472,10 +490,10 @@ public void testPartitionedYears() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedMonths() throws Exception {\n     createUnpartitionedTable(spark, tableName);\n \n@@ -492,7 +510,7 @@ public void testUnpartitionedMonths() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n \n     // NOT GT\n     builder = scanBuilder();\n@@ -501,10 +519,10 @@ public void testUnpartitionedMonths() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedMonths() throws Exception {\n     createPartitionedTable(spark, tableName, \"months(ts)\");\n \n@@ -521,7 +539,7 @@ public void testPartitionedMonths() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n \n     // NOT GT\n     builder = scanBuilder();\n@@ -530,10 +548,10 @@ public void testPartitionedMonths() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedDays() throws Exception {\n     createUnpartitionedTable(spark, tableName);\n \n@@ -549,7 +567,7 @@ public void testUnpartitionedDays() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n \n     // NOT LT\n     builder = scanBuilder();\n@@ -558,10 +576,10 @@ public void testUnpartitionedDays() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedDays() throws Exception {\n     createPartitionedTable(spark, tableName, \"days(ts)\");\n \n@@ -577,7 +595,7 @@ public void testPartitionedDays() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n \n     // NOT LT\n     builder = scanBuilder();\n@@ -586,10 +604,10 @@ public void testPartitionedDays() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedHours() throws Exception {\n     createUnpartitionedTable(spark, tableName);\n \n@@ -605,7 +623,7 @@ public void testUnpartitionedHours() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(8);\n+    assertThat(scan.planInputPartitions()).hasSize(8);\n \n     // NOT GTEQ\n     builder = scanBuilder();\n@@ -614,10 +632,10 @@ public void testUnpartitionedHours() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(2);\n+    assertThat(scan.planInputPartitions()).hasSize(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedHours() throws Exception {\n     createPartitionedTable(spark, tableName, \"hours(ts)\");\n \n@@ -633,7 +651,7 @@ public void testPartitionedHours() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(8);\n+    assertThat(scan.planInputPartitions()).hasSize(8);\n \n     // NOT GTEQ\n     builder = scanBuilder();\n@@ -642,10 +660,10 @@ public void testPartitionedHours() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(2);\n+    assertThat(scan.planInputPartitions()).hasSize(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedBucketLong() throws Exception {\n     createUnpartitionedTable(spark, tableName);\n \n@@ -657,7 +675,7 @@ public void testUnpartitionedBucketLong() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n \n     // NOT GTEQ\n     builder = scanBuilder();\n@@ -666,10 +684,10 @@ public void testUnpartitionedBucketLong() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedBucketLong() throws Exception {\n     createPartitionedTable(spark, tableName, \"bucket(5, id)\");\n \n@@ -681,7 +699,7 @@ public void testPartitionedBucketLong() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(6);\n+    assertThat(scan.planInputPartitions()).hasSize(6);\n \n     // NOT GTEQ\n     builder = scanBuilder();\n@@ -690,10 +708,10 @@ public void testPartitionedBucketLong() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(4);\n+    assertThat(scan.planInputPartitions()).hasSize(4);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedBucketString() throws Exception {\n     createUnpartitionedTable(spark, tableName);\n \n@@ -705,7 +723,7 @@ public void testUnpartitionedBucketString() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n \n     // NOT LTEQ\n     builder = scanBuilder();\n@@ -714,10 +732,10 @@ public void testUnpartitionedBucketString() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedBucketString() throws Exception {\n     createPartitionedTable(spark, tableName, \"bucket(5, data)\");\n \n@@ -729,7 +747,7 @@ public void testPartitionedBucketString() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(6);\n+    assertThat(scan.planInputPartitions()).hasSize(6);\n \n     // NOT LTEQ\n     builder = scanBuilder();\n@@ -738,10 +756,10 @@ public void testPartitionedBucketString() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(4);\n+    assertThat(scan.planInputPartitions()).hasSize(4);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedTruncateString() throws Exception {\n     createUnpartitionedTable(spark, tableName);\n \n@@ -753,7 +771,7 @@ public void testUnpartitionedTruncateString() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n \n     // NOT NotEqual\n     builder = scanBuilder();\n@@ -762,10 +780,10 @@ public void testUnpartitionedTruncateString() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedTruncateString() throws Exception {\n     createPartitionedTable(spark, tableName, \"truncate(4, data)\");\n \n@@ -777,7 +795,7 @@ public void testPartitionedTruncateString() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n \n     // NOT NotEqual\n     builder = scanBuilder();\n@@ -786,10 +804,10 @@ public void testPartitionedTruncateString() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedIsNull() throws Exception {\n     createUnpartitionedTable(spark, tableName);\n \n@@ -801,7 +819,7 @@ public void testUnpartitionedIsNull() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(0);\n+    assertThat(scan.planInputPartitions()).isEmpty();\n \n     // NOT IsNull\n     builder = scanBuilder();\n@@ -810,10 +828,10 @@ public void testUnpartitionedIsNull() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedIsNull() throws Exception {\n     createPartitionedTable(spark, tableName, \"truncate(4, data)\");\n \n@@ -825,7 +843,7 @@ public void testPartitionedIsNull() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(0);\n+    assertThat(scan.planInputPartitions()).isEmpty();\n \n     // NOT IsNULL\n     builder = scanBuilder();\n@@ -834,10 +852,10 @@ public void testPartitionedIsNull() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedIsNotNull() throws Exception {\n     createUnpartitionedTable(spark, tableName);\n \n@@ -849,7 +867,7 @@ public void testUnpartitionedIsNotNull() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n \n     // NOT IsNotNull\n     builder = scanBuilder();\n@@ -858,10 +876,10 @@ public void testUnpartitionedIsNotNull() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(0);\n+    assertThat(scan.planInputPartitions()).isEmpty();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedIsNotNull() throws Exception {\n     createPartitionedTable(spark, tableName, \"truncate(4, data)\");\n \n@@ -873,7 +891,7 @@ public void testPartitionedIsNotNull() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n \n     // NOT IsNotNULL\n     builder = scanBuilder();\n@@ -882,10 +900,10 @@ public void testPartitionedIsNotNull() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(0);\n+    assertThat(scan.planInputPartitions()).isEmpty();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedAnd() throws Exception {\n     createUnpartitionedTable(spark, tableName);\n \n@@ -903,7 +921,7 @@ public void testUnpartitionedAnd() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n \n     // NOT (years(ts) = 47 AND bucket(id, 5) >= 2)\n     builder = scanBuilder();\n@@ -912,10 +930,10 @@ public void testUnpartitionedAnd() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedAnd() throws Exception {\n     createPartitionedTable(spark, tableName, \"years(ts), bucket(5, id)\");\n \n@@ -933,7 +951,7 @@ public void testPartitionedAnd() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(1);\n+    assertThat(scan.planInputPartitions()).hasSize(1);\n \n     // NOT (years(ts) = 47 AND bucket(id, 5) >= 2)\n     builder = scanBuilder();\n@@ -942,10 +960,10 @@ public void testPartitionedAnd() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(9);\n+    assertThat(scan.planInputPartitions()).hasSize(9);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedOr() throws Exception {\n     createUnpartitionedTable(spark, tableName);\n \n@@ -963,7 +981,7 @@ public void testUnpartitionedOr() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n \n     // NOT (years(ts) = 47 OR bucket(id, 5) >= 2)\n     builder = scanBuilder();\n@@ -972,10 +990,10 @@ public void testUnpartitionedOr() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedOr() throws Exception {\n     createPartitionedTable(spark, tableName, \"years(ts), bucket(5, id)\");\n \n@@ -993,7 +1011,7 @@ public void testPartitionedOr() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(6);\n+    assertThat(scan.planInputPartitions()).hasSize(6);\n \n     // NOT (years(ts) = 48 OR bucket(id, 5) >= 2)\n     builder = scanBuilder();\n@@ -1002,7 +1020,7 @@ public void testPartitionedOr() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(4);\n+    assertThat(scan.planInputPartitions()).hasSize(4);\n   }\n \n   private SparkScanBuilder scanBuilder() throws Exception {\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkDistributionAndOrderingUtil.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkDistributionAndOrderingUtil.java\nindex 39ef72c6bb1d..ca86350346cd 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkDistributionAndOrderingUtil.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkDistributionAndOrderingUtil.java\n@@ -32,6 +32,7 @@\n import static org.assertj.core.api.Assertions.assertThat;\n \n import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.spark.sql.connector.distributions.Distribution;\n@@ -43,7 +44,9 @@\n import org.apache.spark.sql.connector.write.RowLevelOperation.Command;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkDistributionAndOrderingUtil extends TestBaseWithCatalog {\n \n   private static final Distribution UNSPECIFIED_DISTRIBUTION = Distributions.unspecified();\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\nindex 7404b18d14b2..0a51d5b16f50 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\n@@ -24,6 +24,7 @@\n \n import java.math.BigDecimal;\n import java.util.List;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.spark.Spark3Util;\n@@ -41,7 +42,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestDataFrameWriterV2 extends TestBaseWithCatalog {\n   @BeforeEach\n   public void createTable() {\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogCacheExpiration.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogCacheExpiration.java\nindex 2a9bbca40f94..f16db1972e7b 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogCacheExpiration.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogCacheExpiration.java\n@@ -23,6 +23,7 @@\n import java.util.Map;\n import org.apache.iceberg.CachingCatalog;\n import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n@@ -32,7 +33,9 @@\n import org.apache.spark.sql.connector.catalog.TableCatalog;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkCatalogCacheExpiration extends TestBaseWithCatalog {\n \n   private static final Map<String, String> SESSION_CATALOG_CONFIG =\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPlanningUtil.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPlanningUtil.java\nindex 65c6790e5b49..e4602532f029 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPlanningUtil.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkPlanningUtil.java\n@@ -28,6 +28,7 @@\n import org.apache.iceberg.DataTask;\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.MockFileScanTask;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.ScanTask;\n import org.apache.iceberg.ScanTaskGroup;\n@@ -39,8 +40,10 @@\n import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.iceberg.types.Types;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n import org.mockito.Mockito;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkPlanningUtil extends TestBaseWithCatalog {\n \n   private static final Schema SCHEMA =\n@@ -68,8 +71,7 @@ public void testFileScanTaskWithoutDeletes() {\n     String[][] locations = SparkPlanningUtil.assignExecutors(taskGroups, EXECUTOR_LOCATIONS);\n \n     // should not assign executors if there are no deletes\n-    assertThat(locations.length).isEqualTo(1);\n-    assertThat(locations[0]).isEmpty();\n+    assertThat(locations).hasDimensions(1, 0);\n   }\n \n   @TestTemplate\n@@ -137,9 +139,7 @@ public void testFileScanTaskWithUnpartitionedDeletes() {\n     String[][] locations = SparkPlanningUtil.assignExecutors(taskGroups, EXECUTOR_LOCATIONS);\n \n     // should not assign executors if the table is unpartitioned\n-    assertThat(locations.length).isEqualTo(2);\n-    assertThat(locations[0]).isEmpty();\n-    assertThat(locations[1]).isEmpty();\n+    assertThat(locations).hasDimensions(2, 0);\n   }\n \n   @TestTemplate\n@@ -155,8 +155,7 @@ public void testDataTasks() {\n     String[][] locations = SparkPlanningUtil.assignExecutors(taskGroups, EXECUTOR_LOCATIONS);\n \n     // should not assign executors for data tasks\n-    assertThat(locations.length).isEqualTo(1);\n-    assertThat(locations[0]).isEmpty();\n+    assertThat(locations).hasDimensions(1, 0);\n   }\n \n   @TestTemplate\n@@ -168,8 +167,7 @@ public void testUnknownTasks() {\n     String[][] locations = SparkPlanningUtil.assignExecutors(taskGroups, EXECUTOR_LOCATIONS);\n \n     // should not assign executors for unknown tasks\n-    assertThat(locations.length).isEqualTo(1);\n-    assertThat(locations[0]).isEmpty();\n+    assertThat(locations).hasDimensions(1, 0);\n   }\n \n   private static DataFile mockDataFile(StructLike partition) {\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadMetrics.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadMetrics.java\nindex 895861e95948..57b2c7146b79 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadMetrics.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReadMetrics.java\n@@ -23,6 +23,7 @@\n \n import java.util.List;\n import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n@@ -31,8 +32,10 @@\n import org.apache.spark.sql.execution.metric.SQLMetric;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n import scala.collection.JavaConverters;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkReadMetrics extends TestBaseWithCatalog {\n \n   @AfterEach\n@@ -57,30 +60,62 @@ public void testReadMetricsForV1Table() throws NoSuchTableException {\n     Map<String, SQLMetric> metricsMap =\n         JavaConverters.mapAsJavaMapConverter(sparkPlans.get(0).metrics()).asJava();\n     // Common\n-    assertThat(metricsMap.get(\"totalPlanningDuration\").value()).isNotEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalPlanningDuration\", sqlMetric -> assertThat(sqlMetric.value()).isNotEqualTo(0));\n \n     // data manifests\n-    assertThat(metricsMap.get(\"totalDataManifest\").value()).isEqualTo(2);\n-    assertThat(metricsMap.get(\"scannedDataManifests\").value()).isEqualTo(2);\n-    assertThat(metricsMap.get(\"skippedDataManifests\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDataManifest\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(2));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"scannedDataManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(2));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDataManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n \n     // data files\n-    assertThat(metricsMap.get(\"resultDataFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"skippedDataFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"totalDataFileSize\").value()).isNotEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"resultDataFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDataFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDataFileSize\", sqlMetric -> assertThat(sqlMetric.value()).isNotEqualTo(0));\n \n     // delete manifests\n-    assertThat(metricsMap.get(\"totalDeleteManifests\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"scannedDeleteManifests\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"skippedDeleteManifests\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"scannedDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n \n     // delete files\n-    assertThat(metricsMap.get(\"totalDeleteFileSize\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"resultDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"equalityDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"indexedDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"positionalDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"skippedDeleteFiles\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDeleteFileSize\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"resultDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"equalityDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"indexedDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"positionalDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n   }\n \n   @TestTemplate\n@@ -101,30 +136,62 @@ public void testReadMetricsForV2Table() throws NoSuchTableException {\n         JavaConverters.mapAsJavaMapConverter(sparkPlans.get(0).metrics()).asJava();\n \n     // Common\n-    assertThat(metricsMap.get(\"totalPlanningDuration\").value()).isNotEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalPlanningDuration\", sqlMetric -> assertThat(sqlMetric.value()).isNotEqualTo(0));\n \n     // data manifests\n-    assertThat(metricsMap.get(\"totalDataManifest\").value()).isEqualTo(2);\n-    assertThat(metricsMap.get(\"scannedDataManifests\").value()).isEqualTo(2);\n-    assertThat(metricsMap.get(\"skippedDataManifests\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDataManifest\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(2));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"scannedDataManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(2));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDataManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n \n     // data files\n-    assertThat(metricsMap.get(\"resultDataFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"skippedDataFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"totalDataFileSize\").value()).isNotEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"resultDataFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDataFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDataFileSize\", sqlMetric -> assertThat(sqlMetric.value()).isNotEqualTo(0));\n \n     // delete manifests\n-    assertThat(metricsMap.get(\"totalDeleteManifests\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"scannedDeleteManifests\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"skippedDeleteManifests\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"scannedDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n \n     // delete files\n-    assertThat(metricsMap.get(\"totalDeleteFileSize\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"resultDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"equalityDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"indexedDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"positionalDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"skippedDeleteFiles\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDeleteFileSize\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"resultDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"equalityDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"indexedDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"positionalDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n   }\n \n   @TestTemplate\n@@ -152,29 +219,61 @@ public void testDeleteMetrics() throws NoSuchTableException {\n         JavaConverters.mapAsJavaMapConverter(sparkPlans.get(0).metrics()).asJava();\n \n     // Common\n-    assertThat(metricsMap.get(\"totalPlanningDuration\").value()).isNotEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalPlanningDuration\", sqlMetric -> assertThat(sqlMetric.value()).isNotEqualTo(0));\n \n     // data manifests\n-    assertThat(metricsMap.get(\"totalDataManifest\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"scannedDataManifests\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"skippedDataManifests\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDataManifest\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"scannedDataManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDataManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n \n     // data files\n-    assertThat(metricsMap.get(\"resultDataFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"skippedDataFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"totalDataFileSize\").value()).isNotEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"resultDataFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDataFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDataFileSize\", sqlMetric -> assertThat(sqlMetric.value()).isNotEqualTo(0));\n \n     // delete manifests\n-    assertThat(metricsMap.get(\"totalDeleteManifests\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"scannedDeleteManifests\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"skippedDeleteManifests\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"scannedDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDeleteManifests\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n \n     // delete files\n-    assertThat(metricsMap.get(\"totalDeleteFileSize\").value()).isNotEqualTo(0);\n-    assertThat(metricsMap.get(\"resultDeleteFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"equalityDeleteFiles\").value()).isEqualTo(0);\n-    assertThat(metricsMap.get(\"indexedDeleteFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"positionalDeleteFiles\").value()).isEqualTo(1);\n-    assertThat(metricsMap.get(\"skippedDeleteFiles\").value()).isEqualTo(0);\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"totalDeleteFileSize\", sqlMetric -> assertThat(sqlMetric.value()).isNotEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"resultDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"equalityDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"indexedDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"positionalDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(1));\n+    assertThat(metricsMap)\n+        .hasEntrySatisfying(\n+            \"skippedDeleteFiles\", sqlMetric -> assertThat(sqlMetric.value()).isEqualTo(0));\n   }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkScan.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkScan.java\nindex 8aecd785bbca..1ddf9318f608 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkScan.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkScan.java\n@@ -452,7 +452,7 @@ public void testUnpartitionedYears() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n \n     // NOT Equal\n     builder = scanBuilder();\n@@ -462,7 +462,7 @@ public void testUnpartitionedYears() throws Exception {\n     scan = builder.build().toBatch();\n \n     // notEq can't be answered using column bounds because they are not exact\n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n   }\n \n   @TestTemplate\n@@ -481,7 +481,7 @@ public void testPartitionedYears() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n \n     // NOT Equal\n     builder = scanBuilder();\n@@ -490,7 +490,7 @@ public void testPartitionedYears() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n   }\n \n   @TestTemplate\n@@ -510,7 +510,7 @@ public void testUnpartitionedMonths() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n \n     // NOT GT\n     builder = scanBuilder();\n@@ -519,7 +519,7 @@ public void testUnpartitionedMonths() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n   }\n \n   @TestTemplate\n@@ -539,7 +539,7 @@ public void testPartitionedMonths() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n \n     // NOT GT\n     builder = scanBuilder();\n@@ -548,7 +548,7 @@ public void testPartitionedMonths() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n   }\n \n   @TestTemplate\n@@ -567,7 +567,7 @@ public void testUnpartitionedDays() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n \n     // NOT LT\n     builder = scanBuilder();\n@@ -576,7 +576,7 @@ public void testUnpartitionedDays() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n   }\n \n   @TestTemplate\n@@ -595,7 +595,7 @@ public void testPartitionedDays() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n \n     // NOT LT\n     builder = scanBuilder();\n@@ -604,7 +604,7 @@ public void testPartitionedDays() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n   }\n \n   @TestTemplate\n@@ -623,7 +623,7 @@ public void testUnpartitionedHours() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(8);\n+    assertThat(scan.planInputPartitions()).hasSize(8);\n \n     // NOT GTEQ\n     builder = scanBuilder();\n@@ -632,7 +632,7 @@ public void testUnpartitionedHours() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(2);\n+    assertThat(scan.planInputPartitions()).hasSize(2);\n   }\n \n   @TestTemplate\n@@ -651,7 +651,7 @@ public void testPartitionedHours() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(8);\n+    assertThat(scan.planInputPartitions()).hasSize(8);\n \n     // NOT GTEQ\n     builder = scanBuilder();\n@@ -660,7 +660,7 @@ public void testPartitionedHours() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(2);\n+    assertThat(scan.planInputPartitions()).hasSize(2);\n   }\n \n   @TestTemplate\n@@ -675,7 +675,7 @@ public void testUnpartitionedBucketLong() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n \n     // NOT GTEQ\n     builder = scanBuilder();\n@@ -684,7 +684,7 @@ public void testUnpartitionedBucketLong() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n   }\n \n   @TestTemplate\n@@ -699,7 +699,7 @@ public void testPartitionedBucketLong() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(6);\n+    assertThat(scan.planInputPartitions()).hasSize(6);\n \n     // NOT GTEQ\n     builder = scanBuilder();\n@@ -708,7 +708,7 @@ public void testPartitionedBucketLong() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(4);\n+    assertThat(scan.planInputPartitions()).hasSize(4);\n   }\n \n   @TestTemplate\n@@ -723,7 +723,7 @@ public void testUnpartitionedBucketString() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n \n     // NOT LTEQ\n     builder = scanBuilder();\n@@ -732,7 +732,7 @@ public void testUnpartitionedBucketString() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n   }\n \n   @TestTemplate\n@@ -747,7 +747,7 @@ public void testPartitionedBucketString() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(6);\n+    assertThat(scan.planInputPartitions()).hasSize(6);\n \n     // NOT LTEQ\n     builder = scanBuilder();\n@@ -756,7 +756,7 @@ public void testPartitionedBucketString() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(4);\n+    assertThat(scan.planInputPartitions()).hasSize(4);\n   }\n \n   @TestTemplate\n@@ -771,7 +771,7 @@ public void testUnpartitionedTruncateString() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n \n     // NOT NotEqual\n     builder = scanBuilder();\n@@ -780,7 +780,7 @@ public void testUnpartitionedTruncateString() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n   }\n \n   @TestTemplate\n@@ -795,7 +795,7 @@ public void testPartitionedTruncateString() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n \n     // NOT NotEqual\n     builder = scanBuilder();\n@@ -804,7 +804,7 @@ public void testPartitionedTruncateString() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n   }\n \n   @TestTemplate\n@@ -819,7 +819,7 @@ public void testUnpartitionedIsNull() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(0);\n+    assertThat(scan.planInputPartitions()).isEmpty();\n \n     // NOT IsNull\n     builder = scanBuilder();\n@@ -828,7 +828,7 @@ public void testUnpartitionedIsNull() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n   }\n \n   @TestTemplate\n@@ -843,7 +843,7 @@ public void testPartitionedIsNull() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(0);\n+    assertThat(scan.planInputPartitions()).isEmpty();\n \n     // NOT IsNULL\n     builder = scanBuilder();\n@@ -852,7 +852,7 @@ public void testPartitionedIsNull() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n   }\n \n   @TestTemplate\n@@ -867,7 +867,7 @@ public void testUnpartitionedIsNotNull() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n \n     // NOT IsNotNull\n     builder = scanBuilder();\n@@ -876,7 +876,7 @@ public void testUnpartitionedIsNotNull() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(0);\n+    assertThat(scan.planInputPartitions()).isEmpty();\n   }\n \n   @TestTemplate\n@@ -891,7 +891,7 @@ public void testPartitionedIsNotNull() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n \n     // NOT IsNotNULL\n     builder = scanBuilder();\n@@ -900,7 +900,7 @@ public void testPartitionedIsNotNull() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(0);\n+    assertThat(scan.planInputPartitions()).isEmpty();\n   }\n \n   @TestTemplate\n@@ -921,7 +921,7 @@ public void testUnpartitionedAnd() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(5);\n+    assertThat(scan.planInputPartitions()).hasSize(5);\n \n     // NOT (years(ts) = 47 AND bucket(id, 5) >= 2)\n     builder = scanBuilder();\n@@ -930,7 +930,7 @@ public void testUnpartitionedAnd() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n   }\n \n   @TestTemplate\n@@ -951,7 +951,7 @@ public void testPartitionedAnd() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(1);\n+    assertThat(scan.planInputPartitions()).hasSize(1);\n \n     // NOT (years(ts) = 47 AND bucket(id, 5) >= 2)\n     builder = scanBuilder();\n@@ -960,7 +960,7 @@ public void testPartitionedAnd() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(9);\n+    assertThat(scan.planInputPartitions()).hasSize(9);\n   }\n \n   @TestTemplate\n@@ -981,7 +981,7 @@ public void testUnpartitionedOr() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n \n     // NOT (years(ts) = 47 OR bucket(id, 5) >= 2)\n     builder = scanBuilder();\n@@ -990,7 +990,7 @@ public void testUnpartitionedOr() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(10);\n+    assertThat(scan.planInputPartitions()).hasSize(10);\n   }\n \n   @TestTemplate\n@@ -1011,7 +1011,7 @@ public void testPartitionedOr() throws Exception {\n     pushFilters(builder, predicate);\n     Batch scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(6);\n+    assertThat(scan.planInputPartitions()).hasSize(6);\n \n     // NOT (years(ts) = 48 OR bucket(id, 5) >= 2)\n     builder = scanBuilder();\n@@ -1020,7 +1020,7 @@ public void testPartitionedOr() throws Exception {\n     pushFilters(builder, predicate);\n     scan = builder.build().toBatch();\n \n-    assertThat(scan.planInputPartitions().length).isEqualTo(4);\n+    assertThat(scan.planInputPartitions()).hasSize(4);\n   }\n \n   private SparkScanBuilder scanBuilder() throws Exception {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-13007",
    "pr_id": 13007,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 16,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkCatalogTestBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestCompressionSettings.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestRuntimeFiltering.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreamingRead3.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkCatalogTestBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestCompressionSettings.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestRuntimeFiltering.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreamingRead3.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java"
    ],
    "base_commit": "df866c51af8f99d213037f0bd49b1d7dd061f7b5",
    "head_commit": "6e04283462f27b5759cad792c29b26e54d7bba43",
    "repo_url": "https://github.com/apache/iceberg/pull/13007",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/13007",
    "dockerfile": "",
    "pr_merged_at": "2025-05-08T09:16:46.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkCatalogTestBase.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkCatalogTestBase.java\ndeleted file mode 100644\nindex 6b2b9a1b8082..000000000000\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/SparkCatalogTestBase.java\n+++ /dev/null\n@@ -1,72 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.iceberg.spark;\n-\n-import java.util.Map;\n-import org.apache.iceberg.CatalogProperties;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.junit.Rule;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n-public abstract class SparkCatalogTestBase extends SparkTestBaseWithCatalog {\n-\n-  // these parameters are broken out to avoid changes that need to modify lots of test suites\n-  @Parameterized.Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n-  public static Object[][] parameters() {\n-    return new Object[][] {\n-      {\n-        SparkCatalogConfig.HIVE.catalogName(),\n-        SparkCatalogConfig.HIVE.implementation(),\n-        SparkCatalogConfig.HIVE.properties()\n-      },\n-      {\n-        SparkCatalogConfig.HADOOP.catalogName(),\n-        SparkCatalogConfig.HADOOP.implementation(),\n-        SparkCatalogConfig.HADOOP.properties()\n-      },\n-      {\n-        SparkCatalogConfig.SPARK.catalogName(),\n-        SparkCatalogConfig.SPARK.implementation(),\n-        SparkCatalogConfig.SPARK.properties()\n-      },\n-      {\n-        SparkCatalogConfig.REST.catalogName(),\n-        SparkCatalogConfig.REST.implementation(),\n-        ImmutableMap.builder()\n-            .putAll(SparkCatalogConfig.REST.properties())\n-            .put(CatalogProperties.URI, REST_SERVER_RULE.uri())\n-            .build()\n-      }\n-    };\n-  }\n-\n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n-\n-  public SparkCatalogTestBase(SparkCatalogConfig config) {\n-    super(config);\n-  }\n-\n-  public SparkCatalogTestBase(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-}\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestCompressionSettings.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestCompressionSettings.java\nindex 14e8fc34c3db..24a14bb64d86 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestCompressionSettings.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestCompressionSettings.java\n@@ -18,6 +18,9 @@\n  */\n package org.apache.iceberg.spark.source;\n \n+import static org.apache.iceberg.FileFormat.AVRO;\n+import static org.apache.iceberg.FileFormat.ORC;\n+import static org.apache.iceberg.FileFormat.PARQUET;\n import static org.apache.iceberg.RowLevelOperationMode.MERGE_ON_READ;\n import static org.apache.iceberg.TableProperties.AVRO_COMPRESSION;\n import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n@@ -50,6 +53,9 @@\n import org.apache.iceberg.ManifestFile;\n import org.apache.iceberg.ManifestFiles;\n import org.apache.iceberg.ManifestReader;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.actions.SizeBasedFileRewritePlanner;\n@@ -58,8 +64,8 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.SparkCatalogConfig;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n import org.apache.iceberg.spark.SparkWriteOptions;\n import org.apache.iceberg.spark.actions.SparkActions;\n import org.apache.orc.OrcFile;\n@@ -69,73 +75,98 @@\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SparkSession;\n-import org.junit.AfterClass;\n-import org.junit.Before;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n-public class TestCompressionSettings extends SparkCatalogTestBase {\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestCompressionSettings extends CatalogTestBase {\n \n   private static final Configuration CONF = new Configuration();\n   private static final String TABLE_NAME = \"testWriteData\";\n \n   private static SparkSession spark = null;\n \n-  private final FileFormat format;\n-  private final ImmutableMap<String, String> properties;\n+  @Parameter(index = 3)\n+  private FileFormat format;\n+\n+  @Parameter(index = 4)\n+  private Map<String, String> properties;\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private java.nio.file.Path temp;\n \n-  @Parameterized.Parameters(name = \"format = {0}, properties = {1}\")\n+  @Parameters(\n+      name =\n+          \"catalogName = {0}, implementation = {1}, config = {2}, format = {3}, properties = {4}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {\"parquet\", ImmutableMap.of(COMPRESSION_CODEC, \"zstd\", COMPRESSION_LEVEL, \"1\")},\n-      {\"parquet\", ImmutableMap.of(COMPRESSION_CODEC, \"gzip\")},\n-      {\"orc\", ImmutableMap.of(COMPRESSION_CODEC, \"zstd\", COMPRESSION_STRATEGY, \"speed\")},\n-      {\"orc\", ImmutableMap.of(COMPRESSION_CODEC, \"zstd\", COMPRESSION_STRATEGY, \"compression\")},\n-      {\"avro\", ImmutableMap.of(COMPRESSION_CODEC, \"snappy\", COMPRESSION_LEVEL, \"3\")}\n+      {\n+        SparkCatalogConfig.SPARK.catalogName(),\n+        SparkCatalogConfig.SPARK.implementation(),\n+        SparkCatalogConfig.SPARK.properties(),\n+        PARQUET,\n+        ImmutableMap.of(COMPRESSION_CODEC, \"zstd\", COMPRESSION_LEVEL, \"1\")\n+      },\n+      {\n+        SparkCatalogConfig.SPARK.catalogName(),\n+        SparkCatalogConfig.SPARK.implementation(),\n+        SparkCatalogConfig.SPARK.properties(),\n+        PARQUET,\n+        ImmutableMap.of(COMPRESSION_CODEC, \"gzip\")\n+      },\n+      {\n+        SparkCatalogConfig.SPARK.catalogName(),\n+        SparkCatalogConfig.SPARK.implementation(),\n+        SparkCatalogConfig.SPARK.properties(),\n+        ORC,\n+        ImmutableMap.of(COMPRESSION_CODEC, \"zstd\", COMPRESSION_STRATEGY, \"speed\")\n+      },\n+      {\n+        SparkCatalogConfig.SPARK.catalogName(),\n+        SparkCatalogConfig.SPARK.implementation(),\n+        SparkCatalogConfig.SPARK.properties(),\n+        ORC,\n+        ImmutableMap.of(COMPRESSION_CODEC, \"zstd\", COMPRESSION_STRATEGY, \"compression\")\n+      },\n+      {\n+        SparkCatalogConfig.SPARK.catalogName(),\n+        SparkCatalogConfig.SPARK.implementation(),\n+        SparkCatalogConfig.SPARK.properties(),\n+        AVRO,\n+        ImmutableMap.of(COMPRESSION_CODEC, \"snappy\", COMPRESSION_LEVEL, \"3\")\n+      }\n     };\n   }\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestCompressionSettings.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n   }\n \n-  @Before\n+  @BeforeEach\n   public void resetSpecificConfigurations() {\n     spark.conf().unset(COMPRESSION_CODEC);\n     spark.conf().unset(COMPRESSION_LEVEL);\n     spark.conf().unset(COMPRESSION_STRATEGY);\n   }\n \n-  @Parameterized.AfterParam\n-  public static void clearSourceCache() {\n+  @AfterEach\n+  public void afterEach() {\n     spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", TABLE_NAME));\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestCompressionSettings.spark;\n     TestCompressionSettings.spark = null;\n     currentSpark.stop();\n   }\n \n-  public TestCompressionSettings(String format, ImmutableMap properties) {\n-    super(\n-        SparkCatalogConfig.SPARK.catalogName(),\n-        SparkCatalogConfig.SPARK.implementation(),\n-        SparkCatalogConfig.SPARK.properties());\n-    this.format = FileFormat.fromString(format);\n-    this.properties = properties;\n-  }\n-\n-  @Test\n+  @TestTemplate\n   public void testWriteDataWithDifferentSetting() throws Exception {\n     sql(\"CREATE TABLE %s (id int, data string) USING iceberg\", TABLE_NAME);\n     Map<String, String> tableProperties = Maps.newHashMap();\n@@ -168,6 +199,8 @@ public void testWriteDataWithDifferentSetting() throws Exception {\n       spark.conf().set(entry.getKey(), entry.getValue());\n     }\n \n+    assertSparkConf();\n+\n     df.select(\"id\", \"data\")\n         .writeTo(TABLE_NAME)\n         .option(SparkWriteOptions.WRITE_FORMAT, format.toString())\n@@ -230,4 +263,13 @@ private String getCompressionType(InputFile inputFile) throws Exception {\n         return fileReader.getMetaString(DataFileConstants.CODEC);\n     }\n   }\n+\n+  private void assertSparkConf() {\n+    String[] propertiesToCheck = {COMPRESSION_CODEC, COMPRESSION_LEVEL, COMPRESSION_STRATEGY};\n+    for (String prop : propertiesToCheck) {\n+      String expected = properties.getOrDefault(prop, null);\n+      String actual = spark.conf().get(prop, null);\n+      assertThat(actual).isEqualToIgnoringCase(expected);\n+    }\n+  }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java\nindex b669c91313f3..dc7d87b9036d 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java\n@@ -21,31 +21,28 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.SparkWriteOptions;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n-import org.junit.After;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestRequiredDistributionAndOrdering extends SparkCatalogTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRequiredDistributionAndOrdering extends CatalogTestBase {\n \n-  public TestRequiredDistributionAndOrdering(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void dropTestTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultLocalSort() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 STRING, c3 STRING) \"\n@@ -74,7 +71,7 @@ public void testDefaultLocalSort() throws NoSuchTableException {\n         sql(\"SELECT count(*) FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionColumnsArePrependedForRangeDistribution() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 STRING, c3 STRING) \"\n@@ -110,7 +107,7 @@ public void testPartitionColumnsArePrependedForRangeDistribution() throws NoSuch\n         sql(\"SELECT count(*) FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortOrderIncludesPartitionColumns() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 STRING, c3 STRING) \"\n@@ -142,7 +139,7 @@ public void testSortOrderIncludesPartitionColumns() throws NoSuchTableException\n         sql(\"SELECT count(*) FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDisabledDistributionAndOrdering() {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 STRING, c3 STRING) \"\n@@ -176,7 +173,7 @@ public void testDisabledDistributionAndOrdering() {\n                 + \"and by partition within each spec. Either cluster the incoming records or switch to fanout writers.\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashDistribution() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 STRING, c3 STRING) \"\n@@ -212,7 +209,7 @@ public void testHashDistribution() throws NoSuchTableException {\n         sql(\"SELECT count(*) FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortBucketTransformsWithoutExtensions() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 STRING, c3 STRING) \"\n@@ -238,7 +235,7 @@ public void testSortBucketTransformsWithoutExtensions() throws NoSuchTableExcept\n     assertEquals(\"Rows must match\", expected, sql(\"SELECT * FROM %s ORDER BY c1\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeDistributionWithQuotedColumnsNames() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 STRING, `c.3` STRING) \"\n@@ -274,7 +271,7 @@ public void testRangeDistributionWithQuotedColumnsNames() throws NoSuchTableExce\n         sql(\"SELECT count(*) FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashDistributionWithQuotedColumnsNames() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 STRING, `c``3` STRING) \"\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestRuntimeFiltering.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestRuntimeFiltering.java\nindex b09c995b30fa..e7346e270f38 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestRuntimeFiltering.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestRuntimeFiltering.java\n@@ -22,6 +22,7 @@\n import static org.apache.iceberg.PlanningMode.LOCAL;\n import static org.apache.spark.sql.functions.date_add;\n import static org.apache.spark.sql.functions.expr;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.IOException;\n import java.io.UncheckedIOException;\n@@ -29,6 +30,9 @@\n import java.util.Set;\n import org.apache.commons.lang3.StringUtils;\n import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PlanningMode;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.expressions.Expression;\n@@ -36,38 +40,47 @@\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.spark.SparkCatalogConfig;\n import org.apache.iceberg.spark.SparkWriteOptions;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n-public class TestRuntimeFiltering extends SparkTestBaseWithCatalog {\n-\n-  @Parameterized.Parameters(name = \"planningMode = {0}\")\n-  public static Object[] parameters() {\n-    return new Object[] {LOCAL, DISTRIBUTED};\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRuntimeFiltering extends TestBaseWithCatalog {\n+\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}, planningMode = {3}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties(),\n+        LOCAL\n+      },\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties(),\n+        DISTRIBUTED\n+      }\n+    };\n   }\n \n-  private final PlanningMode planningMode;\n-\n-  public TestRuntimeFiltering(PlanningMode planningMode) {\n-    this.planningMode = planningMode;\n-  }\n+  @Parameter(index = 3)\n+  private PlanningMode planningMode;\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS dim\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testIdentityPartitionedTable() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -106,7 +119,7 @@ public void testIdentityPartitionedTable() throws NoSuchTableException {\n         sql(query));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBucketedTable() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -145,7 +158,7 @@ public void testBucketedTable() throws NoSuchTableException {\n         sql(query));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRenamedSourceColumnTable() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -186,7 +199,7 @@ public void testRenamedSourceColumnTable() throws NoSuchTableException {\n         sql(query));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMultipleRuntimeFilters() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -229,7 +242,7 @@ public void testMultipleRuntimeFilters() throws NoSuchTableException {\n         sql(query));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCaseSensitivityOfRuntimeFilters() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -273,7 +286,7 @@ public void testCaseSensitivityOfRuntimeFilters() throws NoSuchTableException {\n         sql(caseInsensitiveQuery));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBucketedTableWithMultipleSpecs() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) USING iceberg\",\n@@ -325,7 +338,7 @@ public void testBucketedTableWithMultipleSpecs() throws NoSuchTableException {\n         sql(query));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSourceColumnWithDots() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (`i.d` BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -369,7 +382,7 @@ public void testSourceColumnWithDots() throws NoSuchTableException {\n         sql(query));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSourceColumnWithBackticks() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (`i``d` BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n@@ -410,7 +423,7 @@ public void testSourceColumnWithBackticks() throws NoSuchTableException {\n         sql(query));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedTable() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) USING iceberg\",\n@@ -458,7 +471,7 @@ private void assertQueryContainsRuntimeFilters(\n     List<Row> output = spark.sql(\"EXPLAIN EXTENDED \" + query).collectAsList();\n     String plan = output.get(0).getString(0);\n     int actualFilterCount = StringUtils.countMatches(plan, \"dynamicpruningexpression\");\n-    Assert.assertEquals(errorMessage, expectedFilterCount, actualFilterCount);\n+    assertThat(actualFilterCount).as(errorMessage).isEqualTo(expectedFilterCount);\n   }\n \n   // delete files that don't match the filter to ensure dynamic filtering works and only required\n@@ -490,9 +503,8 @@ private void deleteNotMatchingFiles(Expression filter, int expectedDeletedFileCo\n       throw new UncheckedIOException(e);\n     }\n \n-    Assert.assertEquals(\n-        \"Deleted unexpected number of files\",\n-        expectedDeletedFileCount,\n-        deletedFileLocations.size());\n+    assertThat(deletedFileLocations)\n+        .as(\"Deleted unexpected number of files\")\n+        .hasSize(expectedDeletedFileCount);\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java\nindex c27671311374..fd155a6bcaf3 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java\n@@ -18,25 +18,28 @@\n  */\n package org.apache.iceberg.spark.source;\n \n-import java.util.Map;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import org.apache.hadoop.conf.Configurable;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.KryoHelpers;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TestHelpers;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.SparkCatalog;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.connector.catalog.Identifier;\n import org.apache.spark.sql.connector.catalog.TableCatalog;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSparkCatalogHadoopOverrides extends SparkCatalogTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkCatalogHadoopOverrides extends CatalogTestBase {\n \n   private static final String CONFIG_TO_OVERRIDE = \"fs.s3a.buffer.dir\";\n   // prepend \"hadoop.\" so that the test base formats SQLConf correctly\n@@ -44,7 +47,7 @@ public class TestSparkCatalogHadoopOverrides extends SparkCatalogTestBase {\n   private static final String HADOOP_PREFIXED_CONFIG_TO_OVERRIDE = \"hadoop.\" + CONFIG_TO_OVERRIDE;\n   private static final String CONFIG_OVERRIDE_VALUE = \"/tmp-overridden\";\n \n-  @Parameterized.Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n       {\n@@ -77,41 +80,36 @@ public static Object[][] parameters() {\n     };\n   }\n \n-  public TestSparkCatalogHadoopOverrides(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Before\n+  @BeforeEach\n   public void createTable() {\n     sql(\"CREATE TABLE IF NOT EXISTS %s (id bigint) USING iceberg\", tableName(tableIdent.name()));\n   }\n \n-  @After\n+  @AfterEach\n   public void dropTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName(tableIdent.name()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTableFromCatalogHasOverrides() throws Exception {\n     Table table = getIcebergTableFromSparkCatalog();\n     Configuration conf = ((Configurable) table.io()).getConf();\n     String actualCatalogOverride = conf.get(CONFIG_TO_OVERRIDE, \"/whammies\");\n-    Assert.assertEquals(\n-        \"Iceberg tables from spark should have the overridden hadoop configurations from the spark config\",\n-        CONFIG_OVERRIDE_VALUE,\n-        actualCatalogOverride);\n+    assertThat(actualCatalogOverride)\n+        .as(\n+            \"Iceberg tables from spark should have the overridden hadoop configurations from the spark config\")\n+        .isEqualTo(CONFIG_OVERRIDE_VALUE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void ensureRoundTripSerializedTableRetainsHadoopConfig() throws Exception {\n     Table table = getIcebergTableFromSparkCatalog();\n     Configuration originalConf = ((Configurable) table.io()).getConf();\n     String actualCatalogOverride = originalConf.get(CONFIG_TO_OVERRIDE, \"/whammies\");\n-    Assert.assertEquals(\n-        \"Iceberg tables from spark should have the overridden hadoop configurations from the spark config\",\n-        CONFIG_OVERRIDE_VALUE,\n-        actualCatalogOverride);\n+    assertThat(actualCatalogOverride)\n+        .as(\n+            \"Iceberg tables from spark should have the overridden hadoop configurations from the spark config\")\n+        .isEqualTo(CONFIG_OVERRIDE_VALUE);\n \n     // Now convert to SerializableTable and ensure overridden property is still present.\n     Table serializableTable = SerializableTableWithSize.copyOf(table);\n@@ -119,19 +117,19 @@ public void ensureRoundTripSerializedTableRetainsHadoopConfig() throws Exception\n         KryoHelpers.roundTripSerialize(SerializableTableWithSize.copyOf(table));\n     Configuration configFromKryoSerde = ((Configurable) kryoSerializedTable.io()).getConf();\n     String kryoSerializedCatalogOverride = configFromKryoSerde.get(CONFIG_TO_OVERRIDE, \"/whammies\");\n-    Assert.assertEquals(\n-        \"Tables serialized with Kryo serialization should retain overridden hadoop configuration properties\",\n-        CONFIG_OVERRIDE_VALUE,\n-        kryoSerializedCatalogOverride);\n+    assertThat(kryoSerializedCatalogOverride)\n+        .as(\n+            \"Tables serialized with Kryo serialization should retain overridden hadoop configuration properties\")\n+        .isEqualTo(CONFIG_OVERRIDE_VALUE);\n \n     // Do the same for Java based serde\n     Table javaSerializedTable = TestHelpers.roundTripSerialize(serializableTable);\n     Configuration configFromJavaSerde = ((Configurable) javaSerializedTable.io()).getConf();\n     String javaSerializedCatalogOverride = configFromJavaSerde.get(CONFIG_TO_OVERRIDE, \"/whammies\");\n-    Assert.assertEquals(\n-        \"Tables serialized with Java serialization should retain overridden hadoop configuration properties\",\n-        CONFIG_OVERRIDE_VALUE,\n-        javaSerializedCatalogOverride);\n+    assertThat(javaSerializedCatalogOverride)\n+        .as(\n+            \"Tables serialized with Java serialization should retain overridden hadoop configuration properties\")\n+        .isEqualTo(CONFIG_OVERRIDE_VALUE);\n   }\n \n   @SuppressWarnings(\"ThrowSpecificity\")\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java\nindex 241293f367aa..b0029c09ab66 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java\n@@ -18,38 +18,35 @@\n  */\n package org.apache.iceberg.spark.source;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import java.io.IOException;\n import java.util.List;\n-import java.util.Map;\n import java.util.UUID;\n import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.ScanTaskSetManager;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n import org.apache.iceberg.spark.SparkReadOptions;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n-\n-public class TestSparkStagedScan extends SparkCatalogTestBase {\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-  public TestSparkStagedScan(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkStagedScan extends CatalogTestBase {\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTaskSetLoading() throws NoSuchTableException, IOException {\n     sql(\"CREATE TABLE %s (id INT, data STRING) USING iceberg\", tableName);\n \n@@ -59,7 +56,7 @@ public void testTaskSetLoading() throws NoSuchTableException, IOException {\n     df.writeTo(tableName).append();\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should produce 1 snapshot\", 1, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should produce 1 snapshot\").hasSize(1);\n \n     try (CloseableIterable<FileScanTask> fileScanTasks = table.newScan().planFiles()) {\n       ScanTaskSetManager taskSetManager = ScanTaskSetManager.get();\n@@ -84,7 +81,7 @@ public void testTaskSetLoading() throws NoSuchTableException, IOException {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTaskSetPlanning() throws NoSuchTableException, IOException {\n     sql(\"CREATE TABLE %s (id INT, data STRING) USING iceberg\", tableName);\n \n@@ -95,7 +92,7 @@ public void testTaskSetPlanning() throws NoSuchTableException, IOException {\n     df.coalesce(1).writeTo(tableName).append();\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should produce 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should produce 1 snapshot\").hasSize(2);\n \n     try (CloseableIterable<FileScanTask> fileScanTasks = table.newScan().planFiles()) {\n       ScanTaskSetManager taskSetManager = ScanTaskSetManager.get();\n@@ -111,7 +108,9 @@ public void testTaskSetPlanning() throws NoSuchTableException, IOException {\n               .option(SparkReadOptions.SCAN_TASK_SET_ID, setID)\n               .option(SparkReadOptions.SPLIT_SIZE, tasks.get(0).file().fileSizeInBytes())\n               .load(tableName);\n-      Assert.assertEquals(\"Num partitions should match\", 2, scanDF.javaRDD().getNumPartitions());\n+      assertThat(scanDF.javaRDD().getNumPartitions())\n+          .as(\"Num partitions should match\")\n+          .isEqualTo(2);\n \n       // load the staged file set and make sure we combine both files into a single split\n       scanDF =\n@@ -121,7 +120,9 @@ public void testTaskSetPlanning() throws NoSuchTableException, IOException {\n               .option(SparkReadOptions.SCAN_TASK_SET_ID, setID)\n               .option(SparkReadOptions.SPLIT_SIZE, Long.MAX_VALUE)\n               .load(tableName);\n-      Assert.assertEquals(\"Num partitions should match\", 1, scanDF.javaRDD().getNumPartitions());\n+      assertThat(scanDF.javaRDD().getNumPartitions())\n+          .as(\"Num partitions should match\")\n+          .isEqualTo(1);\n     }\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java\nindex 616a196872de..4a386ee861d6 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java\n@@ -18,34 +18,32 @@\n  */\n package org.apache.iceberg.spark.source;\n \n-import java.util.Map;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.connector.catalog.CatalogManager;\n import org.apache.spark.sql.connector.catalog.Identifier;\n import org.apache.spark.sql.connector.catalog.TableCatalog;\n-import org.junit.After;\n import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSparkTable extends SparkCatalogTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkTable extends CatalogTestBase {\n \n-  public TestSparkTable(String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Before\n+  @BeforeEach\n   public void createTable() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTableEquality() throws NoSuchTableException {\n     CatalogManager catalogManager = spark.sessionState().catalogManager();\n     TableCatalog catalog = (TableCatalog) catalogManager.catalog(catalogName);\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreamingRead3.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreamingRead3.java\nindex b7d415de3454..2544a8f73954 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreamingRead3.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreamingRead3.java\n@@ -37,6 +37,7 @@\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.Files;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableMetadata;\n@@ -49,7 +50,7 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.SparkReadOptions;\n import org.apache.spark.api.java.function.VoidFunction2;\n import org.apache.spark.sql.Dataset;\n@@ -59,20 +60,14 @@\n import org.apache.spark.sql.streaming.DataStreamWriter;\n import org.apache.spark.sql.streaming.OutputMode;\n import org.apache.spark.sql.streaming.StreamingQuery;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.BeforeClass;\n-import org.junit.Test;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-@RunWith(Parameterized.class)\n-public final class TestStructuredStreamingRead3 extends SparkCatalogTestBase {\n-  public TestStructuredStreamingRead3(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n+@ExtendWith(ParameterizedTestExtension.class)\n+public final class TestStructuredStreamingRead3 extends CatalogTestBase {\n \n   private Table table;\n \n@@ -114,13 +109,13 @@ public TestStructuredStreamingRead3(\n               Lists.newArrayList(\n                   new SimpleRecord(15, \"fifteen\"), new SimpleRecord(16, \"sixteen\"))));\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void setupSpark() {\n     // disable AQE as tests assume that writes generate a particular number of files\n     spark.conf().set(SQLConf.ADAPTIVE_EXECUTION_ENABLED().key(), \"false\");\n   }\n \n-  @Before\n+  @BeforeEach\n   public void setupTable() {\n     sql(\n         \"CREATE TABLE %s \"\n@@ -132,19 +127,19 @@ public void setupTable() {\n     microBatches.set(0);\n   }\n \n-  @After\n+  @AfterEach\n   public void stopStreams() throws TimeoutException {\n     for (StreamingQuery query : spark.streams().active()) {\n       query.stop();\n     }\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadStreamOnIcebergTableWithMultipleSnapshots() throws Exception {\n     List<List<SimpleRecord>> expected = TEST_DATA_MULTIPLE_SNAPSHOTS;\n     appendDataAsMultipleSnapshots(expected);\n@@ -155,37 +150,38 @@ public void testReadStreamOnIcebergTableWithMultipleSnapshots() throws Exception\n     assertThat(actual).containsExactlyInAnyOrderElementsOf(Iterables.concat(expected));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadStreamOnIcebergTableWithMultipleSnapshots_WithNumberOfFiles_1()\n       throws Exception {\n     appendDataAsMultipleSnapshots(TEST_DATA_MULTIPLE_SNAPSHOTS);\n \n-    Assert.assertEquals(\n-        6,\n-        microBatchCount(\n-            ImmutableMap.of(SparkReadOptions.STREAMING_MAX_FILES_PER_MICRO_BATCH, \"1\")));\n+    assertThat(\n+            microBatchCount(\n+                ImmutableMap.of(SparkReadOptions.STREAMING_MAX_FILES_PER_MICRO_BATCH, \"1\")))\n+        .isEqualTo(6);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadStreamOnIcebergTableWithMultipleSnapshots_WithNumberOfFiles_2()\n       throws Exception {\n     appendDataAsMultipleSnapshots(TEST_DATA_MULTIPLE_SNAPSHOTS);\n \n-    Assert.assertEquals(\n-        3,\n-        microBatchCount(\n-            ImmutableMap.of(SparkReadOptions.STREAMING_MAX_FILES_PER_MICRO_BATCH, \"2\")));\n+    assertThat(\n+            microBatchCount(\n+                ImmutableMap.of(SparkReadOptions.STREAMING_MAX_FILES_PER_MICRO_BATCH, \"2\")))\n+        .isEqualTo(3);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadStreamOnIcebergTableWithMultipleSnapshots_WithNumberOfRows_1()\n       throws Exception {\n     appendDataAsMultipleSnapshots(TEST_DATA_MULTIPLE_SNAPSHOTS);\n \n     // only 1 micro-batch will be formed and we will read data partially\n-    Assert.assertEquals(\n-        1,\n-        microBatchCount(ImmutableMap.of(SparkReadOptions.STREAMING_MAX_ROWS_PER_MICRO_BATCH, \"1\")));\n+    assertThat(\n+            microBatchCount(\n+                ImmutableMap.of(SparkReadOptions.STREAMING_MAX_ROWS_PER_MICRO_BATCH, \"1\")))\n+        .isEqualTo(1);\n \n     StreamingQuery query = startStream(SparkReadOptions.STREAMING_MAX_ROWS_PER_MICRO_BATCH, \"1\");\n \n@@ -196,17 +192,18 @@ public void testReadStreamOnIcebergTableWithMultipleSnapshots_WithNumberOfRows_1\n             Lists.newArrayList(TEST_DATA_MULTIPLE_SNAPSHOTS.get(0).get(0)));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadStreamOnIcebergTableWithMultipleSnapshots_WithNumberOfRows_4()\n       throws Exception {\n     appendDataAsMultipleSnapshots(TEST_DATA_MULTIPLE_SNAPSHOTS);\n \n-    Assert.assertEquals(\n-        2,\n-        microBatchCount(ImmutableMap.of(SparkReadOptions.STREAMING_MAX_ROWS_PER_MICRO_BATCH, \"4\")));\n+    assertThat(\n+            microBatchCount(\n+                ImmutableMap.of(SparkReadOptions.STREAMING_MAX_ROWS_PER_MICRO_BATCH, \"4\")))\n+        .isEqualTo(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadStreamOnIcebergThenAddData() throws Exception {\n     List<List<SimpleRecord>> expected = TEST_DATA_MULTIPLE_SNAPSHOTS;\n \n@@ -218,7 +215,7 @@ public void testReadStreamOnIcebergThenAddData() throws Exception {\n     assertThat(actual).containsExactlyInAnyOrderElementsOf(Iterables.concat(expected));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadingStreamFromTimestamp() throws Exception {\n     List<SimpleRecord> dataBeforeTimestamp =\n         Lists.newArrayList(\n@@ -245,7 +242,7 @@ public void testReadingStreamFromTimestamp() throws Exception {\n     assertThat(actual).containsExactlyInAnyOrderElementsOf(Iterables.concat(expected));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadingStreamFromFutureTimetsamp() throws Exception {\n     long futureTimestamp = System.currentTimeMillis() + 10000;\n \n@@ -277,7 +274,7 @@ public void testReadingStreamFromFutureTimetsamp() throws Exception {\n     assertThat(actual).containsExactlyInAnyOrderElementsOf(data);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadingStreamFromTimestampFutureWithExistingSnapshots() throws Exception {\n     List<SimpleRecord> dataBeforeTimestamp =\n         Lists.newArrayList(\n@@ -290,7 +287,7 @@ public void testReadingStreamFromTimestampFutureWithExistingSnapshots() throws E\n     StreamingQuery query =\n         startStream(SparkReadOptions.STREAM_FROM_TIMESTAMP, Long.toString(streamStartTimestamp));\n     List<SimpleRecord> actual = rowsAvailable(query);\n-    Assert.assertEquals(Collections.emptyList(), actual);\n+    assertThat(actual).isEmpty();\n \n     // Stream should contain data added after the timestamp elapses\n     waitUntilAfter(streamStartTimestamp);\n@@ -300,7 +297,7 @@ public void testReadingStreamFromTimestampFutureWithExistingSnapshots() throws E\n         .containsExactlyInAnyOrderElementsOf(Iterables.concat(expected));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadingStreamFromTimestampOfExistingSnapshot() throws Exception {\n     List<List<SimpleRecord>> expected = TEST_DATA_MULTIPLE_SNAPSHOTS;\n \n@@ -322,7 +319,7 @@ public void testReadingStreamFromTimestampOfExistingSnapshot() throws Exception\n     assertThat(actual).containsExactlyInAnyOrderElementsOf(Iterables.concat(expected));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadingStreamWithExpiredSnapshotFromTimestamp() throws TimeoutException {\n     List<SimpleRecord> firstSnapshotRecordList = Lists.newArrayList(new SimpleRecord(1, \"one\"));\n \n@@ -351,11 +348,11 @@ public void testReadingStreamWithExpiredSnapshotFromTimestamp() throws TimeoutEx\n     assertThat(actual).containsExactlyInAnyOrderElementsOf(expectedRecordList);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testResumingStreamReadFromCheckpoint() throws Exception {\n-    File writerCheckpointFolder = temp.newFolder(\"writer-checkpoint-folder\");\n+    File writerCheckpointFolder = temp.resolve(\"writer-checkpoint-folder\").toFile();\n     File writerCheckpoint = new File(writerCheckpointFolder, \"writer-checkpoint\");\n-    File output = temp.newFolder();\n+    File output = temp.resolve(\"junit\").toFile();\n \n     DataStreamWriter querySource =\n         spark\n@@ -391,11 +388,11 @@ public void testResumingStreamReadFromCheckpoint() throws Exception {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFailReadingCheckpointInvalidSnapshot() throws IOException, TimeoutException {\n-    File writerCheckpointFolder = temp.newFolder(\"writer-checkpoint-folder\");\n+    File writerCheckpointFolder = temp.resolve(\"writer-checkpoint-folder\").toFile();\n     File writerCheckpoint = new File(writerCheckpointFolder, \"writer-checkpoint\");\n-    File output = temp.newFolder();\n+    File output = temp.resolve(\"junit\").toFile();\n \n     DataStreamWriter querySource =\n         spark\n@@ -431,7 +428,7 @@ public void testFailReadingCheckpointInvalidSnapshot() throws IOException, Timeo\n                 firstSnapshotid));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testParquetOrcAvroDataInOneTable() throws Exception {\n     List<SimpleRecord> parquetFileRecords =\n         Lists.newArrayList(\n@@ -453,14 +450,14 @@ public void testParquetOrcAvroDataInOneTable() throws Exception {\n             Iterables.concat(parquetFileRecords, orcFileRecords, avroFileRecords));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadStreamFromEmptyTable() throws Exception {\n     StreamingQuery stream = startStream();\n     List<SimpleRecord> actual = rowsAvailable(stream);\n-    Assert.assertEquals(Collections.emptyList(), actual);\n+    assertThat(actual).isEmpty();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadStreamWithSnapshotTypeOverwriteErrorsOut() throws Exception {\n     // upgrade table to version 2 - to facilitate creation of Snapshot of type OVERWRITE.\n     TableOperations ops = ((BaseTable) table).operations();\n@@ -481,14 +478,14 @@ public void testReadStreamWithSnapshotTypeOverwriteErrorsOut() throws Exception\n     DeleteFile eqDeletes =\n         FileHelpers.writeDeleteFile(\n             table,\n-            Files.localOutput(temp.newFile()),\n+            Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             TestHelpers.Row.of(0),\n             dataDeletes,\n             deleteRowSchema);\n \n     DataFile dataFile =\n         DataFiles.builder(table.spec())\n-            .withPath(temp.newFile().toString())\n+            .withPath(File.createTempFile(\"junit\", null, temp.toFile()).getPath())\n             .withFileSizeInBytes(10)\n             .withRecordCount(1)\n             .withFormat(FileFormat.PARQUET)\n@@ -498,7 +495,7 @@ public void testReadStreamWithSnapshotTypeOverwriteErrorsOut() throws Exception\n \n     // check pre-condition - that the above Delete file write - actually resulted in snapshot of\n     // type OVERWRITE\n-    Assert.assertEquals(DataOperations.OVERWRITE, table.currentSnapshot().operation());\n+    assertThat(table.currentSnapshot().operation()).isEqualTo(DataOperations.OVERWRITE);\n \n     StreamingQuery query = startStream();\n \n@@ -508,7 +505,7 @@ public void testReadStreamWithSnapshotTypeOverwriteErrorsOut() throws Exception\n         .hasMessageStartingWith(\"Cannot process overwrite snapshot\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadStreamWithSnapshotTypeReplaceIgnoresReplace() throws Exception {\n     // fill table with some data\n     List<List<SimpleRecord>> expected = TEST_DATA_MULTIPLE_SNAPSHOTS;\n@@ -518,14 +515,14 @@ public void testReadStreamWithSnapshotTypeReplaceIgnoresReplace() throws Excepti\n     table.rewriteManifests().clusterBy(f -> 1).commit();\n \n     // check pre-condition\n-    Assert.assertEquals(DataOperations.REPLACE, table.currentSnapshot().operation());\n+    assertThat(table.currentSnapshot().operation()).isEqualTo(DataOperations.REPLACE);\n \n     StreamingQuery query = startStream();\n     List<SimpleRecord> actual = rowsAvailable(query);\n     assertThat(actual).containsExactlyInAnyOrderElementsOf(Iterables.concat(expected));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadStreamWithSnapshotTypeDeleteErrorsOut() throws Exception {\n     table.updateSpec().removeField(\"id_bucket\").addField(ref(\"id\")).commit();\n \n@@ -538,7 +535,7 @@ public void testReadStreamWithSnapshotTypeDeleteErrorsOut() throws Exception {\n \n     // check pre-condition - that the above delete operation on table resulted in Snapshot of Type\n     // DELETE.\n-    Assert.assertEquals(DataOperations.DELETE, table.currentSnapshot().operation());\n+    assertThat(table.currentSnapshot().operation()).isEqualTo(DataOperations.DELETE);\n \n     StreamingQuery query = startStream();\n \n@@ -548,7 +545,7 @@ public void testReadStreamWithSnapshotTypeDeleteErrorsOut() throws Exception {\n         .hasMessageStartingWith(\"Cannot process delete snapshot\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadStreamWithSnapshotTypeDeleteAndSkipDeleteOption() throws Exception {\n     table.updateSpec().removeField(\"id_bucket\").addField(ref(\"id\")).commit();\n \n@@ -561,14 +558,14 @@ public void testReadStreamWithSnapshotTypeDeleteAndSkipDeleteOption() throws Exc\n \n     // check pre-condition - that the above delete operation on table resulted in Snapshot of Type\n     // DELETE.\n-    Assert.assertEquals(DataOperations.DELETE, table.currentSnapshot().operation());\n+    assertThat(table.currentSnapshot().operation()).isEqualTo(DataOperations.DELETE);\n \n     StreamingQuery query = startStream(SparkReadOptions.STREAMING_SKIP_DELETE_SNAPSHOTS, \"true\");\n     assertThat(rowsAvailable(query))\n         .containsExactlyInAnyOrderElementsOf(Iterables.concat(dataAcrossSnapshots));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadStreamWithSnapshotTypeDeleteAndSkipOverwriteOption() throws Exception {\n     table.updateSpec().removeField(\"id_bucket\").addField(ref(\"id\")).commit();\n \n@@ -578,7 +575,7 @@ public void testReadStreamWithSnapshotTypeDeleteAndSkipOverwriteOption() throws\n \n     DataFile dataFile =\n         DataFiles.builder(table.spec())\n-            .withPath(temp.newFile().toString())\n+            .withPath(File.createTempFile(\"junit\", null, temp.toFile()).getPath())\n             .withFileSizeInBytes(10)\n             .withRecordCount(1)\n             .withFormat(FileFormat.PARQUET)\n@@ -593,7 +590,7 @@ public void testReadStreamWithSnapshotTypeDeleteAndSkipOverwriteOption() throws\n \n     // check pre-condition - that the above delete operation on table resulted in Snapshot of Type\n     // OVERWRITE.\n-    Assert.assertEquals(DataOperations.OVERWRITE, table.currentSnapshot().operation());\n+    assertThat(table.currentSnapshot().operation()).isEqualTo(DataOperations.OVERWRITE);\n \n     StreamingQuery query = startStream(SparkReadOptions.STREAMING_SKIP_OVERWRITE_SNAPSHOTS, \"true\");\n     assertThat(rowsAvailable(query))\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java\nindex 55fd2cefe2e6..5dbfc7fa6c0f 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java\n@@ -21,6 +21,7 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n@@ -31,7 +32,9 @@\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestRequiredDistributionAndOrdering extends CatalogTestBase {\n \n   @AfterEach\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java\nindex c031f2991fed..fd155a6bcaf3 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java\n@@ -23,6 +23,7 @@\n import org.apache.hadoop.conf.Configurable;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.KryoHelpers;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TestHelpers;\n@@ -35,7 +36,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkCatalogHadoopOverrides extends CatalogTestBase {\n \n   private static final String CONFIG_TO_OVERRIDE = \"fs.s3a.buffer.dir\";\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java\nindex 6ce2ce623835..e444b7cb1f7c 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java\n@@ -24,6 +24,7 @@\n import java.util.List;\n import java.util.UUID;\n import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n@@ -35,7 +36,9 @@\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkStagedScan extends CatalogTestBase {\n \n   @AfterEach\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java\nindex 46ee484b39ea..d14b1a52cf82 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java\n@@ -20,6 +20,7 @@\n \n import static org.assertj.core.api.Assertions.assertThat;\n \n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.connector.catalog.CatalogManager;\n@@ -28,7 +29,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkTable extends CatalogTestBase {\n \n   @BeforeEach\n\ndiff --git a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java\nindex 55fd2cefe2e6..5dbfc7fa6c0f 100644\n--- a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java\n+++ b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestRequiredDistributionAndOrdering.java\n@@ -21,6 +21,7 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n@@ -31,7 +32,9 @@\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestRequiredDistributionAndOrdering extends CatalogTestBase {\n \n   @AfterEach\n\ndiff --git a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java\nindex c031f2991fed..fd155a6bcaf3 100644\n--- a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java\n+++ b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkCatalogHadoopOverrides.java\n@@ -23,6 +23,7 @@\n import org.apache.hadoop.conf.Configurable;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.KryoHelpers;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TestHelpers;\n@@ -35,7 +36,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkCatalogHadoopOverrides extends CatalogTestBase {\n \n   private static final String CONFIG_TO_OVERRIDE = \"fs.s3a.buffer.dir\";\n\ndiff --git a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java\nindex 6ce2ce623835..e444b7cb1f7c 100644\n--- a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java\n+++ b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkStagedScan.java\n@@ -24,6 +24,7 @@\n import java.util.List;\n import java.util.UUID;\n import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n@@ -35,7 +36,9 @@\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkStagedScan extends CatalogTestBase {\n \n   @AfterEach\n\ndiff --git a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java\nindex 46ee484b39ea..d14b1a52cf82 100644\n--- a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java\n+++ b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkTable.java\n@@ -20,6 +20,7 @@\n \n import static org.assertj.core.api.Assertions.assertThat;\n \n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.connector.catalog.CatalogManager;\n@@ -28,7 +29,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkTable extends CatalogTestBase {\n \n   @BeforeEach\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12998",
    "pr_id": 12998,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 19,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestFileRewriteCoordinator.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkCachedTableCatalog.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkCatalogOperations.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestMigrateTableAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHiveTables.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTableReadableMetrics.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTablesWithPartitionEvolution.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestFileRewriteCoordinator.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkCachedTableCatalog.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkCatalogOperations.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTableReadableMetrics.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestFileRewriteCoordinator.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkCachedTableCatalog.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkCatalogOperations.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestMigrateTableAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHiveTables.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTableReadableMetrics.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTablesWithPartitionEvolution.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestFileRewriteCoordinator.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkCachedTableCatalog.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkCatalogOperations.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTableReadableMetrics.java"
    ],
    "base_commit": "542a4d764683695217dc7741aec9887e1f947e8c",
    "head_commit": "f802fe3a2b7711ffebc96a75bcd807421ef7cfe3",
    "repo_url": "https://github.com/apache/iceberg/pull/12998",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12998",
    "dockerfile": "",
    "pr_merged_at": "2025-05-08T05:31:29.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestFileRewriteCoordinator.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestFileRewriteCoordinator.java\nindex 20a2d5c93a01..ae94f350195b 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestFileRewriteCoordinator.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestFileRewriteCoordinator.java\n@@ -18,6 +18,8 @@\n  */\n package org.apache.iceberg.spark;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import java.io.IOException;\n import java.util.List;\n import java.util.Map;\n@@ -26,34 +28,30 @@\n import java.util.stream.Collectors;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.spark.source.SimpleRecord;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Encoders;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n-\n-public class TestFileRewriteCoordinator extends SparkCatalogTestBase {\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-  public TestFileRewriteCoordinator(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestFileRewriteCoordinator extends CatalogTestBase {\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackRewrite() throws NoSuchTableException, IOException {\n     sql(\"CREATE TABLE %s (id INT, data STRING) USING iceberg\", tableName);\n \n@@ -64,7 +62,7 @@ public void testBinPackRewrite() throws NoSuchTableException, IOException {\n     df.coalesce(1).writeTo(tableName).append();\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should produce 4 snapshots\", 4, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should produce 4 snapshots\").hasSize(4);\n \n     Dataset<Row> fileDF =\n         spark.read().format(\"iceberg\").load(tableName(tableIdent.name() + \".files\"));\n@@ -106,14 +104,15 @@ public void testBinPackRewrite() throws NoSuchTableException, IOException {\n     table.refresh();\n \n     Map<String, String> summary = table.currentSnapshot().summary();\n-    Assert.assertEquals(\"Deleted files count must match\", \"4\", summary.get(\"deleted-data-files\"));\n-    Assert.assertEquals(\"Added files count must match\", \"2\", summary.get(\"added-data-files\"));\n+    assertThat(summary)\n+        .containsEntry(\"deleted-data-files\", \"4\")\n+        .containsEntry(\"added-data-files\", \"2\");\n \n     Object rowCount = scalarSql(\"SELECT count(*) FROM %s\", tableName);\n-    Assert.assertEquals(\"Row count must match\", 4000L, rowCount);\n+    assertThat(rowCount).as(\"Row count must match\").isEqualTo(4000L);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortRewrite() throws NoSuchTableException, IOException {\n     sql(\"CREATE TABLE %s (id INT, data STRING) USING iceberg\", tableName);\n \n@@ -124,7 +123,7 @@ public void testSortRewrite() throws NoSuchTableException, IOException {\n     df.coalesce(1).writeTo(tableName).append();\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should produce 4 snapshots\", 4, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should produce 4 snapshots\").hasSize(4);\n \n     try (CloseableIterable<FileScanTask> fileScanTasks = table.newScan().planFiles()) {\n       String fileSetID = UUID.randomUUID().toString();\n@@ -176,14 +175,15 @@ public void testSortRewrite() throws NoSuchTableException, IOException {\n     table.refresh();\n \n     Map<String, String> summary = table.currentSnapshot().summary();\n-    Assert.assertEquals(\"Deleted files count must match\", \"4\", summary.get(\"deleted-data-files\"));\n-    Assert.assertEquals(\"Added files count must match\", \"2\", summary.get(\"added-data-files\"));\n+    assertThat(summary)\n+        .containsEntry(\"deleted-data-files\", \"4\")\n+        .containsEntry(\"added-data-files\", \"2\");\n \n     Object rowCount = scalarSql(\"SELECT count(*) FROM %s\", tableName);\n-    Assert.assertEquals(\"Row count must match\", 4000L, rowCount);\n+    assertThat(rowCount).as(\"Row count must match\").isEqualTo(4000L);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCommitMultipleRewrites() throws NoSuchTableException, IOException {\n     sql(\"CREATE TABLE %s (id INT, data STRING) USING iceberg\", tableName);\n \n@@ -253,14 +253,15 @@ public void testCommitMultipleRewrites() throws NoSuchTableException, IOExceptio\n \n     table.refresh();\n \n-    Assert.assertEquals(\"Should produce 5 snapshots\", 5, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should produce 5 snapshots\").hasSize(5);\n \n     Map<String, String> summary = table.currentSnapshot().summary();\n-    Assert.assertEquals(\"Deleted files count must match\", \"4\", summary.get(\"deleted-data-files\"));\n-    Assert.assertEquals(\"Added files count must match\", \"2\", summary.get(\"added-data-files\"));\n+    assertThat(summary)\n+        .containsEntry(\"deleted-data-files\", \"4\")\n+        .containsEntry(\"added-data-files\", \"2\");\n \n     Object rowCount = scalarSql(\"SELECT count(*) FROM %s\", tableName);\n-    Assert.assertEquals(\"Row count must match\", 4000L, rowCount);\n+    assertThat(rowCount).as(\"Row count must match\").isEqualTo(4000L);\n   }\n \n   private Dataset<Row> newDF(int numRecords) {\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkCachedTableCatalog.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkCachedTableCatalog.java\nindex 23e8717fb8c3..228bf43b89b1 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkCachedTableCatalog.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkCachedTableCatalog.java\n@@ -18,32 +18,43 @@\n  */\n package org.apache.iceberg.spark;\n \n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.junit.AfterClass;\n-import org.junit.BeforeClass;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSparkCachedTableCatalog extends SparkTestBaseWithCatalog {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkCachedTableCatalog extends TestBaseWithCatalog {\n \n   private static final SparkTableCache TABLE_CACHE = SparkTableCache.get();\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void setupCachedTableCatalog() {\n     spark.conf().set(\"spark.sql.catalog.testcache\", SparkCachedTableCatalog.class.getName());\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void unsetCachedTableCatalog() {\n     spark.conf().unset(\"spark.sql.catalog.testcache\");\n   }\n \n-  public TestSparkCachedTableCatalog() {\n-    super(SparkCatalogConfig.HIVE);\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  protected static Object[][] parameters() {\n+    return new Object[][] {\n+      {\n+        SparkCatalogConfig.HIVE.catalogName(),\n+        SparkCatalogConfig.HIVE.implementation(),\n+        SparkCatalogConfig.HIVE.properties()\n+      },\n+    };\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimeTravel() {\n     sql(\"CREATE TABLE %s (id INT, dep STRING) USING iceberg\", tableName);\n \n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkCatalogOperations.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkCatalogOperations.java\nindex 0836271a7c22..8c76cbc3e872 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkCatalogOperations.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkCatalogOperations.java\n@@ -18,37 +18,37 @@\n  */\n package org.apache.iceberg.spark;\n \n-import java.util.Map;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.atIndex;\n+\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.catalog.Catalog;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.connector.catalog.Column;\n import org.apache.spark.sql.connector.catalog.Identifier;\n import org.apache.spark.sql.connector.catalog.Table;\n import org.apache.spark.sql.connector.catalog.TableChange;\n import org.apache.spark.sql.types.DataTypes;\n-import org.apache.spark.sql.types.StructField;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSparkCatalogOperations extends SparkCatalogTestBase {\n-  public TestSparkCatalogOperations(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkCatalogOperations extends CatalogTestBase {\n \n-  @Before\n+  @BeforeEach\n   public void createTable() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAlterTable() throws NoSuchTableException {\n     BaseCatalog catalog = (BaseCatalog) spark.sessionState().catalogManager().catalog(catalogName);\n     Identifier identifier = Identifier.of(tableIdent.namespace().levels(), tableIdent.name());\n@@ -62,24 +62,20 @@ public void testAlterTable() throws NoSuchTableException {\n             TableChange.addColumn(new String[] {fieldName}, DataTypes.StringType, true),\n             TableChange.setProperty(propsKey, propsValue));\n \n-    Assert.assertNotNull(\"Should return updated table\", table);\n+    assertThat(table).as(\"Should return updated table\").isNotNull();\n \n-    StructField expectedField = DataTypes.createStructField(fieldName, DataTypes.StringType, true);\n-    Assert.assertEquals(\n-        \"Adding a column to a table should return the updated table with the new column\",\n-        table.schema().fields()[2],\n-        expectedField);\n+    Column expectedField = Column.create(fieldName, DataTypes.StringType, true);\n+    assertThat(table.columns())\n+        .as(\"Adding a column to a table should return the updated table with the new column\")\n+        .contains(expectedField, atIndex(2));\n \n-    Assert.assertTrue(\n-        \"Adding a property to a table should return the updated table with the new property\",\n-        table.properties().containsKey(propsKey));\n-    Assert.assertEquals(\n-        \"Altering a table to add a new property should add the correct value\",\n-        propsValue,\n-        table.properties().get(propsKey));\n+    assertThat(table.properties())\n+        .as(\n+            \"Adding a property to a table should return the updated table with the new property with the new correct value\")\n+        .containsEntry(propsKey, propsValue);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidateTable() {\n     // load table to CachingCatalog\n     sql(\"SELECT count(1) FROM %s\", tableName);\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java\nindex 00faecf9a830..ae7ed3af651e 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java\n@@ -51,21 +51,24 @@\n import java.util.List;\n import java.util.Map;\n import org.apache.iceberg.DistributionMode;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.UpdateProperties;\n import org.apache.iceberg.deletes.DeleteGranularity;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSparkWriteConf extends SparkTestBaseWithCatalog {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkWriteConf extends TestBaseWithCatalog {\n \n-  @Before\n+  @BeforeEach\n   public void before() {\n+    super.before();\n     sql(\n         \"CREATE TABLE %s (id BIGINT, data STRING, date DATE, ts TIMESTAMP) \"\n             + \"USING iceberg \"\n@@ -73,12 +76,12 @@ public void before() {\n         tableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void after() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDurationConf() {\n     Table table = validationCatalog.loadTable(tableIdent);\n     String confName = \"spark.sql.iceberg.some-duration-conf\";\n@@ -100,7 +103,7 @@ public void testDurationConf() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteGranularityDefault() {\n     Table table = validationCatalog.loadTable(tableIdent);\n     SparkWriteConf writeConf = new SparkWriteConf(spark, table, ImmutableMap.of());\n@@ -109,7 +112,7 @@ public void testDeleteGranularityDefault() {\n     assertThat(value).isEqualTo(DeleteGranularity.PARTITION);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteGranularityTableProperty() {\n     Table table = validationCatalog.loadTable(tableIdent);\n \n@@ -124,7 +127,7 @@ public void testDeleteGranularityTableProperty() {\n     assertThat(value).isEqualTo(DeleteGranularity.FILE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteGranularityWriteOption() {\n     Table table = validationCatalog.loadTable(tableIdent);\n \n@@ -142,7 +145,7 @@ public void testDeleteGranularityWriteOption() {\n     assertThat(value).isEqualTo(DeleteGranularity.FILE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteGranularityInvalidValue() {\n     Table table = validationCatalog.loadTable(tableIdent);\n \n@@ -155,7 +158,7 @@ public void testDeleteGranularityInvalidValue() {\n         .hasMessageContaining(\"Unknown delete granularity\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkWriteConfDistributionDefault() {\n     Table table = validationCatalog.loadTable(tableIdent);\n \n@@ -164,7 +167,7 @@ public void testSparkWriteConfDistributionDefault() {\n     checkMode(DistributionMode.HASH, writeConf);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkWriteConfDistributionModeWithWriteOption() {\n     Table table = validationCatalog.loadTable(tableIdent);\n \n@@ -175,7 +178,7 @@ public void testSparkWriteConfDistributionModeWithWriteOption() {\n     checkMode(DistributionMode.NONE, writeConf);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkWriteConfDistributionModeWithSessionConfig() {\n     withSQLConf(\n         ImmutableMap.of(SparkSQLProperties.DISTRIBUTION_MODE, DistributionMode.NONE.modeName()),\n@@ -186,7 +189,7 @@ public void testSparkWriteConfDistributionModeWithSessionConfig() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkWriteConfDistributionModeWithTableProperties() {\n     Table table = validationCatalog.loadTable(tableIdent);\n \n@@ -202,7 +205,7 @@ public void testSparkWriteConfDistributionModeWithTableProperties() {\n     checkMode(DistributionMode.NONE, writeConf);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkWriteConfDistributionModeWithTblPropAndSessionConfig() {\n     withSQLConf(\n         ImmutableMap.of(SparkSQLProperties.DISTRIBUTION_MODE, DistributionMode.NONE.modeName()),\n@@ -223,7 +226,7 @@ public void testSparkWriteConfDistributionModeWithTblPropAndSessionConfig() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkWriteConfDistributionModeWithWriteOptionAndSessionConfig() {\n     withSQLConf(\n         ImmutableMap.of(SparkSQLProperties.DISTRIBUTION_MODE, DistributionMode.RANGE.modeName()),\n@@ -240,7 +243,7 @@ public void testSparkWriteConfDistributionModeWithWriteOptionAndSessionConfig()\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkWriteConfDistributionModeWithEverything() {\n     withSQLConf(\n         ImmutableMap.of(SparkSQLProperties.DISTRIBUTION_MODE, DistributionMode.RANGE.modeName()),\n@@ -265,7 +268,7 @@ public void testSparkWriteConfDistributionModeWithEverything() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkConfOverride() {\n     List<List<Map<String, String>>> propertiesSuites =\n         Lists.newArrayList(\n@@ -338,7 +341,7 @@ public void testSparkConfOverride() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDataPropsDefaultsAsDeleteProps() {\n     List<List<Map<String, String>>> propertiesSuites =\n         Lists.newArrayList(\n@@ -407,7 +410,7 @@ public void testDataPropsDefaultsAsDeleteProps() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteFileWriteConf() {\n     List<List<Map<String, String>>> propertiesSuites =\n         Lists.newArrayList(\n@@ -502,9 +505,9 @@ private void testWriteProperties(List<Map<String, String>> propertiesSuite) {\n           SparkWriteConf writeConf = new SparkWriteConf(spark, table, ImmutableMap.of());\n           Map<String, String> writeProperties = writeConf.writeProperties();\n           Map<String, String> expectedProperties = propertiesSuite.get(2);\n-          Assert.assertEquals(expectedProperties.size(), writeConf.writeProperties().size());\n+          assertThat(writeConf.writeProperties()).hasSameSizeAs(expectedProperties);\n           for (Map.Entry<String, String> entry : writeProperties.entrySet()) {\n-            Assert.assertEquals(entry.getValue(), expectedProperties.get(entry.getKey()));\n+            assertThat(expectedProperties).containsEntry(entry.getKey(), entry.getValue());\n           }\n \n           table.refresh();\n@@ -518,12 +521,12 @@ private void testWriteProperties(List<Map<String, String>> propertiesSuite) {\n   }\n \n   private void checkMode(DistributionMode expectedMode, SparkWriteConf writeConf) {\n-    Assert.assertEquals(expectedMode, writeConf.distributionMode());\n-    Assert.assertEquals(expectedMode, writeConf.copyOnWriteDistributionMode(DELETE));\n-    Assert.assertEquals(expectedMode, writeConf.positionDeltaDistributionMode(DELETE));\n-    Assert.assertEquals(expectedMode, writeConf.copyOnWriteDistributionMode(UPDATE));\n-    Assert.assertEquals(expectedMode, writeConf.positionDeltaDistributionMode(UPDATE));\n-    Assert.assertEquals(expectedMode, writeConf.copyOnWriteDistributionMode(MERGE));\n-    Assert.assertEquals(expectedMode, writeConf.positionDeltaDistributionMode(MERGE));\n+    assertThat(writeConf.distributionMode()).isEqualTo(expectedMode);\n+    assertThat(writeConf.copyOnWriteDistributionMode(DELETE)).isEqualTo(expectedMode);\n+    assertThat(writeConf.positionDeltaDistributionMode(DELETE)).isEqualTo(expectedMode);\n+    assertThat(writeConf.copyOnWriteDistributionMode(UPDATE)).isEqualTo(expectedMode);\n+    assertThat(writeConf.positionDeltaDistributionMode(UPDATE)).isEqualTo(expectedMode);\n+    assertThat(writeConf.copyOnWriteDistributionMode(MERGE)).isEqualTo(expectedMode);\n+    assertThat(writeConf.positionDeltaDistributionMode(MERGE)).isEqualTo(expectedMode);\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestMigrateTableAction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestMigrateTableAction.java\nindex 7bed72b7cc2c..94afa50cf4b8 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestMigrateTableAction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestMigrateTableAction.java\n@@ -18,34 +18,32 @@\n  */\n package org.apache.iceberg.spark.actions;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.IOException;\n-import java.util.Map;\n+import java.nio.file.Files;\n import java.util.concurrent.Executors;\n import java.util.concurrent.atomic.AtomicInteger;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.spark.CatalogTestBase;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestMigrateTableAction extends SparkCatalogTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestMigrateTableAction extends CatalogTestBase {\n \n-  public TestMigrateTableAction(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMigrateWithParallelTasks() throws IOException {\n-    String location = temp.newFolder().toURI().toString();\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         tableName, location);\n@@ -65,6 +63,6 @@ public void testMigrateWithParallelTasks() throws IOException {\n                   return thread;\n                 }))\n         .execute();\n-    Assert.assertEquals(migrationThreadsIndex.get(), 2);\n+    assertThat(migrationThreadsIndex.get()).isEqualTo(2);\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java\nindex 013b8d4386af..99d3f38ee7eb 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java\n@@ -31,6 +31,7 @@\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Snapshot;\n@@ -45,8 +46,8 @@\n import org.apache.iceberg.relocated.com.google.common.math.LongMath;\n import org.apache.iceberg.spark.CommitMetadata;\n import org.apache.iceberg.spark.SparkReadOptions;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n import org.apache.iceberg.spark.SparkWriteOptions;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.SnapshotUtil;\n import org.apache.spark.sql.Column;\n@@ -57,14 +58,13 @@\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.functions;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestDataSourceOptions extends SparkTestBaseWithCatalog {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestDataSourceOptions extends TestBaseWithCatalog {\n \n   private static final Configuration CONF = new Configuration();\n   private static final Schema SCHEMA =\n@@ -72,23 +72,21 @@ public class TestDataSourceOptions extends SparkTestBaseWithCatalog {\n           optional(1, \"id\", Types.IntegerType.get()), optional(2, \"data\", Types.StringType.get()));\n   private static SparkSession spark = null;\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n-\n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestDataSourceOptions.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestDataSourceOptions.spark;\n     TestDataSourceOptions.spark = null;\n     currentSpark.stop();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWriteFormatOptionOverridesTableProperties() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -111,14 +109,14 @@ public void testWriteFormatOptionOverridesTableProperties() throws IOException {\n       tasks.forEach(\n           task -> {\n             FileFormat fileFormat = FileFormat.fromFileName(task.file().location());\n-            Assert.assertEquals(FileFormat.PARQUET, fileFormat);\n+            assertThat(fileFormat).isEqualTo(FileFormat.PARQUET);\n           });\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoWriteFormatOption() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -136,14 +134,14 @@ public void testNoWriteFormatOption() throws IOException {\n       tasks.forEach(\n           task -> {\n             FileFormat fileFormat = FileFormat.fromFileName(task.file().location());\n-            Assert.assertEquals(FileFormat.AVRO, fileFormat);\n+            assertThat(fileFormat).isEqualTo(FileFormat.AVRO);\n           });\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHadoopOptions() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n     Configuration sparkHadoopConf = spark.sessionState().newHadoopConf();\n     String originalDefaultFS = sparkHadoopConf.get(\"fs.default.name\");\n \n@@ -177,15 +175,15 @@ public void testHadoopOptions() throws IOException {\n       List<SimpleRecord> resultRecords =\n           resultDf.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n \n-      Assert.assertEquals(\"Records should match\", expectedRecords, resultRecords);\n+      assertThat(resultRecords).as(\"Records should match\").isEqualTo(expectedRecords);\n     } finally {\n       sparkHadoopConf.set(\"fs.default.name\", originalDefaultFS);\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSplitOptionsOverridesTableProperties() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -209,7 +207,7 @@ public void testSplitOptionsOverridesTableProperties() throws IOException {\n \n     List<DataFile> files =\n         Lists.newArrayList(icebergTable.currentSnapshot().addedDataFiles(icebergTable.io()));\n-    Assert.assertEquals(\"Should have written 1 file\", 1, files.size());\n+    assertThat(files).as(\"Should have written 1 file\").hasSize(1);\n \n     long fileSize = files.get(0).fileSizeInBytes();\n     long splitSize = LongMath.divide(fileSize, 2, RoundingMode.CEILING);\n@@ -221,12 +219,14 @@ public void testSplitOptionsOverridesTableProperties() throws IOException {\n             .option(SparkReadOptions.SPLIT_SIZE, String.valueOf(splitSize))\n             .load(tableLocation);\n \n-    Assert.assertEquals(\"Spark partitions should match\", 2, resultDf.javaRDD().getNumPartitions());\n+    assertThat(resultDf.javaRDD().getNumPartitions())\n+        .as(\"Spark partitions should match\")\n+        .isEqualTo(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testIncrementalScanOptions() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -330,9 +330,9 @@ public void testIncrementalScanOptions() throws IOException {\n     assertThat(row2.getInt(1)).as(\"max value should match\").isEqualTo(3);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMetadataSplitSizeOptionOverrideTableProperties() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -349,7 +349,7 @@ public void testMetadataSplitSizeOptionOverrideTableProperties() throws IOExcept\n \n     List<ManifestFile> manifests = table.currentSnapshot().allManifests(table.io());\n \n-    Assert.assertEquals(\"Must be 2 manifests\", 2, manifests.size());\n+    assertThat(manifests).as(\"Must be 2 manifests\").hasSize(2);\n \n     // set the target metadata split size so each manifest ends up in a separate split\n     table\n@@ -358,7 +358,7 @@ public void testMetadataSplitSizeOptionOverrideTableProperties() throws IOExcept\n         .commit();\n \n     Dataset<Row> entriesDf = spark.read().format(\"iceberg\").load(tableLocation + \"#entries\");\n-    Assert.assertEquals(\"Num partitions must match\", 2, entriesDf.javaRDD().getNumPartitions());\n+    assertThat(entriesDf.javaRDD().getNumPartitions()).as(\"Num partitions must match\").isEqualTo(2);\n \n     // override the table property using options\n     entriesDf =\n@@ -367,12 +367,12 @@ public void testMetadataSplitSizeOptionOverrideTableProperties() throws IOExcept\n             .format(\"iceberg\")\n             .option(SparkReadOptions.SPLIT_SIZE, String.valueOf(128 * 1024 * 1024))\n             .load(tableLocation + \"#entries\");\n-    Assert.assertEquals(\"Num partitions must match\", 1, entriesDf.javaRDD().getNumPartitions());\n+    assertThat(entriesDf.javaRDD().getNumPartitions()).as(\"Num partitions must match\").isEqualTo(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultMetadataSplitSize() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -401,12 +401,12 @@ public void testDefaultMetadataSplitSize() throws IOException {\n     Dataset<Row> metadataDf = spark.read().format(\"iceberg\").load(tableLocation + \"#entries\");\n \n     int partitionNum = metadataDf.javaRDD().getNumPartitions();\n-    Assert.assertEquals(\"Spark partitions should match\", expectedSplits, partitionNum);\n+    assertThat(partitionNum).as(\"Spark partitions should match\").isEqualTo(expectedSplits);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExtraSnapshotMetadata() throws IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n     HadoopTables tables = new HadoopTables(CONF);\n     tables.create(SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap(), tableLocation);\n \n@@ -424,13 +424,14 @@ public void testExtraSnapshotMetadata() throws IOException {\n \n     Table table = tables.load(tableLocation);\n \n-    Assert.assertTrue(table.currentSnapshot().summary().get(\"extra-key\").equals(\"someValue\"));\n-    Assert.assertTrue(table.currentSnapshot().summary().get(\"another-key\").equals(\"anotherValue\"));\n+    assertThat(table.currentSnapshot().summary())\n+        .containsEntry(\"extra-key\", \"someValue\")\n+        .containsEntry(\"another-key\", \"anotherValue\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExtraSnapshotMetadataWithSQL() throws InterruptedException, IOException {\n-    String tableLocation = temp.newFolder(\"iceberg-table\").toString();\n+    String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n     HadoopTables tables = new HadoopTables(CONF);\n \n     Table table =\n@@ -465,15 +466,15 @@ public void testExtraSnapshotMetadataWithSQL() throws InterruptedException, IOEx\n     writerThread.join();\n \n     List<Snapshot> snapshots = Lists.newArrayList(table.snapshots());\n-    Assert.assertEquals(2, snapshots.size());\n-    Assert.assertNull(snapshots.get(0).summary().get(\"writer-thread\"));\n+    assertThat(snapshots).hasSize(2);\n+    assertThat(snapshots.get(0).summary()).doesNotContainKey(\"writer-thread\");\n     assertThat(snapshots.get(1).summary())\n         .containsEntry(\"writer-thread\", \"test-extra-commit-message-writer-thread\")\n         .containsEntry(\"extra-key\", \"someValue\")\n         .containsEntry(\"another-key\", \"anotherValue\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExtraSnapshotMetadataWithDelete()\n       throws InterruptedException, NoSuchTableException {\n     spark.sessionState().conf().setConfString(\"spark.sql.shuffle.partitions\", \"1\");\n@@ -508,8 +509,7 @@ public void testExtraSnapshotMetadataWithDelete()\n \n     Table table = validationCatalog.loadTable(tableIdent);\n     List<Snapshot> snapshots = Lists.newArrayList(table.snapshots());\n-    Assert.assertEquals(2, snapshots.size());\n-    Assert.assertNull(snapshots.get(0).summary().get(\"writer-thread\"));\n+    assertThat(snapshots.get(0).summary()).doesNotContainKey(\"writer-thread\");\n     assertThat(snapshots.get(1).summary())\n         .containsEntry(\"writer-thread\", \"test-extra-commit-message-delete-thread\")\n         .containsEntry(\"extra-key\", \"someValue\")\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java\nindex 746415818c84..701c421a253b 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java\n@@ -26,20 +26,17 @@\n import org.apache.iceberg.Table;\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.hadoop.HadoopTables;\n-import org.junit.Before;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.io.TempDir;\n \n public class TestIcebergSourceHadoopTables extends TestIcebergSourceTablesBase {\n \n   private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n-\n-  File tableDir = null;\n+  @TempDir private File tableDir;\n   String tableLocation = null;\n \n-  @Before\n-  public void setupTable() throws Exception {\n-    this.tableDir = temp.newFolder();\n-    tableDir.delete(); // created by table create\n-\n+  @BeforeEach\n+  public void setupTable() {\n     this.tableLocation = tableDir.toURI().toString();\n   }\n \n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHiveTables.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHiveTables.java\nindex 2a264b74b0e2..9120bbcc35a3 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHiveTables.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHiveTables.java\n@@ -27,14 +27,14 @@\n import org.apache.iceberg.Table;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.catalog.TableIdentifier;\n-import org.junit.After;\n-import org.junit.BeforeClass;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeAll;\n \n public class TestIcebergSourceHiveTables extends TestIcebergSourceTablesBase {\n \n   private static TableIdentifier currentIdentifier;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void start() {\n     Namespace db = Namespace.of(\"db\");\n     if (!catalog.namespaceExists(db)) {\n@@ -42,7 +42,7 @@ public static void start() {\n     }\n   }\n \n-  @After\n+  @AfterEach\n   public void dropTable() throws IOException {\n     if (!catalog.tableExists(currentIdentifier)) {\n       return;\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java\nindex 19e8e8146d86..09219963f7a3 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java\n@@ -79,8 +79,8 @@\n import org.apache.iceberg.spark.SparkSQLProperties;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.iceberg.spark.SparkTableUtil;\n-import org.apache.iceberg.spark.SparkTestBase;\n import org.apache.iceberg.spark.SparkWriteOptions;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.spark.actions.SparkActions;\n import org.apache.iceberg.spark.data.TestHelpers;\n import org.apache.iceberg.types.Types;\n@@ -96,13 +96,11 @@\n import org.apache.spark.sql.functions;\n import org.apache.spark.sql.internal.SQLConf;\n import org.apache.spark.sql.types.StructType;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n-public abstract class TestIcebergSourceTablesBase extends SparkTestBase {\n+public abstract class TestIcebergSourceTablesBase extends TestBase {\n \n   private static final Schema SCHEMA =\n       new Schema(\n@@ -121,7 +119,7 @@ public abstract class TestIcebergSourceTablesBase extends SparkTestBase {\n \n   private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"id\").build();\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir protected Path temp;\n \n   public abstract Table createTable(\n       TableIdentifier ident, Schema schema, PartitionSpec spec, Map<String, String> properties);\n@@ -134,7 +132,7 @@ public abstract Table createTable(\n \n   public abstract void dropTable(TableIdentifier ident) throws IOException;\n \n-  @After\n+  @AfterEach\n   public void removeTable() {\n     spark.sql(\"DROP TABLE IF EXISTS parquet_table\");\n   }\n@@ -164,7 +162,7 @@ public synchronized void testTablesSupport() {\n     List<SimpleRecord> actualRecords =\n         resultDf.orderBy(\"id\").as(Encoders.bean(SimpleRecord.class)).collectAsList();\n \n-    Assert.assertEquals(\"Records should match\", expectedRecords, actualRecords);\n+    assertThat(actualRecords).as(\"Records should match\").isEqualTo(expectedRecords);\n   }\n \n   @Test\n@@ -191,8 +189,7 @@ public void testEntriesTable() throws Exception {\n \n     Snapshot snapshot = table.currentSnapshot();\n \n-    Assert.assertEquals(\n-        \"Should only contain one manifest\", 1, snapshot.allManifests(table.io()).size());\n+    assertThat(snapshot.allManifests(table.io())).as(\"Should only contain one manifest\").hasSize(1);\n \n     InputFile manifest = table.io().newInputFile(snapshot.allManifests(table.io()).get(0).path());\n     List<GenericData.Record> expected = Lists.newArrayList();\n@@ -209,8 +206,8 @@ public void testEntriesTable() throws Exception {\n           });\n     }\n \n-    Assert.assertEquals(\"Entries table should have one row\", 1, expected.size());\n-    Assert.assertEquals(\"Actual results should have one row\", 1, actual.size());\n+    assertThat(expected).as(\"Entries table should have one row\").hasSize(1);\n+    assertThat(actual).as(\"Actual results should have one row\").hasSize(1);\n     TestHelpers.assertEqualsSafe(\n         TestHelpers.nonDerivedSchema(entriesTableDs), expected.get(0), actual.get(0));\n   }\n@@ -240,8 +237,7 @@ public void testEntriesTablePartitionedPrune() {\n             .select(\"status\")\n             .collectAsList();\n \n-    Assert.assertEquals(\"Results should contain only one status\", 1, actual.size());\n-    Assert.assertEquals(\"That status should be Added (1)\", 1, actual.get(0).getInt(0));\n+    assertThat(actual).singleElement().satisfies(row -> assertThat(row.getInt(0)).isEqualTo(1));\n   }\n \n   @Test\n@@ -412,8 +408,8 @@ public void testAllEntriesTable() throws Exception {\n \n     expected.sort(Comparator.comparing(o -> (Long) o.get(\"snapshot_id\")));\n \n-    Assert.assertEquals(\"Entries table should have 3 rows\", 3, expected.size());\n-    Assert.assertEquals(\"Actual results should have 3 rows\", 3, actual.size());\n+    assertThat(expected).as(\"Entries table should have 3 rows\").hasSize(3);\n+    assertThat(actual).as(\"Actual results should have 3 rows\").hasSize(3);\n     for (int i = 0; i < expected.size(); i += 1) {\n       TestHelpers.assertEqualsSafe(\n           TestHelpers.nonDerivedSchema(entriesTableDs), expected.get(i), actual.get(i));\n@@ -438,16 +434,20 @@ public void testCountEntriesTable() {\n     final int expectedEntryCount = 1;\n \n     // count entries\n-    Assert.assertEquals(\n-        \"Count should return \" + expectedEntryCount,\n-        expectedEntryCount,\n-        spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier, \"entries\")).count());\n+    assertThat(\n+            spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier, \"entries\")).count())\n+        .as(\"Count should return \" + expectedEntryCount)\n+        .isEqualTo(expectedEntryCount);\n \n     // count all_entries\n-    Assert.assertEquals(\n-        \"Count should return \" + expectedEntryCount,\n-        expectedEntryCount,\n-        spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier, \"all_entries\")).count());\n+    assertThat(\n+            spark\n+                .read()\n+                .format(\"iceberg\")\n+                .load(loadLocation(tableIdentifier, \"all_entries\"))\n+                .count())\n+        .as(\"Count should return \" + expectedEntryCount)\n+        .isEqualTo(expectedEntryCount);\n   }\n \n   @Test\n@@ -496,8 +496,8 @@ public void testFilesTable() throws Exception {\n       }\n     }\n \n-    Assert.assertEquals(\"Files table should have one row\", 1, expected.size());\n-    Assert.assertEquals(\"Actual results should have one row\", 1, actual.size());\n+    assertThat(expected).as(\"Files table should have one row\").hasSize(1);\n+    assertThat(actual).as(\"Actual results should have one row\").hasSize(1);\n \n     TestHelpers.assertEqualsSafe(\n         TestHelpers.nonDerivedSchema(filesTableDs), expected.get(0), actual.get(0));\n@@ -514,7 +514,7 @@ public void testFilesTableWithSnapshotIdInheritance() throws Exception {\n         String.format(\n             \"CREATE TABLE parquet_table (data string, id int) \"\n                 + \"USING parquet PARTITIONED BY (id) LOCATION '%s'\",\n-            temp.newFolder()));\n+            temp.toFile()));\n \n     List<SimpleRecord> records =\n         Lists.newArrayList(new SimpleRecord(1, \"a\"), new SimpleRecord(2, \"b\"));\n@@ -552,8 +552,8 @@ public void testFilesTableWithSnapshotIdInheritance() throws Exception {\n     }\n \n     Types.StructType struct = TestHelpers.nonDerivedSchema(filesTableDs);\n-    Assert.assertEquals(\"Files table should have one row\", 2, expected.size());\n-    Assert.assertEquals(\"Actual results should have one row\", 2, actual.size());\n+    assertThat(expected).as(\"Files table should have 2 rows\").hasSize(2);\n+    assertThat(actual).as(\"Actual results should have 2 rows\").hasSize(2);\n     TestHelpers.assertEqualsSafe(struct, expected.get(0), actual.get(0));\n     TestHelpers.assertEqualsSafe(struct, expected.get(1), actual.get(1));\n   }\n@@ -570,7 +570,7 @@ public void testV1EntriesTableWithSnapshotIdInheritance() throws Exception {\n         String.format(\n             \"CREATE TABLE parquet_table (data string, id int) \"\n                 + \"USING parquet PARTITIONED BY (id) LOCATION '%s'\",\n-            temp.newFolder()));\n+            temp.toFile()));\n \n     List<SimpleRecord> records =\n         Lists.newArrayList(new SimpleRecord(1, \"a\"), new SimpleRecord(2, \"b\"));\n@@ -597,11 +597,21 @@ public void testV1EntriesTableWithSnapshotIdInheritance() throws Exception {\n \n     long snapshotId = table.currentSnapshot().snapshotId();\n \n-    Assert.assertEquals(\"Entries table should have 2 rows\", 2, actual.size());\n-    Assert.assertEquals(\"Sequence number must match\", 0, actual.get(0).getLong(0));\n-    Assert.assertEquals(\"Snapshot id must match\", snapshotId, actual.get(0).getLong(1));\n-    Assert.assertEquals(\"Sequence number must match\", 0, actual.get(1).getLong(0));\n-    Assert.assertEquals(\"Snapshot id must match\", snapshotId, actual.get(1).getLong(1));\n+    assertThat(actual).as(\"Entries table should have 2 rows\").hasSize(2);\n+    assertThat(actual)\n+        .first()\n+        .satisfies(\n+            row -> {\n+              assertThat(row.getLong(0)).isEqualTo(0);\n+              assertThat(row.getLong(1)).isEqualTo(snapshotId);\n+            });\n+    assertThat(actual)\n+        .element(1)\n+        .satisfies(\n+            row -> {\n+              assertThat(row.getLong(0)).isEqualTo(0);\n+              assertThat(row.getLong(1)).isEqualTo(snapshotId);\n+            });\n   }\n \n   @Test\n@@ -654,8 +664,8 @@ public void testFilesUnpartitionedTable() throws Exception {\n       }\n     }\n \n-    Assert.assertEquals(\"Files table should have one row\", 1, expected.size());\n-    Assert.assertEquals(\"Actual results should have one row\", 1, actual.size());\n+    assertThat(expected).as(\"Files table should have one row\").hasSize(1);\n+    assertThat(actual).as(\"Actual results should have one row\").hasSize(1);\n     TestHelpers.assertEqualsSafe(\n         TestHelpers.nonDerivedSchema(filesTableDs), expected.get(0), actual.get(0));\n   }\n@@ -706,12 +716,11 @@ public void testAllMetadataTablesWithStagedCommits() {\n             .load(loadLocation(tableIdentifier, \"all_entries\"))\n             .collectAsList();\n \n-    Assert.assertTrue(\n-        \"Stage table should have some snapshots\", table.snapshots().iterator().hasNext());\n-    Assert.assertNull(\"Stage table should have null currentSnapshot\", table.currentSnapshot());\n-    Assert.assertEquals(\"Actual results should have two rows\", 2, actualAllData.size());\n-    Assert.assertEquals(\"Actual results should have two rows\", 2, actualAllManifests.size());\n-    Assert.assertEquals(\"Actual results should have two rows\", 2, actualAllEntries.size());\n+    assertThat(table.snapshots().iterator()).as(\"Stage table should have some snapshots\").hasNext();\n+    assertThat(table.currentSnapshot()).as(\"Stage table should have null currentSnapshot\").isNull();\n+    assertThat(actualAllData).as(\"Actual results should have two rows\").hasSize(2);\n+    assertThat(actualAllManifests).as(\"Actual results should have two rows\").hasSize(2);\n+    assertThat(actualAllEntries).as(\"Actual results should have two rows\").hasSize(2);\n   }\n \n   @Test\n@@ -769,8 +778,8 @@ public void testAllDataFilesTable() throws Exception {\n \n     expected.sort(Comparator.comparing(o -> o.get(\"file_path\").toString()));\n \n-    Assert.assertEquals(\"Files table should have two rows\", 2, expected.size());\n-    Assert.assertEquals(\"Actual results should have two rows\", 2, actual.size());\n+    assertThat(expected).as(\"Files table should have two rows\").hasSize(2);\n+    assertThat(actual).as(\"Actual results should have two rows\").hasSize(2);\n     for (int i = 0; i < expected.size(); i += 1) {\n       TestHelpers.assertEqualsSafe(\n           TestHelpers.nonDerivedSchema(filesTableDs), expected.get(i), actual.get(i));\n@@ -861,7 +870,7 @@ public void testHistoryTable() {\n                 .set(\"is_current_ancestor\", true)\n                 .build());\n \n-    Assert.assertEquals(\"History table should have a row for each commit\", 4, actual.size());\n+    assertThat(actual).as(\"History table should have a row for each commit\").hasSize(4);\n     TestHelpers.assertEqualsSafe(historyTable.schema().asStruct(), expected.get(0), actual.get(0));\n     TestHelpers.assertEqualsSafe(historyTable.schema().asStruct(), expected.get(1), actual.get(1));\n     TestHelpers.assertEqualsSafe(historyTable.schema().asStruct(), expected.get(2), actual.get(2));\n@@ -940,7 +949,7 @@ public void testSnapshotsTable() {\n                         \"total-data-files\", \"0\"))\n                 .build());\n \n-    Assert.assertEquals(\"Snapshots table should have a row for each snapshot\", 2, actual.size());\n+    assertThat(actual).as(\"Snapshots table should have a row for each snapshot\").hasSize(2);\n     TestHelpers.assertEqualsSafe(snapTable.schema().asStruct(), expected.get(0), actual.get(0));\n     TestHelpers.assertEqualsSafe(snapTable.schema().asStruct(), expected.get(1), actual.get(1));\n   }\n@@ -1013,7 +1022,7 @@ public void testPrunedSnapshotsTable() {\n                         \"total-data-files\", \"0\"))\n                 .build());\n \n-    Assert.assertEquals(\"Snapshots table should have a row for each snapshot\", 2, actual.size());\n+    assertThat(actual).as(\"Snapshots table should have a row for each snapshot\").hasSize(2);\n     TestHelpers.assertEqualsSafe(projectedSchema.asStruct(), expected.get(0), actual.get(0));\n     TestHelpers.assertEqualsSafe(projectedSchema.asStruct(), expected.get(1), actual.get(1));\n   }\n@@ -1098,7 +1107,7 @@ public void testManifestsTable() {\n                                     .build()))\n                     .build());\n \n-    Assert.assertEquals(\"Manifests table should have two manifest rows\", 2, actual.size());\n+    assertThat(actual).as(\"Manifests table should have two manifest rows\").hasSize(2);\n     TestHelpers.assertEqualsSafe(manifestTable.schema().asStruct(), expected.get(0), actual.get(0));\n     TestHelpers.assertEqualsSafe(manifestTable.schema().asStruct(), expected.get(1), actual.get(1));\n   }\n@@ -1179,7 +1188,7 @@ public void testPruneManifestsTable() {\n                                     .build()))\n                     .build());\n \n-    Assert.assertEquals(\"Manifests table should have one manifest row\", 1, actual.size());\n+    assertThat(actual).as(\"Manifests table should have one manifest row\").hasSize(1);\n     TestHelpers.assertEqualsSafe(projectedSchema.asStruct(), expected.get(0), actual.get(0));\n   }\n \n@@ -1231,7 +1240,7 @@ public void testAllManifestsTable() {\n             .sorted(Comparator.comparing(o -> o.get(\"path\").toString()))\n             .collect(Collectors.toList());\n \n-    Assert.assertEquals(\"Manifests table should have 5 manifest rows\", 5, actual.size());\n+    assertThat(actual).as(\"Manifests table should have 5 manifest rows\").hasSize(5);\n     for (int i = 0; i < expected.size(); i += 1) {\n       TestHelpers.assertEqualsSafe(\n           manifestTable.schema().asStruct(), expected.get(i), actual.get(i));\n@@ -1294,10 +1303,9 @@ public void testUnpartitionedPartitionsTable() {\n \n     Table partitionsTable = loadTable(tableIdentifier, \"partitions\");\n \n-    Assert.assertEquals(\n-        \"Schema should not have partition field\",\n-        expectedSchema,\n-        partitionsTable.schema().asStruct());\n+    assertThat(expectedSchema)\n+        .as(\"Schema should not have partition field\")\n+        .isEqualTo(partitionsTable.schema().asStruct());\n \n     GenericRecordBuilder builder =\n         new GenericRecordBuilder(AvroSchemaUtil.convert(partitionsTable.schema(), \"partitions\"));\n@@ -1323,7 +1331,7 @@ public void testUnpartitionedPartitionsTable() {\n             .load(loadLocation(tableIdentifier, \"partitions\"))\n             .collectAsList();\n \n-    Assert.assertEquals(\"Unpartitioned partitions table should have one row\", 1, actual.size());\n+    assertThat(actual).as(\"Unpartitioned partitions table should have one row\").hasSize(1);\n     TestHelpers.assertEqualsSafe(expectedSchema, expectedRow, actual.get(0));\n   }\n \n@@ -1404,8 +1412,8 @@ public void testPartitionsTable() {\n             .set(\"last_updated_snapshot_id\", secondCommitId)\n             .build());\n \n-    Assert.assertEquals(\"Partitions table should have two rows\", 2, expected.size());\n-    Assert.assertEquals(\"Actual results should have two rows\", 2, actual.size());\n+    assertThat(expected).as(\"Partitions table should have two rows\").hasSize(2);\n+    assertThat(actual).as(\"Actual results should have two rows\").hasSize(2);\n     for (int i = 0; i < 2; i += 1) {\n       TestHelpers.assertEqualsSafe(\n           partitionsTable.schema().asStruct(), expected.get(i), actual.get(i));\n@@ -1421,7 +1429,7 @@ public void testPartitionsTable() {\n             .orderBy(\"partition.id\")\n             .collectAsList();\n \n-    Assert.assertEquals(\"Actual results should have one row\", 1, actualAfterFirstCommit.size());\n+    assertThat(actualAfterFirstCommit).as(\"Actual results should have one row\").hasSize(1);\n     TestHelpers.assertEqualsSafe(\n         partitionsTable.schema().asStruct(), expected.get(0), actualAfterFirstCommit.get(0));\n \n@@ -1433,7 +1441,7 @@ public void testPartitionsTable() {\n             .load(loadLocation(tableIdentifier, \"partitions\"))\n             .filter(\"partition.id < 2\")\n             .collectAsList();\n-    Assert.assertEquals(\"Actual results should have one row\", 1, filtered.size());\n+    assertThat(filtered).as(\"Actual results should have one row\").hasSize(1);\n     TestHelpers.assertEqualsSafe(\n         partitionsTable.schema().asStruct(), expected.get(0), filtered.get(0));\n \n@@ -1444,7 +1452,7 @@ public void testPartitionsTable() {\n             .load(loadLocation(tableIdentifier, \"partitions\"))\n             .filter(\"partition.id < 2 or record_count=1\")\n             .collectAsList();\n-    Assert.assertEquals(\"Actual results should have two row\", 2, nonFiltered.size());\n+    assertThat(nonFiltered).as(\"Actual results should have two rows\").hasSize(2);\n     for (int i = 0; i < 2; i += 1) {\n       TestHelpers.assertEqualsSafe(\n           partitionsTable.schema().asStruct(), expected.get(i), actual.get(i));\n@@ -1485,12 +1493,10 @@ public void testPartitionsTableLastUpdatedSnapshot() {\n     // check if rewrite manifest does not override metadata about data file's creating snapshot\n     RewriteManifests.Result rewriteManifestResult =\n         SparkActions.get().rewriteManifests(table).execute();\n-    Assert.assertEquals(\n-        \"rewrite replaced 2 manifests\",\n-        2,\n-        Iterables.size(rewriteManifestResult.rewrittenManifests()));\n-    Assert.assertEquals(\n-        \"rewrite added 1 manifests\", 1, Iterables.size(rewriteManifestResult.addedManifests()));\n+    assertThat(rewriteManifestResult.rewrittenManifests())\n+        .as(\"rewrite replaced 2 manifests\")\n+        .hasSize(2);\n+    assertThat(rewriteManifestResult.addedManifests()).as(\"rewrite added 1 manifests\").hasSize(1);\n \n     List<Row> actual =\n         spark\n@@ -1542,8 +1548,8 @@ public void testPartitionsTableLastUpdatedSnapshot() {\n             .set(\"last_updated_snapshot_id\", secondCommitId)\n             .build());\n \n-    Assert.assertEquals(\"Partitions table should have two rows\", 2, expected.size());\n-    Assert.assertEquals(\"Actual results should have two rows\", 2, actual.size());\n+    assertThat(expected).as(\"Partitions table should have two rows\").hasSize(2);\n+    assertThat(actual).as(\"Actual results should have two rows\").hasSize(2);\n     for (int i = 0; i < 2; i += 1) {\n       TestHelpers.assertEqualsSafe(\n           partitionsTable.schema().asStruct(), expected.get(i), actual.get(i));\n@@ -1557,7 +1563,7 @@ public void testPartitionsTableLastUpdatedSnapshot() {\n             .load(loadLocation(tableIdentifier, \"partitions\"))\n             .filter(\"partition.id < 2\")\n             .collectAsList();\n-    Assert.assertEquals(\"Actual results should have one row\", 1, filtered.size());\n+    assertThat(filtered).as(\"Actual results should have one row\").hasSize(1);\n     TestHelpers.assertEqualsSafe(\n         partitionsTable.schema().asStruct(), expected.get(0), filtered.get(0));\n \n@@ -1588,8 +1594,7 @@ public void testPartitionsTableLastUpdatedSnapshot() {\n             .format(\"iceberg\")\n             .load(loadLocation(tableIdentifier, \"partitions\"))\n             .collectAsList();\n-    Assert.assertEquals(\n-        \"Actual results should have two row\", 2, actualAfterSnapshotExpiration.size());\n+    assertThat(actualAfterSnapshotExpiration).as(\"Actual results should have two rows\").hasSize(2);\n     for (int i = 0; i < 2; i += 1) {\n       TestHelpers.assertEqualsSafe(\n           partitionsTable.schema().asStruct(),\n@@ -1645,7 +1650,7 @@ public void testPartitionsTableDeleteStats() {\n             .load(loadLocation(tableIdentifier, \"partitions\"))\n             .orderBy(\"partition.id\")\n             .collectAsList();\n-    Assert.assertEquals(\"Actual results should have two rows\", 2, actual.size());\n+    assertThat(actual).as(\"Actual results should have two rows\").hasSize(2);\n \n     GenericRecordBuilder builder =\n         new GenericRecordBuilder(AvroSchemaUtil.convert(partitionsTable.schema(), \"partitions\"));\n@@ -1705,7 +1710,7 @@ public void testPartitionsTableDeleteStats() {\n             .load(loadLocation(tableIdentifier, \"partitions\"))\n             .orderBy(\"partition.id\")\n             .collectAsList();\n-    Assert.assertEquals(\"Actual results should have two rows\", 2, actual.size());\n+    assertThat(actual).as(\"Actual results should have two rows\").hasSize(2);\n     expected.remove(0);\n     expected.add(\n         0,\n@@ -1747,8 +1752,9 @@ public synchronized void testSnapshotReadAfterAddColumn() {\n     table.refresh();\n \n     Dataset<Row> resultDf = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n-    Assert.assertEquals(\n-        \"Records should match\", originalRecords, resultDf.orderBy(\"id\").collectAsList());\n+    assertThat(resultDf.orderBy(\"id\").collectAsList())\n+        .as(\"Records should match\")\n+        .containsExactlyElementsOf(originalRecords);\n \n     Snapshot snapshotBeforeAddColumn = table.currentSnapshot();\n \n@@ -1777,8 +1783,9 @@ public synchronized void testSnapshotReadAfterAddColumn() {\n             RowFactory.create(5, \"xyz\", \"C\"));\n \n     Dataset<Row> resultDf2 = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n-    Assert.assertEquals(\n-        \"Records should match\", updatedRecords, resultDf2.orderBy(\"id\").collectAsList());\n+    assertThat(resultDf2.orderBy(\"id\").collectAsList())\n+        .as(\"Records should match\")\n+        .containsExactlyElementsOf(updatedRecords);\n \n     Dataset<Row> resultDf3 =\n         spark\n@@ -1786,9 +1793,10 @@ public synchronized void testSnapshotReadAfterAddColumn() {\n             .format(\"iceberg\")\n             .option(SparkReadOptions.SNAPSHOT_ID, snapshotBeforeAddColumn.snapshotId())\n             .load(loadLocation(tableIdentifier));\n-    Assert.assertEquals(\n-        \"Records should match\", originalRecords, resultDf3.orderBy(\"id\").collectAsList());\n-    Assert.assertEquals(\"Schemas should match\", originalSparkSchema, resultDf3.schema());\n+    assertThat(resultDf3.orderBy(\"id\").collectAsList())\n+        .as(\"Records should match\")\n+        .containsExactlyElementsOf(originalRecords);\n+    assertThat(resultDf3.schema()).as(\"Schemas should match\").isEqualTo(originalSparkSchema);\n   }\n \n   @Test\n@@ -1814,8 +1822,9 @@ public synchronized void testSnapshotReadAfterDropColumn() {\n     table.refresh();\n \n     Dataset<Row> resultDf = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n-    Assert.assertEquals(\n-        \"Records should match\", originalRecords, resultDf.orderBy(\"id\").collectAsList());\n+    assertThat(resultDf.orderBy(\"id\").collectAsList())\n+        .as(\"Records should match\")\n+        .containsExactlyElementsOf(originalRecords);\n \n     long tsBeforeDropColumn = waitUntilAfter(System.currentTimeMillis());\n     table.updateSchema().deleteColumn(\"data\").commit();\n@@ -1843,8 +1852,9 @@ public synchronized void testSnapshotReadAfterDropColumn() {\n             RowFactory.create(5, \"C\"));\n \n     Dataset<Row> resultDf2 = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n-    Assert.assertEquals(\n-        \"Records should match\", updatedRecords, resultDf2.orderBy(\"id\").collectAsList());\n+    assertThat(resultDf2.orderBy(\"id\").collectAsList())\n+        .as(\"Records should match\")\n+        .containsExactlyElementsOf(updatedRecords);\n \n     Dataset<Row> resultDf3 =\n         spark\n@@ -1852,9 +1862,10 @@ public synchronized void testSnapshotReadAfterDropColumn() {\n             .format(\"iceberg\")\n             .option(SparkReadOptions.AS_OF_TIMESTAMP, tsBeforeDropColumn)\n             .load(loadLocation(tableIdentifier));\n-    Assert.assertEquals(\n-        \"Records should match\", originalRecords, resultDf3.orderBy(\"id\").collectAsList());\n-    Assert.assertEquals(\"Schemas should match\", originalSparkSchema, resultDf3.schema());\n+    assertThat(resultDf3.orderBy(\"id\").collectAsList())\n+        .as(\"Records should match\")\n+        .containsExactlyElementsOf(originalRecords);\n+    assertThat(resultDf3.schema()).as(\"Schemas should match\").isEqualTo(originalSparkSchema);\n \n     // At tsAfterDropColumn, there has been a schema change, but no new snapshot,\n     // so the snapshot as of tsAfterDropColumn is the same as that as of tsBeforeDropColumn.\n@@ -1864,9 +1875,10 @@ public synchronized void testSnapshotReadAfterDropColumn() {\n             .format(\"iceberg\")\n             .option(SparkReadOptions.AS_OF_TIMESTAMP, tsAfterDropColumn)\n             .load(loadLocation(tableIdentifier));\n-    Assert.assertEquals(\n-        \"Records should match\", originalRecords, resultDf4.orderBy(\"id\").collectAsList());\n-    Assert.assertEquals(\"Schemas should match\", originalSparkSchema, resultDf4.schema());\n+    assertThat(resultDf4.orderBy(\"id\").collectAsList())\n+        .as(\"Records should match\")\n+        .containsExactlyElementsOf(originalRecords);\n+    assertThat(resultDf4.schema()).as(\"Schemas should match\").isEqualTo(originalSparkSchema);\n   }\n \n   @Test\n@@ -1890,8 +1902,9 @@ public synchronized void testSnapshotReadAfterAddAndDropColumn() {\n     table.refresh();\n \n     Dataset<Row> resultDf = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n-    Assert.assertEquals(\n-        \"Records should match\", originalRecords, resultDf.orderBy(\"id\").collectAsList());\n+    assertThat(resultDf.orderBy(\"id\").collectAsList())\n+        .as(\"Records should match\")\n+        .containsExactlyElementsOf(originalRecords);\n \n     Snapshot snapshotBeforeAddColumn = table.currentSnapshot();\n \n@@ -1920,8 +1933,9 @@ public synchronized void testSnapshotReadAfterAddAndDropColumn() {\n             RowFactory.create(5, \"xyz\", \"C\"));\n \n     Dataset<Row> resultDf2 = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n-    Assert.assertEquals(\n-        \"Records should match\", updatedRecords, resultDf2.orderBy(\"id\").collectAsList());\n+    assertThat(resultDf2.orderBy(\"id\").collectAsList())\n+        .as(\"Records should match\")\n+        .containsExactlyElementsOf(updatedRecords);\n \n     table.updateSchema().deleteColumn(\"data\").commit();\n \n@@ -1934,8 +1948,9 @@ public synchronized void testSnapshotReadAfterAddAndDropColumn() {\n             RowFactory.create(5, \"C\"));\n \n     Dataset<Row> resultDf3 = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n-    Assert.assertEquals(\n-        \"Records should match\", recordsAfterDropColumn, resultDf3.orderBy(\"id\").collectAsList());\n+    assertThat(resultDf3.orderBy(\"id\").collectAsList())\n+        .as(\"Records should match\")\n+        .containsExactlyElementsOf(recordsAfterDropColumn);\n \n     Dataset<Row> resultDf4 =\n         spark\n@@ -1943,9 +1958,10 @@ public synchronized void testSnapshotReadAfterAddAndDropColumn() {\n             .format(\"iceberg\")\n             .option(SparkReadOptions.SNAPSHOT_ID, snapshotBeforeAddColumn.snapshotId())\n             .load(loadLocation(tableIdentifier));\n-    Assert.assertEquals(\n-        \"Records should match\", originalRecords, resultDf4.orderBy(\"id\").collectAsList());\n-    Assert.assertEquals(\"Schemas should match\", originalSparkSchema, resultDf4.schema());\n+    assertThat(resultDf4.orderBy(\"id\").collectAsList())\n+        .as(\"Records should match\")\n+        .containsExactlyElementsOf(originalRecords);\n+    assertThat(resultDf4.schema()).as(\"Schemas should match\").isEqualTo(originalSparkSchema);\n   }\n \n   @Test\n@@ -1976,19 +1992,16 @@ public void testRemoveOrphanFilesActionSupport() throws InterruptedException {\n             .location(table.location() + \"/metadata\")\n             .olderThan(System.currentTimeMillis())\n             .execute();\n-    Assert.assertTrue(\n-        \"Should not delete any metadata files\", Iterables.isEmpty(result1.orphanFileLocations()));\n+    assertThat(result1.orphanFileLocations()).as(\"Should not delete any metadata files\").isEmpty();\n \n     DeleteOrphanFiles.Result result2 =\n         actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n-    Assert.assertEquals(\n-        \"Should delete 1 data file\", 1, Iterables.size(result2.orphanFileLocations()));\n+    assertThat(result2.orphanFileLocations()).as(\"Should delete 1 data file\").hasSize(1);\n \n     Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n     List<SimpleRecord> actualRecords =\n         resultDF.as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-\n-    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n+    assertThat(actualRecords).as(\"Rows must match\").containsExactlyInAnyOrderElementsOf(records);\n   }\n \n   @Test\n@@ -2033,7 +2046,7 @@ public void testFilesTablePartitionId() {\n             .map(r -> (Integer) r.getAs(DataFile.SPEC_ID.name()))\n             .collect(Collectors.toList());\n \n-    Assert.assertEquals(\"Should have two partition specs\", ImmutableList.of(spec0, spec1), actual);\n+    assertThat(actual).as(\"Should have two partition specs\").containsExactly(spec0, spec1);\n   }\n \n   @Test\n@@ -2067,7 +2080,7 @@ public void testAllManifestTableSnapshotFiltering() {\n \n     table.refresh();\n     Snapshot snapshot2 = table.currentSnapshot();\n-    Assert.assertEquals(\"Should have two manifests\", 2, snapshot2.allManifests(table.io()).size());\n+    assertThat(snapshot2.allManifests(table.io())).as(\"Should have two manifests\").hasSize(2);\n     snapshotIdToManifests.addAll(\n         snapshot2.allManifests(table.io()).stream()\n             .map(manifest -> Pair.of(snapshot2.snapshotId(), manifest))\n@@ -2109,7 +2122,7 @@ public void testAllManifestTableSnapshotFiltering() {\n             .sorted(Comparator.comparing(o -> o.get(\"path\").toString()))\n             .collect(Collectors.toList());\n \n-    Assert.assertEquals(\"Manifests table should have 3 manifest rows\", 3, actual.size());\n+    assertThat(actual).as(\"Manifests table should have 3 manifest rows\").hasSize(3);\n     for (int i = 0; i < expected.size(); i += 1) {\n       TestHelpers.assertEqualsSafe(\n           manifestTable.schema().asStruct(), expected.get(i), actual.get(i));\n@@ -2118,7 +2131,7 @@ public void testAllManifestTableSnapshotFiltering() {\n \n   @Test\n   public void testTableWithInt96Timestamp() throws IOException {\n-    File parquetTableDir = temp.newFolder(\"table_timestamp_int96\");\n+    File parquetTableDir = temp.resolve(\"table_timestamp_int96\").toFile();\n     String parquetTableLocation = parquetTableDir.toURI().toString();\n     Schema schema =\n         new Schema(\n@@ -2178,8 +2191,8 @@ public void testImportSparkTableWithMissingFilesFailure() throws IOException {\n     TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"missing_files_test\");\n     Table table = createTable(tableIdentifier, SCHEMA, SPEC);\n \n-    File parquetTablePath = temp.newFolder(\"table_missing_files\");\n-    String parquetTableLocation = parquetTablePath.toURI().toString();\n+    File parquetTableDir = temp.resolve(\"table_missing_files\").toFile();\n+    String parquetTableLocation = parquetTableDir.toURI().toString();\n     spark.sql(\n         String.format(\n             \"CREATE TABLE parquet_table (data string, id int) \"\n@@ -2194,7 +2207,7 @@ public void testImportSparkTableWithMissingFilesFailure() throws IOException {\n \n     // Add a Spark partition of which location is missing\n     spark.sql(\"ALTER TABLE parquet_table ADD PARTITION (id = 1234)\");\n-    Path partitionLocationPath = parquetTablePath.toPath().resolve(\"id=1234\");\n+    Path partitionLocationPath = parquetTableDir.toPath().resolve(\"id=1234\");\n     java.nio.file.Files.delete(partitionLocationPath);\n \n     String stagingLocation = table.location() + \"/metadata\";\n@@ -2217,7 +2230,7 @@ public void testImportSparkTableWithIgnoreMissingFilesEnabled() throws IOExcepti\n     TableIdentifier tableIdentifier = TableIdentifier.of(\"db\", \"missing_files_test\");\n     Table table = createTable(tableIdentifier, SCHEMA, SPEC);\n \n-    File parquetTableDir = temp.newFolder(\"table_missing_files\");\n+    File parquetTableDir = temp.resolve(\"table_missing_files\").toFile();\n     String parquetTableLocation = parquetTableDir.toURI().toString();\n     spark.sql(\n         String.format(\n@@ -2415,7 +2428,7 @@ private DeleteFile writeEqDeleteFile(Table table, String dataValue) {\n     try {\n       return FileHelpers.writeDeleteFile(\n           table,\n-          Files.localOutput(temp.newFile()),\n+          Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n           org.apache.iceberg.TestHelpers.Row.of(1),\n           deletes,\n           deleteRowSchema);\n@@ -2430,16 +2443,14 @@ private long totalSizeInBytes(Iterable<DataFile> dataFiles) {\n \n   private void assertDataFilePartitions(\n       List<DataFile> dataFiles, List<Integer> expectedPartitionIds) {\n-    Assert.assertEquals(\n-        \"Table should have \" + expectedPartitionIds.size() + \" data files\",\n-        expectedPartitionIds.size(),\n-        dataFiles.size());\n+    assertThat(dataFiles)\n+        .as(\"Table should have \" + expectedPartitionIds.size() + \" data files\")\n+        .hasSameSizeAs(expectedPartitionIds);\n \n     for (int i = 0; i < dataFiles.size(); ++i) {\n-      Assert.assertEquals(\n-          \"Data file should have partition of id \" + expectedPartitionIds.get(i),\n-          expectedPartitionIds.get(i).intValue(),\n-          dataFiles.get(i).partition().get(0, Integer.class).intValue());\n+      assertThat(dataFiles.get(i).partition().get(0, Integer.class).intValue())\n+          .as(\"Data file should have partition of id \" + expectedPartitionIds.get(i))\n+          .isEqualTo(expectedPartitionIds.get(i).intValue());\n     }\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTableReadableMetrics.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTableReadableMetrics.java\nindex 9075257fa9f1..c21ccd0100db 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTableReadableMetrics.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTableReadableMetrics.java\n@@ -29,6 +29,8 @@\n import java.util.Map;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.Files;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -41,19 +43,17 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.spark.SparkCatalogConfig;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.Pair;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n-import org.junit.After;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestMetadataTableReadableMetrics extends SparkTestBaseWithCatalog {\n-\n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestMetadataTableReadableMetrics extends TestBaseWithCatalog {\n \n   private static final Types.StructType LEAF_STRUCT_TYPE =\n       Types.StructType.of(\n@@ -78,9 +78,16 @@ public class TestMetadataTableReadableMetrics extends SparkTestBaseWithCatalog {\n           optional(8, \"fixedCol\", Types.FixedType.ofLength(3)),\n           optional(9, \"binaryCol\", Types.BinaryType.get()));\n \n-  public TestMetadataTableReadableMetrics() {\n-    // only SparkCatalog supports metadata table sql queries\n-    super(SparkCatalogConfig.HIVE);\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  protected static Object[][] parameters() {\n+    return new Object[][] {\n+      {\n+        // only SparkCatalog supports metadata table sql queries\n+        SparkCatalogConfig.HIVE.catalogName(),\n+        SparkCatalogConfig.HIVE.implementation(),\n+        SparkCatalogConfig.HIVE.properties()\n+      },\n+    };\n   }\n \n   protected String tableName() {\n@@ -124,8 +131,7 @@ private Table createPrimitiveTable() throws IOException {\n             createPrimitiveRecord(\n                 false, 2, 2L, Float.NaN, 2.0D, new BigDecimal(\"2.00\"), \"2\", null, null));\n \n-    DataFile dataFile =\n-        FileHelpers.writeDataFile(table, Files.localOutput(temp.newFile()), records);\n+    DataFile dataFile = FileHelpers.writeDataFile(table, Files.localOutput(temp.toFile()), records);\n     table.newAppend().appendFile(dataFile).commit();\n     return table;\n   }\n@@ -143,13 +149,12 @@ private Pair<Table, DataFile> createNestedTable() throws IOException {\n             createNestedRecord(0L, 0.0),\n             createNestedRecord(1L, Double.NaN),\n             createNestedRecord(null, null));\n-    DataFile dataFile =\n-        FileHelpers.writeDataFile(table, Files.localOutput(temp.newFile()), records);\n+    DataFile dataFile = FileHelpers.writeDataFile(table, Files.localOutput(temp.toFile()), records);\n     table.newAppend().appendFile(dataFile).commit();\n     return Pair.of(table, dataFile);\n   }\n \n-  @After\n+  @AfterEach\n   public void dropTable() {\n     sql(\"DROP TABLE %s\", tableName);\n   }\n@@ -192,7 +197,7 @@ private GenericRecord createNestedRecord(Long longCol, Double doubleCol) {\n     return record;\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPrimitiveColumns() throws Exception {\n     Table table = createPrimitiveTable();\n     DataFile dataFile = table.currentSnapshot().addedDataFiles(table.io()).iterator().next();\n@@ -291,7 +296,7 @@ public void testPrimitiveColumns() throws Exception {\n     assertEquals(\"Row should match for entries table\", expected, entriesReadableMetrics);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSelectPrimitiveValues() throws Exception {\n     createPrimitiveTable();\n \n@@ -330,7 +335,7 @@ public void testSelectPrimitiveValues() throws Exception {\n         sql(\"SELECT readable_metrics.longCol.value_count, status FROM %s.entries\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSelectNestedValues() throws Exception {\n     createNestedTable();\n \n@@ -351,7 +356,7 @@ public void testSelectNestedValues() throws Exception {\n         entriesReadableMetrics);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNestedValues() throws Exception {\n     Pair<Table, DataFile> table = createNestedTable();\n     int longColId =\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTablesWithPartitionEvolution.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTablesWithPartitionEvolution.java\nindex 13abdad9ef92..a417454b45dc 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTablesWithPartitionEvolution.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTablesWithPartitionEvolution.java\n@@ -29,14 +29,18 @@\n import static org.apache.iceberg.TableProperties.DEFAULT_FILE_FORMAT;\n import static org.apache.iceberg.TableProperties.FORMAT_VERSION;\n import static org.apache.iceberg.TableProperties.MANIFEST_MERGE_ENABLED;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.util.Arrays;\n import java.util.List;\n-import java.util.Map;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.HasTableOperations;\n import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableMetadata;\n@@ -46,24 +50,20 @@\n import org.apache.iceberg.expressions.Expressions;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.SparkCatalog;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n import org.apache.spark.sql.types.DataType;\n import org.apache.spark.sql.types.StructType;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Test;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-import org.junit.runners.Parameterized.Parameters;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-@RunWith(Parameterized.class)\n-public class TestMetadataTablesWithPartitionEvolution extends SparkCatalogTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestMetadataTablesWithPartitionEvolution extends CatalogTestBase {\n \n   @Parameters(name = \"catalog = {0}, impl = {1}, conf = {2}, fileFormat = {3}, formatVersion = {4}\")\n   public static Object[][] parameters() {\n@@ -119,26 +119,18 @@ public static Object[][] parameters() {\n     };\n   }\n \n-  private final FileFormat fileFormat;\n-  private final int formatVersion;\n-\n-  public TestMetadataTablesWithPartitionEvolution(\n-      String catalogName,\n-      String implementation,\n-      Map<String, String> config,\n-      FileFormat fileFormat,\n-      int formatVersion) {\n-    super(catalogName, implementation, config);\n-    this.fileFormat = fileFormat;\n-    this.formatVersion = formatVersion;\n-  }\n+  @Parameter(index = 3)\n+  private FileFormat fileFormat;\n+\n+  @Parameter(index = 4)\n+  private int formatVersion;\n \n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilesMetadataTable() throws ParseException {\n     createTable(\"id bigint NOT NULL, category string, data string\");\n \n@@ -147,8 +139,9 @@ public void testFilesMetadataTable() throws ParseException {\n     // verify the metadata tables while the current spec is still unpartitioned\n     for (MetadataTableType tableType : Arrays.asList(FILES, ALL_DATA_FILES)) {\n       Dataset<Row> df = loadMetadataTable(tableType);\n-      Assert.assertTrue(\n-          \"Partition must be skipped\", df.schema().getFieldIndex(\"partition\").isEmpty());\n+      assertThat(df.schema().getFieldIndex(\"partition\").isEmpty())\n+          .as(\"Partition must be skipped\")\n+          .isTrue();\n     }\n \n     Table table = validationCatalog.loadTable(tableIdent);\n@@ -199,7 +192,7 @@ public void testFilesMetadataTable() throws ParseException {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilesMetadataTableFilter() throws ParseException {\n     createTable(\"id bigint NOT NULL, category string, data string\");\n     sql(\"ALTER TABLE %s SET TBLPROPERTIES ('%s' 'false')\", tableName, MANIFEST_MERGE_ENABLED);\n@@ -210,8 +203,9 @@ public void testFilesMetadataTableFilter() throws ParseException {\n     // verify the metadata tables while the current spec is still unpartitioned\n     for (MetadataTableType tableType : Arrays.asList(FILES, ALL_DATA_FILES)) {\n       Dataset<Row> df = loadMetadataTable(tableType);\n-      Assert.assertTrue(\n-          \"Partition must be skipped\", df.schema().getFieldIndex(\"partition\").isEmpty());\n+      assertThat(df.schema().getFieldIndex(\"partition\").isEmpty())\n+          .as(\"Partition must be skipped\")\n+          .isTrue();\n     }\n \n     Table table = validationCatalog.loadTable(tableIdent);\n@@ -290,7 +284,7 @@ public void testFilesMetadataTableFilter() throws ParseException {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testEntriesMetadataTable() throws ParseException {\n     createTable(\"id bigint NOT NULL, category string, data string\");\n \n@@ -300,7 +294,7 @@ public void testEntriesMetadataTable() throws ParseException {\n     for (MetadataTableType tableType : Arrays.asList(ENTRIES, ALL_ENTRIES)) {\n       Dataset<Row> df = loadMetadataTable(tableType);\n       StructType dataFileType = (StructType) df.schema().apply(\"data_file\").dataType();\n-      Assert.assertTrue(\"Partition must be skipped\", dataFileType.getFieldIndex(\"\").isEmpty());\n+      assertThat(dataFileType.getFieldIndex(\"\").isEmpty()).as(\"Partition must be skipped\").isTrue();\n     }\n \n     Table table = validationCatalog.loadTable(tableIdent);\n@@ -351,7 +345,7 @@ public void testEntriesMetadataTable() throws ParseException {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionsTableAddRemoveFields() throws ParseException {\n     createTable(\"id bigint NOT NULL, category string, data string\");\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'c1', 'd1')\", tableName);\n@@ -359,8 +353,9 @@ public void testPartitionsTableAddRemoveFields() throws ParseException {\n \n     // verify the metadata tables while the current spec is still unpartitioned\n     Dataset<Row> df = loadMetadataTable(PARTITIONS);\n-    Assert.assertTrue(\n-        \"Partition must be skipped\", df.schema().getFieldIndex(\"partition\").isEmpty());\n+    assertThat(df.schema().getFieldIndex(\"partition\").isEmpty())\n+        .as(\"Partition must be skipped\")\n+        .isTrue();\n \n     Table table = validationCatalog.loadTable(tableIdent);\n \n@@ -406,7 +401,7 @@ public void testPartitionsTableAddRemoveFields() throws ParseException {\n         PARTITIONS);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionsTableRenameFields() throws ParseException {\n     createTable(\"id bigint NOT NULL, category string, data string\");\n \n@@ -433,7 +428,7 @@ public void testPartitionsTableRenameFields() throws ParseException {\n         PARTITIONS);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionsTableSwitchFields() throws Exception {\n     createTable(\"id bigint NOT NULL, category string, data string\");\n \n@@ -495,7 +490,7 @@ public void testPartitionsTableSwitchFields() throws Exception {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionTableFilterAddRemoveFields() throws ParseException {\n     // Create un-partitioned table\n     createTable(\"id bigint NOT NULL, category string, data string\");\n@@ -549,13 +544,13 @@ public void testPartitionTableFilterAddRemoveFields() throws ParseException {\n         \"partition.category = 'c2'\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionTableFilterSwitchFields() throws Exception {\n     // Re-added partition fields currently not re-associated:\n     // https://github.com/apache/iceberg/issues/4292\n     // In V1, dropped partition fields show separately when field is re-added\n     // In V2, re-added field currently conflicts with its deleted form\n-    Assume.assumeTrue(formatVersion == 1);\n+    assumeThat(formatVersion).isEqualTo(1);\n \n     createTable(\"id bigint NOT NULL, category string, data string\");\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'c1', 'd1')\", tableName);\n@@ -595,7 +590,7 @@ public void testPartitionTableFilterSwitchFields() throws Exception {\n         \"partition.data = 'd1'\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionsTableFilterRenameFields() throws ParseException {\n     createTable(\"id bigint NOT NULL, category string, data string\");\n \n@@ -618,7 +613,7 @@ public void testPartitionsTableFilterRenameFields() throws ParseException {\n         \"partition.category_another_name = 'c1'\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMetadataTablesWithUnknownTransforms() {\n     createTable(\"id bigint NOT NULL, category string, data string\");\n \n@@ -647,7 +642,7 @@ public void testMetadataTablesWithUnknownTransforms() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionColumnNamedPartition() {\n     sql(\n         \"CREATE TABLE %s (id int, partition int) USING iceberg PARTITIONED BY (partition)\",\n@@ -655,7 +650,7 @@ public void testPartitionColumnNamedPartition() {\n     sql(\"INSERT INTO %s VALUES (1, 1), (2, 1), (3, 2), (2, 2)\", tableName);\n     List<Object[]> expected = ImmutableList.of(row(1, 1), row(2, 1), row(3, 2), row(2, 2));\n     assertEquals(\"Should return all expected rows\", expected, sql(\"SELECT * FROM %s\", tableName));\n-    Assert.assertEquals(2, sql(\"SELECT * FROM %s.files\", tableName).size());\n+    assertThat(sql(\"SELECT * FROM %s.files\", tableName)).hasSize(2);\n   }\n \n   private void assertPartitions(\n@@ -681,14 +676,14 @@ private void assertPartitions(\n       case FILES:\n       case ALL_DATA_FILES:\n         DataType actualFilesType = df.schema().apply(\"partition\").dataType();\n-        Assert.assertEquals(\"Partition type must match\", expectedType, actualFilesType);\n+        assertThat(actualFilesType).as(\"Partition type must match\").isEqualTo(expectedType);\n         break;\n \n       case ENTRIES:\n       case ALL_ENTRIES:\n         StructType dataFileType = (StructType) df.schema().apply(\"data_file\").dataType();\n         DataType actualEntriesType = dataFileType.apply(\"partition\").dataType();\n-        Assert.assertEquals(\"Partition type must match\", expectedType, actualEntriesType);\n+        assertThat(actualEntriesType).as(\"Partition type must match\").isEqualTo(expectedType);\n         break;\n \n       default:\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestFileRewriteCoordinator.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestFileRewriteCoordinator.java\nindex 666634a06c02..085eedf45d1d 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestFileRewriteCoordinator.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestFileRewriteCoordinator.java\n@@ -28,6 +28,7 @@\n import java.util.stream.Collectors;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.FileScanTask;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n@@ -41,7 +42,9 @@\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestFileRewriteCoordinator extends CatalogTestBase {\n \n   @AfterEach\n@@ -102,10 +105,9 @@ public void testBinPackRewrite() throws NoSuchTableException, IOException {\n     table.refresh();\n \n     Map<String, String> summary = table.currentSnapshot().summary();\n-    assertThat(summary.get(\"deleted-data-files\"))\n-        .as(\"Deleted files count must match\")\n-        .isEqualTo(\"4\");\n-    assertThat(summary.get(\"added-data-files\")).as(\"Added files count must match\").isEqualTo(\"2\");\n+    assertThat(summary)\n+        .containsEntry(\"deleted-data-files\", \"4\")\n+        .containsEntry(\"added-data-files\", \"2\");\n \n     Object rowCount = scalarSql(\"SELECT count(*) FROM %s\", tableName);\n     assertThat(rowCount).as(\"Row count must match\").isEqualTo(4000L);\n@@ -174,10 +176,9 @@ public void testSortRewrite() throws NoSuchTableException, IOException {\n     table.refresh();\n \n     Map<String, String> summary = table.currentSnapshot().summary();\n-    assertThat(summary.get(\"deleted-data-files\"))\n-        .as(\"Deleted files count must match\")\n-        .isEqualTo(\"4\");\n-    assertThat(summary.get(\"added-data-files\")).as(\"Added files count must match\").isEqualTo(\"2\");\n+    assertThat(summary)\n+        .containsEntry(\"deleted-data-files\", \"4\")\n+        .containsEntry(\"added-data-files\", \"2\");\n \n     Object rowCount = scalarSql(\"SELECT count(*) FROM %s\", tableName);\n     assertThat(rowCount).as(\"Row count must match\").isEqualTo(4000L);\n@@ -256,10 +257,9 @@ public void testCommitMultipleRewrites() throws NoSuchTableException, IOExceptio\n     assertThat(table.snapshots()).as(\"Should produce 5 snapshots\").hasSize(5);\n \n     Map<String, String> summary = table.currentSnapshot().summary();\n-    assertThat(summary.get(\"deleted-data-files\"))\n-        .as(\"Deleted files count must match\")\n-        .isEqualTo(\"4\");\n-    assertThat(summary.get(\"added-data-files\")).as(\"Added files count must match\").isEqualTo(\"2\");\n+    assertThat(summary)\n+        .containsEntry(\"deleted-data-files\", \"4\")\n+        .containsEntry(\"added-data-files\", \"2\");\n \n     Object rowCount = scalarSql(\"SELECT count(*) FROM %s\", tableName);\n     assertThat(rowCount).as(\"Row count must match\").isEqualTo(4000L);\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkCachedTableCatalog.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkCachedTableCatalog.java\nindex eaf230865957..228bf43b89b1 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkCachedTableCatalog.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkCachedTableCatalog.java\n@@ -18,6 +18,7 @@\n  */\n package org.apache.iceberg.spark;\n \n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n@@ -25,7 +26,9 @@\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkCachedTableCatalog extends TestBaseWithCatalog {\n \n   private static final SparkTableCache TABLE_CACHE = SparkTableCache.get();\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkCatalogOperations.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkCatalogOperations.java\nindex 5b460a0b527a..d7ead52880ee 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkCatalogOperations.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkCatalogOperations.java\n@@ -19,8 +19,10 @@\n package org.apache.iceberg.spark;\n \n import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.atIndex;\n \n import java.util.concurrent.ThreadLocalRandom;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.catalog.Catalog;\n@@ -36,7 +38,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkCatalogOperations extends CatalogTestBase {\n   private static final boolean USE_NULLABLE_QUERY_SCHEMA =\n       ThreadLocalRandom.current().nextBoolean();\n@@ -108,9 +112,9 @@ public void testAlterTable() throws NoSuchTableException {\n     assertThat(table).as(\"Should return updated table\").isNotNull();\n \n     Column expectedField = Column.create(fieldName, DataTypes.StringType, true);\n-    assertThat(table.columns()[2])\n+    assertThat(table.columns())\n         .as(\"Adding a column to a table should return the updated table with the new column\")\n-        .isEqualTo(expectedField);\n+        .contains(expectedField, atIndex(2));\n \n     assertThat(table.properties())\n         .as(\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java\nindex 6041c22e2465..a9b5d1a237b4 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java\n@@ -52,6 +52,7 @@\n import java.util.Map;\n import org.apache.iceberg.DistributionMode;\n import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.UpdateProperties;\n@@ -62,7 +63,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkWriteConf extends TestBaseWithCatalog {\n \n   @BeforeEach\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java\nindex c4ba96e63403..a7702b169a60 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataSourceOptions.java\n@@ -24,7 +24,6 @@\n \n import java.io.IOException;\n import java.math.RoundingMode;\n-import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n import org.apache.hadoop.conf.Configuration;\n@@ -32,6 +31,7 @@\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Snapshot;\n@@ -61,8 +61,9 @@\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.TestTemplate;\n-import org.junit.jupiter.api.io.TempDir;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestDataSourceOptions extends TestBaseWithCatalog {\n \n   private static final Configuration CONF = new Configuration();\n@@ -71,8 +72,6 @@ public class TestDataSourceOptions extends TestBaseWithCatalog {\n           optional(1, \"id\", Types.IntegerType.get()), optional(2, \"data\", Types.StringType.get()));\n   private static SparkSession spark = null;\n \n-  @TempDir private Path temp;\n-\n   @BeforeAll\n   public static void startSpark() {\n     TestDataSourceOptions.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n@@ -372,7 +371,7 @@ public void testMetadataSplitSizeOptionOverrideTableProperties() throws IOExcept\n   }\n \n   @TestTemplate\n-  public void testDefaultMetadataSplitSize() throws IOException {\n+  public void testDefaultMetadataSplitSize() {\n     String tableLocation = temp.resolve(\"iceberg-table\").toFile().toString();\n \n     HadoopTables tables = new HadoopTables(CONF);\n@@ -468,7 +467,7 @@ public void testExtraSnapshotMetadataWithSQL() throws InterruptedException, IOEx\n \n     List<Snapshot> snapshots = Lists.newArrayList(table.snapshots());\n     assertThat(snapshots).hasSize(2);\n-    assertThat(snapshots.get(0).summary().get(\"writer-thread\")).isNull();\n+    assertThat(snapshots.get(0).summary()).doesNotContainKey(\"writer-thread\");\n     assertThat(snapshots.get(1).summary())\n         .containsEntry(\"writer-thread\", \"test-extra-commit-message-writer-thread\")\n         .containsEntry(\"extra-key\", \"someValue\")\n@@ -512,7 +511,7 @@ public void testExtraSnapshotMetadataWithDelete()\n     List<Snapshot> snapshots = Lists.newArrayList(table.snapshots());\n \n     assertThat(snapshots).hasSize(2);\n-    assertThat(snapshots.get(0).summary().get(\"writer-thread\")).isNull();\n+    assertThat(snapshots.get(0).summary()).doesNotContainKey(\"writer-thread\");\n     assertThat(snapshots.get(1).summary())\n         .containsEntry(\"writer-thread\", \"test-extra-commit-message-delete-thread\")\n         .containsEntry(\"extra-key\", \"someValue\")\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java\nindex 35d6e119e86f..701c421a253b 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceHadoopTables.java\n@@ -36,7 +36,7 @@ public class TestIcebergSourceHadoopTables extends TestIcebergSourceTablesBase {\n   String tableLocation = null;\n \n   @BeforeEach\n-  public void setupTable() throws Exception {\n+  public void setupTable() {\n     this.tableLocation = tableDir.toURI().toString();\n   }\n \n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java\nindex 225c9540a05e..63994726cafb 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestIcebergSourceTablesBase.java\n@@ -237,8 +237,7 @@ public void testEntriesTablePartitionedPrune() {\n             .select(\"status\")\n             .collectAsList();\n \n-    assertThat(actual).as(\"Results should contain only one status\").hasSize(1);\n-    assertThat(actual.get(0).getInt(0)).as(\"That status should be Added (1)\").isEqualTo(1);\n+    assertThat(actual).singleElement().satisfies(row -> assertThat(row.getInt(0)).isEqualTo(1));\n   }\n \n   @Test\n@@ -600,10 +599,20 @@ public void testV1EntriesTableWithSnapshotIdInheritance() throws Exception {\n     long snapshotId = table.currentSnapshot().snapshotId();\n \n     assertThat(actual).as(\"Entries table should have 2 rows\").hasSize(2);\n-    assertThat(actual.get(0).getLong(0)).as(\"Sequence number must match\").isEqualTo(0);\n-    assertThat(actual.get(0).getLong(1)).as(\"Snapshot id must match\").isEqualTo(snapshotId);\n-    assertThat(actual.get(1).getLong(0)).as(\"Sequence number must match\").isEqualTo(0);\n-    assertThat(actual.get(1).getLong(1)).as(\"Snapshot id must match\").isEqualTo(snapshotId);\n+    assertThat(actual)\n+        .first()\n+        .satisfies(\n+            row -> {\n+              assertThat(row.getLong(0)).isEqualTo(0);\n+              assertThat(row.getLong(1)).isEqualTo(snapshotId);\n+            });\n+    assertThat(actual)\n+        .element(1)\n+        .satisfies(\n+            row -> {\n+              assertThat(row.getLong(0)).isEqualTo(0);\n+              assertThat(row.getLong(1)).isEqualTo(snapshotId);\n+            });\n   }\n \n   @Test\n@@ -1490,7 +1499,6 @@ public void testPartitionsTableLastUpdatedSnapshot() {\n     assertThat(rewriteManifestResult.rewrittenManifests())\n         .as(\"rewrite replaced 2 manifests\")\n         .hasSize(2);\n-\n     assertThat(rewriteManifestResult.addedManifests()).as(\"rewrite added 1 manifests\").hasSize(1);\n \n     List<Row> actual =\n@@ -1747,9 +1755,9 @@ public synchronized void testSnapshotReadAfterAddColumn() {\n     table.refresh();\n \n     Dataset<Row> resultDf = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n-    assertThat(originalRecords)\n+    assertThat(resultDf.orderBy(\"id\").collectAsList())\n         .as(\"Records should match\")\n-        .isEqualTo(resultDf.orderBy(\"id\").collectAsList());\n+        .containsExactlyElementsOf(originalRecords);\n \n     Snapshot snapshotBeforeAddColumn = table.currentSnapshot();\n \n@@ -1778,9 +1786,9 @@ public synchronized void testSnapshotReadAfterAddColumn() {\n             RowFactory.create(5, \"xyz\", \"C\"));\n \n     Dataset<Row> resultDf2 = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n-    assertThat(updatedRecords)\n+    assertThat(resultDf2.orderBy(\"id\").collectAsList())\n         .as(\"Records should match\")\n-        .isEqualTo(resultDf2.orderBy(\"id\").collectAsList());\n+        .containsExactlyElementsOf(updatedRecords);\n \n     Dataset<Row> resultDf3 =\n         spark\n@@ -1788,11 +1796,9 @@ public synchronized void testSnapshotReadAfterAddColumn() {\n             .format(\"iceberg\")\n             .option(SparkReadOptions.SNAPSHOT_ID, snapshotBeforeAddColumn.snapshotId())\n             .load(loadLocation(tableIdentifier));\n-\n-    assertThat(originalRecords)\n+    assertThat(resultDf3.orderBy(\"id\").collectAsList())\n         .as(\"Records should match\")\n-        .isEqualTo(resultDf3.orderBy(\"id\").collectAsList());\n-\n+        .containsExactlyElementsOf(originalRecords);\n     assertThat(resultDf3.schema()).as(\"Schemas should match\").isEqualTo(originalSparkSchema);\n   }\n \n@@ -1819,10 +1825,9 @@ public synchronized void testSnapshotReadAfterDropColumn() {\n     table.refresh();\n \n     Dataset<Row> resultDf = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n-\n     assertThat(resultDf.orderBy(\"id\").collectAsList())\n         .as(\"Records should match\")\n-        .isEqualTo(originalRecords);\n+        .containsExactlyElementsOf(originalRecords);\n \n     long tsBeforeDropColumn = waitUntilAfter(System.currentTimeMillis());\n     table.updateSchema().deleteColumn(\"data\").commit();\n@@ -1852,7 +1857,7 @@ public synchronized void testSnapshotReadAfterDropColumn() {\n     Dataset<Row> resultDf2 = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n     assertThat(resultDf2.orderBy(\"id\").collectAsList())\n         .as(\"Records should match\")\n-        .isEqualTo(updatedRecords);\n+        .containsExactlyElementsOf(updatedRecords);\n \n     Dataset<Row> resultDf3 =\n         spark\n@@ -1860,11 +1865,9 @@ public synchronized void testSnapshotReadAfterDropColumn() {\n             .format(\"iceberg\")\n             .option(SparkReadOptions.AS_OF_TIMESTAMP, tsBeforeDropColumn)\n             .load(loadLocation(tableIdentifier));\n-\n     assertThat(resultDf3.orderBy(\"id\").collectAsList())\n         .as(\"Records should match\")\n-        .isEqualTo(originalRecords);\n-\n+        .containsExactlyElementsOf(originalRecords);\n     assertThat(resultDf3.schema()).as(\"Schemas should match\").isEqualTo(originalSparkSchema);\n \n     // At tsAfterDropColumn, there has been a schema change, but no new snapshot,\n@@ -1875,11 +1878,9 @@ public synchronized void testSnapshotReadAfterDropColumn() {\n             .format(\"iceberg\")\n             .option(SparkReadOptions.AS_OF_TIMESTAMP, tsAfterDropColumn)\n             .load(loadLocation(tableIdentifier));\n-\n     assertThat(resultDf4.orderBy(\"id\").collectAsList())\n         .as(\"Records should match\")\n-        .isEqualTo(originalRecords);\n-\n+        .containsExactlyElementsOf(originalRecords);\n     assertThat(resultDf4.schema()).as(\"Schemas should match\").isEqualTo(originalSparkSchema);\n   }\n \n@@ -1904,10 +1905,9 @@ public synchronized void testSnapshotReadAfterAddAndDropColumn() {\n     table.refresh();\n \n     Dataset<Row> resultDf = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n-\n     assertThat(resultDf.orderBy(\"id\").collectAsList())\n         .as(\"Records should match\")\n-        .isEqualTo(originalRecords);\n+        .containsExactlyElementsOf(originalRecords);\n \n     Snapshot snapshotBeforeAddColumn = table.currentSnapshot();\n \n@@ -1936,10 +1936,9 @@ public synchronized void testSnapshotReadAfterAddAndDropColumn() {\n             RowFactory.create(5, \"xyz\", \"C\"));\n \n     Dataset<Row> resultDf2 = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n-\n     assertThat(resultDf2.orderBy(\"id\").collectAsList())\n         .as(\"Records should match\")\n-        .isEqualTo(updatedRecords);\n+        .containsExactlyElementsOf(updatedRecords);\n \n     table.updateSchema().deleteColumn(\"data\").commit();\n \n@@ -1952,10 +1951,9 @@ public synchronized void testSnapshotReadAfterAddAndDropColumn() {\n             RowFactory.create(5, \"C\"));\n \n     Dataset<Row> resultDf3 = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n-\n     assertThat(resultDf3.orderBy(\"id\").collectAsList())\n         .as(\"Records should match\")\n-        .isEqualTo(recordsAfterDropColumn);\n+        .containsExactlyElementsOf(recordsAfterDropColumn);\n \n     Dataset<Row> resultDf4 =\n         spark\n@@ -1963,11 +1961,9 @@ public synchronized void testSnapshotReadAfterAddAndDropColumn() {\n             .format(\"iceberg\")\n             .option(SparkReadOptions.SNAPSHOT_ID, snapshotBeforeAddColumn.snapshotId())\n             .load(loadLocation(tableIdentifier));\n-\n     assertThat(resultDf4.orderBy(\"id\").collectAsList())\n         .as(\"Records should match\")\n-        .isEqualTo(originalRecords);\n-\n+        .containsExactlyElementsOf(originalRecords);\n     assertThat(resultDf4.schema()).as(\"Schemas should match\").isEqualTo(originalSparkSchema);\n   }\n \n@@ -1999,19 +1995,16 @@ public void testRemoveOrphanFilesActionSupport() throws InterruptedException {\n             .location(table.location() + \"/metadata\")\n             .olderThan(System.currentTimeMillis())\n             .execute();\n-\n     assertThat(result1.orphanFileLocations()).as(\"Should not delete any metadata files\").isEmpty();\n \n     DeleteOrphanFiles.Result result2 =\n         actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n-\n     assertThat(result2.orphanFileLocations()).as(\"Should delete 1 data file\").hasSize(1);\n \n     Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(loadLocation(tableIdentifier));\n     List<SimpleRecord> actualRecords =\n         resultDF.as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-\n-    assertThat(actualRecords).as(\"Rows must match\").isEqualTo(records);\n+    assertThat(actualRecords).as(\"Rows must match\").containsExactlyInAnyOrderElementsOf(records);\n   }\n \n   @Test\n@@ -2056,9 +2049,7 @@ public void testFilesTablePartitionId() {\n             .map(r -> (Integer) r.getAs(DataFile.SPEC_ID.name()))\n             .collect(Collectors.toList());\n \n-    assertThat(actual)\n-        .as(\"Should have two partition specs\")\n-        .isEqualTo(ImmutableList.of(spec0, spec1));\n+    assertThat(actual).as(\"Should have two partition specs\").containsExactly(spec0, spec1);\n   }\n \n   @Test\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTableReadableMetrics.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTableReadableMetrics.java\nindex 547ab32eac24..c21ccd0100db 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTableReadableMetrics.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestMetadataTableReadableMetrics.java\n@@ -24,12 +24,12 @@\n import java.io.IOException;\n import java.math.BigDecimal;\n import java.nio.ByteBuffer;\n-import java.nio.file.Path;\n import java.util.Base64;\n import java.util.List;\n import java.util.Map;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.Files;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n@@ -50,12 +50,11 @@\n import org.apache.spark.sql.Row;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n-import org.junit.jupiter.api.io.TempDir;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestMetadataTableReadableMetrics extends TestBaseWithCatalog {\n \n-  @TempDir private Path temp;\n-\n   private static final Types.StructType LEAF_STRUCT_TYPE =\n       Types.StructType.of(\n           optional(1, \"leafLongCol\", Types.LongType.get()),\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12949",
    "pr_id": 12949,
    "issue_id": 12926,
    "repo": "apache/iceberg",
    "problem_statement": "Core, Release: 1.9.0 Build Version() returns Unspecified due to incorrect packaged iceberg-build.properties\n### Apache Iceberg version\n\n1.9.0 (latest release)\n\n### Query engine\n\nNone\n\n### Please describe the bug üêû\n\nApache Iceberg 1.9.0 (java library)\n\niceberg-api-1.9.0.jar contains iceberg-build.properties\n\nThe contents of `iceberg-build.properties` is incorrect \n\n```\n$ cat iceberg-build.properties \n\n```\n\nOutput:\n```\ngit.branch=1.9.x\ngit.build.version=unspecified\ngit.closest.tag.name=apache-iceberg-1.9.0-rc2\ngit.commit.id=7dbafb438ee1e68d0047bebcb587265d7d87d8a1\ngit.commit.id.abbrev=7dbafb4\ngit.commit.message.short=API\\: Don't check underlying error msg on AIOOBE (\\#12867)\ngit.commit.time=2025-04-22T15\\:14\\:07+0200\ngit.tags=apache-iceberg-1.9.0-rc2\n```\n\n\nüèì  \"git.build.version\" should be \"1.9.0\"\n\n\n\n\n\n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [x] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 169,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "api/src/test/java/org/apache/iceberg/TestIcebergBuild.java",
      "build.gradle"
    ],
    "pr_changed_test_files": [
      "api/src/test/java/org/apache/iceberg/TestIcebergBuild.java"
    ],
    "base_commit": "bea3f8b58cebe1458d8edb2172287cae0a04cb38",
    "head_commit": "38d2c3dcda956c6f0f617585e76a8d08bdde1584",
    "repo_url": "https://github.com/apache/iceberg/pull/12949",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12949",
    "dockerfile": "",
    "pr_merged_at": "2025-05-02T23:49:23.000Z",
    "patch": "diff --git a/build.gradle b/build.gradle\nindex 95b82f36301d..998f2ee9ea6d 100644\n--- a/build.gradle\n+++ b/build.gradle\n@@ -95,6 +95,7 @@ gitProperties {\n   failOnNoGitDirectory = true\n   keys = ['git.branch', 'git.build.version', 'git.closest.tag.name','git.commit.id.abbrev', 'git.commit.id',\n           'git.commit.message.short', 'git.commit.time', 'git.tags']\n+  version = projectVersion\n }\n generateGitProperties.outputs.upToDateWhen { false }\n \n@@ -233,6 +234,8 @@ subprojects {\n       events \"failed\"\n       exceptionFormat \"full\"\n     }\n+\n+    systemProperty 'project.version', project.version\n   }\n \n   plugins.withType(ScalaPlugin.class) {\n",
    "test_patch": "diff --git a/api/src/test/java/org/apache/iceberg/TestIcebergBuild.java b/api/src/test/java/org/apache/iceberg/TestIcebergBuild.java\nindex 584bddb132d4..36ae9c90642c 100644\n--- a/api/src/test/java/org/apache/iceberg/TestIcebergBuild.java\n+++ b/api/src/test/java/org/apache/iceberg/TestIcebergBuild.java\n@@ -19,7 +19,11 @@\n package org.apache.iceberg;\n \n import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n import java.util.Locale;\n import java.util.regex.Pattern;\n import org.junit.jupiter.api.Test;\n@@ -37,6 +41,34 @@ public void testFullVersion() {\n                 + \")\");\n   }\n \n+  @Test\n+  public void testVersionNotUnspecified() {\n+    assertThat(IcebergBuild.version()).isNotEqualTo(\"unspecified\");\n+  }\n+\n+  @Test\n+  public void testVersionMatchesSystemProperty() {\n+    assumeThat(System.getProperty(\"project.version\")).isNotNull();\n+    assertThat(IcebergBuild.version())\n+        .isEqualTo(System.getProperty(\"project.version\"))\n+        .as(\"IcebergBuild.version() should match system property project.version\");\n+  }\n+\n+  /**\n+   * This test is for Source Releases. When we have a source release we use a version.txt file in\n+   * the parent directory of this module to actually set the \"version\" which should be included in\n+   * the gradle build properties used by IcebergBuild.\n+   */\n+  @Test\n+  public void testVersionMatchesFile() throws IOException {\n+    Path versionPath = Paths.get(\"../version.txt\").toAbsolutePath();\n+    assumeThat(java.nio.file.Files.exists(versionPath)).isTrue();\n+    String versionText = java.nio.file.Files.readString(versionPath).trim();\n+    assertThat(IcebergBuild.version())\n+        .isEqualTo(versionText)\n+        .as(\"IcebergBuild.version() should match version file\");\n+  }\n+\n   @Test\n   public void testVersion() {\n     assertThat(IcebergBuild.version()).as(\"Should not use unknown version\").isNotEqualTo(\"unknown\");\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12945",
    "pr_id": 12945,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 14,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWrites.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesAsSelect.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToBranch.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToWapBranch.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWrites.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWritesToBranch.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/UnpartitionedWritesTestBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesAsSelect.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToBranch.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToWapBranch.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWritesToBranch.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/UnpartitionedWritesTestBase.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWrites.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesAsSelect.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToBranch.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToWapBranch.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWrites.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWritesToBranch.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/UnpartitionedWritesTestBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesAsSelect.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToBranch.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToWapBranch.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWritesToBranch.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/UnpartitionedWritesTestBase.java"
    ],
    "base_commit": "bd9353ab0bdd88f588e4e3be5773e25ce52faf51",
    "head_commit": "49ec7b0a0bec79982dc6ce8f912debae2e7f2f34",
    "repo_url": "https://github.com/apache/iceberg/pull/12945",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12945",
    "dockerfile": "",
    "pr_merged_at": "2025-05-05T09:59:19.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java\nindex 77dccbf1e064..c2fa8dc0ce24 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java\n@@ -18,30 +18,29 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import java.util.Arrays;\n import java.util.List;\n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.expressions.Expressions;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.source.SimpleRecord;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.functions;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n-\n-public abstract class PartitionedWritesTestBase extends SparkCatalogTestBase {\n-  public PartitionedWritesTestBase(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public abstract class PartitionedWritesTestBase extends CatalogTestBase {\n \n-  @Before\n+  @BeforeEach\n   public void createTables() {\n     sql(\n         \"CREATE TABLE %s (id bigint, data string) USING iceberg PARTITIONED BY (truncate(id, 3))\",\n@@ -49,22 +48,22 @@ public void createTables() {\n     sql(\"INSERT INTO %s VALUES (1, 'a'), (2, 'b'), (3, 'c')\", tableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInsertAppend() {\n-    Assert.assertEquals(\n-        \"Should have 3 rows\", 3L, scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Rows before insert\")\n+        .isEqualTo(3L);\n \n     sql(\"INSERT INTO %s VALUES (4, 'd'), (5, 'e')\", commitTarget());\n \n-    Assert.assertEquals(\n-        \"Should have 5 rows after insert\",\n-        5L,\n-        scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 5 rows after insert\")\n+        .isEqualTo(5L);\n \n     List<Object[]> expected =\n         ImmutableList.of(row(1L, \"a\"), row(2L, \"b\"), row(3L, \"c\"), row(4L, \"d\"), row(5L, \"e\"));\n@@ -75,18 +74,18 @@ public void testInsertAppend() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInsertOverwrite() {\n-    Assert.assertEquals(\n-        \"Should have 3 rows\", 3L, scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Rows before overwrite\")\n+        .isEqualTo(3L);\n \n     // 4 and 5 replace 3 in the partition (id - (id % 3)) = 3\n     sql(\"INSERT OVERWRITE %s VALUES (4, 'd'), (5, 'e')\", commitTarget());\n \n-    Assert.assertEquals(\n-        \"Should have 4 rows after overwrite\",\n-        4L,\n-        scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 4 rows after overwrite\")\n+        .isEqualTo(4L);\n \n     List<Object[]> expected =\n         ImmutableList.of(row(1L, \"a\"), row(2L, \"b\"), row(4L, \"d\"), row(5L, \"e\"));\n@@ -97,20 +96,20 @@ public void testInsertOverwrite() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDataFrameV2Append() throws NoSuchTableException {\n-    Assert.assertEquals(\n-        \"Should have 3 rows\", 3L, scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 3 rows\")\n+        .isEqualTo(3L);\n \n     List<SimpleRecord> data = ImmutableList.of(new SimpleRecord(4, \"d\"), new SimpleRecord(5, \"e\"));\n     Dataset<Row> ds = spark.createDataFrame(data, SimpleRecord.class);\n \n     ds.writeTo(commitTarget()).append();\n \n-    Assert.assertEquals(\n-        \"Should have 5 rows after insert\",\n-        5L,\n-        scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 5 rows after insert\")\n+        .isEqualTo(5L);\n \n     List<Object[]> expected =\n         ImmutableList.of(row(1L, \"a\"), row(2L, \"b\"), row(3L, \"c\"), row(4L, \"d\"), row(5L, \"e\"));\n@@ -121,20 +120,20 @@ public void testDataFrameV2Append() throws NoSuchTableException {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDataFrameV2DynamicOverwrite() throws NoSuchTableException {\n-    Assert.assertEquals(\n-        \"Should have 3 rows\", 3L, scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 3 rows\")\n+        .isEqualTo(3L);\n \n     List<SimpleRecord> data = ImmutableList.of(new SimpleRecord(4, \"d\"), new SimpleRecord(5, \"e\"));\n     Dataset<Row> ds = spark.createDataFrame(data, SimpleRecord.class);\n \n     ds.writeTo(commitTarget()).overwritePartitions();\n \n-    Assert.assertEquals(\n-        \"Should have 4 rows after overwrite\",\n-        4L,\n-        scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 4 rows after overwrite\")\n+        .isEqualTo(4L);\n \n     List<Object[]> expected =\n         ImmutableList.of(row(1L, \"a\"), row(2L, \"b\"), row(4L, \"d\"), row(5L, \"e\"));\n@@ -145,20 +144,20 @@ public void testDataFrameV2DynamicOverwrite() throws NoSuchTableException {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDataFrameV2Overwrite() throws NoSuchTableException {\n-    Assert.assertEquals(\n-        \"Should have 3 rows\", 3L, scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 3 rows\")\n+        .isEqualTo(3L);\n \n     List<SimpleRecord> data = ImmutableList.of(new SimpleRecord(4, \"d\"), new SimpleRecord(5, \"e\"));\n     Dataset<Row> ds = spark.createDataFrame(data, SimpleRecord.class);\n \n     ds.writeTo(commitTarget()).overwrite(functions.col(\"id\").$less(3));\n \n-    Assert.assertEquals(\n-        \"Should have 3 rows after overwrite\",\n-        3L,\n-        scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 3 rows after overwrite\")\n+        .isEqualTo(3L);\n \n     List<Object[]> expected = ImmutableList.of(row(3L, \"c\"), row(4L, \"d\"), row(5L, \"e\"));\n \n@@ -168,10 +167,11 @@ public void testDataFrameV2Overwrite() throws NoSuchTableException {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testViewsReturnRecentResults() {\n-    Assert.assertEquals(\n-        \"Should have 3 rows\", 3L, scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 3 rows\")\n+        .isEqualTo(3L);\n \n     Dataset<Row> query = spark.sql(\"SELECT * FROM \" + commitTarget() + \" WHERE id = 1\");\n     query.createOrReplaceTempView(\"tmp\");\n@@ -207,7 +207,7 @@ protected void assertPartitionMetadata(\n         rowsToJava(actualPartitionRows.collectAsList()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWriteWithOutputSpec() throws NoSuchTableException {\n     Table table = validationCatalog.loadTable(tableIdent);\n \n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWrites.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWrites.java\nindex a18bd997250b..800d17dd4559 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWrites.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWrites.java\n@@ -18,12 +18,4 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n-import java.util.Map;\n-\n-public class TestPartitionedWrites extends PartitionedWritesTestBase {\n-\n-  public TestPartitionedWrites(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-}\n+public class TestPartitionedWrites extends PartitionedWritesTestBase {}\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesAsSelect.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesAsSelect.java\nindex 3ffd38b83c3b..fde0f0a39a9f 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesAsSelect.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesAsSelect.java\n@@ -18,34 +18,54 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import java.util.List;\n import java.util.stream.IntStream;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.spark.IcebergSpark;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.spark.SparkCatalogConfig;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.spark.sql.types.DataTypes;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n-\n-public class TestPartitionedWritesAsSelect extends SparkTestBaseWithCatalog {\n-\n-  private final String targetTable = tableName(\"target_table\");\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestPartitionedWritesAsSelect extends TestBaseWithCatalog {\n+\n+  @Parameter(index = 3)\n+  private String targetTable;\n+\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}, targetTable = {3}\")\n+  protected static Object[][] parameters() {\n+    return new Object[][] {\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties(),\n+        SparkCatalogConfig.HADOOP.catalogName() + \".default.target_table\"\n+      },\n+    };\n+  }\n \n-  @Before\n+  @BeforeEach\n   public void createTables() {\n     sql(\n         \"CREATE TABLE %s (id bigint, data string, category string, ts timestamp) USING iceberg\",\n         tableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS %s\", targetTable);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInsertAsSelectAppend() {\n     insertData(3);\n     List<Object[]> expected = currentData();\n@@ -58,10 +78,9 @@ public void testInsertAsSelectAppend() {\n     sql(\n         \"INSERT INTO %s SELECT id, data, category, ts FROM %s ORDER BY ts,category\",\n         targetTable, tableName);\n-    Assert.assertEquals(\n-        \"Should have 15 rows after insert\",\n-        3 * 5L,\n-        scalarSql(\"SELECT count(*) FROM %s\", targetTable));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", targetTable))\n+        .as(\"Should have 15 rows after insert\")\n+        .isEqualTo(3 * 5L);\n \n     assertEquals(\n         \"Row data should match expected\",\n@@ -69,7 +88,7 @@ public void testInsertAsSelectAppend() {\n         sql(\"SELECT * FROM %s ORDER BY id\", targetTable));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInsertAsSelectWithBucket() {\n     insertData(3);\n     List<Object[]> expected = currentData();\n@@ -83,10 +102,9 @@ public void testInsertAsSelectWithBucket() {\n     sql(\n         \"INSERT INTO %s SELECT id, data, category, ts FROM %s ORDER BY iceberg_bucket8(data)\",\n         targetTable, tableName);\n-    Assert.assertEquals(\n-        \"Should have 15 rows after insert\",\n-        3 * 5L,\n-        scalarSql(\"SELECT count(*) FROM %s\", targetTable));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", targetTable))\n+        .as(\"Should have 15 rows after insert\")\n+        .isEqualTo(3 * 5L);\n \n     assertEquals(\n         \"Row data should match expected\",\n@@ -94,7 +112,7 @@ public void testInsertAsSelectWithBucket() {\n         sql(\"SELECT * FROM %s ORDER BY id\", targetTable));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInsertAsSelectWithTruncate() {\n     insertData(3);\n     List<Object[]> expected = currentData();\n@@ -110,10 +128,9 @@ public void testInsertAsSelectWithTruncate() {\n         \"INSERT INTO %s SELECT id, data, category, ts FROM %s \"\n             + \"ORDER BY iceberg_truncate_string4(data),iceberg_truncate_long4(id)\",\n         targetTable, tableName);\n-    Assert.assertEquals(\n-        \"Should have 15 rows after insert\",\n-        3 * 5L,\n-        scalarSql(\"SELECT count(*) FROM %s\", targetTable));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", targetTable))\n+        .as(\"Should have 15 rows after insert\")\n+        .isEqualTo(3 * 5L);\n \n     assertEquals(\n         \"Row data should match expected\",\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToBranch.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToBranch.java\nindex c6cde7a5524e..154c6181a594 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToBranch.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToBranch.java\n@@ -18,20 +18,14 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n-import java.util.Map;\n import org.apache.iceberg.Table;\n-import org.junit.Before;\n+import org.junit.jupiter.api.BeforeEach;\n \n public class TestPartitionedWritesToBranch extends PartitionedWritesTestBase {\n \n   private static final String BRANCH = \"test\";\n \n-  public TestPartitionedWritesToBranch(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Before\n+  @BeforeEach\n   @Override\n   public void createTables() {\n     super.createTables();\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToWapBranch.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToWapBranch.java\nindex 064ad0f1785c..45268b78f893 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToWapBranch.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToWapBranch.java\n@@ -21,26 +21,23 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n-import java.util.Map;\n import java.util.UUID;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.spark.SparkSQLProperties;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestPartitionedWritesToWapBranch extends PartitionedWritesTestBase {\n \n   private static final String BRANCH = \"test\";\n \n-  public TestPartitionedWritesToWapBranch(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Before\n+  @BeforeEach\n   @Override\n   public void createTables() {\n     spark.conf().set(SparkSQLProperties.WAP_BRANCH, BRANCH);\n@@ -50,7 +47,7 @@ public void createTables() {\n     sql(\"INSERT INTO %s VALUES (1, 'a'), (2, 'b'), (3, 'c')\", tableName);\n   }\n \n-  @After\n+  @AfterEach\n   @Override\n   public void removeTables() {\n     super.removeTables();\n@@ -58,17 +55,12 @@ public void removeTables() {\n     spark.conf().unset(SparkSQLProperties.WAP_ID);\n   }\n \n-  @Override\n-  protected String commitTarget() {\n-    return tableName;\n-  }\n-\n   @Override\n   protected String selectTarget() {\n     return String.format(\"%s VERSION AS OF '%s'\", tableName, BRANCH);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBranchAndWapBranchCannotBothBeSetForWrite() {\n     Table table = validationCatalog.loadTable(tableIdent);\n     table.manageSnapshots().createBranch(\"test2\", table.refs().get(BRANCH).snapshotId()).commit();\n@@ -80,7 +72,7 @@ public void testBranchAndWapBranchCannotBothBeSetForWrite() {\n             BRANCH);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWapIdAndWapBranchCannotBothBeSetForWrite() {\n     String wapId = UUID.randomUUID().toString();\n     spark.conf().set(SparkSQLProperties.WAP_ID, wapId);\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWrites.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWrites.java\nindex d01ccab00f55..7d9dfe95efc0 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWrites.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWrites.java\n@@ -18,12 +18,4 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n-import java.util.Map;\n-\n-public class TestUnpartitionedWrites extends UnpartitionedWritesTestBase {\n-\n-  public TestUnpartitionedWrites(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-}\n+public class TestUnpartitionedWrites extends UnpartitionedWritesTestBase {}\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWritesToBranch.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWritesToBranch.java\nindex 891813e3816e..4991a9fd553e 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWritesToBranch.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWritesToBranch.java\n@@ -20,21 +20,20 @@\n \n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.exceptions.ValidationException;\n-import org.junit.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestUnpartitionedWritesToBranch extends UnpartitionedWritesTestBase {\n \n   private static final String BRANCH = \"test\";\n \n-  public TestUnpartitionedWritesToBranch(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n   @Override\n+  @BeforeEach\n   public void createTables() {\n     super.createTables();\n     Table table = validationCatalog.loadTable(tableIdent);\n@@ -52,7 +51,7 @@ protected String selectTarget() {\n     return String.format(\"%s VERSION AS OF '%s'\", tableName, BRANCH);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInsertIntoNonExistingBranchFails() {\n     assertThatThrownBy(\n             () -> sql(\"INSERT INTO %s.branch_not_exist VALUES (4, 'd'), (5, 'e')\", tableName))\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/UnpartitionedWritesTestBase.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/UnpartitionedWritesTestBase.java\nindex c4534f8b67fd..756f9c755ca0 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/UnpartitionedWritesTestBase.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/UnpartitionedWritesTestBase.java\n@@ -18,51 +18,49 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.util.List;\n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.source.SimpleRecord;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.functions;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Before;\n-import org.junit.Test;\n-\n-public abstract class UnpartitionedWritesTestBase extends SparkCatalogTestBase {\n-  public UnpartitionedWritesTestBase(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public abstract class UnpartitionedWritesTestBase extends CatalogTestBase {\n \n-  @Before\n+  @BeforeEach\n   public void createTables() {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO %s VALUES (1, 'a'), (2, 'b'), (3, 'c')\", tableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInsertAppend() {\n-    Assert.assertEquals(\n-        \"Should have 3 rows\", 3L, scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 3 rows\")\n+        .isEqualTo(3L);\n \n     sql(\"INSERT INTO %s VALUES (4, 'd'), (5, 'e')\", commitTarget());\n \n-    Assert.assertEquals(\n-        \"Should have 5 rows after insert\",\n-        5L,\n-        scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 5 rows\")\n+        .isEqualTo(5L);\n \n     List<Object[]> expected =\n         ImmutableList.of(row(1L, \"a\"), row(2L, \"b\"), row(3L, \"c\"), row(4L, \"d\"), row(5L, \"e\"));\n@@ -73,17 +71,17 @@ public void testInsertAppend() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInsertOverwrite() {\n-    Assert.assertEquals(\n-        \"Should have 3 rows\", 3L, scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 3 rows\")\n+        .isEqualTo(3L);\n \n     sql(\"INSERT OVERWRITE %s VALUES (4, 'd'), (5, 'e')\", commitTarget());\n \n-    Assert.assertEquals(\n-        \"Should have 2 rows after overwrite\",\n-        2L,\n-        scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 2 rows after overwrite\")\n+        .isEqualTo(2L);\n \n     List<Object[]> expected = ImmutableList.of(row(4L, \"d\"), row(5L, \"e\"));\n \n@@ -93,9 +91,9 @@ public void testInsertOverwrite() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInsertAppendAtSnapshot() {\n-    Assume.assumeTrue(tableName.equals(commitTarget()));\n+    assumeThat(tableName.equals(commitTarget())).isTrue();\n     long snapshotId = validationCatalog.loadTable(tableIdent).currentSnapshot().snapshotId();\n     String prefix = \"snapshot_id_\";\n \n@@ -106,9 +104,9 @@ public void testInsertAppendAtSnapshot() {\n         .hasMessageStartingWith(\"Cannot write to table at a specific snapshot\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInsertOverwriteAtSnapshot() {\n-    Assume.assumeTrue(tableName.equals(commitTarget()));\n+    assumeThat(tableName.equals(commitTarget())).isTrue();\n     long snapshotId = validationCatalog.loadTable(tableIdent).currentSnapshot().snapshotId();\n     String prefix = \"snapshot_id_\";\n \n@@ -121,20 +119,20 @@ public void testInsertOverwriteAtSnapshot() {\n         .hasMessageStartingWith(\"Cannot write to table at a specific snapshot\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDataFrameV2Append() throws NoSuchTableException {\n-    Assert.assertEquals(\n-        \"Should have 3 rows\", 3L, scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 3 rows\")\n+        .isEqualTo(3L);\n \n     List<SimpleRecord> data = ImmutableList.of(new SimpleRecord(4, \"d\"), new SimpleRecord(5, \"e\"));\n     Dataset<Row> ds = spark.createDataFrame(data, SimpleRecord.class);\n \n     ds.writeTo(commitTarget()).append();\n \n-    Assert.assertEquals(\n-        \"Should have 5 rows after insert\",\n-        5L,\n-        scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 5 rows after insert\")\n+        .isEqualTo(5L);\n \n     List<Object[]> expected =\n         ImmutableList.of(row(1L, \"a\"), row(2L, \"b\"), row(3L, \"c\"), row(4L, \"d\"), row(5L, \"e\"));\n@@ -145,20 +143,20 @@ public void testDataFrameV2Append() throws NoSuchTableException {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDataFrameV2DynamicOverwrite() throws NoSuchTableException {\n-    Assert.assertEquals(\n-        \"Should have 3 rows\", 3L, scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 3 rows\")\n+        .isEqualTo(3L);\n \n     List<SimpleRecord> data = ImmutableList.of(new SimpleRecord(4, \"d\"), new SimpleRecord(5, \"e\"));\n     Dataset<Row> ds = spark.createDataFrame(data, SimpleRecord.class);\n \n     ds.writeTo(commitTarget()).overwritePartitions();\n \n-    Assert.assertEquals(\n-        \"Should have 2 rows after overwrite\",\n-        2L,\n-        scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 2 rows after overwrite\")\n+        .isEqualTo(2L);\n \n     List<Object[]> expected = ImmutableList.of(row(4L, \"d\"), row(5L, \"e\"));\n \n@@ -168,20 +166,20 @@ public void testDataFrameV2DynamicOverwrite() throws NoSuchTableException {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDataFrameV2Overwrite() throws NoSuchTableException {\n-    Assert.assertEquals(\n-        \"Should have 3 rows\", 3L, scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 3 rows\")\n+        .isEqualTo(3L);\n \n     List<SimpleRecord> data = ImmutableList.of(new SimpleRecord(4, \"d\"), new SimpleRecord(5, \"e\"));\n     Dataset<Row> ds = spark.createDataFrame(data, SimpleRecord.class);\n \n     ds.writeTo(commitTarget()).overwrite(functions.col(\"id\").$less$eq(3));\n \n-    Assert.assertEquals(\n-        \"Should have 2 rows after overwrite\",\n-        2L,\n-        scalarSql(\"SELECT count(*) FROM %s\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n+        .as(\"Should have 2 rows after overwrite\")\n+        .isEqualTo(2L);\n \n     List<Object[]> expected = ImmutableList.of(row(4L, \"d\"), row(5L, \"e\"));\n \n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java\nindex 88d18113f19c..c2fa8dc0ce24 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java\n@@ -22,6 +22,7 @@\n \n import java.util.Arrays;\n import java.util.List;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.expressions.Expressions;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n@@ -34,7 +35,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public abstract class PartitionedWritesTestBase extends CatalogTestBase {\n \n   @BeforeEach\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesAsSelect.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesAsSelect.java\nindex 373ca9996efd..fde0f0a39a9f 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesAsSelect.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesAsSelect.java\n@@ -23,6 +23,7 @@\n import java.util.List;\n import java.util.stream.IntStream;\n import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.spark.IcebergSpark;\n import org.apache.iceberg.spark.SparkCatalogConfig;\n@@ -31,7 +32,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestPartitionedWritesAsSelect extends TestBaseWithCatalog {\n \n   @Parameter(index = 3)\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToBranch.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToBranch.java\nindex 154c6181a594..0a90d3ffec75 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToBranch.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToBranch.java\n@@ -18,9 +18,12 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestPartitionedWritesToBranch extends PartitionedWritesTestBase {\n \n   private static final String BRANCH = \"test\";\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToWapBranch.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToWapBranch.java\nindex 45c0eb763653..45268b78f893 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToWapBranch.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestPartitionedWritesToWapBranch.java\n@@ -22,6 +22,7 @@\n \n import java.util.List;\n import java.util.UUID;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.exceptions.ValidationException;\n@@ -29,7 +30,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestPartitionedWritesToWapBranch extends PartitionedWritesTestBase {\n \n   private static final String BRANCH = \"test\";\n@@ -52,11 +55,6 @@ public void removeTables() {\n     spark.conf().unset(SparkSQLProperties.WAP_ID);\n   }\n \n-  @Override\n-  protected String commitTarget() {\n-    return tableName;\n-  }\n-\n   @Override\n   protected String selectTarget() {\n     return String.format(\"%s VERSION AS OF '%s'\", tableName, BRANCH);\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWritesToBranch.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWritesToBranch.java\nindex 3df5e9cdf5da..4991a9fd553e 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWritesToBranch.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestUnpartitionedWritesToBranch.java\n@@ -20,11 +20,14 @@\n \n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.exceptions.ValidationException;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestUnpartitionedWritesToBranch extends UnpartitionedWritesTestBase {\n \n   private static final String BRANCH = \"test\";\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/UnpartitionedWritesTestBase.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/UnpartitionedWritesTestBase.java\nindex ab87b89a3529..756f9c755ca0 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/UnpartitionedWritesTestBase.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/UnpartitionedWritesTestBase.java\n@@ -23,6 +23,7 @@\n import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.util.List;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.source.SimpleRecord;\n@@ -33,7 +34,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public abstract class UnpartitionedWritesTestBase extends CatalogTestBase {\n \n   @BeforeEach\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12941",
    "pr_id": 12941,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 15,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkBucketFunction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkDaysFunction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkHoursFunction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkMonthsFunction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkTruncateFunction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkYearsFunction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestStoragePartitionedJoins.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestTimestampWithoutZone.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkBucketFunction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkDaysFunction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkHoursFunction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkMonthsFunction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkTruncateFunction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkYearsFunction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestTimestampWithoutZone.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkBucketFunction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkDaysFunction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkHoursFunction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkMonthsFunction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkTruncateFunction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkYearsFunction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestStoragePartitionedJoins.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestTimestampWithoutZone.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkBucketFunction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkDaysFunction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkHoursFunction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkMonthsFunction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkTruncateFunction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkYearsFunction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestTimestampWithoutZone.java"
    ],
    "base_commit": "767688a33b1bf0af01e1dd06b141f9b891f8570e",
    "head_commit": "49aa1c4c5aecfc7cbd076c8bc4c1c1a0c0e69aa5",
    "repo_url": "https://github.com/apache/iceberg/pull/12941",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12941",
    "dockerfile": "",
    "pr_merged_at": "2025-04-30T14:02:30.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkBucketFunction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkBucketFunction.java\nindex d5525a06c0a0..0dcb986375bf 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkBucketFunction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkBucketFunction.java\n@@ -24,185 +24,175 @@\n import java.math.BigDecimal;\n import java.nio.ByteBuffer;\n import java.nio.charset.StandardCharsets;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.expressions.Literal;\n import org.apache.iceberg.relocated.com.google.common.io.BaseEncoding;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.iceberg.spark.functions.BucketFunction;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.AnalysisException;\n import org.apache.spark.sql.types.DataTypes;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSparkBucketFunction extends SparkTestBaseWithCatalog {\n-  @Before\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkBucketFunction extends TestBaseWithCatalog {\n+  @BeforeEach\n   public void useCatalog() {\n     sql(\"USE %s\", catalogName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSpecValues() {\n-    Assert.assertEquals(\n-        \"Spec example: hash(34) = 2017239379\",\n-        2017239379,\n-        new BucketFunction.BucketInt(DataTypes.IntegerType).hash(34));\n-\n-    Assert.assertEquals(\n-        \"Spec example: hash(34L) = 2017239379\",\n-        2017239379,\n-        new BucketFunction.BucketLong(DataTypes.LongType).hash(34L));\n-\n-    Assert.assertEquals(\n-        \"Spec example: hash(decimal2(14.20)) = -500754589\",\n-        -500754589,\n-        new BucketFunction.BucketDecimal(DataTypes.createDecimalType(9, 2))\n-            .hash(new BigDecimal(\"14.20\")));\n+    assertThat(new BucketFunction.BucketInt(DataTypes.IntegerType).hash(34))\n+        .as(\"Spec example: hash(34) = 2017239379\")\n+        .isEqualTo(2017239379);\n+\n+    assertThat(new BucketFunction.BucketLong(DataTypes.IntegerType).hash(34L))\n+        .as(\"Spec example: hash(34L) = 2017239379\")\n+        .isEqualTo(2017239379);\n+\n+    assertThat(\n+            new BucketFunction.BucketDecimal(DataTypes.createDecimalType(9, 2))\n+                .hash(new BigDecimal(\"14.20\")))\n+        .as(\"Spec example: hash(decimal2(14.20)) = -500754589\")\n+        .isEqualTo(-500754589);\n \n     Literal<Integer> date = Literal.of(\"2017-11-16\").to(Types.DateType.get());\n-    Assert.assertEquals(\n-        \"Spec example: hash(2017-11-16) = -653330422\",\n-        -653330422,\n-        new BucketFunction.BucketInt(DataTypes.DateType).hash(date.value()));\n+    assertThat(new BucketFunction.BucketInt(DataTypes.DateType).hash(date.value()))\n+        .as(\"Spec example: hash(2017-11-16) = -653330422\")\n+        .isEqualTo(-653330422);\n \n     Literal<Long> timestampVal =\n         Literal.of(\"2017-11-16T22:31:08\").to(Types.TimestampType.withoutZone());\n-    Assert.assertEquals(\n-        \"Spec example: hash(2017-11-16T22:31:08) = -2047944441\",\n-        -2047944441,\n-        new BucketFunction.BucketLong(DataTypes.TimestampType).hash(timestampVal.value()));\n+    assertThat(new BucketFunction.BucketLong(DataTypes.TimestampType).hash(timestampVal.value()))\n+        .as(\"Spec example: hash(2017-11-16T22:31:08) = -2047944441\")\n+        .isEqualTo(-2047944441);\n \n     Literal<Long> timestampntzVal =\n         Literal.of(\"2017-11-16T22:31:08\").to(Types.TimestampType.withoutZone());\n-    Assert.assertEquals(\n-        \"Spec example: hash(2017-11-16T22:31:08) = -2047944441\",\n-        -2047944441,\n-        new BucketFunction.BucketLong(DataTypes.TimestampNTZType).hash(timestampntzVal.value()));\n+    assertThat(\n+            new BucketFunction.BucketLong(DataTypes.TimestampNTZType).hash(timestampntzVal.value()))\n+        .as(\"Spec example: hash(2017-11-16T22:31:08) = -2047944441\")\n+        .isEqualTo(-2047944441);\n \n-    Assert.assertEquals(\n-        \"Spec example: hash(\\\"iceberg\\\") = 1210000089\",\n-        1210000089,\n-        new BucketFunction.BucketString().hash(\"iceberg\"));\n+    assertThat(new BucketFunction.BucketString().hash(\"iceberg\"))\n+        .as(\"Spec example: hash(\\\"iceberg\\\") = 1210000089\")\n+        .isEqualTo(1210000089);\n \n-    Assert.assertEquals(\n-        \"Verify that the hash string and hash raw bytes produce the same result\",\n-        new BucketFunction.BucketString().hash(\"iceberg\"),\n-        new BucketFunction.BucketString().hash(\"iceberg\".getBytes(StandardCharsets.UTF_8)));\n+    assertThat(new BucketFunction.BucketString().hash(\"iceberg\".getBytes(StandardCharsets.UTF_8)))\n+        .as(\"Verify that the hash string and hash raw bytes produce the same result\")\n+        .isEqualTo(new BucketFunction.BucketString().hash(\"iceberg\"));\n \n     ByteBuffer bytes = ByteBuffer.wrap(new byte[] {0, 1, 2, 3});\n-    Assert.assertEquals(\n-        \"Spec example: hash([00 01 02 03]) = -188683207\",\n-        -188683207,\n-        new BucketFunction.BucketBinary().hash(bytes));\n+    assertThat(new BucketFunction.BucketBinary().hash(bytes))\n+        .as(\"Spec example: hash([00 01 02 03]) = -188683207\")\n+        .isEqualTo(-188683207);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBucketIntegers() {\n-    Assert.assertEquals(\n-        \"Byte type should bucket similarly to integer\",\n-        3,\n-        scalarSql(\"SELECT system.bucket(10, 8Y)\"));\n-    Assert.assertEquals(\n-        \"Short type should bucket similarly to integer\",\n-        3,\n-        scalarSql(\"SELECT system.bucket(10, 8S)\"));\n+    assertThat(scalarSql(\"SELECT system.bucket(10, 8Y)\"))\n+        .as(\"Byte type should bucket similarly to integer\")\n+        .isEqualTo(3);\n+    assertThat(scalarSql(\"SELECT system.bucket(10, 8S)\"))\n+        .as(\"Short type should bucket similarly to integer\")\n+        .isEqualTo(3);\n     // Integers\n-    Assert.assertEquals(3, scalarSql(\"SELECT system.bucket(10, 8)\"));\n-    Assert.assertEquals(79, scalarSql(\"SELECT system.bucket(100, 34)\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.bucket(1, CAST(null AS INT))\"));\n+    assertThat(scalarSql(\"SELECT system.bucket(10, 8)\")).isEqualTo(3);\n+    assertThat(scalarSql(\"SELECT system.bucket(100, 34)\")).isEqualTo(79);\n+    assertThat(scalarSql(\"SELECT system.bucket(1, CAST(null AS INT))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBucketDates() {\n-    Assert.assertEquals(3, scalarSql(\"SELECT system.bucket(10, date('1970-01-09'))\"));\n-    Assert.assertEquals(79, scalarSql(\"SELECT system.bucket(100, date('1970-02-04'))\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.bucket(1, CAST(null AS DATE))\"));\n+    assertThat(scalarSql(\"SELECT system.bucket(10, date('1970-01-09'))\")).isEqualTo(3);\n+    assertThat(scalarSql(\"SELECT system.bucket(100, date('1970-02-04'))\")).isEqualTo(79);\n+    assertThat(scalarSql(\"SELECT system.bucket(1, CAST(null AS DATE))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBucketLong() {\n-    Assert.assertEquals(79, scalarSql(\"SELECT system.bucket(100, 34L)\"));\n-    Assert.assertEquals(76, scalarSql(\"SELECT system.bucket(100, 0L)\"));\n-    Assert.assertEquals(97, scalarSql(\"SELECT system.bucket(100, -34L)\"));\n-    Assert.assertEquals(0, scalarSql(\"SELECT system.bucket(2, -1L)\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.bucket(2, CAST(null AS LONG))\"));\n+    assertThat(scalarSql(\"SELECT system.bucket(100, 34L)\")).isEqualTo(79);\n+    assertThat(scalarSql(\"SELECT system.bucket(100, 0L)\")).isEqualTo(76);\n+    assertThat(scalarSql(\"SELECT system.bucket(100, -34L)\")).isEqualTo(97);\n+    assertThat(scalarSql(\"SELECT system.bucket(2, -1L)\")).isEqualTo(0);\n+    assertThat(scalarSql(\"SELECT system.bucket(2, CAST(null AS LONG))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBucketDecimal() {\n-    Assert.assertEquals(56, scalarSql(\"SELECT system.bucket(64, CAST('12.34' as DECIMAL(9, 2)))\"));\n-    Assert.assertEquals(13, scalarSql(\"SELECT system.bucket(18, CAST('12.30' as DECIMAL(9, 2)))\"));\n-    Assert.assertEquals(2, scalarSql(\"SELECT system.bucket(16, CAST('12.999' as DECIMAL(9, 3)))\"));\n-    Assert.assertEquals(21, scalarSql(\"SELECT system.bucket(32, CAST('0.05' as DECIMAL(5, 2)))\"));\n-    Assert.assertEquals(85, scalarSql(\"SELECT system.bucket(128, CAST('0.05' as DECIMAL(9, 2)))\"));\n-    Assert.assertEquals(3, scalarSql(\"SELECT system.bucket(18, CAST('0.05' as DECIMAL(9, 2)))\"));\n-\n-    Assert.assertNull(\n-        \"Null input should return null\",\n-        scalarSql(\"SELECT system.bucket(2, CAST(null AS decimal))\"));\n+    assertThat(scalarSql(\"SELECT system.bucket(64, CAST('12.34' as DECIMAL(9, 2)))\")).isEqualTo(56);\n+    assertThat(scalarSql(\"SELECT system.bucket(18, CAST('12.30' as DECIMAL(9, 2)))\")).isEqualTo(13);\n+    assertThat(scalarSql(\"SELECT system.bucket(16, CAST('12.999' as DECIMAL(9, 3)))\")).isEqualTo(2);\n+    assertThat(scalarSql(\"SELECT system.bucket(32, CAST('0.05' as DECIMAL(5, 2)))\")).isEqualTo(21);\n+    assertThat(scalarSql(\"SELECT system.bucket(128, CAST('0.05' as DECIMAL(9, 2)))\")).isEqualTo(85);\n+    assertThat(scalarSql(\"SELECT system.bucket(18, CAST('0.05' as DECIMAL(9, 2)))\")).isEqualTo(3);\n+\n+    assertThat(scalarSql(\"SELECT system.bucket(2, CAST(null AS decimal))\"))\n+        .as(\"Null input should return null\")\n+        .isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBucketTimestamp() {\n-    Assert.assertEquals(\n-        99, scalarSql(\"SELECT system.bucket(100, TIMESTAMP '1997-01-01 00:00:00 UTC+00:00')\"));\n-    Assert.assertEquals(\n-        85, scalarSql(\"SELECT system.bucket(100, TIMESTAMP '1997-01-31 09:26:56 UTC+00:00')\"));\n-    Assert.assertEquals(\n-        62, scalarSql(\"SELECT system.bucket(100, TIMESTAMP '2022-08-08 00:00:00 UTC+00:00')\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.bucket(2, CAST(null AS timestamp))\"));\n+    assertThat(scalarSql(\"SELECT system.bucket(100, TIMESTAMP '1997-01-01 00:00:00 UTC+00:00')\"))\n+        .isEqualTo(99);\n+    assertThat(scalarSql(\"SELECT system.bucket(100, TIMESTAMP '1997-01-31 09:26:56 UTC+00:00')\"))\n+        .isEqualTo(85);\n+    assertThat(scalarSql(\"SELECT system.bucket(100, TIMESTAMP '2022-08-08 00:00:00 UTC+00:00')\"))\n+        .isEqualTo(62);\n+    assertThat(scalarSql(\"SELECT system.bucket(2, CAST(null AS timestamp))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBucketString() {\n-    Assert.assertEquals(4, scalarSql(\"SELECT system.bucket(5, 'abcdefg')\"));\n-    Assert.assertEquals(122, scalarSql(\"SELECT system.bucket(128, 'abc')\"));\n-    Assert.assertEquals(54, scalarSql(\"SELECT system.bucket(64, 'abcde')\"));\n-    Assert.assertEquals(8, scalarSql(\"SELECT system.bucket(12, 'ÊµãËØï')\"));\n-    Assert.assertEquals(1, scalarSql(\"SELECT system.bucket(16, 'ÊµãËØïraulËØïÊµã')\"));\n-    Assert.assertEquals(\n-        \"Varchar should work like string\",\n-        1,\n-        scalarSql(\"SELECT system.bucket(16, CAST('ÊµãËØïraulËØïÊµã' AS varchar(8)))\"));\n-    Assert.assertEquals(\n-        \"Char should work like string\",\n-        1,\n-        scalarSql(\"SELECT system.bucket(16, CAST('ÊµãËØïraulËØïÊµã' AS char(8)))\"));\n-    Assert.assertEquals(\n-        \"Should not fail on the empty string\", 0, scalarSql(\"SELECT system.bucket(16, '')\"));\n-    Assert.assertNull(\n-        \"Null input should return null as output\",\n-        scalarSql(\"SELECT system.bucket(16, CAST(null AS string))\"));\n+    assertThat(scalarSql(\"SELECT system.bucket(5, 'abcdefg')\")).isEqualTo(4);\n+    assertThat(scalarSql(\"SELECT system.bucket(128, 'abc')\")).isEqualTo(122);\n+    assertThat(scalarSql(\"SELECT system.bucket(64, 'abcde')\")).isEqualTo(54);\n+    assertThat(scalarSql(\"SELECT system.bucket(12, 'ÊµãËØï')\")).isEqualTo(8);\n+    assertThat(scalarSql(\"SELECT system.bucket(16, 'ÊµãËØïraulËØïÊµã')\")).isEqualTo(1);\n+    assertThat(scalarSql(\"SELECT system.bucket(16, CAST('ÊµãËØïraulËØïÊµã' AS varchar(8)))\"))\n+        .as(\"Varchar should work like string\")\n+        .isEqualTo(1);\n+    assertThat(scalarSql(\"SELECT system.bucket(16, CAST('ÊµãËØïraulËØïÊµã' AS char(8)))\"))\n+        .as(\"Char should work like string\")\n+        .isEqualTo(1);\n+    assertThat(scalarSql(\"SELECT system.bucket(16, '')\"))\n+        .as(\"Should not fail on the empty string\")\n+        .isEqualTo(0);\n+    assertThat(scalarSql(\"SELECT system.bucket(16, CAST(null AS string))\"))\n+        .as(\"Null input should return null as output\")\n+        .isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBucketBinary() {\n-    Assert.assertEquals(\n-        1, scalarSql(\"SELECT system.bucket(10, X'0102030405060708090a0b0c0d0e0f')\"));\n-    Assert.assertEquals(10, scalarSql(\"SELECT system.bucket(12, %s)\", asBytesLiteral(\"abcdefg\")));\n-    Assert.assertEquals(13, scalarSql(\"SELECT system.bucket(18, %s)\", asBytesLiteral(\"abc\\0\\0\")));\n-    Assert.assertEquals(42, scalarSql(\"SELECT system.bucket(48, %s)\", asBytesLiteral(\"abc\")));\n-    Assert.assertEquals(3, scalarSql(\"SELECT system.bucket(16, %s)\", asBytesLiteral(\"ÊµãËØï_\")));\n-\n-    Assert.assertNull(\n-        \"Null input should return null as output\",\n-        scalarSql(\"SELECT system.bucket(100, CAST(null AS binary))\"));\n+    assertThat(scalarSql(\"SELECT system.bucket(10, X'0102030405060708090a0b0c0d0e0f')\"))\n+        .isEqualTo(1);\n+    assertThat(scalarSql(\"SELECT system.bucket(12, %s)\", asBytesLiteral(\"abcdefg\"))).isEqualTo(10);\n+    assertThat(scalarSql(\"SELECT system.bucket(18, %s)\", asBytesLiteral(\"abc\\0\\0\"))).isEqualTo(13);\n+    assertThat(scalarSql(\"SELECT system.bucket(48, %s)\", asBytesLiteral(\"abc\"))).isEqualTo(42);\n+    assertThat(scalarSql(\"SELECT system.bucket(16, %s)\", asBytesLiteral(\"ÊµãËØï_\"))).isEqualTo(3);\n+\n+    assertThat(scalarSql(\"SELECT system.bucket(100, CAST(null AS binary))\"))\n+        .as(\"Null input should return null as output\")\n+        .isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNumBucketsAcceptsShortAndByte() {\n-    Assert.assertEquals(\n-        \"Short types should be usable for the number of buckets field\",\n-        1,\n-        scalarSql(\"SELECT system.bucket(5S, 1L)\"));\n-\n-    Assert.assertEquals(\n-        \"Byte types should be allowed for the number of buckets field\",\n-        1,\n-        scalarSql(\"SELECT system.bucket(5Y, 1)\"));\n+    assertThat(scalarSql(\"SELECT system.bucket(5S, 1L)\"))\n+        .as(\"Short types should be usable for the number of buckets field\")\n+        .isEqualTo(1);\n+\n+    assertThat(scalarSql(\"SELECT system.bucket(5Y, 1)\"))\n+        .as(\"Byte types should be allowed for the number of buckets field\")\n+        .isEqualTo(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWrongNumberOfArguments() {\n     assertThatThrownBy(() -> scalarSql(\"SELECT system.bucket()\"))\n         .isInstanceOf(AnalysisException.class)\n@@ -220,7 +210,7 @@ public void testWrongNumberOfArguments() {\n             \"Function 'bucket' cannot process input: (int, bigint, int): Wrong number of inputs (expected numBuckets and value)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidTypesCannotBeUsedForNumberOfBuckets() {\n     assertThatThrownBy(() -> scalarSql(\"SELECT system.bucket(CAST('12.34' as DECIMAL(9, 2)), 10)\"))\n         .isInstanceOf(AnalysisException.class)\n@@ -250,7 +240,7 @@ public void testInvalidTypesCannotBeUsedForNumberOfBuckets() {\n             \"Function 'bucket' cannot process input: (interval day to second, int): Expected number of buckets to be tinyint, shortint or int\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidTypesForBucketColumn() {\n     assertThatThrownBy(() -> scalarSql(\"SELECT system.bucket(10, cast(12.3456 as float))\"))\n         .isInstanceOf(AnalysisException.class)\n@@ -287,7 +277,7 @@ public void testInvalidTypesForBucketColumn() {\n             \"Function 'bucket' cannot process input: (int, interval day to second)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testThatMagicFunctionsAreInvoked() {\n     // TinyInt\n     assertThat(scalarSql(\"EXPLAIN EXTENDED SELECT system.bucket(5, 6Y)\"))\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkDaysFunction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkDaysFunction.java\nindex f12bdd59ab3c..cfec6a33ab14 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkDaysFunction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkDaysFunction.java\n@@ -18,74 +18,68 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.sql.Date;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.spark.sql.AnalysisException;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSparkDaysFunction extends SparkTestBaseWithCatalog {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkDaysFunction extends TestBaseWithCatalog {\n \n-  @Before\n+  @BeforeEach\n   public void useCatalog() {\n     sql(\"USE %s\", catalogName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDates() {\n-    Assert.assertEquals(\n-        \"Expected to produce 2017-12-01\",\n-        Date.valueOf(\"2017-12-01\"),\n-        scalarSql(\"SELECT system.days(date('2017-12-01'))\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 1970-01-01\",\n-        Date.valueOf(\"1970-01-01\"),\n-        scalarSql(\"SELECT system.days(date('1970-01-01'))\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 1969-12-31\",\n-        Date.valueOf(\"1969-12-31\"),\n-        scalarSql(\"SELECT system.days(date('1969-12-31'))\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.days(CAST(null AS DATE))\"));\n+    assertThat(scalarSql(\"SELECT system.days(date('2017-12-01'))\"))\n+        .as(\"Expected to produce 2017-12-01\")\n+        .isEqualTo(Date.valueOf(\"2017-12-01\"));\n+    assertThat(scalarSql(\"SELECT system.days(date('1970-01-01'))\"))\n+        .as(\"Expected to produce 1970-01-01\")\n+        .isEqualTo(Date.valueOf(\"1970-01-01\"));\n+    assertThat(scalarSql(\"SELECT system.days(date('1969-12-31'))\"))\n+        .as(\"Expected to produce 1969-12-31\")\n+        .isEqualTo(Date.valueOf(\"1969-12-31\"));\n+    assertThat(scalarSql(\"SELECT system.days(CAST(null AS DATE))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimestamps() {\n-    Assert.assertEquals(\n-        \"Expected to produce 2017-12-01\",\n-        Date.valueOf(\"2017-12-01\"),\n-        scalarSql(\"SELECT system.days(TIMESTAMP '2017-12-01 10:12:55.038194 UTC+00:00')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 1970-01-01\",\n-        Date.valueOf(\"1970-01-01\"),\n-        scalarSql(\"SELECT system.days(TIMESTAMP '1970-01-01 00:00:01.000001 UTC+00:00')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 1969-12-31\",\n-        Date.valueOf(\"1969-12-31\"),\n-        scalarSql(\"SELECT system.days(TIMESTAMP '1969-12-31 23:59:58.999999 UTC+00:00')\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.days(CAST(null AS TIMESTAMP))\"));\n+    assertThat(scalarSql(\"SELECT system.days(TIMESTAMP '2017-12-01 10:12:55.038194 UTC+00:00')\"))\n+        .as(\"Expected to produce 2017-12-01\")\n+        .isEqualTo(Date.valueOf(\"2017-12-01\"));\n+    assertThat(scalarSql(\"SELECT system.days(TIMESTAMP '1970-01-01 00:00:01.000001 UTC+00:00')\"))\n+        .as(\"Expected to produce 1970-01-01\")\n+        .isEqualTo(Date.valueOf(\"1970-01-01\"));\n+    assertThat(scalarSql(\"SELECT system.days(TIMESTAMP '1969-12-31 23:59:58.999999 UTC+00:00')\"))\n+        .as(\"Expected to produce 1969-12-31\")\n+        .isEqualTo(Date.valueOf(\"1969-12-31\"));\n+    assertThat(scalarSql(\"SELECT system.days(CAST(null AS TIMESTAMP))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimestampNtz() {\n-    Assert.assertEquals(\n-        \"Expected to produce 2017-12-01\",\n-        Date.valueOf(\"2017-12-01\"),\n-        scalarSql(\"SELECT system.days(TIMESTAMP_NTZ '2017-12-01 10:12:55.038194 UTC')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 1970-01-01\",\n-        Date.valueOf(\"1970-01-01\"),\n-        scalarSql(\"SELECT system.days(TIMESTAMP_NTZ '1970-01-01 00:00:01.000001 UTC')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 1969-12-31\",\n-        Date.valueOf(\"1969-12-31\"),\n-        scalarSql(\"SELECT system.days(TIMESTAMP_NTZ '1969-12-31 23:59:58.999999 UTC')\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.days(CAST(null AS TIMESTAMP_NTZ))\"));\n+    assertThat(scalarSql(\"SELECT system.days(TIMESTAMP_NTZ '2017-12-01 10:12:55.038194 UTC')\"))\n+        .as(\"Expected to produce 2017-12-01\")\n+        .isEqualTo(Date.valueOf(\"2017-12-01\"));\n+    assertThat(scalarSql(\"SELECT system.days(TIMESTAMP_NTZ '1970-01-01 00:00:01.000001 UTC')\"))\n+        .as(\"Expected to produce 1970-01-01\")\n+        .isEqualTo(Date.valueOf(\"1970-01-01\"));\n+    assertThat(scalarSql(\"SELECT system.days(TIMESTAMP_NTZ '1969-12-31 23:59:58.999999 UTC')\"))\n+        .as(\"Expected to produce 1969-12-31\")\n+        .isEqualTo(Date.valueOf(\"1969-12-31\"));\n+    assertThat(scalarSql(\"SELECT system.days(CAST(null AS TIMESTAMP_NTZ))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWrongNumberOfArguments() {\n     assertThatThrownBy(() -> scalarSql(\"SELECT system.days()\"))\n         .isInstanceOf(AnalysisException.class)\n@@ -98,7 +92,7 @@ public void testWrongNumberOfArguments() {\n             \"Function 'days' cannot process input: (date, date): Wrong number of inputs\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidInputTypes() {\n     assertThatThrownBy(() -> scalarSql(\"SELECT system.days(1)\"))\n         .isInstanceOf(AnalysisException.class)\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkHoursFunction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkHoursFunction.java\nindex c2f16e454a5c..eeeb9d1a1e78 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkHoursFunction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkHoursFunction.java\n@@ -18,56 +18,53 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.spark.sql.AnalysisException;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSparkHoursFunction extends SparkTestBaseWithCatalog {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkHoursFunction extends TestBaseWithCatalog {\n \n-  @Before\n+  @BeforeEach\n   public void useCatalog() {\n     sql(\"USE %s\", catalogName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimestamps() {\n-    Assert.assertEquals(\n-        \"Expected to produce 17501 * 24 + 10\",\n-        420034,\n-        scalarSql(\"SELECT system.hours(TIMESTAMP '2017-12-01 10:12:55.038194 UTC+00:00')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 0 * 24 + 0 = 0\",\n-        0,\n-        scalarSql(\"SELECT system.hours(TIMESTAMP '1970-01-01 00:00:01.000001 UTC+00:00')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce -1\",\n-        -1,\n-        scalarSql(\"SELECT system.hours(TIMESTAMP '1969-12-31 23:59:58.999999 UTC+00:00')\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.hours(CAST(null AS TIMESTAMP))\"));\n+    assertThat(scalarSql(\"SELECT system.hours(TIMESTAMP '2017-12-01 10:12:55.038194 UTC+00:00')\"))\n+        .as(\"Expected to produce 17501 * 24 + 10\")\n+        .isEqualTo(420034);\n+    assertThat(scalarSql(\"SELECT system.hours(TIMESTAMP '1970-01-01 00:00:01.000001 UTC+00:00')\"))\n+        .as(\"Expected to produce 0 * 24 + 0 = 0\")\n+        .isEqualTo(0);\n+    assertThat(scalarSql(\"SELECT system.hours(TIMESTAMP '1969-12-31 23:59:58.999999 UTC+00:00')\"))\n+        .as(\"Expected to produce -1\")\n+        .isEqualTo(-1);\n+    assertThat(scalarSql(\"SELECT system.hours(CAST(null AS TIMESTAMP))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimestampsNtz() {\n-    Assert.assertEquals(\n-        \"Expected to produce 17501 * 24 + 10\",\n-        420034,\n-        scalarSql(\"SELECT system.hours(TIMESTAMP_NTZ '2017-12-01 10:12:55.038194 UTC')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 0 * 24 + 0 = 0\",\n-        0,\n-        scalarSql(\"SELECT system.hours(TIMESTAMP_NTZ '1970-01-01 00:00:01.000001 UTC')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce -1\",\n-        -1,\n-        scalarSql(\"SELECT system.hours(TIMESTAMP_NTZ '1969-12-31 23:59:58.999999 UTC')\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.hours(CAST(null AS TIMESTAMP_NTZ))\"));\n+    assertThat(scalarSql(\"SELECT system.hours(TIMESTAMP_NTZ '2017-12-01 10:12:55.038194 UTC')\"))\n+        .as(\"Expected to produce 17501 * 24 + 10\")\n+        .isEqualTo(420034);\n+    assertThat(scalarSql(\"SELECT system.hours(TIMESTAMP_NTZ '1970-01-01 00:00:01.000001 UTC')\"))\n+        .as(\"Expected to produce 0 * 24 + 0 = 0\")\n+        .isEqualTo(0);\n+    assertThat(scalarSql(\"SELECT system.hours(TIMESTAMP_NTZ '1969-12-31 23:59:58.999999 UTC')\"))\n+        .as(\"Expected to produce -1\")\n+        .isEqualTo(-1);\n+    assertThat(scalarSql(\"SELECT system.hours(CAST(null AS TIMESTAMP_NTZ))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWrongNumberOfArguments() {\n     assertThatThrownBy(() -> scalarSql(\"SELECT system.hours()\"))\n         .isInstanceOf(AnalysisException.class)\n@@ -81,7 +78,7 @@ public void testWrongNumberOfArguments() {\n             \"Function 'hours' cannot process input: (date, date): Wrong number of inputs\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidInputTypes() {\n     assertThatThrownBy(() -> scalarSql(\"SELECT system.hours(1)\"))\n         .isInstanceOf(AnalysisException.class)\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkMonthsFunction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkMonthsFunction.java\nindex 60b93c466698..68952d189359 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkMonthsFunction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkMonthsFunction.java\n@@ -21,70 +21,65 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.iceberg.spark.functions.MonthsFunction;\n import org.apache.spark.sql.AnalysisException;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSparkMonthsFunction extends SparkTestBaseWithCatalog {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkMonthsFunction extends TestBaseWithCatalog {\n \n-  @Before\n+  @BeforeEach\n   public void useCatalog() {\n     sql(\"USE %s\", catalogName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDates() {\n-    Assert.assertEquals(\n-        \"Expected to produce 47 * 12 + 11 = 575\",\n-        575,\n-        scalarSql(\"SELECT system.months(date('2017-12-01'))\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 0 * 12 + 0 = 0\",\n-        0,\n-        scalarSql(\"SELECT system.months(date('1970-01-01'))\"));\n-    Assert.assertEquals(\n-        \"Expected to produce -1\", -1, scalarSql(\"SELECT system.months(date('1969-12-31'))\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.months(CAST(null AS DATE))\"));\n+    assertThat(scalarSql(\"SELECT system.months(date('2017-12-01'))\"))\n+        .as(\"Expected to produce 47 * 12 + 11 = 575\")\n+        .isEqualTo(575);\n+    assertThat(scalarSql(\"SELECT system.months(date('1970-01-01'))\"))\n+        .as(\"Expected to produce 0 * 12 + 0 = 0\")\n+        .isEqualTo(0);\n+    assertThat(scalarSql(\"SELECT system.months(date('1969-12-31'))\"))\n+        .as(\"Expected to produce -1\")\n+        .isEqualTo(-1);\n+    assertThat(scalarSql(\"SELECT system.months(CAST(null AS DATE))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimestamps() {\n-    Assert.assertEquals(\n-        \"Expected to produce 47 * 12 + 11 = 575\",\n-        575,\n-        scalarSql(\"SELECT system.months(TIMESTAMP '2017-12-01 10:12:55.038194 UTC+00:00')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 0 * 12 + 0 = 0\",\n-        0,\n-        scalarSql(\"SELECT system.months(TIMESTAMP '1970-01-01 00:00:01.000001 UTC+00:00')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce -1\",\n-        -1,\n-        scalarSql(\"SELECT system.months(TIMESTAMP '1969-12-31 23:59:58.999999 UTC+00:00')\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.months(CAST(null AS TIMESTAMP))\"));\n+    assertThat(scalarSql(\"SELECT system.months(TIMESTAMP '2017-12-01 10:12:55.038194 UTC+00:00')\"))\n+        .as(\"Expected to produce 47 * 12 + 11 = 575\")\n+        .isEqualTo(575);\n+    assertThat(scalarSql(\"SELECT system.months(TIMESTAMP '1970-01-01 00:00:01.000001 UTC+00:00')\"))\n+        .as(\"Expected to produce 0 * 12 + 0 = 0\")\n+        .isEqualTo(0);\n+    assertThat(scalarSql(\"SELECT system.months(TIMESTAMP '1969-12-31 23:59:58.999999 UTC+00:00')\"))\n+        .as(\"Expected to produce -1\")\n+        .isEqualTo(-1);\n+    assertThat(scalarSql(\"SELECT system.months(CAST(null AS TIMESTAMP))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimestampNtz() {\n-    Assert.assertEquals(\n-        \"Expected to produce 47 * 12 + 11 = 575\",\n-        575,\n-        scalarSql(\"SELECT system.months(TIMESTAMP_NTZ '2017-12-01 10:12:55.038194 UTC')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 0 * 12 + 0 = 0\",\n-        0,\n-        scalarSql(\"SELECT system.months(TIMESTAMP_NTZ '1970-01-01 00:00:01.000001 UTC')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce -1\",\n-        -1,\n-        scalarSql(\"SELECT system.months(TIMESTAMP_NTZ '1969-12-31 23:59:58.999999 UTC')\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.months(CAST(null AS TIMESTAMP_NTZ))\"));\n+    assertThat(scalarSql(\"SELECT system.months(TIMESTAMP_NTZ '2017-12-01 10:12:55.038194 UTC')\"))\n+        .as(\"Expected to produce 47 * 12 + 11 = 575\")\n+        .isEqualTo(575);\n+    assertThat(scalarSql(\"SELECT system.months(TIMESTAMP_NTZ '1970-01-01 00:00:01.000001 UTC')\"))\n+        .as(\"Expected to produce 0 * 12 + 0 = 0\")\n+        .isEqualTo(0);\n+    assertThat(scalarSql(\"SELECT system.months(TIMESTAMP_NTZ '1969-12-31 23:59:58.999999 UTC')\"))\n+        .as(\"Expected to produce -1\")\n+        .isEqualTo(-1);\n+    assertThat(scalarSql(\"SELECT system.months(CAST(null AS TIMESTAMP_NTZ))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWrongNumberOfArguments() {\n     assertThatThrownBy(() -> scalarSql(\"SELECT system.months()\"))\n         .isInstanceOf(AnalysisException.class)\n@@ -98,7 +93,7 @@ public void testWrongNumberOfArguments() {\n             \"Function 'months' cannot process input: (date, date): Wrong number of inputs\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidInputTypes() {\n     assertThatThrownBy(() -> scalarSql(\"SELECT system.months(1)\"))\n         .isInstanceOf(AnalysisException.class)\n@@ -111,7 +106,7 @@ public void testInvalidInputTypes() {\n             \"Function 'months' cannot process input: (bigint): Expected value to be date or timestamp\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testThatMagicFunctionsAreInvoked() {\n     String dateValue = \"date('2017-12-01')\";\n     String dateTransformClass = MonthsFunction.DateToMonthsFunction.class.getName();\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkTruncateFunction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkTruncateFunction.java\nindex 686b46888f18..af4adbe5b32d 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkTruncateFunction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkTruncateFunction.java\n@@ -23,249 +23,232 @@\n \n import java.math.BigDecimal;\n import java.nio.charset.StandardCharsets;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.relocated.com.google.common.io.BaseEncoding;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.spark.sql.AnalysisException;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSparkTruncateFunction extends SparkTestBaseWithCatalog {\n-  public TestSparkTruncateFunction() {}\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkTruncateFunction extends TestBaseWithCatalog {\n \n-  @Before\n+  @BeforeEach\n   public void useCatalog() {\n     sql(\"USE %s\", catalogName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTruncateTinyInt() {\n-    Assert.assertEquals((byte) 0, scalarSql(\"SELECT system.truncate(10, 0Y)\"));\n-    Assert.assertEquals((byte) 0, scalarSql(\"SELECT system.truncate(10, 1Y)\"));\n-    Assert.assertEquals((byte) 0, scalarSql(\"SELECT system.truncate(10, 5Y)\"));\n-    Assert.assertEquals((byte) 0, scalarSql(\"SELECT system.truncate(10, 9Y)\"));\n-    Assert.assertEquals((byte) 10, scalarSql(\"SELECT system.truncate(10, 10Y)\"));\n-    Assert.assertEquals((byte) 10, scalarSql(\"SELECT system.truncate(10, 11Y)\"));\n-    Assert.assertEquals((byte) -10, scalarSql(\"SELECT system.truncate(10, -1Y)\"));\n-    Assert.assertEquals((byte) -10, scalarSql(\"SELECT system.truncate(10, -5Y)\"));\n-    Assert.assertEquals((byte) -10, scalarSql(\"SELECT system.truncate(10, -10Y)\"));\n-    Assert.assertEquals((byte) -20, scalarSql(\"SELECT system.truncate(10, -11Y)\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 0Y)\")).isEqualTo((byte) 0);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 1Y)\")).isEqualTo((byte) 0);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 5Y)\")).isEqualTo((byte) 0);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 9Y)\")).isEqualTo((byte) 0);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 10Y)\")).isEqualTo((byte) 10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 11Y)\")).isEqualTo((byte) 10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -1Y)\")).isEqualTo((byte) -10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -5Y)\")).isEqualTo((byte) -10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -10Y)\")).isEqualTo((byte) -10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -11Y)\")).isEqualTo((byte) -20);\n \n     // Check that different widths can be used\n-    Assert.assertEquals((byte) -2, scalarSql(\"SELECT system.truncate(2, -1Y)\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(2, -1Y)\")).isEqualTo((byte) -2);\n \n-    Assert.assertNull(\n-        \"Null input should return null\",\n-        scalarSql(\"SELECT system.truncate(2, CAST(null AS tinyint))\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(2, CAST(null AS tinyint))\"))\n+        .as(\"Null input should return null\")\n+        .isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTruncateSmallInt() {\n-    Assert.assertEquals((short) 0, scalarSql(\"SELECT system.truncate(10, 0S)\"));\n-    Assert.assertEquals((short) 0, scalarSql(\"SELECT system.truncate(10, 1S)\"));\n-    Assert.assertEquals((short) 0, scalarSql(\"SELECT system.truncate(10, 5S)\"));\n-    Assert.assertEquals((short) 0, scalarSql(\"SELECT system.truncate(10, 9S)\"));\n-    Assert.assertEquals((short) 10, scalarSql(\"SELECT system.truncate(10, 10S)\"));\n-    Assert.assertEquals((short) 10, scalarSql(\"SELECT system.truncate(10, 11S)\"));\n-    Assert.assertEquals((short) -10, scalarSql(\"SELECT system.truncate(10, -1S)\"));\n-    Assert.assertEquals((short) -10, scalarSql(\"SELECT system.truncate(10, -5S)\"));\n-    Assert.assertEquals((short) -10, scalarSql(\"SELECT system.truncate(10, -10S)\"));\n-    Assert.assertEquals((short) -20, scalarSql(\"SELECT system.truncate(10, -11S)\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 0S)\")).isEqualTo((short) 0);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 1S)\")).isEqualTo((short) 0);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 5S)\")).isEqualTo((short) 0);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 9S)\")).isEqualTo((short) 0);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 10S)\")).isEqualTo((short) 10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 11S)\")).isEqualTo((short) 10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -1S)\")).isEqualTo((short) -10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -5S)\")).isEqualTo((short) -10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -10S)\")).isEqualTo((short) -10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -11S)\")).isEqualTo((short) -20);\n \n     // Check that different widths can be used\n-    Assert.assertEquals((short) -2, scalarSql(\"SELECT system.truncate(2, -1S)\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(2, -1S)\")).isEqualTo((short) -2);\n \n-    Assert.assertNull(\n-        \"Null input should return null\",\n-        scalarSql(\"SELECT system.truncate(2, CAST(null AS smallint))\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(2, CAST(null AS smallint))\"))\n+        .as(\"Null input should return null\")\n+        .isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTruncateInt() {\n-    Assert.assertEquals(0, scalarSql(\"SELECT system.truncate(10, 0)\"));\n-    Assert.assertEquals(0, scalarSql(\"SELECT system.truncate(10, 1)\"));\n-    Assert.assertEquals(0, scalarSql(\"SELECT system.truncate(10, 5)\"));\n-    Assert.assertEquals(0, scalarSql(\"SELECT system.truncate(10, 9)\"));\n-    Assert.assertEquals(10, scalarSql(\"SELECT system.truncate(10, 10)\"));\n-    Assert.assertEquals(10, scalarSql(\"SELECT system.truncate(10, 11)\"));\n-    Assert.assertEquals(-10, scalarSql(\"SELECT system.truncate(10, -1)\"));\n-    Assert.assertEquals(-10, scalarSql(\"SELECT system.truncate(10, -5)\"));\n-    Assert.assertEquals(-10, scalarSql(\"SELECT system.truncate(10, -10)\"));\n-    Assert.assertEquals(-20, scalarSql(\"SELECT system.truncate(10, -11)\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 0)\")).isEqualTo(0);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 1)\")).isEqualTo(0);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 5)\")).isEqualTo(0);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 9)\")).isEqualTo(0);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 10)\")).isEqualTo(10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 11)\")).isEqualTo(10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -1)\")).isEqualTo(-10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -5)\")).isEqualTo(-10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -10)\")).isEqualTo(-10);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -11)\")).isEqualTo(-20);\n \n     // Check that different widths can be used\n-    Assert.assertEquals(-2, scalarSql(\"SELECT system.truncate(2, -1)\"));\n-    Assert.assertEquals(0, scalarSql(\"SELECT system.truncate(300, 1)\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(2, -1)\")).isEqualTo(-2);\n+    assertThat(scalarSql(\"SELECT system.truncate(300, 1)\")).isEqualTo(0);\n \n-    Assert.assertNull(\n-        \"Null input should return null\", scalarSql(\"SELECT system.truncate(2, CAST(null AS int))\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(2, CAST(null AS int))\"))\n+        .as(\"Null input should return null\")\n+        .isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTruncateBigInt() {\n-    Assert.assertEquals(0L, scalarSql(\"SELECT system.truncate(10, 0L)\"));\n-    Assert.assertEquals(0L, scalarSql(\"SELECT system.truncate(10, 1L)\"));\n-    Assert.assertEquals(0L, scalarSql(\"SELECT system.truncate(10, 5L)\"));\n-    Assert.assertEquals(0L, scalarSql(\"SELECT system.truncate(10, 9L)\"));\n-    Assert.assertEquals(10L, scalarSql(\"SELECT system.truncate(10, 10L)\"));\n-    Assert.assertEquals(10L, scalarSql(\"SELECT system.truncate(10, 11L)\"));\n-    Assert.assertEquals(-10L, scalarSql(\"SELECT system.truncate(10, -1L)\"));\n-    Assert.assertEquals(-10L, scalarSql(\"SELECT system.truncate(10, -5L)\"));\n-    Assert.assertEquals(-10L, scalarSql(\"SELECT system.truncate(10, -10L)\"));\n-    Assert.assertEquals(-20L, scalarSql(\"SELECT system.truncate(10, -11L)\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 0L)\")).isEqualTo(0L);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 1L)\")).isEqualTo(0L);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 5L)\")).isEqualTo(0L);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 9L)\")).isEqualTo(0L);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 10L)\")).isEqualTo(10L);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 11L)\")).isEqualTo(10L);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -1L)\")).isEqualTo(-10L);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -5L)\")).isEqualTo(-10L);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -10L)\")).isEqualTo(-10L);\n+    assertThat(scalarSql(\"SELECT system.truncate(10, -11L)\")).isEqualTo(-20L);\n \n     // Check that different widths can be used\n-    Assert.assertEquals(-2L, scalarSql(\"SELECT system.truncate(2, -1L)\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(2, -1L)\")).isEqualTo(-2L);\n \n-    Assert.assertNull(\n-        \"Null input should return null\",\n-        scalarSql(\"SELECT system.truncate(2, CAST(null AS bigint))\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(2, CAST(null AS bigint))\"))\n+        .as(\"Null input should return null\")\n+        .isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTruncateDecimal() {\n     // decimal truncation works by applying the decimal scale to the width: ie 10 scale 2 = 0.10\n-    Assert.assertEquals(\n-        new BigDecimal(\"12.30\"),\n-        scalarSql(\"SELECT system.truncate(10, CAST(%s as DECIMAL(9, 2)))\", \"12.34\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(10, CAST(%s as DECIMAL(9, 2)))\", \"12.34\"))\n+        .isEqualTo(new BigDecimal(\"12.30\"));\n \n-    Assert.assertEquals(\n-        new BigDecimal(\"12.30\"),\n-        scalarSql(\"SELECT system.truncate(10, CAST(%s as DECIMAL(9, 2)))\", \"12.30\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(10, CAST(%s as DECIMAL(9, 2)))\", \"12.30\"))\n+        .isEqualTo(new BigDecimal(\"12.30\"));\n \n-    Assert.assertEquals(\n-        new BigDecimal(\"12.290\"),\n-        scalarSql(\"SELECT system.truncate(10, CAST(%s as DECIMAL(9, 3)))\", \"12.299\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(10, CAST(%s as DECIMAL(9, 3)))\", \"12.299\"))\n+        .isEqualTo(new BigDecimal(\"12.290\"));\n \n-    Assert.assertEquals(\n-        new BigDecimal(\"0.03\"),\n-        scalarSql(\"SELECT system.truncate(3, CAST(%s as DECIMAL(5, 2)))\", \"0.05\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(3, CAST(%s as DECIMAL(5, 2)))\", \"0.05\"))\n+        .isEqualTo(new BigDecimal(\"0.03\"));\n \n-    Assert.assertEquals(\n-        new BigDecimal(\"0.00\"),\n-        scalarSql(\"SELECT system.truncate(10, CAST(%s as DECIMAL(9, 2)))\", \"0.05\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(10, CAST(%s as DECIMAL(9, 2)))\", \"0.05\"))\n+        .isEqualTo(new BigDecimal(\"0.00\"));\n \n-    Assert.assertEquals(\n-        new BigDecimal(\"-0.10\"),\n-        scalarSql(\"SELECT system.truncate(10, CAST(%s as DECIMAL(9, 2)))\", \"-0.05\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(10, CAST(%s as DECIMAL(9, 2)))\", \"-0.05\"))\n+        .isEqualTo(new BigDecimal(\"-0.10\"));\n \n-    Assert.assertEquals(\n-        \"Implicit decimal scale and precision should be allowed\",\n-        new BigDecimal(\"12345.3480\"),\n-        scalarSql(\"SELECT system.truncate(10, 12345.3482)\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(10, 12345.3482)\"))\n+        .as(\"Implicit decimal scale and precision should be allowed\")\n+        .isEqualTo(new BigDecimal(\"12345.3480\"));\n \n     BigDecimal truncatedDecimal =\n         (BigDecimal) scalarSql(\"SELECT system.truncate(10, CAST(%s as DECIMAL(6, 4)))\", \"-0.05\");\n-    Assert.assertEquals(\n-        \"Truncating a decimal should return a decimal with the same scale\",\n-        4,\n-        truncatedDecimal.scale());\n-\n-    Assert.assertEquals(\n-        \"Truncating a decimal should return a decimal with the correct scale\",\n-        BigDecimal.valueOf(-500, 4),\n-        truncatedDecimal);\n-\n-    Assert.assertNull(\n-        \"Null input should return null\",\n-        scalarSql(\"SELECT system.truncate(2, CAST(null AS decimal))\"));\n+    assertThat(truncatedDecimal.scale())\n+        .as(\"Truncating a decimal should return a decimal with the same scale\")\n+        .isEqualTo(4);\n+\n+    assertThat(truncatedDecimal)\n+        .as(\"Truncating a decimal should return a decimal with the correct scale\")\n+        .isEqualTo(BigDecimal.valueOf(-500, 4));\n+\n+    assertThat(scalarSql(\"SELECT system.truncate(2, CAST(null AS decimal))\"))\n+        .as(\"Null input should return null\")\n+        .isNull();\n   }\n \n   @SuppressWarnings(\"checkstyle:AvoidEscapedUnicodeCharacters\")\n-  @Test\n+  @TestTemplate\n   public void testTruncateString() {\n-    Assert.assertEquals(\n-        \"Should system.truncate strings longer than length\",\n-        \"abcde\",\n-        scalarSql(\"SELECT system.truncate(5, 'abcdefg')\"));\n-\n-    Assert.assertEquals(\n-        \"Should not pad strings shorter than length\",\n-        \"abc\",\n-        scalarSql(\"SELECT system.truncate(5, 'abc')\"));\n-\n-    Assert.assertEquals(\n-        \"Should not alter strings equal to length\",\n-        \"abcde\",\n-        scalarSql(\"SELECT system.truncate(5, 'abcde')\"));\n-\n-    Assert.assertEquals(\n-        \"Strings with multibyte unicode characters should should truncate along codepoint boundaries\",\n-        \"„Ç§„É≠\",\n-        scalarSql(\"SELECT system.truncate(2, '„Ç§„É≠„Éè„Éã„Éõ„Éò„Éà')\"));\n-\n-    Assert.assertEquals(\n-        \"Strings with multibyte unicode characters should truncate along codepoint boundaries\",\n-        \"„Ç§„É≠„Éè\",\n-        scalarSql(\"SELECT system.truncate(3, '„Ç§„É≠„Éè„Éã„Éõ„Éò„Éà')\"));\n-\n-    Assert.assertEquals(\n-        \"Strings with multibyte unicode characters should not alter input with fewer codepoints than width\",\n-        \"„Ç§„É≠„Éè„Éã„Éõ„Éò„Éà\",\n-        scalarSql(\"SELECT system.truncate(7, '„Ç§„É≠„Éè„Éã„Éõ„Éò„Éà')\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(5, 'abcdefg')\"))\n+        .as(\"Should system.truncate strings longer than length\")\n+        .isEqualTo(\"abcde\");\n+\n+    assertThat(scalarSql(\"SELECT system.truncate(5, 'abc')\"))\n+        .as(\"Should not pad strings shorter than length\")\n+        .isEqualTo(\"abc\");\n+\n+    assertThat(scalarSql(\"SELECT system.truncate(5, 'abcde')\"))\n+        .as(\"Should not alter strings equal to length\")\n+        .isEqualTo(\"abcde\");\n+\n+    assertThat(scalarSql(\"SELECT system.truncate(2, '„Ç§„É≠„Éè„Éã„Éõ„Éò„Éà')\"))\n+        .as(\"Strings with multibyte unicode characters should truncate along codepoint boundaries\")\n+        .isEqualTo(\"„Ç§„É≠\");\n+\n+    assertThat(scalarSql(\"SELECT system.truncate(3, '„Ç§„É≠„Éè„Éã„Éõ„Éò„Éà')\"))\n+        .as(\"Strings with multibyte unicode characters should truncate along codepoint boundaries\")\n+        .isEqualTo(\"„Ç§„É≠„Éè\");\n+\n+    assertThat(scalarSql(\"SELECT system.truncate(7, '„Ç§„É≠„Éè„Éã„Éõ„Éò„Éà')\"))\n+        .as(\n+            \"Strings with multibyte unicode characters should not alter input with fewer codepoints than width\")\n+        .isEqualTo(\"„Ç§„É≠„Éè„Éã„Éõ„Éò„Éà\");\n \n     String stringWithTwoCodePointsEachFourBytes = \"\\uD800\\uDC00\\uD800\\uDC00\";\n-    Assert.assertEquals(\n-        \"String truncation on four byte codepoints should work as expected\",\n-        \"\\uD800\\uDC00\",\n-        scalarSql(\"SELECT system.truncate(1, '%s')\", stringWithTwoCodePointsEachFourBytes));\n-\n-    Assert.assertEquals(\n-        \"Should handle three-byte UTF-8 characters appropriately\",\n-        \"Êµã\",\n-        scalarSql(\"SELECT system.truncate(1, 'ÊµãËØï')\"));\n-\n-    Assert.assertEquals(\n-        \"Should handle three-byte UTF-8 characters mixed with two byte utf-8 characters\",\n-        \"ÊµãËØïra\",\n-        scalarSql(\"SELECT system.truncate(4, 'ÊµãËØïraulËØïÊµã')\"));\n-\n-    Assert.assertEquals(\n-        \"Should not fail on the empty string\", \"\", scalarSql(\"SELECT system.truncate(10, '')\"));\n-\n-    Assert.assertNull(\n-        \"Null input should return null as output\",\n-        scalarSql(\"SELECT system.truncate(3, CAST(null AS string))\"));\n-\n-    Assert.assertEquals(\n-        \"Varchar should work like string\",\n-        \"ÊµãËØïra\",\n-        scalarSql(\"SELECT system.truncate(4, CAST('ÊµãËØïraulËØïÊµã' AS varchar(8)))\"));\n-\n-    Assert.assertEquals(\n-        \"Char should work like string\",\n-        \"ÊµãËØïra\",\n-        scalarSql(\"SELECT system.truncate(4, CAST('ÊµãËØïraulËØïÊµã' AS char(8)))\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(1, '%s')\", stringWithTwoCodePointsEachFourBytes))\n+        .as(\"String truncation on four byte codepoints should work as expected\")\n+        .isEqualTo(\"\\uD800\\uDC00\");\n+\n+    assertThat(scalarSql(\"SELECT system.truncate(1, 'ÊµãËØï')\"))\n+        .as(\"Should handle three-byte UTF-8 characters appropriately\")\n+        .isEqualTo(\"Êµã\");\n+\n+    assertThat(scalarSql(\"SELECT system.truncate(4, 'ÊµãËØïraulËØïÊµã')\"))\n+        .as(\"Should handle three-byte UTF-8 characters mixed with two byte utf-8 characters\")\n+        .isEqualTo(\"ÊµãËØïra\");\n+\n+    assertThat(scalarSql(\"SELECT system.truncate(10, '')\"))\n+        .as(\"Should not fail on the empty string\")\n+        .isEqualTo(\"\");\n+\n+    assertThat(scalarSql(\"SELECT system.truncate(3, CAST(null AS string))\"))\n+        .as(\"Null input should return null as output\")\n+        .isNull();\n+\n+    assertThat(scalarSql(\"SELECT system.truncate(4, CAST('ÊµãËØïraulËØïÊµã' AS varchar(8)))\"))\n+        .as(\"Varchar should work like string\")\n+        .isEqualTo(\"ÊµãËØïra\");\n+\n+    assertThat(scalarSql(\"SELECT system.truncate(4, CAST('ÊµãËØïraulËØïÊµã' AS char(8)))\"))\n+        .as(\"Char should work like string\")\n+        .isEqualTo(\"ÊµãËØïra\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTruncateBinary() {\n-    Assert.assertArrayEquals(\n-        new byte[] {1, 2, 3, 4, 5, 6, 7, 8, 9, 10},\n-        (byte[]) scalarSql(\"SELECT system.truncate(10, X'0102030405060708090a0b0c0d0e0f')\"));\n-    Assert.assertArrayEquals(\n-        \"Should return the same input when value is equal to truncation width\",\n-        \"abc\".getBytes(StandardCharsets.UTF_8),\n-        (byte[]) scalarSql(\"SELECT system.truncate(3, %s)\", asBytesLiteral(\"abcdefg\")));\n-    Assert.assertArrayEquals(\n-        \"Should not truncate, pad, or trim the input when its length is less than the width\",\n-        \"abc\\0\\0\".getBytes(StandardCharsets.UTF_8),\n-        (byte[]) scalarSql(\"SELECT system.truncate(10, %s)\", asBytesLiteral(\"abc\\0\\0\")));\n-    Assert.assertArrayEquals(\n-        \"Should not pad the input when its length is equal to the width\",\n-        \"abc\".getBytes(StandardCharsets.UTF_8),\n-        (byte[]) scalarSql(\"SELECT system.truncate(3, %s)\", asBytesLiteral(\"abc\")));\n-    Assert.assertArrayEquals(\n-        \"Should handle three-byte UTF-8 characters appropriately\",\n-        \"ÊµãËØï\".getBytes(StandardCharsets.UTF_8),\n-        (byte[]) scalarSql(\"SELECT system.truncate(6, %s)\", asBytesLiteral(\"ÊµãËØï_\")));\n-\n-    Assert.assertNull(\n-        \"Null input should return null as output\",\n-        scalarSql(\"SELECT system.truncate(3, CAST(null AS binary))\"));\n+    assertThat((byte[]) scalarSql(\"SELECT system.truncate(10, X'0102030405060708090a0b0c0d0e0f')\"))\n+        .isEqualTo(new byte[] {1, 2, 3, 4, 5, 6, 7, 8, 9, 10});\n+\n+    assertThat((byte[]) scalarSql(\"SELECT system.truncate(3, %s)\", asBytesLiteral(\"abcdefg\")))\n+        .as(\"Should return the same input when value is equal to truncation width\")\n+        .isEqualTo(\"abc\".getBytes(StandardCharsets.UTF_8));\n+\n+    assertThat((byte[]) scalarSql(\"SELECT system.truncate(10, %s)\", asBytesLiteral(\"abc\\0\\0\")))\n+        .as(\"Should not truncate, pad, or trim the input when its length is less than the width\")\n+        .isEqualTo(\"abc\\0\\0\".getBytes(StandardCharsets.UTF_8));\n+\n+    assertThat((byte[]) scalarSql(\"SELECT system.truncate(3, %s)\", asBytesLiteral(\"abc\")))\n+        .as(\"Should not pad the input when its length is equal to the width\")\n+        .isEqualTo(\"abc\".getBytes(StandardCharsets.UTF_8));\n+\n+    assertThat((byte[]) scalarSql(\"SELECT system.truncate(6, %s)\", asBytesLiteral(\"ÊµãËØï_\")))\n+        .as(\"Should handle three-byte UTF-8 characters appropriately\")\n+        .isEqualTo(\"ÊµãËØï\".getBytes(StandardCharsets.UTF_8));\n+\n+    assertThat(scalarSql(\"SELECT system.truncate(3, CAST(null AS binary))\"))\n+        .as(\"Null input should return null as output\")\n+        .isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTruncateUsingDataframeForWidthWithVaryingWidth() {\n     // This situation is atypical but allowed. Typically, width is static as data is partitioned on\n     // one width.\n@@ -278,26 +261,23 @@ public void testTruncateUsingDataframeForWidthWithVaryingWidth() {\n             .selectExpr(\"system.truncate(width, value) as truncated_value\")\n             .filter(\"truncated_value == 0\")\n             .count();\n-    Assert.assertEquals(\n-        \"A truncate function with variable widths should be usable on dataframe columns\",\n-        rumRows,\n-        numNonZero);\n+    assertThat(numNonZero)\n+        .as(\"A truncate function with variable widths should be usable on dataframe columns\")\n+        .isEqualTo(rumRows);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWidthAcceptsShortAndByte() {\n-    Assert.assertEquals(\n-        \"Short types should be usable for the width field\",\n-        0L,\n-        scalarSql(\"SELECT system.truncate(5S, 1L)\"));\n-\n-    Assert.assertEquals(\n-        \"Byte types should be allowed for the width field\",\n-        0,\n-        scalarSql(\"SELECT system.truncate(5Y, 1)\"));\n+    assertThat(scalarSql(\"SELECT system.truncate(5S, 1L)\"))\n+        .as(\"Short types should be usable for the width field\")\n+        .isEqualTo(0L);\n+\n+    assertThat(scalarSql(\"SELECT system.truncate(5Y, 1)\"))\n+        .as(\"Byte types should be allowed for the width field\")\n+        .isEqualTo(0);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWrongNumberOfArguments() {\n     assertThatThrownBy(() -> scalarSql(\"SELECT system.truncate()\"))\n         .isInstanceOf(AnalysisException.class)\n@@ -315,7 +295,7 @@ public void testWrongNumberOfArguments() {\n             \"Function 'truncate' cannot process input: (int, bigint, int): Wrong number of inputs (expected width and value)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidTypesCannotBeUsedForWidth() {\n     assertThatThrownBy(\n             () -> scalarSql(\"SELECT system.truncate(CAST('12.34' as DECIMAL(9, 2)), 10)\"))\n@@ -343,7 +323,7 @@ public void testInvalidTypesCannotBeUsedForWidth() {\n             \"Function 'truncate' cannot process input: (interval day to second, int): Expected truncation width to be tinyint, shortint or int\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidTypesForTruncationColumn() {\n     assertThatThrownBy(() -> scalarSql(\"SELECT system.truncate(10, cast(12.3456 as float))\"))\n         .isInstanceOf(AnalysisException.class)\n@@ -385,7 +365,7 @@ public void testInvalidTypesForTruncationColumn() {\n             \"Function 'truncate' cannot process input: (int, interval day to second): Expected truncation col to be tinyint, shortint, int, bigint, decimal, string, or binary\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMagicFunctionsResolveForTinyIntAndSmallIntWidths() {\n     // Magic functions have staticinvoke in the explain output. Nonmagic calls use\n     // applyfunctionexpression instead.\n@@ -403,7 +383,7 @@ public void testMagicFunctionsResolveForTinyIntAndSmallIntWidths() {\n             \"staticinvoke(class org.apache.iceberg.spark.functions.TruncateFunction$TruncateBigInt\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testThatMagicFunctionsAreInvoked() {\n     // Magic functions have `staticinvoke` in the explain output.\n     // Non-magic calls have `applyfunctionexpression` instead.\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkYearsFunction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkYearsFunction.java\nindex ecd3f6cc7db7..8cf62b2b48f3 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkYearsFunction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkYearsFunction.java\n@@ -21,72 +21,62 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.iceberg.spark.functions.YearsFunction;\n import org.apache.spark.sql.AnalysisException;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n \n-public class TestSparkYearsFunction extends SparkTestBaseWithCatalog {\n+public class TestSparkYearsFunction extends TestBaseWithCatalog {\n \n-  @Before\n+  @BeforeEach\n   public void useCatalog() {\n     sql(\"USE %s\", catalogName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDates() {\n-    Assert.assertEquals(\n-        \"Expected to produce 2017 - 1970 = 47\",\n-        47,\n-        scalarSql(\"SELECT system.years(date('2017-12-01'))\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 1970 - 1970 = 0\",\n-        0,\n-        scalarSql(\"SELECT system.years(date('1970-01-01'))\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 1969 - 1970 = -1\",\n-        -1,\n-        scalarSql(\"SELECT system.years(date('1969-12-31'))\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.years(CAST(null AS DATE))\"));\n+    assertThat(scalarSql(\"SELECT system.years(date('2017-12-01'))\"))\n+        .as(\"Expected to produce 2017 - 1970 = 47\")\n+        .isEqualTo(47);\n+    assertThat(scalarSql(\"SELECT system.years(date('1970-01-01'))\"))\n+        .as(\"Expected to produce 1970 - 1970 = 0\")\n+        .isEqualTo(0);\n+    assertThat(scalarSql(\"SELECT system.years(date('1969-12-31'))\"))\n+        .as(\"Expected to produce 1969 - 1970 = -1\")\n+        .isEqualTo(-1);\n+    assertThat(scalarSql(\"SELECT system.years(CAST(null AS DATE))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimestamps() {\n-    Assert.assertEquals(\n-        \"Expected to produce 2017 - 1970 = 47\",\n-        47,\n-        scalarSql(\"SELECT system.years(TIMESTAMP '2017-12-01 10:12:55.038194 UTC+00:00')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 1970 - 1970 = 0\",\n-        0,\n-        scalarSql(\"SELECT system.years(TIMESTAMP '1970-01-01 00:00:01.000001 UTC+00:00')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 1969 - 1970 = -1\",\n-        -1,\n-        scalarSql(\"SELECT system.years(TIMESTAMP '1969-12-31 23:59:58.999999 UTC+00:00')\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.years(CAST(null AS TIMESTAMP))\"));\n+    assertThat(scalarSql(\"SELECT system.years(TIMESTAMP '2017-12-01 10:12:55.038194 UTC+00:00')\"))\n+        .as(\"Expected to produce 2017 - 1970 = 47\")\n+        .isEqualTo(47);\n+    assertThat(scalarSql(\"SELECT system.years(TIMESTAMP '1970-01-01 00:00:01.000001 UTC+00:00')\"))\n+        .as(\"Expected to produce 1970 - 1970 = 0\")\n+        .isEqualTo(0);\n+    assertThat(scalarSql(\"SELECT system.years(TIMESTAMP '1969-12-31 23:59:58.999999 UTC+00:00')\"))\n+        .as(\"Expected to produce 1969 - 1970 = -1\")\n+        .isEqualTo(-1);\n+    assertThat(scalarSql(\"SELECT system.years(CAST(null AS TIMESTAMP))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimestampNtz() {\n-    Assert.assertEquals(\n-        \"Expected to produce 2017 - 1970 = 47\",\n-        47,\n-        scalarSql(\"SELECT system.years(TIMESTAMP_NTZ '2017-12-01 10:12:55.038194 UTC')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 1970 - 1970 = 0\",\n-        0,\n-        scalarSql(\"SELECT system.years(TIMESTAMP_NTZ '1970-01-01 00:00:01.000001 UTC')\"));\n-    Assert.assertEquals(\n-        \"Expected to produce 1969 - 1970 = -1\",\n-        -1,\n-        scalarSql(\"SELECT system.years(TIMESTAMP_NTZ '1969-12-31 23:59:58.999999 UTC')\"));\n-    Assert.assertNull(scalarSql(\"SELECT system.years(CAST(null AS TIMESTAMP_NTZ))\"));\n+    assertThat(scalarSql(\"SELECT system.years(TIMESTAMP_NTZ '2017-12-01 10:12:55.038194 UTC')\"))\n+        .as(\"Expected to produce 2017 - 1970 = 47\")\n+        .isEqualTo(47);\n+    assertThat(scalarSql(\"SELECT system.years(TIMESTAMP_NTZ '1970-01-01 00:00:01.000001 UTC')\"))\n+        .as(\"Expected to produce 1970 - 1970 = 0\")\n+        .isEqualTo(0);\n+    assertThat(scalarSql(\"SELECT system.years(TIMESTAMP_NTZ '1969-12-31 23:59:58.999999 UTC')\"))\n+        .as(\"Expected to produce 1969 - 1970 = -1\")\n+        .isEqualTo(-1);\n+    assertThat(scalarSql(\"SELECT system.years(CAST(null AS TIMESTAMP_NTZ))\")).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWrongNumberOfArguments() {\n     assertThatThrownBy(() -> scalarSql(\"SELECT system.years()\"))\n         .isInstanceOf(AnalysisException.class)\n@@ -100,7 +90,7 @@ public void testWrongNumberOfArguments() {\n             \"Function 'years' cannot process input: (date, date): Wrong number of inputs\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidInputTypes() {\n     assertThatThrownBy(() -> scalarSql(\"SELECT system.years(1)\"))\n         .isInstanceOf(AnalysisException.class)\n@@ -113,7 +103,7 @@ public void testInvalidInputTypes() {\n             \"Function 'years' cannot process input: (bigint): Expected value to be date or timestamp\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testThatMagicFunctionsAreInvoked() {\n     String dateValue = \"date('2017-12-01')\";\n     String dateTransformClass = YearsFunction.DateToYearsFunction.class.getName();\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestStoragePartitionedJoins.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestStoragePartitionedJoins.java\nindex 8a1ec5060f6a..4a60e66ba4d0 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestStoragePartitionedJoins.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestStoragePartitionedJoins.java\n@@ -20,11 +20,15 @@\n \n import static org.apache.iceberg.PlanningMode.DISTRIBUTED;\n import static org.apache.iceberg.PlanningMode.LOCAL;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import java.util.List;\n import java.util.Map;\n import java.util.concurrent.atomic.AtomicReference;\n import org.apache.commons.lang3.StringUtils;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PlanningMode;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -32,10 +36,11 @@\n import org.apache.iceberg.expressions.Expressions;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkCatalogConfig;\n import org.apache.iceberg.spark.SparkSQLProperties;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n import org.apache.iceberg.spark.SparkWriteOptions;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.iceberg.spark.data.RandomData;\n import org.apache.spark.api.java.JavaRDD;\n import org.apache.spark.sql.Dataset;\n@@ -44,19 +49,30 @@\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.internal.SQLConf;\n import org.apache.spark.sql.types.StructType;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.BeforeClass;\n-import org.junit.Test;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n-public class TestStoragePartitionedJoins extends SparkTestBaseWithCatalog {\n-\n-  @Parameterized.Parameters(name = \"planningMode = {0}\")\n-  public static Object[] parameters() {\n-    return new Object[] {LOCAL, DISTRIBUTED};\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestStoragePartitionedJoins extends TestBaseWithCatalog {\n+\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}, planningMode = {3}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties(),\n+        LOCAL\n+      },\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties(),\n+        DISTRIBUTED\n+      },\n+    };\n   }\n \n   private static final String OTHER_TABLE_NAME = \"other_table\";\n@@ -96,18 +112,15 @@ public static Object[] parameters() {\n           SparkSQLProperties.PRESERVE_DATA_GROUPING,\n           \"true\");\n \n-  private final PlanningMode planningMode;\n-\n-  public TestStoragePartitionedJoins(PlanningMode planningMode) {\n-    this.planningMode = planningMode;\n-  }\n+  @Parameter(index = 3)\n+  private PlanningMode planningMode;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void setupSparkConf() {\n     spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS %s\", tableName(OTHER_TABLE_NAME));\n@@ -115,107 +128,107 @@ public void removeTables() {\n \n   // TODO: add tests for truncate transforms once SPARK-40295 is released\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithBucketingOnByteColumn() throws NoSuchTableException {\n     checkJoin(\"byte_col\", \"TINYINT\", \"bucket(4, byte_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithBucketingOnShortColumn() throws NoSuchTableException {\n     checkJoin(\"short_col\", \"SMALLINT\", \"bucket(4, short_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithBucketingOnIntColumn() throws NoSuchTableException {\n     checkJoin(\"int_col\", \"INT\", \"bucket(16, int_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithBucketingOnLongColumn() throws NoSuchTableException {\n     checkJoin(\"long_col\", \"BIGINT\", \"bucket(16, long_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithBucketingOnTimestampColumn() throws NoSuchTableException {\n     checkJoin(\"timestamp_col\", \"TIMESTAMP\", \"bucket(16, timestamp_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithBucketingOnTimestampNtzColumn() throws NoSuchTableException {\n     checkJoin(\"timestamp_col\", \"TIMESTAMP_NTZ\", \"bucket(16, timestamp_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithBucketingOnDateColumn() throws NoSuchTableException {\n     checkJoin(\"date_col\", \"DATE\", \"bucket(8, date_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithBucketingOnDecimalColumn() throws NoSuchTableException {\n     checkJoin(\"decimal_col\", \"DECIMAL(20, 2)\", \"bucket(8, decimal_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithBucketingOnBinaryColumn() throws NoSuchTableException {\n     checkJoin(\"binary_col\", \"BINARY\", \"bucket(8, binary_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithYearsOnTimestampColumn() throws NoSuchTableException {\n     checkJoin(\"timestamp_col\", \"TIMESTAMP\", \"years(timestamp_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithYearsOnTimestampNtzColumn() throws NoSuchTableException {\n     checkJoin(\"timestamp_col\", \"TIMESTAMP_NTZ\", \"years(timestamp_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithYearsOnDateColumn() throws NoSuchTableException {\n     checkJoin(\"date_col\", \"DATE\", \"years(date_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithMonthsOnTimestampColumn() throws NoSuchTableException {\n     checkJoin(\"timestamp_col\", \"TIMESTAMP\", \"months(timestamp_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithMonthsOnTimestampNtzColumn() throws NoSuchTableException {\n     checkJoin(\"timestamp_col\", \"TIMESTAMP_NTZ\", \"months(timestamp_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithMonthsOnDateColumn() throws NoSuchTableException {\n     checkJoin(\"date_col\", \"DATE\", \"months(date_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithDaysOnTimestampColumn() throws NoSuchTableException {\n     checkJoin(\"timestamp_col\", \"TIMESTAMP\", \"days(timestamp_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithDaysOnTimestampNtzColumn() throws NoSuchTableException {\n     checkJoin(\"timestamp_col\", \"TIMESTAMP_NTZ\", \"days(timestamp_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithDaysOnDateColumn() throws NoSuchTableException {\n     checkJoin(\"date_col\", \"DATE\", \"days(date_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithHoursOnTimestampColumn() throws NoSuchTableException {\n     checkJoin(\"timestamp_col\", \"TIMESTAMP\", \"hours(timestamp_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithHoursOnTimestampNtzColumn() throws NoSuchTableException {\n     checkJoin(\"timestamp_col\", \"TIMESTAMP_NTZ\", \"hours(timestamp_col)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithMultipleTransformTypes() throws NoSuchTableException {\n     String createTableStmt =\n         \"CREATE TABLE %s (\"\n@@ -304,7 +317,7 @@ public void testJoinsWithMultipleTransformTypes() throws NoSuchTableException {\n         tableName(OTHER_TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithCompatibleSpecEvolution() {\n     // create a table with an empty spec\n     sql(\n@@ -357,7 +370,7 @@ public void testJoinsWithCompatibleSpecEvolution() {\n         tableName(OTHER_TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithIncompatibleSpecs() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, int_col INT, dep STRING)\"\n@@ -397,7 +410,7 @@ public void testJoinsWithIncompatibleSpecs() {\n         tableName(OTHER_TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithUnpartitionedTables() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, int_col INT, dep STRING)\"\n@@ -437,7 +450,7 @@ public void testJoinsWithUnpartitionedTables() {\n         tableName(OTHER_TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithEmptyTable() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, int_col INT, dep STRING)\"\n@@ -469,7 +482,7 @@ public void testJoinsWithEmptyTable() {\n         tableName(OTHER_TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithOneSplitTables() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, int_col INT, dep STRING)\"\n@@ -503,7 +516,7 @@ public void testJoinsWithOneSplitTables() {\n         tableName(OTHER_TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testJoinsWithMismatchingPartitionKeys() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, int_col INT, dep STRING)\"\n@@ -537,7 +550,7 @@ public void testJoinsWithMismatchingPartitionKeys() {\n         tableName(OTHER_TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregates() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, int_col INT, dep STRING)\"\n@@ -628,10 +641,9 @@ private void assertPartitioningAwarePlan(\n         () -> {\n           String plan = executeAndKeepPlan(query, args).toString();\n           int actualNumShuffles = StringUtils.countMatches(plan, \"Exchange\");\n-          Assert.assertEquals(\n-              \"Number of shuffles with enabled SPJ must match\",\n-              expectedNumShufflesWithSPJ,\n-              actualNumShuffles);\n+          assertThat(actualNumShuffles)\n+              .as(\"Number of shuffles with enabled SPJ must match\")\n+              .isEqualTo(expectedNumShufflesWithSPJ);\n \n           rowsWithSPJ.set(sql(query, args));\n         });\n@@ -641,10 +653,9 @@ private void assertPartitioningAwarePlan(\n         () -> {\n           String plan = executeAndKeepPlan(query, args).toString();\n           int actualNumShuffles = StringUtils.countMatches(plan, \"Exchange\");\n-          Assert.assertEquals(\n-              \"Number of shuffles with disabled SPJ must match\",\n-              expectedNumShufflesWithoutSPJ,\n-              actualNumShuffles);\n+          assertThat(actualNumShuffles)\n+              .as(\"Number of shuffles with disabled SPJ must match\")\n+              .isEqualTo(expectedNumShufflesWithoutSPJ);\n \n           rowsWithoutSPJ.set(sql(query, args));\n         });\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestTimestampWithoutZone.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestTimestampWithoutZone.java\nindex e34d253dd39e..a890b59d30b9 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestTimestampWithoutZone.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestTimestampWithoutZone.java\n@@ -18,21 +18,23 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.sql.Timestamp;\n import java.time.LocalDateTime;\n import java.util.Arrays;\n import java.util.List;\n-import java.util.Map;\n import java.util.stream.Collectors;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.relocated.com.google.common.base.Joiner;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.SparkReadOptions;\n import org.apache.iceberg.spark.SparkSQLProperties;\n import org.apache.iceberg.spark.SparkSessionCatalog;\n@@ -41,17 +43,15 @@\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n import org.joda.time.DateTime;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestTimestampWithoutZone extends SparkCatalogTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestTimestampWithoutZone extends CatalogTestBase {\n \n   private static final String NEW_TABLE_NAME = \"created_table\";\n-  private final Map<String, String> config;\n-\n   private static final Schema SCHEMA =\n       new Schema(\n           Types.NestedField.required(1, \"id\", Types.LongType.get()),\n@@ -64,7 +64,7 @@ public class TestTimestampWithoutZone extends SparkCatalogTestBase {\n           row(2L, toLocalDateTime(\"2021-01-01T00:00:00.0\"), toTimestamp(\"2021-02-01T00:00:00.0\")),\n           row(3L, toLocalDateTime(\"2021-01-01T00:00:00.0\"), toTimestamp(\"2021-02-01T00:00:00.0\")));\n \n-  @Parameterized.Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n       {\n@@ -79,24 +79,18 @@ public static Object[][] parameters() {\n     };\n   }\n \n-  public TestTimestampWithoutZone(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-    this.config = config;\n-  }\n-\n-  @Before\n+  @BeforeEach\n   public void createTables() {\n     validationCatalog.createTable(tableIdent, SCHEMA);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     validationCatalog.dropTable(tableIdent, true);\n     sql(\"DROP TABLE IF EXISTS %s\", NEW_TABLE_NAME);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeprecatedTimezoneProperty() {\n     withSQLConf(\n         ImmutableMap.of(SparkSQLProperties.USE_TIMESTAMP_WITHOUT_TIME_ZONE_IN_NEW_TABLES, \"true\"),\n@@ -107,7 +101,7 @@ public void testDeprecatedTimezoneProperty() {\n                         .sessionState()\n                         .catalogManager()\n                         .currentCatalog()\n-                        .initialize(catalog.name(), new CaseInsensitiveStringMap(config));\n+                        .initialize(catalog.name(), new CaseInsensitiveStringMap(catalogConfig));\n                   })\n               .isInstanceOf(UnsupportedOperationException.class)\n               .hasMessage(\n@@ -115,7 +109,7 @@ public void testDeprecatedTimezoneProperty() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadWithDeprecatedTimezoneProperty() {\n     withSQLConf(\n         ImmutableMap.of(SparkSQLProperties.HANDLE_TIMESTAMP_WITHOUT_TIMEZONE, \"true\"),\n@@ -130,7 +124,7 @@ public void testReadWithDeprecatedTimezoneProperty() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadWithDeprecatedTimezonePropertyReadOption() {\n     assertThatThrownBy(\n             () -> {\n@@ -145,7 +139,7 @@ public void testReadWithDeprecatedTimezonePropertyReadOption() {\n             \"Option handle-timestamp-without-timezone is not supported in Spark 3.4 due to the introduction of native support for timestamp without timezone.\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWriteWithDeprecatedTimezoneProperty() {\n     withSQLConf(\n         ImmutableMap.of(\n@@ -166,7 +160,7 @@ public void testWriteWithDeprecatedTimezoneProperty() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWriteWithDeprecatedTimezonePropertyReadOption() {\n     assertThatThrownBy(\n             () -> {\n@@ -213,7 +207,7 @@ public void testWriteWithDeprecatedTimezonePropertyReadOption() {\n   8\n    */\n \n-  @Test\n+  @TestTemplate\n   public void testAppendTimestampWithoutZone() {\n     // Both NTZ\n     sql(\n@@ -227,7 +221,7 @@ public void testAppendTimestampWithoutZone() {\n                     toLocalDateTime(\"2021-02-01T00:00:00.0\")))));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAppendTimestampWithZone() {\n     // Both TZ\n     sql(\n@@ -241,16 +235,15 @@ public void testAppendTimestampWithZone() {\n                     toTimestamp(\"2021-02-01T00:00:00.0\")))));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateAsSelectWithTimestampWithoutZone() {\n     sql(\"INSERT INTO %s VALUES %s\", tableName, rowToSqlValues(values));\n \n     sql(\"CREATE TABLE %s USING iceberg AS SELECT * FROM %s\", NEW_TABLE_NAME, tableName);\n \n-    Assert.assertEquals(\n-        \"Should have \" + values.size() + \" row\",\n-        (long) values.size(),\n-        scalarSql(\"SELECT count(*) FROM %s\", NEW_TABLE_NAME));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", NEW_TABLE_NAME))\n+        .as(\"Should have \" + values.size() + \" row\")\n+        .isEqualTo((long) values.size());\n \n     assertEquals(\n         \"Row data should match expected\",\n@@ -258,16 +251,15 @@ public void testCreateAsSelectWithTimestampWithoutZone() {\n         sql(\"SELECT * FROM %s ORDER BY id\", NEW_TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateNewTableShouldHaveTimestampWithZoneIcebergType() {\n     sql(\"INSERT INTO %s VALUES %s\", tableName, rowToSqlValues(values));\n \n     sql(\"CREATE TABLE %s USING iceberg AS SELECT * FROM %s\", NEW_TABLE_NAME, tableName);\n \n-    Assert.assertEquals(\n-        \"Should have \" + values.size() + \" row\",\n-        (long) values.size(),\n-        scalarSql(\"SELECT count(*) FROM %s\", NEW_TABLE_NAME));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", NEW_TABLE_NAME))\n+        .as(\"Should have \" + values.size() + \" row\")\n+        .isEqualTo((long) values.size());\n \n     assertEquals(\n         \"Data from created table should match data from base table\",\n@@ -279,21 +271,20 @@ public void testCreateNewTableShouldHaveTimestampWithZoneIcebergType() {\n     assertFieldsType(createdTable.schema(), Types.TimestampType.withZone(), \"tsz\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateNewTableShouldHaveTimestampWithoutZoneIcebergType() {\n     spark\n         .sessionState()\n         .catalogManager()\n         .currentCatalog()\n-        .initialize(catalog.name(), new CaseInsensitiveStringMap(config));\n+        .initialize(catalog.name(), new CaseInsensitiveStringMap(catalogConfig));\n     sql(\"INSERT INTO %s VALUES %s\", tableName, rowToSqlValues(values));\n \n     sql(\"CREATE TABLE %s USING iceberg AS SELECT * FROM %s\", NEW_TABLE_NAME, tableName);\n \n-    Assert.assertEquals(\n-        \"Should have \" + values.size() + \" row\",\n-        (long) values.size(),\n-        scalarSql(\"SELECT count(*) FROM %s\", NEW_TABLE_NAME));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s\", NEW_TABLE_NAME))\n+        .as(\"Should have \" + values.size() + \" row\")\n+        .isEqualTo((long) values.size());\n \n     assertEquals(\n         \"Row data should match expected\",\n@@ -342,6 +333,6 @@ private void assertFieldsType(Schema actual, Type.PrimitiveType expected, String\n         .select(fields)\n         .asStruct()\n         .fields()\n-        .forEach(field -> Assert.assertEquals(expected, field.type()));\n+        .forEach(field -> assertThat(field.type()).isEqualTo(expected));\n   }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkBucketFunction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkBucketFunction.java\nindex 5b179f7f0f04..0dcb986375bf 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkBucketFunction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkBucketFunction.java\n@@ -24,6 +24,7 @@\n import java.math.BigDecimal;\n import java.nio.ByteBuffer;\n import java.nio.charset.StandardCharsets;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.expressions.Literal;\n import org.apache.iceberg.relocated.com.google.common.io.BaseEncoding;\n import org.apache.iceberg.spark.TestBaseWithCatalog;\n@@ -33,7 +34,9 @@\n import org.apache.spark.sql.types.DataTypes;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkBucketFunction extends TestBaseWithCatalog {\n   @BeforeEach\n   public void useCatalog() {\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkDaysFunction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkDaysFunction.java\nindex 36cf196351b8..cfec6a33ab14 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkDaysFunction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkDaysFunction.java\n@@ -22,11 +22,14 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.sql.Date;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.spark.sql.AnalysisException;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkDaysFunction extends TestBaseWithCatalog {\n \n   @BeforeEach\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkHoursFunction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkHoursFunction.java\nindex 17380747b4c0..eeeb9d1a1e78 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkHoursFunction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkHoursFunction.java\n@@ -21,11 +21,14 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.spark.sql.AnalysisException;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkHoursFunction extends TestBaseWithCatalog {\n \n   @BeforeEach\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkMonthsFunction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkMonthsFunction.java\nindex 1a00950124f0..68952d189359 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkMonthsFunction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkMonthsFunction.java\n@@ -21,12 +21,15 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.iceberg.spark.functions.MonthsFunction;\n import org.apache.spark.sql.AnalysisException;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkMonthsFunction extends TestBaseWithCatalog {\n \n   @BeforeEach\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkTruncateFunction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkTruncateFunction.java\nindex 25f3770d01e4..af4adbe5b32d 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkTruncateFunction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkTruncateFunction.java\n@@ -23,12 +23,15 @@\n \n import java.math.BigDecimal;\n import java.nio.charset.StandardCharsets;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.relocated.com.google.common.io.BaseEncoding;\n import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.spark.sql.AnalysisException;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkTruncateFunction extends TestBaseWithCatalog {\n \n   @BeforeEach\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkYearsFunction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkYearsFunction.java\nindex 8cf62b2b48f3..6f806f65d963 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkYearsFunction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSparkYearsFunction.java\n@@ -21,12 +21,15 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.iceberg.spark.functions.YearsFunction;\n import org.apache.spark.sql.AnalysisException;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSparkYearsFunction extends TestBaseWithCatalog {\n \n   @BeforeEach\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestTimestampWithoutZone.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestTimestampWithoutZone.java\nindex 44d895dd44c5..f721ff3decf0 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestTimestampWithoutZone.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestTimestampWithoutZone.java\n@@ -25,6 +25,7 @@\n import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -41,7 +42,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestTimestampWithoutZone extends CatalogTestBase {\n \n   private static final String NEW_TABLE_NAME = \"created_table\";\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12938",
    "pr_id": 12938,
    "issue_id": 13049,
    "repo": "apache/iceberg",
    "problem_statement": "Exclude JUnit4 dependency from classpath\n### Feature Request / Improvement\n\nWe should apply the diff\n```\ndiff --git a/build.gradle b/build.gradle\nindex cd79e45a7f..ed7133cc7e 100644\n--- a/build.gradle\n+++ b/build.gradle\n@@ -182,6 +182,7 @@ subprojects {\n       exclude group: 'com.sun.jersey'\n       exclude group: 'com.sun.jersey.contribs'\n       exclude group: 'org.pentaho', module: 'pentaho-aggdesigner-algorithm'\n+      exclude group: 'junit'\n     }\n```\n\nto completely exclude JUnit4 from the classpath.\n\n\nThere are unfortunately still a few other projects that depend on JUnit4\n* Testcontainers depends on JUnit4 when using `GenericContainer`. As indicated by https://github.com/testcontainers/testcontainers-java/issues/970 this should be fixed in Testcontainers 2.0\n* Flink currently also depends on JUnit4 as indicated in https://github.com/apache/iceberg/issues/12937 (Flink's `InternalMiniClusterExtension` still depends on having JUnit4 on the classpath)\n\n### Query engine\n\nNone\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [ ] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 167,
    "test_files_count": 5,
    "non_test_files_count": 9,
    "pr_changed_files": [
      ".baseline/checkstyle/checkstyle.xml",
      "build.gradle",
      "data/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java",
      "flink/v1.19/build.gradle",
      "flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java",
      "flink/v1.20/build.gradle",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java",
      "flink/v2.0/build.gradle",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java",
      "gradle/libs.versions.toml",
      "spark/v3.4/build.gradle",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/rest/RESTServerRule.java",
      "spark/v3.5/build.gradle",
      "spark/v4.0/build.gradle"
    ],
    "pr_changed_test_files": [
      "data/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java",
      "flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java",
      "flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/rest/RESTServerRule.java"
    ],
    "base_commit": "6c98a9e470d44f77e1939a1a333bf197a57949a6",
    "head_commit": "ff74122e98089c577588bbca02269299121d3a0d",
    "repo_url": "https://github.com/apache/iceberg/pull/12938",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12938",
    "dockerfile": "",
    "pr_merged_at": "2025-05-15T06:41:34.000Z",
    "patch": "diff --git a/.baseline/checkstyle/checkstyle.xml b/.baseline/checkstyle/checkstyle.xml\nindex 4f5cbcef5feb..8410a9034538 100644\n--- a/.baseline/checkstyle/checkstyle.xml\n+++ b/.baseline/checkstyle/checkstyle.xml\n@@ -167,7 +167,6 @@\n                 org.apache.spark.sql.functions.*,\n                 org.apache.spark.sql.connector.iceberg.write.RowLevelOperation.Command.*,\n                 org.apache.spark.sql.connector.write.RowLevelOperation.Command.*,\n-                org.junit.Assert.*,\n                 org.assertj.core.api.Assertions.*,\n                 org.assertj.core.api.Assumptions.*\"/>\n         </module>\n@@ -201,10 +200,6 @@\n             <property name=\"setterCanReturnItsClass\" value=\"true\"/>\n         </module>\n         <module name=\"HideUtilityClassConstructor\"/> <!-- Java Coding Guidelines: Private constructors -->\n-        <module name=\"IllegalImport\"> <!-- Java Coding Guidelines: Use JUnit 4-style test classes and assertions -->\n-            <property name=\"illegalPkgs\" value=\"junit.framework\"/>\n-            <message key=\"import.illegal\" value=\"Use JUnit 4-style (org.junit.*) test classes and assertions instead of JUnit 3 (junit.framework.*).\"/>\n-        </module>\n         <module name=\"IllegalImport\"> <!-- Only relevant for pre-Java 11 because javafx is gone completely in Java 11 -->\n             <property name=\"id\" value=\"BanJavafx\"/>\n             <property name=\"illegalPkgs\" value=\"javafx\"/>\n@@ -459,6 +454,11 @@\n             <property name=\"illegalPkgs\" value=\"org.junit.jupiter.api.Assertions\"/>\n             <message key=\"import.illegal\" value=\"Prefer using org.assertj.core.api.Assertions instead.\"/>\n         </module>\n+        <module name=\"IllegalImport\">\n+            <property name=\"id\" value=\"BanJUnit4Usage\"/>\n+            <property name=\"illegalClasses\" value=\"org.junit.Assert, org.junit.Assume, org.junit.Test\"/>\n+            <message key=\"import.illegal\" value=\"Prefer using JUnit5 / AssertJ instead.\"/>\n+        </module>\n         <module name=\"RegexpSinglelineJava\">\n             <property name=\"ignoreComments\" value=\"true\"/>\n             <property name=\"format\" value=\"@Json(S|Des)erialize\"/>\n\ndiff --git a/build.gradle b/build.gradle\nindex cda76fb31f93..d35088e85ddf 100644\n--- a/build.gradle\n+++ b/build.gradle\n@@ -371,7 +371,9 @@ project(':iceberg-core') {\n     testImplementation libs.sqlite.jdbc\n     testImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')\n     testImplementation libs.esotericsoftware.kryo\n-    testImplementation libs.guava.testlib\n+    testImplementation(libs.guava.testlib) {\n+      exclude group: 'junit'\n+    }\n     testImplementation libs.awaitility\n   }\n }\n@@ -413,7 +415,6 @@ project(':iceberg-data') {\n \n     testImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')\n     testImplementation project(path: ':iceberg-core', configuration: 'testArtifacts')\n-    testImplementation libs.junit.vintage.engine\n   }\n \n   test {\n\ndiff --git a/flink/v1.19/build.gradle b/flink/v1.19/build.gradle\nindex 599ba085e4c4..f0fd048a6c70 100644\n--- a/flink/v1.19/build.gradle\n+++ b/flink/v1.19/build.gradle\n@@ -174,7 +174,6 @@ project(\":iceberg-flink:iceberg-flink-runtime-${flinkMajorVersion}\") {\n     // for integration testing with the flink-runtime-jar\n     // all of those dependencies are required because the integration test extends FlinkTestBase\n     integrationCompileOnly project(':iceberg-api')\n-    integrationImplementation libs.junit.vintage.engine\n     integrationImplementation libs.assertj.core\n     integrationImplementation project(path: \":iceberg-flink:iceberg-flink-${flinkMajorVersion}\", configuration: \"testArtifacts\")\n     integrationImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')\n\ndiff --git a/flink/v1.20/build.gradle b/flink/v1.20/build.gradle\nindex 3e308d22b021..7c4989312ef3 100644\n--- a/flink/v1.20/build.gradle\n+++ b/flink/v1.20/build.gradle\n@@ -174,7 +174,6 @@ project(\":iceberg-flink:iceberg-flink-runtime-${flinkMajorVersion}\") {\n     // for integration testing with the flink-runtime-jar\n     // all of those dependencies are required because the integration test extends FlinkTestBase\n     integrationCompileOnly project(':iceberg-api')\n-    integrationImplementation libs.junit.vintage.engine\n     integrationImplementation libs.assertj.core\n     integrationImplementation project(path: \":iceberg-flink:iceberg-flink-${flinkMajorVersion}\", configuration: \"testArtifacts\")\n     integrationImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')\n\ndiff --git a/flink/v2.0/build.gradle b/flink/v2.0/build.gradle\nindex fbe206f1b3ca..dfbaa8ff4184 100644\n--- a/flink/v2.0/build.gradle\n+++ b/flink/v2.0/build.gradle\n@@ -177,7 +177,6 @@ project(\":iceberg-flink:iceberg-flink-runtime-${flinkMajorVersion}\") {\n     // for integration testing with the flink-runtime-jar\n     // all of those dependencies are required because the integration test extends FlinkTestBase\n     integrationCompileOnly project(':iceberg-api')\n-    integrationImplementation libs.junit.vintage.engine\n     integrationImplementation libs.assertj.core\n     integrationImplementation project(path: \":iceberg-flink:iceberg-flink-${flinkMajorVersion}\", configuration: \"testArtifacts\")\n     integrationImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')\n\ndiff --git a/gradle/libs.versions.toml b/gradle/libs.versions.toml\nindex 784e5023b5f8..e1f76abe2ec4 100644\n--- a/gradle/libs.versions.toml\n+++ b/gradle/libs.versions.toml\n@@ -202,7 +202,6 @@ junit-jupiter-engine = { module = \"org.junit.jupiter:junit-jupiter-engine\", vers\n junit-platform-launcher = { module = \"org.junit.platform:junit-platform-launcher\", version.ref = \"junit-platform\" }\n junit-suite-api = { module = \"org.junit.platform:junit-platform-suite-api\", version.ref = \"junit-platform\" }\n junit-suite-engine = { module = \"org.junit.platform:junit-platform-suite-engine\", version.ref = \"junit-platform\" }\n-junit-vintage-engine = { module = \"org.junit.vintage:junit-vintage-engine\", version.ref = \"junit\" }\n kryo-shaded = { module = \"com.esotericsoftware:kryo-shaded\", version.ref = \"kryo-shaded\" }\n mockito-core = { module = \"org.mockito:mockito-core\", version.ref = \"mockito\" }\n mockito-inline = { module = \"org.mockito:mockito-inline\", version.ref = \"mockito\" }\n\ndiff --git a/spark/v3.4/build.gradle b/spark/v3.4/build.gradle\nindex 587e5eb70ad8..32abdf2962b2 100644\n--- a/spark/v3.4/build.gradle\n+++ b/spark/v3.4/build.gradle\n@@ -112,7 +112,6 @@ project(\":iceberg-spark:iceberg-spark-${sparkMajorVersion}_${scalaVersion}\") {\n     }\n     testImplementation libs.sqlite.jdbc\n     testImplementation libs.awaitility\n-    testImplementation libs.junit.vintage.engine\n     // runtime dependencies for running REST Catalog based integration test\n     testRuntimeOnly libs.jetty.servlet\n   }\n@@ -185,7 +184,6 @@ project(\":iceberg-spark:iceberg-spark-extensions-${sparkMajorVersion}_${scalaVer\n     testImplementation libs.avro.avro\n     testImplementation libs.parquet.hadoop\n     testImplementation libs.awaitility\n-    testImplementation libs.junit.vintage.engine\n     testImplementation \"org.apache.datafusion:comet-spark-spark${sparkMajorVersion}_${scalaVersion}:${libs.versions.comet.get()}\"\n \n     // Required because we remove antlr plugin dependencies from the compile configuration, see note above\n@@ -258,7 +256,6 @@ project(\":iceberg-spark:iceberg-spark-runtime-${sparkMajorVersion}_${scalaVersio\n \n     integrationImplementation \"org.scala-lang.modules:scala-collection-compat_${scalaVersion}:${libs.versions.scala.collection.compat.get()}\"\n     integrationImplementation \"org.apache.spark:spark-hive_${scalaVersion}:${libs.versions.spark34.get()}\"\n-    integrationImplementation libs.junit.vintage.engine\n     integrationImplementation libs.junit.jupiter\n     integrationImplementation libs.junit.platform.launcher\n     integrationImplementation libs.slf4j.simple\n\ndiff --git a/spark/v3.5/build.gradle b/spark/v3.5/build.gradle\nindex b3ee288795ba..af7a1d74d1b3 100644\n--- a/spark/v3.5/build.gradle\n+++ b/spark/v3.5/build.gradle\n@@ -256,7 +256,6 @@ project(\":iceberg-spark:iceberg-spark-runtime-${sparkMajorVersion}_${scalaVersio\n \n     integrationImplementation \"org.scala-lang.modules:scala-collection-compat_${scalaVersion}:${libs.versions.scala.collection.compat.get()}\"\n     integrationImplementation \"org.apache.spark:spark-hive_${scalaVersion}:${libs.versions.spark35.get()}\"\n-    integrationImplementation libs.junit.vintage.engine\n     integrationImplementation libs.junit.jupiter\n     integrationImplementation libs.junit.platform.launcher\n     integrationImplementation libs.slf4j.simple\n\ndiff --git a/spark/v4.0/build.gradle b/spark/v4.0/build.gradle\nindex a6eb8b8077f2..61276369c5a3 100644\n--- a/spark/v4.0/build.gradle\n+++ b/spark/v4.0/build.gradle\n@@ -256,7 +256,6 @@ project(\":iceberg-spark:iceberg-spark-runtime-${sparkMajorVersion}_${scalaVersio\n \n     integrationImplementation \"org.scala-lang.modules:scala-collection-compat_${scalaVersion}:${libs.versions.scala.collection.compat.get()}\"\n     integrationImplementation \"org.apache.spark:spark-hive_${scalaVersion}:${libs.versions.spark40.get()}\"\n-    integrationImplementation libs.junit.vintage.engine\n     integrationImplementation libs.junit.jupiter\n     integrationImplementation libs.junit.platform.launcher\n     integrationImplementation libs.slf4j.simple\n",
    "test_patch": "diff --git a/data/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java b/data/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java\nindex 05f8e0e8e83d..66ddfd84fa3f 100644\n--- a/data/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java\n+++ b/data/src/test/java/org/apache/iceberg/data/GenericAppenderHelper.java\n@@ -35,7 +35,6 @@\n import org.apache.iceberg.Table;\n import org.apache.iceberg.io.FileAppender;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n-import org.junit.rules.TemporaryFolder;\n \n /** Helper for appending {@link DataFile} to a table or appending {@link Record}s to a table. */\n public class GenericAppenderHelper {\n@@ -48,15 +47,6 @@ public class GenericAppenderHelper {\n   private final Path temp;\n   private final Configuration conf;\n \n-  @Deprecated\n-  public GenericAppenderHelper(\n-      Table table, FileFormat fileFormat, TemporaryFolder tmp, Configuration conf) {\n-    this.table = table;\n-    this.fileFormat = fileFormat;\n-    this.temp = tmp.getRoot().toPath();\n-    this.conf = conf;\n-  }\n-\n   public GenericAppenderHelper(Table table, FileFormat fileFormat, Path temp, Configuration conf) {\n     this.table = table;\n     this.fileFormat = fileFormat;\n@@ -64,11 +54,6 @@ public GenericAppenderHelper(Table table, FileFormat fileFormat, Path temp, Conf\n     this.conf = conf;\n   }\n \n-  @Deprecated\n-  public GenericAppenderHelper(Table table, FileFormat fileFormat, TemporaryFolder tmp) {\n-    this(table, fileFormat, tmp, null);\n-  }\n-\n   public GenericAppenderHelper(Table table, FileFormat fileFormat, Path temp) {\n     this(table, fileFormat, temp, null);\n   }\n\ndiff --git a/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java b/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\nindex 96929ec94c2b..828ee5fb87a7 100644\n--- a/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\n+++ b/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\n@@ -21,6 +21,7 @@\n import static org.apache.iceberg.flink.TestFixtures.DATABASE;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.IOException;\n import java.util.Collections;\n@@ -55,7 +56,6 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n-import org.junit.Assume;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n@@ -421,8 +421,9 @@ void testOperatorsUidNameWitUidSuffix() throws Exception {\n \n   @TestTemplate\n   void testErrorOnNullForRequiredField() throws Exception {\n-    Assume.assumeFalse(\n-        \"ORC file format supports null values even for required fields.\", format == FileFormat.ORC);\n+    assumeThat(format)\n+        .as(\"ORC file format supports null values even for required fields.\")\n+        .isNotEqualTo(FileFormat.ORC);\n \n     Schema icebergSchema =\n         new Schema(\n\ndiff --git a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\nindex 96929ec94c2b..828ee5fb87a7 100644\n--- a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\n+++ b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\n@@ -21,6 +21,7 @@\n import static org.apache.iceberg.flink.TestFixtures.DATABASE;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.IOException;\n import java.util.Collections;\n@@ -55,7 +56,6 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n-import org.junit.Assume;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n@@ -421,8 +421,9 @@ void testOperatorsUidNameWitUidSuffix() throws Exception {\n \n   @TestTemplate\n   void testErrorOnNullForRequiredField() throws Exception {\n-    Assume.assumeFalse(\n-        \"ORC file format supports null values even for required fields.\", format == FileFormat.ORC);\n+    assumeThat(format)\n+        .as(\"ORC file format supports null values even for required fields.\")\n+        .isNotEqualTo(FileFormat.ORC);\n \n     Schema icebergSchema =\n         new Schema(\n\ndiff --git a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\nindex b02b3337dc96..9990146a7763 100644\n--- a/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\n+++ b/flink/v2.0/flink/src/test/java/org/apache/iceberg/flink/sink/TestIcebergSink.java\n@@ -21,6 +21,7 @@\n import static org.apache.iceberg.flink.TestFixtures.DATABASE;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.IOException;\n import java.util.Collections;\n@@ -55,7 +56,6 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n-import org.junit.Assume;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n@@ -421,8 +421,9 @@ void testOperatorsUidNameWitUidSuffix() throws Exception {\n \n   @TestTemplate\n   void testErrorOnNullForRequiredField() throws Exception {\n-    Assume.assumeFalse(\n-        \"ORC file format supports null values even for required fields.\", format == FileFormat.ORC);\n+    assumeThat(format)\n+        .as(\"ORC file format supports null values even for required fields.\")\n+        .isNotEqualTo(FileFormat.ORC);\n \n     Schema icebergSchema =\n         new Schema(\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/rest/RESTServerRule.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/rest/RESTServerRule.java\ndeleted file mode 100644\nindex f42106e6d89c..000000000000\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/rest/RESTServerRule.java\n+++ /dev/null\n@@ -1,107 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.iceberg.rest;\n-\n-import java.util.Map;\n-import org.apache.iceberg.CatalogProperties;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n-import org.junit.rules.ExternalResource;\n-\n-/**\n- * This class is to make the {@link RESTCatalogServer} usable for JUnit4 in a similar way to {@link\n- * RESTServerExtension}.\n- */\n-public class RESTServerRule extends ExternalResource {\n-  public static final String FREE_PORT = \"0\";\n-\n-  private volatile RESTCatalogServer localServer;\n-  private RESTCatalog client;\n-  private final Map<String, String> config;\n-\n-  public RESTServerRule() {\n-    config = Maps.newHashMap();\n-  }\n-\n-  public RESTServerRule(Map<String, String> config) {\n-    Map<String, String> conf = Maps.newHashMap(config);\n-    if (conf.containsKey(RESTCatalogServer.REST_PORT)\n-        && conf.get(RESTCatalogServer.REST_PORT).equals(FREE_PORT)) {\n-      conf.put(RESTCatalogServer.REST_PORT, String.valueOf(RCKUtils.findFreePort()));\n-    }\n-    this.config = conf;\n-  }\n-\n-  public Map<String, String> config() {\n-    return config;\n-  }\n-\n-  public RESTCatalog client() {\n-    if (null == client) {\n-      try {\n-        maybeInitClientAndServer();\n-      } catch (Exception e) {\n-        throw new RuntimeException(e);\n-      }\n-    }\n-\n-    return client;\n-  }\n-\n-  public String uri() {\n-    return client().properties().get(CatalogProperties.URI);\n-  }\n-\n-  private void maybeInitClientAndServer() throws Exception {\n-    if (null == localServer) {\n-      synchronized (this) {\n-        if (null == localServer) {\n-          this.localServer = new RESTCatalogServer(config);\n-          this.localServer.start(false);\n-          this.client = RCKUtils.initCatalogClient(config);\n-        }\n-      }\n-    }\n-  }\n-\n-  @Override\n-  protected void before() throws Throwable {\n-    maybeShutdownClientAndServer();\n-    maybeInitClientAndServer();\n-  }\n-\n-  @Override\n-  protected void after() {\n-    maybeShutdownClientAndServer();\n-  }\n-\n-  private void maybeShutdownClientAndServer() {\n-    try {\n-      if (localServer != null) {\n-        localServer.stop();\n-        localServer = null;\n-      }\n-      if (client != null) {\n-        client.close();\n-        client = null;\n-      }\n-    } catch (Exception e) {\n-      throw new RuntimeException(e);\n-    }\n-  }\n-}\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12934",
    "pr_id": 12934,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 17,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestAggregatePushDown.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestDeleteFrom.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestDropTable.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestFilterPushDown.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestNamespaceSQL.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestRefreshTable.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSelect.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestAggregatePushDown.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestDeleteFrom.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestDropTable.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestNamespaceSQL.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestRefreshTable.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSelect.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestAggregatePushDown.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestDeleteFrom.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestDropTable.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestFilterPushDown.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestNamespaceSQL.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestRefreshTable.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSelect.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestAggregatePushDown.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestDeleteFrom.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestDropTable.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestNamespaceSQL.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestRefreshTable.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSelect.java"
    ],
    "base_commit": "242717c8c516dcc49bc4368b77cd3f3af40720c8",
    "head_commit": "f80a09a2d4f290052ad32f19c9e00f9c8e9284e2",
    "repo_url": "https://github.com/apache/iceberg/pull/12934",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12934",
    "dockerfile": "",
    "pr_merged_at": "2025-04-30T09:34:46.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestAggregatePushDown.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestAggregatePushDown.java\nindex 1a4e2f3e1c79..5ce56b4feca7 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestAggregatePushDown.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestAggregatePushDown.java\n@@ -26,8 +26,8 @@\n import java.util.Arrays;\n import java.util.List;\n import java.util.Locale;\n-import java.util.Map;\n import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.exceptions.AlreadyExistsException;\n@@ -35,40 +35,38 @@\n import org.apache.iceberg.hive.TestHiveMetastore;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.SparkReadOptions;\n-import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.execution.ExplainMode;\n import org.apache.spark.sql.functions;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.BeforeClass;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestAggregatePushDown extends SparkCatalogTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestAggregatePushDown extends CatalogTestBase {\n \n-  public TestAggregatePushDown(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @BeforeClass\n+  @BeforeAll\n   public static void startMetastoreAndSpark() {\n-    SparkTestBase.metastore = new TestHiveMetastore();\n+    TestBase.metastore = new TestHiveMetastore();\n     metastore.start();\n-    SparkTestBase.hiveConf = metastore.hiveConf();\n+    TestBase.hiveConf = metastore.hiveConf();\n+\n+    TestBase.spark.close();\n \n-    SparkTestBase.spark =\n+    TestBase.spark =\n         SparkSession.builder()\n             .master(\"local[2]\")\n             .config(\"spark.sql.iceberg.aggregate_pushdown\", \"true\")\n             .enableHiveSupport()\n             .getOrCreate();\n \n-    SparkTestBase.catalog =\n+    TestBase.catalog =\n         (HiveCatalog)\n             CatalogUtil.loadCatalog(\n                 HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n@@ -80,17 +78,17 @@ public static void startMetastoreAndSpark() {\n     }\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDifferentDataTypesAggregatePushDownInPartitionedTable() {\n     testDifferentDataTypesAggregatePushDown(true);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDifferentDataTypesAggregatePushDownInNonPartitionedTable() {\n     testDifferentDataTypesAggregatePushDown(false);\n   }\n@@ -156,8 +154,9 @@ private void testDifferentDataTypesAggregatePushDown(boolean hasPartitionCol) {\n       explainContainsPushDownAggregates = true;\n     }\n \n-    Assert.assertTrue(\n-        \"explain should contain the pushed down aggregates\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"explain should contain the pushed down aggregates\")\n+        .isTrue();\n \n     List<Object[]> actual = sql(select, tableName);\n     List<Object[]> expected = Lists.newArrayList();\n@@ -189,7 +188,7 @@ private void testDifferentDataTypesAggregatePushDown(boolean hasPartitionCol) {\n     assertEquals(\"min/max/count push down\", expected, actual);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDateAndTimestampWithPartition() {\n     sql(\n         \"CREATE TABLE %s (id bigint, data string, d date, ts timestamp) USING iceberg PARTITIONED BY (id)\",\n@@ -216,8 +215,9 @@ public void testDateAndTimestampWithPartition() {\n       explainContainsPushDownAggregates = true;\n     }\n \n-    Assert.assertTrue(\n-        \"explain should contain the pushed down aggregates\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"explain should contain the pushed down aggregates\")\n+        .isTrue();\n \n     List<Object[]> actual = sql(select, tableName);\n     List<Object[]> expected = Lists.newArrayList();\n@@ -233,7 +233,7 @@ public void testDateAndTimestampWithPartition() {\n     assertEquals(\"min/max/count push down\", expected, actual);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregateNotPushDownIfOneCantPushDown() {\n     sql(\"CREATE TABLE %s (id LONG, data DOUBLE) USING iceberg\", tableName);\n     sql(\n@@ -248,8 +248,9 @@ public void testAggregateNotPushDownIfOneCantPushDown() {\n       explainContainsPushDownAggregates = true;\n     }\n \n-    Assert.assertFalse(\n-        \"explain should not contain the pushed down aggregates\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"explain should not contain the pushed down aggregates\")\n+        .isFalse();\n \n     List<Object[]> actual = sql(select, tableName);\n     List<Object[]> expected = Lists.newArrayList();\n@@ -257,7 +258,7 @@ public void testAggregateNotPushDownIfOneCantPushDown() {\n     assertEquals(\"expected and actual should equal\", expected, actual);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregatePushDownWithMetricsMode() {\n     sql(\"CREATE TABLE %s (id LONG, data DOUBLE) USING iceberg\", tableName);\n     sql(\n@@ -283,8 +284,9 @@ public void testAggregatePushDownWithMetricsMode() {\n     }\n \n     // count(data) is not pushed down because the metrics mode is `none`\n-    Assert.assertFalse(\n-        \"explain should not contain the pushed down aggregates\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"explain should not contain the pushed down aggregates\")\n+        .isFalse();\n \n     List<Object[]> actual1 = sql(select1, tableName);\n     List<Object[]> expected1 = Lists.newArrayList();\n@@ -299,8 +301,9 @@ public void testAggregatePushDownWithMetricsMode() {\n     }\n \n     // count(id) is pushed down because the metrics mode is `counts`\n-    Assert.assertTrue(\n-        \"explain should contain the pushed down aggregates\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"explain should contain the pushed down aggregates\")\n+        .isTrue();\n \n     List<Object[]> actual2 = sql(select2, tableName);\n     List<Object[]> expected2 = Lists.newArrayList();\n@@ -317,8 +320,9 @@ public void testAggregatePushDownWithMetricsMode() {\n \n     // COUNT(id), MAX(id) are not pushed down because MAX(id) is not pushed down (metrics mode is\n     // `counts`)\n-    Assert.assertFalse(\n-        \"explain should not contain the pushed down aggregates\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"explain should not contain the pushed down aggregates\")\n+        .isFalse();\n \n     List<Object[]> actual3 = sql(select3, tableName);\n     List<Object[]> expected3 = Lists.newArrayList();\n@@ -326,7 +330,7 @@ public void testAggregatePushDownWithMetricsMode() {\n     assertEquals(\"expected and actual should equal\", expected3, actual3);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregateNotPushDownForStringType() {\n     sql(\"CREATE TABLE %s (id LONG, data STRING) USING iceberg\", tableName);\n     sql(\n@@ -345,8 +349,9 @@ public void testAggregateNotPushDownForStringType() {\n       explainContainsPushDownAggregates = true;\n     }\n \n-    Assert.assertFalse(\n-        \"explain should not contain the pushed down aggregates\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"explain should not contain the pushed down aggregates\")\n+        .isFalse();\n \n     List<Object[]> actual1 = sql(select1, tableName);\n     List<Object[]> expected1 = Lists.newArrayList();\n@@ -360,8 +365,9 @@ public void testAggregateNotPushDownForStringType() {\n       explainContainsPushDownAggregates = true;\n     }\n \n-    Assert.assertTrue(\n-        \"explain should contain the pushed down aggregates\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"explain should contain the pushed down aggregates\")\n+        .isTrue();\n \n     List<Object[]> actual2 = sql(select2, tableName);\n     List<Object[]> expected2 = Lists.newArrayList();\n@@ -379,8 +385,9 @@ public void testAggregateNotPushDownForStringType() {\n       explainContainsPushDownAggregates = true;\n     }\n \n-    Assert.assertTrue(\n-        \"explain should contain the pushed down aggregates\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"explain should contain the pushed down aggregates\")\n+        .isTrue();\n \n     List<Object[]> actual3 = sql(select3, tableName);\n     List<Object[]> expected3 = Lists.newArrayList();\n@@ -388,12 +395,12 @@ public void testAggregateNotPushDownForStringType() {\n     assertEquals(\"expected and actual should equal\", expected3, actual3);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregatePushDownWithDataFilter() {\n     testAggregatePushDownWithFilter(false);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregatePushDownWithPartitionFilter() {\n     testAggregatePushDownWithFilter(true);\n   }\n@@ -429,13 +436,14 @@ private void testAggregatePushDownWithFilter(boolean partitionFilerOnly) {\n \n     if (!partitionFilerOnly) {\n       // Filters are not completely pushed down, we can't push down aggregates\n-      Assert.assertFalse(\n-          \"explain should not contain the pushed down aggregates\",\n-          explainContainsPushDownAggregates);\n+      assertThat(explainContainsPushDownAggregates)\n+          .as(\"explain should not contain the pushed down aggregates\")\n+          .isFalse();\n     } else {\n       // Filters are not completely pushed down, we can push down aggregates\n-      Assert.assertTrue(\n-          \"explain should contain the pushed down aggregates\", explainContainsPushDownAggregates);\n+      assertThat(explainContainsPushDownAggregates)\n+          .as(\"explain should contain the pushed down aggregates\")\n+          .isTrue();\n     }\n \n     List<Object[]> actual = sql(select, tableName);\n@@ -444,7 +452,7 @@ private void testAggregatePushDownWithFilter(boolean partitionFilerOnly) {\n     assertEquals(\"expected and actual should equal\", expected, actual);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregateWithComplexType() {\n     sql(\"CREATE TABLE %s (id INT, complex STRUCT<c1:INT,c2:STRING>) USING iceberg\", tableName);\n     sql(\n@@ -459,8 +467,9 @@ public void testAggregateWithComplexType() {\n       explainContainsPushDownAggregates = true;\n     }\n \n-    Assert.assertFalse(\n-        \"count not pushed down for complex types\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"count not pushed down for complex types\")\n+        .isFalse();\n \n     List<Object[]> actual = sql(select1, tableName);\n     List<Object[]> expected = Lists.newArrayList();\n@@ -475,10 +484,12 @@ public void testAggregateWithComplexType() {\n       explainContainsPushDownAggregates = true;\n     }\n \n-    Assert.assertFalse(\"max not pushed down for complex types\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"max not pushed down for complex types\")\n+        .isFalse();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregationPushdownStructInteger() {\n     sql(\"CREATE TABLE %s (id BIGINT, struct_with_int STRUCT<c1:BIGINT>) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, named_struct(\\\"c1\\\", NULL))\", tableName);\n@@ -495,7 +506,7 @@ public void testAggregationPushdownStructInteger() {\n         \"min(struct_with_int.c1)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregationPushdownNestedStruct() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, struct_with_int STRUCT<c1:STRUCT<c2:STRUCT<c3:STRUCT<c4:BIGINT>>>>) USING iceberg\",\n@@ -522,7 +533,7 @@ public void testAggregationPushdownNestedStruct() {\n         \"min(struct_with_int.c1.c2.c3.c4)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregationPushdownStructTimestamp() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, struct_with_ts STRUCT<c1:TIMESTAMP>) USING iceberg\",\n@@ -551,7 +562,7 @@ public void testAggregationPushdownStructTimestamp() {\n         \"min(struct_with_ts.c1)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregationPushdownOnBucketedColumn() {\n     sql(\n         \"CREATE TABLE %s (id BIGINT, struct_with_int STRUCT<c1:INT>) USING iceberg PARTITIONED BY (bucket(8, id))\",\n@@ -592,7 +603,7 @@ private void assertExplainContains(List<Object[]> explain, String... expectedFra\n                     .contains(fragment));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregatePushDownInDeleteCopyOnWrite() {\n     sql(\"CREATE TABLE %s (id LONG, data INT) USING iceberg\", tableName);\n     sql(\n@@ -610,7 +621,9 @@ public void testAggregatePushDownInDeleteCopyOnWrite() {\n       explainContainsPushDownAggregates = true;\n     }\n \n-    Assert.assertTrue(\"min/max/count pushed down for deleted\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"min/max/count pushed down for deleted\")\n+        .isTrue();\n \n     List<Object[]> actual = sql(select, tableName);\n     List<Object[]> expected = Lists.newArrayList();\n@@ -618,7 +631,7 @@ public void testAggregatePushDownInDeleteCopyOnWrite() {\n     assertEquals(\"min/max/count push down\", expected, actual);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregatePushDownForTimeTravel() {\n     sql(\"CREATE TABLE %s (id LONG, data INT) USING iceberg\", tableName);\n     sql(\n@@ -638,7 +651,7 @@ public void testAggregatePushDownForTimeTravel() {\n     if (explainString1.contains(\"count(id)\")) {\n       explainContainsPushDownAggregates1 = true;\n     }\n-    Assert.assertTrue(\"count pushed down\", explainContainsPushDownAggregates1);\n+    assertThat(explainContainsPushDownAggregates1).as(\"count pushed down\").isTrue();\n \n     List<Object[]> actual1 =\n         sql(\"SELECT count(id) FROM %s VERSION AS OF %s\", tableName, snapshotId);\n@@ -651,13 +664,13 @@ public void testAggregatePushDownForTimeTravel() {\n       explainContainsPushDownAggregates2 = true;\n     }\n \n-    Assert.assertTrue(\"count pushed down\", explainContainsPushDownAggregates2);\n+    assertThat(explainContainsPushDownAggregates2).as(\"count pushed down\").isTrue();\n \n     List<Object[]> actual2 = sql(\"SELECT count(id) FROM %s\", tableName);\n     assertEquals(\"count push down\", expected2, actual2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAllNull() {\n     sql(\"CREATE TABLE %s (id int, data int) USING iceberg PARTITIONED BY (id)\", tableName);\n     sql(\n@@ -679,8 +692,9 @@ public void testAllNull() {\n       explainContainsPushDownAggregates = true;\n     }\n \n-    Assert.assertTrue(\n-        \"explain should contain the pushed down aggregates\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"explain should contain the pushed down aggregates\")\n+        .isTrue();\n \n     List<Object[]> actual = sql(select, tableName);\n     List<Object[]> expected = Lists.newArrayList();\n@@ -688,7 +702,7 @@ public void testAllNull() {\n     assertEquals(\"min/max/count push down\", expected, actual);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAllNaN() {\n     sql(\"CREATE TABLE %s (id int, data float) USING iceberg PARTITIONED BY (id)\", tableName);\n     sql(\n@@ -710,8 +724,9 @@ public void testAllNaN() {\n       explainContainsPushDownAggregates = true;\n     }\n \n-    Assert.assertFalse(\n-        \"explain should not contain the pushed down aggregates\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"explain should not contain the pushed down aggregates\")\n+        .isFalse();\n \n     List<Object[]> actual = sql(select, tableName);\n     List<Object[]> expected = Lists.newArrayList();\n@@ -719,7 +734,7 @@ public void testAllNaN() {\n     assertEquals(\"expected and actual should equal\", expected, actual);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNaN() {\n     sql(\"CREATE TABLE %s (id int, data float) USING iceberg PARTITIONED BY (id)\", tableName);\n     sql(\n@@ -741,8 +756,9 @@ public void testNaN() {\n       explainContainsPushDownAggregates = true;\n     }\n \n-    Assert.assertFalse(\n-        \"explain should not contain the pushed down aggregates\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"explain should not contain the pushed down aggregates\")\n+        .isFalse();\n \n     List<Object[]> actual = sql(select, tableName);\n     List<Object[]> expected = Lists.newArrayList();\n@@ -750,7 +766,7 @@ public void testNaN() {\n     assertEquals(\"expected and actual should equal\", expected, actual);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInfinity() {\n     sql(\n         \"CREATE TABLE %s (id int, data1 float, data2 double, data3 double) USING iceberg PARTITIONED BY (id)\",\n@@ -781,8 +797,9 @@ public void testInfinity() {\n       explainContainsPushDownAggregates = true;\n     }\n \n-    Assert.assertTrue(\n-        \"explain should contain the pushed down aggregates\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"explain should contain the pushed down aggregates\")\n+        .isTrue();\n \n     List<Object[]> actual = sql(select, tableName);\n     List<Object[]> expected = Lists.newArrayList();\n@@ -802,7 +819,7 @@ public void testInfinity() {\n     assertEquals(\"min/max/count push down\", expected, actual);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregatePushDownForIncrementalScan() {\n     sql(\"CREATE TABLE %s (id LONG, data INT) USING iceberg\", tableName);\n     sql(\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java\nindex 927f6e21439f..7fb1fda3364f 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java\n@@ -18,96 +18,105 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n+import static org.apache.iceberg.CatalogUtil.ICEBERG_CATALOG_TYPE;\n+import static org.apache.iceberg.CatalogUtil.ICEBERG_CATALOG_TYPE_REST;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.File;\n-import java.util.Map;\n+import java.nio.file.Files;\n import java.util.UUID;\n import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableOperations;\n import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.hadoop.HadoopCatalog;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.types.Types.NestedField;\n import org.apache.iceberg.types.Types.StructType;\n import org.apache.spark.sql.connector.catalog.TableCatalog;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Test;\n-\n-public class TestCreateTable extends SparkCatalogTestBase {\n-  public TestCreateTable(String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestCreateTable extends CatalogTestBase {\n \n-  @After\n+  @AfterEach\n   public void dropTestTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTransformIgnoreCase() {\n-    Assert.assertFalse(\"Table should not already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n     sql(\n         \"CREATE TABLE IF NOT EXISTS %s (id BIGINT NOT NULL, ts timestamp) \"\n             + \"USING iceberg partitioned by (HOURS(ts))\",\n         tableName);\n-    Assert.assertTrue(\"Table should already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent)).as(\"Table should already exist\").isTrue();\n     sql(\n         \"CREATE TABLE IF NOT EXISTS %s (id BIGINT NOT NULL, ts timestamp) \"\n             + \"USING iceberg partitioned by (hours(ts))\",\n         tableName);\n-    Assert.assertTrue(\"Table should already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent)).as(\"Table should already exist\").isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTransformSingularForm() {\n-    Assert.assertFalse(\"Table should not already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n     sql(\n         \"CREATE TABLE IF NOT EXISTS %s (id BIGINT NOT NULL, ts timestamp) \"\n             + \"USING iceberg partitioned by (hour(ts))\",\n         tableName);\n-    Assert.assertTrue(\"Table should exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent)).as(\"Table should exist\").isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTransformPluralForm() {\n-    Assert.assertFalse(\"Table should not already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n     sql(\n         \"CREATE TABLE IF NOT EXISTS %s (id BIGINT NOT NULL, ts timestamp) \"\n             + \"USING iceberg partitioned by (hours(ts))\",\n         tableName);\n-    Assert.assertTrue(\"Table should exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent)).as(\"Table should exist\").isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateTable() {\n-    Assert.assertFalse(\"Table should not already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n \n     sql(\"CREATE TABLE %s (id BIGINT NOT NULL, data STRING) USING iceberg\", tableName);\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertNotNull(\"Should load the new table\", table);\n+    assertThat(table).as(\"Should load the new table\").isNotNull();\n \n     StructType expectedSchema =\n         StructType.of(\n             NestedField.required(1, \"id\", Types.LongType.get()),\n             NestedField.optional(2, \"data\", Types.StringType.get()));\n-    Assert.assertEquals(\n-        \"Should have the expected schema\", expectedSchema, table.schema().asStruct());\n-    Assert.assertEquals(\"Should not be partitioned\", 0, table.spec().fields().size());\n-    Assert.assertNull(\n-        \"Should not have the default format set\",\n-        table.properties().get(TableProperties.DEFAULT_FILE_FORMAT));\n+    assertThat(table.schema().asStruct())\n+        .as(\"Should have the expected schema\")\n+        .isEqualTo(expectedSchema);\n+    assertThat(table.spec().fields()).as(\"Should not be partitioned\").isEmpty();\n+    assertThat(table.properties()).doesNotContainKey(TableProperties.DEFAULT_FILE_FORMAT);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateTablePartitionedByUUID() {\n     assertThat(validationCatalog.tableExists(tableIdent)).isFalse();\n     Schema schema = new Schema(1, Types.NestedField.optional(1, \"uuid\", Types.UUIDType.get()));\n@@ -126,13 +135,14 @@ public void testCreateTablePartitionedByUUID() {\n \n     sql(\"INSERT INTO %s VALUES('%s')\", tableName, uuid);\n \n-    assertThat(sql(\"SELECT uuid FROM %s\", tableName)).hasSize(1).element(0).isEqualTo(row(uuid));\n+    assertThat(sql(\"SELECT uuid FROM %s\", tableName)).singleElement().isEqualTo(row(uuid));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateTableInRootNamespace() {\n-    Assume.assumeTrue(\n-        \"Hadoop has no default namespace configured\", \"testhadoop\".equals(catalogName));\n+    assumeThat(catalogName)\n+        .as(\"Hadoop has no default namespace configured\")\n+        .isEqualTo(\"testhadoop\");\n \n     try {\n       sql(\"CREATE TABLE %s.table (id bigint) USING iceberg\", catalogName);\n@@ -141,30 +151,30 @@ public void testCreateTableInRootNamespace() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateTableUsingParquet() {\n-    Assume.assumeTrue(\n-        \"Not working with session catalog because Spark will not use v2 for a Parquet table\",\n-        !\"spark_catalog\".equals(catalogName));\n+    assumeThat(catalogName)\n+        .as(\"Not working with session catalog because Spark will not use v2 for a Parquet table\")\n+        .isNotEqualTo(\"spark_catalog\");\n \n-    Assert.assertFalse(\"Table should not already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n \n     sql(\"CREATE TABLE %s (id BIGINT NOT NULL, data STRING) USING parquet\", tableName);\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertNotNull(\"Should load the new table\", table);\n+    assertThat(table).as(\"Should load the new table\").isNotNull();\n \n     StructType expectedSchema =\n         StructType.of(\n             NestedField.required(1, \"id\", Types.LongType.get()),\n             NestedField.optional(2, \"data\", Types.StringType.get()));\n-    Assert.assertEquals(\n-        \"Should have the expected schema\", expectedSchema, table.schema().asStruct());\n-    Assert.assertEquals(\"Should not be partitioned\", 0, table.spec().fields().size());\n-    Assert.assertEquals(\n-        \"Should not have default format parquet\",\n-        \"parquet\",\n-        table.properties().get(TableProperties.DEFAULT_FILE_FORMAT));\n+    assertThat(table.schema().asStruct())\n+        .as(\"Should have the expected schema\")\n+        .isEqualTo(expectedSchema);\n+    assertThat(table.spec().fields()).as(\"Should not be partitioned\").isEmpty();\n+    assertThat(table.properties()).containsEntry(TableProperties.DEFAULT_FILE_FORMAT, \"parquet\");\n \n     assertThatThrownBy(\n             () ->\n@@ -175,9 +185,11 @@ public void testCreateTableUsingParquet() {\n         .hasMessage(\"Unsupported format in USING: crocodile\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateTablePartitionedBy() {\n-    Assert.assertFalse(\"Table should not already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n \n     sql(\n         \"CREATE TABLE %s \"\n@@ -187,7 +199,7 @@ public void testCreateTablePartitionedBy() {\n         tableName);\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertNotNull(\"Should load the new table\", table);\n+    assertThat(table).as(\"Should load the new table\").isNotNull();\n \n     StructType expectedSchema =\n         StructType.of(\n@@ -195,8 +207,9 @@ public void testCreateTablePartitionedBy() {\n             NestedField.optional(2, \"created_at\", Types.TimestampType.withZone()),\n             NestedField.optional(3, \"category\", Types.StringType.get()),\n             NestedField.optional(4, \"data\", Types.StringType.get()));\n-    Assert.assertEquals(\n-        \"Should have the expected schema\", expectedSchema, table.schema().asStruct());\n+    assertThat(table.schema().asStruct())\n+        .as(\"Should have the expected schema\")\n+        .isEqualTo(expectedSchema);\n \n     PartitionSpec expectedSpec =\n         PartitionSpec.builderFor(new Schema(expectedSchema.fields()))\n@@ -204,16 +217,15 @@ public void testCreateTablePartitionedBy() {\n             .bucket(\"id\", 8)\n             .day(\"created_at\")\n             .build();\n-    Assert.assertEquals(\"Should be partitioned correctly\", expectedSpec, table.spec());\n-\n-    Assert.assertNull(\n-        \"Should not have the default format set\",\n-        table.properties().get(TableProperties.DEFAULT_FILE_FORMAT));\n+    assertThat(table.spec()).as(\"Should be partitioned correctly\").isEqualTo(expectedSpec);\n+    assertThat(table.properties()).doesNotContainKey(TableProperties.DEFAULT_FILE_FORMAT);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateTableColumnComments() {\n-    Assert.assertFalse(\"Table should not already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n \n     sql(\n         \"CREATE TABLE %s \"\n@@ -222,23 +234,24 @@ public void testCreateTableColumnComments() {\n         tableName);\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertNotNull(\"Should load the new table\", table);\n+    assertThat(table).as(\"Should load the new table\").isNotNull();\n \n     StructType expectedSchema =\n         StructType.of(\n             NestedField.required(1, \"id\", Types.LongType.get(), \"Unique identifier\"),\n             NestedField.optional(2, \"data\", Types.StringType.get(), \"Data value\"));\n-    Assert.assertEquals(\n-        \"Should have the expected schema\", expectedSchema, table.schema().asStruct());\n-    Assert.assertEquals(\"Should not be partitioned\", 0, table.spec().fields().size());\n-    Assert.assertNull(\n-        \"Should not have the default format set\",\n-        table.properties().get(TableProperties.DEFAULT_FILE_FORMAT));\n+    assertThat(table.schema().asStruct())\n+        .as(\"Should have the expected schema\")\n+        .isEqualTo(expectedSchema);\n+    assertThat(table.spec().fields()).as(\"Should not be partitioned\").isEmpty();\n+    assertThat(table.properties()).doesNotContainKey(TableProperties.DEFAULT_FILE_FORMAT);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateTableComment() {\n-    Assert.assertFalse(\"Table should not already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n \n     sql(\n         \"CREATE TABLE %s \"\n@@ -248,34 +261,33 @@ public void testCreateTableComment() {\n         tableName);\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertNotNull(\"Should load the new table\", table);\n+    assertThat(table).as(\"Should load the new table\").isNotNull();\n \n     StructType expectedSchema =\n         StructType.of(\n             NestedField.required(1, \"id\", Types.LongType.get()),\n             NestedField.optional(2, \"data\", Types.StringType.get()));\n-    Assert.assertEquals(\n-        \"Should have the expected schema\", expectedSchema, table.schema().asStruct());\n-    Assert.assertEquals(\"Should not be partitioned\", 0, table.spec().fields().size());\n-    Assert.assertNull(\n-        \"Should not have the default format set\",\n-        table.properties().get(TableProperties.DEFAULT_FILE_FORMAT));\n-    Assert.assertEquals(\n-        \"Should have the table comment set in properties\",\n-        \"Table doc\",\n-        table.properties().get(TableCatalog.PROP_COMMENT));\n+    assertThat(table.schema().asStruct())\n+        .as(\"Should have the expected schema\")\n+        .isEqualTo(expectedSchema);\n+    assertThat(table.spec().fields()).as(\"Should not be partitioned\").isEmpty();\n+    assertThat(table.properties())\n+        .doesNotContainKey(TableProperties.DEFAULT_FILE_FORMAT)\n+        .containsEntry(TableCatalog.PROP_COMMENT, \"Table doc\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateTableLocation() throws Exception {\n-    Assume.assumeTrue(\n-        \"Cannot set custom locations for Hadoop catalog tables\",\n-        !(validationCatalog instanceof HadoopCatalog));\n+    assumeThat(validationCatalog)\n+        .as(\"Cannot set custom locations for Hadoop catalog tables\")\n+        .isNotInstanceOf(HadoopCatalog.class);\n \n-    Assert.assertFalse(\"Table should not already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n \n-    File tableLocation = temp.newFolder();\n-    Assert.assertTrue(tableLocation.delete());\n+    File tableLocation = Files.createTempDirectory(temp, \"junit\").toFile();\n+    assertThat(tableLocation.delete()).isTrue();\n \n     String location = \"file:\" + tableLocation;\n \n@@ -287,24 +299,25 @@ public void testCreateTableLocation() throws Exception {\n         tableName, location);\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertNotNull(\"Should load the new table\", table);\n+    assertThat(table).as(\"Should load the new table\").isNotNull();\n \n     StructType expectedSchema =\n         StructType.of(\n             NestedField.required(1, \"id\", Types.LongType.get()),\n             NestedField.optional(2, \"data\", Types.StringType.get()));\n-    Assert.assertEquals(\n-        \"Should have the expected schema\", expectedSchema, table.schema().asStruct());\n-    Assert.assertEquals(\"Should not be partitioned\", 0, table.spec().fields().size());\n-    Assert.assertNull(\n-        \"Should not have the default format set\",\n-        table.properties().get(TableProperties.DEFAULT_FILE_FORMAT));\n-    Assert.assertEquals(\"Should have a custom table location\", location, table.location());\n+    assertThat(table.schema().asStruct())\n+        .as(\"Should have the expected schema\")\n+        .isEqualTo(expectedSchema);\n+    assertThat(table.spec().fields()).as(\"Should not be partitioned\").isEmpty();\n+    assertThat(table.properties()).doesNotContainKey(TableProperties.DEFAULT_FILE_FORMAT);\n+    assertThat(table.location()).as(\"Should have a custom table location\").isEqualTo(location);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateTableProperties() {\n-    Assert.assertFalse(\"Table should not already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n \n     sql(\n         \"CREATE TABLE %s \"\n@@ -314,22 +327,69 @@ public void testCreateTableProperties() {\n         tableName);\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertNotNull(\"Should load the new table\", table);\n+    assertThat(table).as(\"Should load the new table\").isNotNull();\n \n     StructType expectedSchema =\n         StructType.of(\n             NestedField.required(1, \"id\", Types.LongType.get()),\n             NestedField.optional(2, \"data\", Types.StringType.get()));\n-    Assert.assertEquals(\n-        \"Should have the expected schema\", expectedSchema, table.schema().asStruct());\n-    Assert.assertEquals(\"Should not be partitioned\", 0, table.spec().fields().size());\n-    Assert.assertEquals(\"Should have property p1\", \"2\", table.properties().get(\"p1\"));\n-    Assert.assertEquals(\"Should have property p2\", \"x\", table.properties().get(\"p2\"));\n+    assertThat(table.schema().asStruct())\n+        .as(\"Should have the expected schema\")\n+        .isEqualTo(expectedSchema);\n+    assertThat(table.spec().fields()).as(\"Should not be partitioned\").isEmpty();\n+    assertThat(table.properties()).containsEntry(\"p1\", \"2\").containsEntry(\"p2\", \"x\");\n+  }\n+\n+  @TestTemplate\n+  public void testCreateTableCommitProperties() {\n+    assumeThat(catalogConfig.get(ICEBERG_CATALOG_TYPE))\n+        .as(\n+            \"need to fix https://github.com/apache/iceberg/issues/11554 before enabling this for the REST catalog\")\n+        .isNotEqualTo(ICEBERG_CATALOG_TYPE_REST);\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n+\n+    assertThatThrownBy(\n+            () ->\n+                sql(\n+                    \"CREATE TABLE %s \"\n+                        + \"(id BIGINT NOT NULL, data STRING) \"\n+                        + \"USING iceberg \"\n+                        + \"TBLPROPERTIES ('commit.retry.num-retries'='x', p2='x')\",\n+                    tableName))\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessage(\"Table property commit.retry.num-retries must have integer value\");\n+\n+    assertThatThrownBy(\n+            () ->\n+                sql(\n+                    \"CREATE TABLE %s \"\n+                        + \"(id BIGINT NOT NULL, data STRING) \"\n+                        + \"USING iceberg \"\n+                        + \"TBLPROPERTIES ('commit.retry.max-wait-ms'='-1')\",\n+                    tableName))\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessage(\"Table property commit.retry.max-wait-ms must have non negative integer value\");\n+\n+    sql(\n+        \"CREATE TABLE %s \"\n+            + \"(id BIGINT NOT NULL, data STRING) \"\n+            + \"USING iceberg \"\n+            + \"TBLPROPERTIES ('commit.retry.num-retries'='1', 'commit.retry.max-wait-ms'='3000')\",\n+        tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    assertThat(table.properties())\n+        .containsEntry(TableProperties.COMMIT_NUM_RETRIES, \"1\")\n+        .containsEntry(TableProperties.COMMIT_MAX_RETRY_WAIT_MS, \"3000\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateTableWithFormatV2ThroughTableProperty() {\n-    Assert.assertFalse(\"Table should not already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n \n     sql(\n         \"CREATE TABLE %s \"\n@@ -339,15 +399,16 @@ public void testCreateTableWithFormatV2ThroughTableProperty() {\n         tableName);\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\n-        \"should create table using format v2\",\n-        2,\n-        ((BaseTable) table).operations().current().formatVersion());\n+    assertThat(((BaseTable) table).operations().current().formatVersion())\n+        .as(\"should create table using format v2\")\n+        .isEqualTo(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpgradeTableWithFormatV2ThroughTableProperty() {\n-    Assert.assertFalse(\"Table should not already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n \n     sql(\n         \"CREATE TABLE %s \"\n@@ -358,15 +419,21 @@ public void testUpgradeTableWithFormatV2ThroughTableProperty() {\n \n     Table table = validationCatalog.loadTable(tableIdent);\n     TableOperations ops = ((BaseTable) table).operations();\n-    Assert.assertEquals(\"should create table using format v1\", 1, ops.refresh().formatVersion());\n+    assertThat(ops.refresh().formatVersion())\n+        .as(\"should create table using format v1\")\n+        .isEqualTo(1);\n \n     sql(\"ALTER TABLE %s SET TBLPROPERTIES ('format-version'='2')\", tableName);\n-    Assert.assertEquals(\"should update table to use format v2\", 2, ops.refresh().formatVersion());\n+    assertThat(ops.refresh().formatVersion())\n+        .as(\"should update table to use format v2\")\n+        .isEqualTo(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDowngradeTableToFormatV1ThroughTablePropertyFails() {\n-    Assert.assertFalse(\"Table should not already exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n \n     sql(\n         \"CREATE TABLE %s \"\n@@ -377,7 +444,9 @@ public void testDowngradeTableToFormatV1ThroughTablePropertyFails() {\n \n     Table table = validationCatalog.loadTable(tableIdent);\n     TableOperations ops = ((BaseTable) table).operations();\n-    Assert.assertEquals(\"should create table using format v2\", 2, ops.refresh().formatVersion());\n+    assertThat(ops.refresh().formatVersion())\n+        .as(\"should create table using format v2\")\n+        .isEqualTo(2);\n \n     assertThatThrownBy(\n             () -> sql(\"ALTER TABLE %s SET TBLPROPERTIES ('format-version'='1')\", tableName))\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java\nindex ff067291c854..855219c16a98 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java\n@@ -21,29 +21,56 @@\n import static org.apache.spark.sql.functions.col;\n import static org.apache.spark.sql.functions.lit;\n import static org.apache.spark.sql.functions.when;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n-import java.util.Map;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.CatalogTestBase;\n+import org.apache.iceberg.spark.SparkCatalogConfig;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.SparkException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n-\n-public class TestCreateTableAsSelect extends SparkCatalogTestBase {\n-\n-  private final String sourceName;\n-\n-  public TestCreateTableAsSelect(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-    this.sourceName = tableName(\"source\");\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestCreateTableAsSelect extends CatalogTestBase {\n+\n+  @Parameter(index = 3)\n+  private String sourceName;\n+\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}, sourceName = {3}\")\n+  protected static Object[][] parameters() {\n+    return new Object[][] {\n+      {\n+        SparkCatalogConfig.HIVE.catalogName(),\n+        SparkCatalogConfig.HIVE.implementation(),\n+        SparkCatalogConfig.HIVE.properties(),\n+        SparkCatalogConfig.HIVE.catalogName() + \".default.source\"\n+      },\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties(),\n+        SparkCatalogConfig.HADOOP.catalogName() + \".default.source\"\n+      },\n+      {\n+        SparkCatalogConfig.SPARK.catalogName(),\n+        SparkCatalogConfig.SPARK.implementation(),\n+        SparkCatalogConfig.SPARK.properties(),\n+        \"default.source\"\n+      }\n+    };\n+  }\n \n+  @BeforeEach\n+  public void createTableIfNotExists() {\n     sql(\n         \"CREATE TABLE IF NOT EXISTS %s (id bigint NOT NULL, data string) \"\n             + \"USING iceberg PARTITIONED BY (truncate(id, 3))\",\n@@ -51,12 +78,12 @@ public TestCreateTableAsSelect(\n     sql(\"INSERT INTO %s VALUES (1, 'a'), (2, 'b'), (3, 'c'), (4, 'd'), (5, 'e')\", sourceName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedCTAS() {\n     sql(\"CREATE TABLE %s USING iceberg AS SELECT * FROM %s\", tableName, sourceName);\n \n@@ -67,18 +94,17 @@ public void testUnpartitionedCTAS() {\n \n     Table ctasTable = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\n-        \"Should have expected nullable schema\",\n-        expectedSchema.asStruct(),\n-        ctasTable.schema().asStruct());\n-    Assert.assertEquals(\"Should be an unpartitioned table\", 0, ctasTable.spec().fields().size());\n+    assertThat(ctasTable.schema().asStruct())\n+        .as(\"Should have expected nullable schema\")\n+        .isEqualTo(expectedSchema.asStruct());\n+    assertThat(ctasTable.spec().fields()).as(\"Should be an unpartitioned table\").isEmpty();\n     assertEquals(\n         \"Should have rows matching the source table\",\n         sql(\"SELECT * FROM %s ORDER BY id\", sourceName),\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedCTAS() {\n     sql(\n         \"CREATE TABLE %s USING iceberg PARTITIONED BY (id) AS SELECT * FROM %s ORDER BY id\",\n@@ -93,18 +119,17 @@ public void testPartitionedCTAS() {\n \n     Table ctasTable = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\n-        \"Should have expected nullable schema\",\n-        expectedSchema.asStruct(),\n-        ctasTable.schema().asStruct());\n-    Assert.assertEquals(\"Should be partitioned by id\", expectedSpec, ctasTable.spec());\n+    assertThat(ctasTable.schema().asStruct())\n+        .as(\"Should have expected nullable schema\")\n+        .isEqualTo(expectedSchema.asStruct());\n+    assertThat(ctasTable.spec()).as(\"Should be partitioned by id\").isEqualTo(expectedSpec);\n     assertEquals(\n         \"Should have rows matching the source table\",\n         sql(\"SELECT * FROM %s ORDER BY id\", sourceName),\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCTASWriteDistributionModeNotRespected() {\n     assertThatThrownBy(\n             () ->\n@@ -116,7 +141,7 @@ public void testCTASWriteDistributionModeNotRespected() {\n             \"Incoming records violate the writer assumption that records are clustered by spec and by partition within each spec\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRTAS() {\n     sql(\n         \"CREATE TABLE %s USING iceberg TBLPROPERTIES ('prop1'='val1', 'prop2'='val2')\"\n@@ -146,11 +171,10 @@ public void testRTAS() {\n     Table rtasTable = validationCatalog.loadTable(tableIdent);\n \n     // the replacement table has a different schema and partition spec than the original\n-    Assert.assertEquals(\n-        \"Should have expected nullable schema\",\n-        expectedSchema.asStruct(),\n-        rtasTable.schema().asStruct());\n-    Assert.assertEquals(\"Should be partitioned by part\", expectedSpec, rtasTable.spec());\n+    assertThat(rtasTable.schema().asStruct())\n+        .as(\"Should have expected nullable schema\")\n+        .isEqualTo(expectedSchema.asStruct());\n+    assertThat(rtasTable.spec()).as(\"Should be partitioned by part\").isEqualTo(expectedSpec);\n \n     assertEquals(\n         \"Should have rows matching the source table\",\n@@ -160,18 +184,14 @@ public void testRTAS() {\n             sourceName),\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n \n-    Assert.assertEquals(\n-        \"Table should have expected snapshots\", 2, Iterables.size(rtasTable.snapshots()));\n-\n-    Assert.assertEquals(\n-        \"Should have updated table property\", \"newval1\", rtasTable.properties().get(\"prop1\"));\n-    Assert.assertEquals(\n-        \"Should have preserved table property\", \"val2\", rtasTable.properties().get(\"prop2\"));\n-    Assert.assertEquals(\n-        \"Should have new table property\", \"val3\", rtasTable.properties().get(\"prop3\"));\n+    assertThat(rtasTable.snapshots()).as(\"Table should have expected snapshots\").hasSize(2);\n+    assertThat(rtasTable.properties())\n+        .containsEntry(\"prop1\", \"newval1\")\n+        .containsEntry(\"prop2\", \"val2\")\n+        .containsEntry(\"prop3\", \"val3\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateRTAS() {\n     sql(\n         \"CREATE OR REPLACE TABLE %s USING iceberg PARTITIONED BY (part) AS \"\n@@ -208,11 +228,10 @@ public void testCreateRTAS() {\n     Table rtasTable = validationCatalog.loadTable(tableIdent);\n \n     // the replacement table has a different schema and partition spec than the original\n-    Assert.assertEquals(\n-        \"Should have expected nullable schema\",\n-        expectedSchema.asStruct(),\n-        rtasTable.schema().asStruct());\n-    Assert.assertEquals(\"Should be partitioned by part\", expectedSpec, rtasTable.spec());\n+    assertThat(rtasTable.schema().asStruct())\n+        .as(\"Should have expected nullable schema\")\n+        .isEqualTo(expectedSchema.asStruct());\n+    assertThat(rtasTable.spec()).as(\"Should be partitioned by part\").isEqualTo(expectedSpec);\n \n     assertEquals(\n         \"Should have rows matching the source table\",\n@@ -222,11 +241,10 @@ public void testCreateRTAS() {\n             sourceName),\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n \n-    Assert.assertEquals(\n-        \"Table should have expected snapshots\", 2, Iterables.size(rtasTable.snapshots()));\n+    assertThat(rtasTable.snapshots()).as(\"Table should have expected snapshots\").hasSize(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDataFrameV2Create() throws Exception {\n     spark.table(sourceName).writeTo(tableName).using(\"iceberg\").create();\n \n@@ -237,18 +255,17 @@ public void testDataFrameV2Create() throws Exception {\n \n     Table ctasTable = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\n-        \"Should have expected nullable schema\",\n-        expectedSchema.asStruct(),\n-        ctasTable.schema().asStruct());\n-    Assert.assertEquals(\"Should be an unpartitioned table\", 0, ctasTable.spec().fields().size());\n+    assertThat(ctasTable.schema().asStruct())\n+        .as(\"Should have expected nullable schema\")\n+        .isEqualTo(expectedSchema.asStruct());\n+    assertThat(ctasTable.spec().fields()).as(\"Should be an unpartitioned table\").isEmpty();\n     assertEquals(\n         \"Should have rows matching the source table\",\n         sql(\"SELECT * FROM %s ORDER BY id\", sourceName),\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDataFrameV2Replace() throws Exception {\n     spark.table(sourceName).writeTo(tableName).using(\"iceberg\").create();\n \n@@ -281,11 +298,10 @@ public void testDataFrameV2Replace() throws Exception {\n     Table rtasTable = validationCatalog.loadTable(tableIdent);\n \n     // the replacement table has a different schema and partition spec than the original\n-    Assert.assertEquals(\n-        \"Should have expected nullable schema\",\n-        expectedSchema.asStruct(),\n-        rtasTable.schema().asStruct());\n-    Assert.assertEquals(\"Should be partitioned by part\", expectedSpec, rtasTable.spec());\n+    assertThat(rtasTable.schema().asStruct())\n+        .as(\"Should have expected nullable schema\")\n+        .isEqualTo(expectedSchema.asStruct());\n+    assertThat(rtasTable.spec()).as(\"Should be partitioned by part\").isEqualTo(expectedSpec);\n \n     assertEquals(\n         \"Should have rows matching the source table\",\n@@ -295,11 +311,10 @@ public void testDataFrameV2Replace() throws Exception {\n             sourceName),\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n \n-    Assert.assertEquals(\n-        \"Table should have expected snapshots\", 2, Iterables.size(rtasTable.snapshots()));\n+    assertThat(rtasTable.snapshots()).as(\"Table should have expected snapshots\").hasSize(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDataFrameV2CreateOrReplace() {\n     spark\n         .table(sourceName)\n@@ -349,11 +364,10 @@ public void testDataFrameV2CreateOrReplace() {\n     Table rtasTable = validationCatalog.loadTable(tableIdent);\n \n     // the replacement table has a different schema and partition spec than the original\n-    Assert.assertEquals(\n-        \"Should have expected nullable schema\",\n-        expectedSchema.asStruct(),\n-        rtasTable.schema().asStruct());\n-    Assert.assertEquals(\"Should be partitioned by part\", expectedSpec, rtasTable.spec());\n+    assertThat(rtasTable.schema().asStruct())\n+        .as(\"Should have expected nullable schema\")\n+        .isEqualTo(expectedSchema.asStruct());\n+    assertThat(rtasTable.spec()).as(\"Should be partitioned by part\").isEqualTo(expectedSpec);\n \n     assertEquals(\n         \"Should have rows matching the source table\",\n@@ -363,11 +377,10 @@ public void testDataFrameV2CreateOrReplace() {\n             sourceName),\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n \n-    Assert.assertEquals(\n-        \"Table should have expected snapshots\", 2, Iterables.size(rtasTable.snapshots()));\n+    assertThat(rtasTable.snapshots()).as(\"Table should have expected snapshots\").hasSize(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateRTASWithPartitionSpecChanging() {\n     sql(\n         \"CREATE OR REPLACE TABLE %s USING iceberg PARTITIONED BY (part) AS \"\n@@ -409,13 +422,12 @@ public void testCreateRTASWithPartitionSpecChanging() {\n             .withSpecId(2) // The Spec is new\n             .build();\n \n-    Assert.assertEquals(\"Should be partitioned by part and id\", expectedSpec, rtasTable.spec());\n+    assertThat(rtasTable.spec()).as(\"Should be partitioned by part and id\").isEqualTo(expectedSpec);\n \n     // the replacement table has a different schema and partition spec than the original\n-    Assert.assertEquals(\n-        \"Should have expected nullable schema\",\n-        expectedSchema.asStruct(),\n-        rtasTable.schema().asStruct());\n+    assertThat(rtasTable.schema().asStruct())\n+        .as(\"Should have expected nullable schema\")\n+        .isEqualTo(expectedSchema.asStruct());\n \n     assertEquals(\n         \"Should have rows matching the source table\",\n@@ -425,7 +437,6 @@ public void testCreateRTASWithPartitionSpecChanging() {\n             sourceName),\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n \n-    Assert.assertEquals(\n-        \"Table should have expected snapshots\", 2, Iterables.size(rtasTable.snapshots()));\n+    assertThat(rtasTable.snapshots()).as(\"Table should have expected snapshots\").hasSize(2);\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestDeleteFrom.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestDeleteFrom.java\nindex d891507da658..bd4a41593c34 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestDeleteFrom.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestDeleteFrom.java\n@@ -18,34 +18,31 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.source.SimpleRecord;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestDeleteFrom extends SparkCatalogTestBase {\n-  public TestDeleteFrom(String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestDeleteFrom extends CatalogTestBase {\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteFromUnpartitionedTable() throws NoSuchTableException {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -75,7 +72,7 @@ public void testDeleteFromUnpartitionedTable() throws NoSuchTableException {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteFromTableAtSnapshot() throws NoSuchTableException {\n     sql(\"CREATE TABLE %s (id bigint, data string) USING iceberg\", tableName);\n \n@@ -92,7 +89,7 @@ public void testDeleteFromTableAtSnapshot() throws NoSuchTableException {\n         .hasMessageStartingWith(\"Cannot delete from table at a specific snapshot\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteFromPartitionedTable() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (id bigint, data string) \"\n@@ -125,7 +122,7 @@ public void testDeleteFromPartitionedTable() throws NoSuchTableException {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteFromWhereFalse() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a'), (2, 'b'), (3, 'c')\", tableName);\n@@ -136,17 +133,16 @@ public void testDeleteFromWhereFalse() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 1 snapshot\", 1, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 1 snapshot\").hasSize(1);\n \n     sql(\"DELETE FROM %s WHERE false\", tableName);\n \n     table.refresh();\n \n-    Assert.assertEquals(\n-        \"Delete should not produce a new snapshot\", 1, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Delete should not produce a new snapshot\").hasSize(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTruncate() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a'), (2, 'b'), (3, 'c')\", tableName);\n@@ -157,7 +153,7 @@ public void testTruncate() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 1 snapshot\", 1, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 1 snapshot\").hasSize(1);\n \n     sql(\"TRUNCATE TABLE %s\", tableName);\n \n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestDropTable.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestDropTable.java\nindex 67dfeb3c8404..07faae52749b 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestDropTable.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestDropTable.java\n@@ -18,47 +18,45 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.io.IOException;\n import java.util.List;\n-import java.util.Map;\n import java.util.stream.Collectors;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.apache.iceberg.spark.CatalogTestBase;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestDropTable extends SparkCatalogTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestDropTable extends CatalogTestBase {\n \n-  public TestDropTable(String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Before\n+  @BeforeEach\n   public void createTable() {\n     sql(\"CREATE TABLE %s (id INT, name STRING) USING iceberg\", tableName);\n     sql(\"INSERT INTO %s VALUES (1, 'test')\", tableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTable() throws IOException {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropTable() throws IOException {\n     dropTableInternal();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropTableGCDisabled() throws IOException {\n     sql(\"ALTER TABLE %s SET TBLPROPERTIES (gc.enabled = false)\", tableName);\n     dropTableInternal();\n@@ -71,22 +69,25 @@ private void dropTableInternal() throws IOException {\n         sql(\"SELECT * FROM %s\", tableName));\n \n     List<String> manifestAndFiles = manifestsAndFiles();\n-    Assert.assertEquals(\n-        \"There should be 2 files for manifests and files\", 2, manifestAndFiles.size());\n-    Assert.assertTrue(\"All files should be existed\", checkFilesExist(manifestAndFiles, true));\n+    assertThat(manifestAndFiles).as(\"There should be 2 files for manifests and files\").hasSize(2);\n+    assertThat(checkFilesExist(manifestAndFiles, true)).as(\"All files should exist\").isTrue();\n \n     sql(\"DROP TABLE %s\", tableName);\n-    Assert.assertFalse(\"Table should not exist\", validationCatalog.tableExists(tableIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent)).as(\"Table should not exist\").isFalse();\n \n     if (catalogName.equals(\"testhadoop\")) {\n       // HadoopCatalog drop table without purge will delete the base table location.\n-      Assert.assertTrue(\"All files should be deleted\", checkFilesExist(manifestAndFiles, false));\n+      assertThat(checkFilesExist(manifestAndFiles, false))\n+          .as(\"All files should be deleted\")\n+          .isTrue();\n     } else {\n-      Assert.assertTrue(\"All files should not be deleted\", checkFilesExist(manifestAndFiles, true));\n+      assertThat(checkFilesExist(manifestAndFiles, true))\n+          .as(\"All files should not be deleted\")\n+          .isTrue();\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPurgeTable() throws IOException {\n     assertEquals(\n         \"Should have expected rows\",\n@@ -94,16 +95,15 @@ public void testPurgeTable() throws IOException {\n         sql(\"SELECT * FROM %s\", tableName));\n \n     List<String> manifestAndFiles = manifestsAndFiles();\n-    Assert.assertEquals(\n-        \"There should be 2 files for manifests and files\", 2, manifestAndFiles.size());\n-    Assert.assertTrue(\"All files should exist\", checkFilesExist(manifestAndFiles, true));\n+    assertThat(manifestAndFiles).as(\"There should be 2 files for manifests and files\").hasSize(2);\n+    assertThat(checkFilesExist(manifestAndFiles, true)).as(\"All files should exist\").isTrue();\n \n     sql(\"DROP TABLE %s PURGE\", tableName);\n-    Assert.assertFalse(\"Table should not exist\", validationCatalog.tableExists(tableIdent));\n-    Assert.assertTrue(\"All files should be deleted\", checkFilesExist(manifestAndFiles, false));\n+    assertThat(validationCatalog.tableExists(tableIdent)).as(\"Table should not exist\").isFalse();\n+    assertThat(checkFilesExist(manifestAndFiles, false)).as(\"All files should be deleted\").isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPurgeTableGCDisabled() throws IOException {\n     sql(\"ALTER TABLE %s SET TBLPROPERTIES (gc.enabled = false)\", tableName);\n \n@@ -113,17 +113,20 @@ public void testPurgeTableGCDisabled() throws IOException {\n         sql(\"SELECT * FROM %s\", tableName));\n \n     List<String> manifestAndFiles = manifestsAndFiles();\n-    Assert.assertEquals(\n-        \"There totally should have 2 files for manifests and files\", 2, manifestAndFiles.size());\n-    Assert.assertTrue(\"All files should be existed\", checkFilesExist(manifestAndFiles, true));\n+    assertThat(manifestAndFiles).as(\"There should be 2 files for manifests and files\").hasSize(2);\n+    assertThat(checkFilesExist(manifestAndFiles, true)).as(\"All files should exist\").isTrue();\n \n     assertThatThrownBy(() -> sql(\"DROP TABLE %s PURGE\", tableName))\n         .isInstanceOf(ValidationException.class)\n         .hasMessageContaining(\n             \"Cannot purge table: GC is disabled (deleting files may corrupt other tables\");\n \n-    Assert.assertTrue(\"Table should not been dropped\", validationCatalog.tableExists(tableIdent));\n-    Assert.assertTrue(\"All files should not be deleted\", checkFilesExist(manifestAndFiles, true));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not been dropped\")\n+        .isTrue();\n+    assertThat(checkFilesExist(manifestAndFiles, true))\n+        .as(\"All files should not be deleted\")\n+        .isTrue();\n   }\n \n   private List<String> manifestsAndFiles() {\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestFilterPushDown.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestFilterPushDown.java\nindex f92055ab7a4d..9d2ce2b388a2 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestFilterPushDown.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestFilterPushDown.java\n@@ -26,38 +26,51 @@\n import java.sql.Timestamp;\n import java.time.Instant;\n import java.util.List;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PlanningMode;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.expressions.Expressions;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.spark.SparkCatalogConfig;\n+import org.apache.iceberg.spark.TestBaseWithCatalog;\n import org.apache.spark.sql.execution.SparkPlan;\n-import org.junit.After;\n-import org.junit.Test;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n-public class TestFilterPushDown extends SparkTestBaseWithCatalog {\n-\n-  @Parameterized.Parameters(name = \"planningMode = {0}\")\n-  public static Object[] parameters() {\n-    return new Object[] {LOCAL, DISTRIBUTED};\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestFilterPushDown extends TestBaseWithCatalog {\n+\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}, planningMode = {0}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties(),\n+        LOCAL\n+      },\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties(),\n+        DISTRIBUTED\n+      }\n+    };\n   }\n \n-  private final PlanningMode planningMode;\n-\n-  public TestFilterPushDown(PlanningMode planningMode) {\n-    this.planningMode = planningMode;\n-  }\n+  @Parameter(index = 3)\n+  private PlanningMode planningMode;\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS tmp_view\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilterPushdownWithDecimalValues() {\n     sql(\n         \"CREATE TABLE %s (id INT, salary DECIMAL(10, 2), dep STRING)\"\n@@ -76,7 +89,7 @@ public void testFilterPushdownWithDecimalValues() {\n         ImmutableList.of(row(2, new BigDecimal(\"100.05\"), \"d1\")));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilterPushdownWithIdentityTransform() {\n     sql(\n         \"CREATE TABLE %s (id INT, salary INT, dep STRING)\"\n@@ -188,7 +201,7 @@ public void testFilterPushdownWithIdentityTransform() {\n         ImmutableList.of(row(5, 500, \"d5\")));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilterPushdownWithHoursTransform() {\n     sql(\n         \"CREATE TABLE %s (id INT, price INT, t TIMESTAMP)\"\n@@ -234,7 +247,7 @@ public void testFilterPushdownWithHoursTransform() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilterPushdownWithDaysTransform() {\n     sql(\n         \"CREATE TABLE %s (id INT, price INT, t TIMESTAMP)\"\n@@ -277,7 +290,7 @@ public void testFilterPushdownWithDaysTransform() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilterPushdownWithMonthsTransform() {\n     sql(\n         \"CREATE TABLE %s (id INT, price INT, t TIMESTAMP)\"\n@@ -320,7 +333,7 @@ public void testFilterPushdownWithMonthsTransform() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilterPushdownWithYearsTransform() {\n     sql(\n         \"CREATE TABLE %s (id INT, price INT, t TIMESTAMP)\"\n@@ -363,7 +376,7 @@ public void testFilterPushdownWithYearsTransform() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilterPushdownWithBucketTransform() {\n     sql(\n         \"CREATE TABLE %s (id INT, salary INT, dep STRING)\"\n@@ -382,7 +395,7 @@ public void testFilterPushdownWithBucketTransform() {\n         ImmutableList.of(row(1, 100, \"d1\")));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilterPushdownWithTruncateTransform() {\n     sql(\n         \"CREATE TABLE %s (id INT, salary INT, dep STRING)\"\n@@ -407,7 +420,7 @@ public void testFilterPushdownWithTruncateTransform() {\n         ImmutableList.of(row(1, 100, \"d1\")));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilterPushdownWithSpecEvolutionAndIdentityTransforms() {\n     sql(\n         \"CREATE TABLE %s (id INT, salary INT, dep STRING, sub_dep STRING)\"\n@@ -448,7 +461,7 @@ public void testFilterPushdownWithSpecEvolutionAndIdentityTransforms() {\n         ImmutableList.of(row(1, 100, \"d1\", \"sd1\")));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilterPushdownWithSpecEvolutionAndTruncateTransform() {\n     sql(\n         \"CREATE TABLE %s (id INT, salary INT, dep STRING)\"\n@@ -489,7 +502,7 @@ public void testFilterPushdownWithSpecEvolutionAndTruncateTransform() {\n         ImmutableList.of(row(1, 100, \"d1\")));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilterPushdownWithSpecEvolutionAndTimeTransforms() {\n     sql(\n         \"CREATE TABLE %s (id INT, price INT, t TIMESTAMP)\"\n@@ -526,7 +539,7 @@ public void testFilterPushdownWithSpecEvolutionAndTimeTransforms() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilterPushdownWithSpecialFloatingPointPartitionValues() {\n     sql(\n         \"CREATE TABLE %s (id INT, salary DOUBLE)\" + \"USING iceberg \" + \"PARTITIONED BY (salary)\",\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestNamespaceSQL.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestNamespaceSQL.java\nindex f7ad4a6e9113..58b56c440aa4 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestNamespaceSQL.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestNamespaceSQL.java\n@@ -18,8 +18,7 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n-import static org.apache.iceberg.CatalogUtil.ICEBERG_CATALOG_TYPE;\n-import static org.apache.iceberg.CatalogUtil.ICEBERG_CATALOG_TYPE_REST;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n import static org.assertj.core.api.Assumptions.assumeThat;\n \n@@ -28,88 +27,127 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.stream.Collectors;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.exceptions.BadRequestException;\n import org.apache.iceberg.exceptions.NamespaceNotEmptyException;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Test;\n-\n-public class TestNamespaceSQL extends SparkCatalogTestBase {\n+import org.apache.iceberg.spark.CatalogTestBase;\n+import org.apache.iceberg.spark.SparkCatalogConfig;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestNamespaceSQL extends CatalogTestBase {\n   private static final Namespace NS = Namespace.of(\"db\");\n \n-  private final String fullNamespace;\n-  private final boolean isHadoopCatalog;\n-\n-  public TestNamespaceSQL(String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-    this.fullNamespace = (\"spark_catalog\".equals(catalogName) ? \"\" : catalogName + \".\") + NS;\n-    this.isHadoopCatalog = \"testhadoop\".equals(catalogName);\n+  @Parameter(index = 3)\n+  private String fullNamespace;\n+\n+  @Parameter(index = 4)\n+  private boolean isHadoopCatalog;\n+\n+  @Parameters(\n+      name =\n+          \"catalogName = {0}, implementation = {1}, config = {2}, fullNameSpace = {3}, isHadoopCatalog = {4}\")\n+  protected static Object[][] parameters() {\n+    return new Object[][] {\n+      {\n+        SparkCatalogConfig.HIVE.catalogName(),\n+        SparkCatalogConfig.HIVE.implementation(),\n+        SparkCatalogConfig.HIVE.properties(),\n+        SparkCatalogConfig.HIVE.catalogName() + \".\" + NS.toString(),\n+        false\n+      },\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties(),\n+        SparkCatalogConfig.HADOOP.catalogName() + \".\" + NS,\n+        true\n+      },\n+      {\n+        SparkCatalogConfig.SPARK.catalogName(),\n+        SparkCatalogConfig.SPARK.implementation(),\n+        SparkCatalogConfig.SPARK.properties(),\n+        NS.toString(),\n+        false\n+      }\n+    };\n   }\n \n-  @After\n+  @AfterEach\n   public void cleanNamespaces() {\n     sql(\"DROP TABLE IF EXISTS %s.table\", fullNamespace);\n     sql(\"DROP NAMESPACE IF EXISTS %s\", fullNamespace);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateNamespace() {\n-    Assert.assertFalse(\n-        \"Namespace should not already exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should not already exist\")\n+        .isFalse();\n \n     sql(\"CREATE NAMESPACE %s\", fullNamespace);\n \n-    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should exist\")\n+        .isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultNamespace() {\n-    Assume.assumeFalse(\"Hadoop has no default namespace configured\", isHadoopCatalog);\n-    assumeThat(catalogConfig.get(ICEBERG_CATALOG_TYPE))\n-        .as(\"REST has no default namespace configured\")\n-        .isNotEqualTo(ICEBERG_CATALOG_TYPE_REST);\n+    assumeThat(isHadoopCatalog).as(\"Hadoop has no default namespace configured\").isFalse();\n \n     sql(\"USE %s\", catalogName);\n \n-    Object[] current = Iterables.getOnlyElement(sql(\"SHOW CURRENT NAMESPACE\"));\n-    Assert.assertEquals(\"Should use the current catalog\", current[0], catalogName);\n-    Assert.assertEquals(\"Should use the configured default namespace\", current[1], \"default\");\n+    assertThat(sql(\"SHOW CURRENT NAMESPACE\"))\n+        .singleElement()\n+        .satisfies(\n+            ns -> {\n+              assertThat(ns).containsExactly(catalogName, \"default\");\n+            });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropEmptyNamespace() {\n-    Assert.assertFalse(\n-        \"Namespace should not already exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should not already exist\")\n+        .isFalse();\n \n     sql(\"CREATE NAMESPACE %s\", fullNamespace);\n \n-    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should exist\")\n+        .isTrue();\n \n     sql(\"DROP NAMESPACE %s\", fullNamespace);\n \n-    Assert.assertFalse(\n-        \"Namespace should have been dropped\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should have been dropped\")\n+        .isFalse();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropNonEmptyNamespace() {\n-    Assume.assumeFalse(\"Session catalog has flaky behavior\", \"spark_catalog\".equals(catalogName));\n+    assumeThat(catalogName).as(\"Session catalog has flaky behavior\").isNotEqualTo(\"spark_catalog\");\n \n-    Assert.assertFalse(\n-        \"Namespace should not already exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should not already exist\")\n+        .isFalse();\n \n     sql(\"CREATE NAMESPACE %s\", fullNamespace);\n     sql(\"CREATE TABLE %s.table (id bigint) USING iceberg\", fullNamespace);\n \n-    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(NS));\n-    Assert.assertTrue(\n-        \"Table should exist\", validationCatalog.tableExists(TableIdentifier.of(NS, \"table\")));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should exist\")\n+        .isTrue();\n+    assertThat(validationCatalog.tableExists(TableIdentifier.of(NS, \"table\")))\n+        .as(\"Table should exist\")\n+        .isTrue();\n \n     assertThatThrownBy(() -> sql(\"DROP NAMESPACE %s\", fullNamespace))\n         .isInstanceOfAny(NamespaceNotEmptyException.class, BadRequestException.class)\n@@ -118,135 +156,148 @@ public void testDropNonEmptyNamespace() {\n     sql(\"DROP TABLE %s.table\", fullNamespace);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testListTables() {\n-    Assert.assertFalse(\n-        \"Namespace should not already exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should not already exist\")\n+        .isFalse();\n \n     sql(\"CREATE NAMESPACE %s\", fullNamespace);\n \n-    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should exist\")\n+        .isTrue();\n \n     List<Object[]> rows = sql(\"SHOW TABLES IN %s\", fullNamespace);\n-    Assert.assertEquals(\"Should not list any tables\", 0, rows.size());\n+    assertThat(rows).as(\"Should not list any tables\").isEmpty();\n \n     sql(\"CREATE TABLE %s.table (id bigint) USING iceberg\", fullNamespace);\n \n-    Object[] row = Iterables.getOnlyElement(sql(\"SHOW TABLES IN %s\", fullNamespace));\n-    Assert.assertEquals(\"Namespace should match\", \"db\", row[0]);\n-    Assert.assertEquals(\"Table name should match\", \"table\", row[1]);\n+    assertThat(sql(\"SHOW TABLES IN %s\", fullNamespace))\n+        .singleElement()\n+        .satisfies(\n+            row -> {\n+              assertThat(row).containsExactly(\"db\", \"table\", false);\n+            });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testListNamespace() {\n-    Assert.assertFalse(\n-        \"Namespace should not already exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should not already exist\")\n+        .isFalse();\n \n     sql(\"CREATE NAMESPACE %s\", fullNamespace);\n \n-    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should exist\")\n+        .isTrue();\n \n     List<Object[]> namespaces = sql(\"SHOW NAMESPACES IN %s\", catalogName);\n \n-    if (isHadoopCatalog\n-        || catalogConfig.get(ICEBERG_CATALOG_TYPE).equals(ICEBERG_CATALOG_TYPE_REST)) {\n-      Assert.assertEquals(\"Should have 1 namespace\", 1, namespaces.size());\n-      Set<String> namespaceNames =\n-          namespaces.stream().map(arr -> arr[0].toString()).collect(Collectors.toSet());\n-      Assert.assertEquals(\"Should have only db namespace\", ImmutableSet.of(\"db\"), namespaceNames);\n+    if (isHadoopCatalog) {\n+      assertThat(namespaces)\n+          .singleElement()\n+          .satisfies(\n+              ns -> {\n+                assertThat(ns).containsExactly(\"db\");\n+              });\n     } else {\n-      Assert.assertEquals(\"Should have 2 namespaces\", 2, namespaces.size());\n+      assertThat(namespaces).as(\"Should have 2 namespaces\").hasSize(2);\n       Set<String> namespaceNames =\n           namespaces.stream().map(arr -> arr[0].toString()).collect(Collectors.toSet());\n-      Assert.assertEquals(\n-          \"Should have default and db namespaces\",\n-          ImmutableSet.of(\"default\", \"db\"),\n-          namespaceNames);\n+      assertThat(namespaceNames)\n+          .as(\"Should have default and db namespaces\")\n+          .containsExactlyInAnyOrder(\"default\", \"db\");\n     }\n \n     List<Object[]> nestedNamespaces = sql(\"SHOW NAMESPACES IN %s\", fullNamespace);\n-\n-    Set<String> nestedNames =\n-        nestedNamespaces.stream().map(arr -> arr[0].toString()).collect(Collectors.toSet());\n-    Assert.assertEquals(\"Should not have nested namespaces\", ImmutableSet.of(), nestedNames);\n+    assertThat(nestedNamespaces).as(\"Should not have nested namespaces\").isEmpty();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateNamespaceWithMetadata() {\n-    Assume.assumeFalse(\"HadoopCatalog does not support namespace metadata\", isHadoopCatalog);\n+    assumeThat(isHadoopCatalog).as(\"HadoopCatalog does not support namespace metadata\").isFalse();\n \n-    Assert.assertFalse(\n-        \"Namespace should not already exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should not already exist\")\n+        .isFalse();\n \n     sql(\"CREATE NAMESPACE %s WITH PROPERTIES ('prop'='value')\", fullNamespace);\n \n-    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should exist\")\n+        .isTrue();\n \n     Map<String, String> nsMetadata = validationNamespaceCatalog.loadNamespaceMetadata(NS);\n \n-    Assert.assertEquals(\n-        \"Namespace should have expected prop value\", \"value\", nsMetadata.get(\"prop\"));\n+    assertThat(nsMetadata).containsEntry(\"prop\", \"value\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateNamespaceWithComment() {\n-    Assume.assumeFalse(\"HadoopCatalog does not support namespace metadata\", isHadoopCatalog);\n+    assumeThat(isHadoopCatalog).as(\"HadoopCatalog does not support namespace metadata\").isFalse();\n \n-    Assert.assertFalse(\n-        \"Namespace should not already exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should not already exist\")\n+        .isFalse();\n \n     sql(\"CREATE NAMESPACE %s COMMENT 'namespace doc'\", fullNamespace);\n \n-    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should exist\")\n+        .isTrue();\n \n     Map<String, String> nsMetadata = validationNamespaceCatalog.loadNamespaceMetadata(NS);\n \n-    Assert.assertEquals(\n-        \"Namespace should have expected comment\", \"namespace doc\", nsMetadata.get(\"comment\"));\n+    assertThat(nsMetadata).containsEntry(\"comment\", \"namespace doc\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateNamespaceWithLocation() throws Exception {\n-    Assume.assumeFalse(\"HadoopCatalog does not support namespace locations\", isHadoopCatalog);\n+    assumeThat(isHadoopCatalog).as(\"HadoopCatalog does not support namespace metadata\").isFalse();\n \n-    Assert.assertFalse(\n-        \"Namespace should not already exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should not already exist\")\n+        .isFalse();\n \n-    File location = temp.newFile();\n-    Assert.assertTrue(location.delete());\n+    File location = File.createTempFile(\"junit\", null, temp.toFile());\n+    assertThat(location.delete()).isTrue();\n \n     sql(\"CREATE NAMESPACE %s LOCATION '%s'\", fullNamespace, location);\n \n-    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should exist\")\n+        .isTrue();\n \n     Map<String, String> nsMetadata = validationNamespaceCatalog.loadNamespaceMetadata(NS);\n \n-    Assert.assertEquals(\n-        \"Namespace should have expected location\",\n-        \"file:\" + location.getPath(),\n-        nsMetadata.get(\"location\"));\n+    assertThat(nsMetadata).containsEntry(\"location\", \"file:\" + location.getPath());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetProperties() {\n-    Assume.assumeFalse(\"HadoopCatalog does not support namespace metadata\", isHadoopCatalog);\n+    assumeThat(isHadoopCatalog).as(\"HadoopCatalog does not support namespace metadata\").isFalse();\n \n-    Assert.assertFalse(\n-        \"Namespace should not already exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should not already exist\")\n+        .isFalse();\n \n     sql(\"CREATE NAMESPACE %s\", fullNamespace);\n \n-    Assert.assertTrue(\"Namespace should exist\", validationNamespaceCatalog.namespaceExists(NS));\n+    assertThat(validationNamespaceCatalog.namespaceExists(NS))\n+        .as(\"Namespace should exist\")\n+        .isTrue();\n \n     Map<String, String> defaultMetadata = validationNamespaceCatalog.loadNamespaceMetadata(NS);\n-    Assert.assertFalse(\n-        \"Default metadata should not have custom property\", defaultMetadata.containsKey(\"prop\"));\n+    assertThat(defaultMetadata)\n+        .as(\"Default metadata should not have custom property\")\n+        .doesNotContainKey(\"prop\");\n \n     sql(\"ALTER NAMESPACE %s SET PROPERTIES ('prop'='value')\", fullNamespace);\n \n     Map<String, String> nsMetadata = validationNamespaceCatalog.loadNamespaceMetadata(NS);\n \n-    Assert.assertEquals(\n-        \"Namespace should have expected prop value\", \"value\", nsMetadata.get(\"prop\"));\n+    assertThat(nsMetadata).containsEntry(\"prop\", \"value\");\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestRefreshTable.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestRefreshTable.java\nindex ccbad7311afd..426c68447724 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestRefreshTable.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestRefreshTable.java\n@@ -19,35 +19,33 @@\n package org.apache.iceberg.spark.sql;\n \n import java.util.List;\n-import java.util.Map;\n import java.util.Set;\n import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.SparkCatalogConfig;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestRefreshTable extends SparkCatalogTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRefreshTable extends CatalogTestBase {\n \n-  public TestRefreshTable(String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Before\n+  @BeforeEach\n   public void createTables() {\n     sql(\"CREATE TABLE %s (key int, value int) USING iceberg\", tableName);\n     sql(\"INSERT INTO %s VALUES (1,1)\", tableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRefreshCommand() {\n     // We are not allowed to change the session catalog after it has been initialized, so build a\n     // new one\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSelect.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSelect.java\nindex 376a89f56261..ab151398204e 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSelect.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestSelect.java\n@@ -18,37 +18,69 @@\n  */\n package org.apache.iceberg.spark.sql;\n \n+import static org.apache.iceberg.TableProperties.SPLIT_SIZE;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.text.SimpleDateFormat;\n+import java.util.Arrays;\n import java.util.Date;\n import java.util.List;\n-import java.util.Map;\n import java.util.concurrent.TimeUnit;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.events.Listeners;\n import org.apache.iceberg.events.ScanEvent;\n import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.expressions.Expressions;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.Spark3Util;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.SparkCatalogConfig;\n import org.apache.iceberg.spark.SparkReadOptions;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSelect extends SparkCatalogTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSelect extends CatalogTestBase {\n   private int scanEventCount = 0;\n   private ScanEvent lastScanEvent = null;\n-  private String binaryTableName = tableName(\"binary_table\");\n \n-  public TestSelect(String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n+  @Parameter(index = 3)\n+  private String binaryTableName;\n+\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}, binaryTableName = {3}\")\n+  protected static Object[][] parameters() {\n+    return new Object[][] {\n+      {\n+        SparkCatalogConfig.HIVE.catalogName(),\n+        SparkCatalogConfig.HIVE.implementation(),\n+        SparkCatalogConfig.HIVE.properties(),\n+        SparkCatalogConfig.HIVE.catalogName() + \".default.binary_table\"\n+      },\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties(),\n+        SparkCatalogConfig.HADOOP.catalogName() + \".default.binary_table\"\n+      },\n+      {\n+        SparkCatalogConfig.SPARK.catalogName(),\n+        SparkCatalogConfig.SPARK.implementation(),\n+        SparkCatalogConfig.SPARK.properties(),\n+        \"default.binary_table\"\n+      }\n+    };\n+  }\n \n+  @BeforeEach\n+  public void createTables() {\n     // register a scan event listener to validate pushdown\n     Listeners.register(\n         event -> {\n@@ -56,10 +88,7 @@ public TestSelect(String catalogName, String implementation, Map<String, String>\n           lastScanEvent = event;\n         },\n         ScanEvent.class);\n-  }\n \n-  @Before\n-  public void createTables() {\n     sql(\"CREATE TABLE %s (id bigint, data string, float float) USING iceberg\", tableName);\n     sql(\"INSERT INTO %s VALUES (1, 'a', 1.0), (2, 'b', 2.0), (3, 'c', float('NaN'))\", tableName);\n \n@@ -67,13 +96,13 @@ public void createTables() {\n     this.lastScanEvent = null;\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS %s\", binaryTableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSelect() {\n     List<Object[]> expected =\n         ImmutableList.of(row(1L, \"a\", 1.0F), row(2L, \"b\", 2.0F), row(3L, \"c\", Float.NaN));\n@@ -81,7 +110,32 @@ public void testSelect() {\n     assertEquals(\"Should return all expected rows\", expected, sql(\"SELECT * FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n+  public void testSelectWithSpecifiedTargetSplitSize() {\n+    List<Object[]> expected =\n+        ImmutableList.of(row(1L, \"a\", 1.0F), row(2L, \"b\", 2.0F), row(3L, \"c\", Float.NaN));\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    table.updateProperties().set(\"read.split.target-size\", \"1024\").commit();\n+    spark.sql(\"REFRESH TABLE \" + tableName);\n+    assertEquals(\"Should return all expected rows\", expected, sql(\"SELECT * FROM %s\", tableName));\n+\n+    // Query failed when `SPLIT_SIZE` < 0\n+    table.updateProperties().set(SPLIT_SIZE, \"-1\").commit();\n+    spark.sql(\"REFRESH TABLE \" + tableName);\n+    assertThatThrownBy(() -> sql(\"SELECT * FROM %s\", tableName))\n+        .hasMessageContaining(\"Split size must be > 0: -1\")\n+        .isInstanceOf(IllegalArgumentException.class);\n+\n+    // Query failed when `SPLIT_SIZE` == 0\n+    table.updateProperties().set(SPLIT_SIZE, \"0\").commit();\n+    spark.sql(\"REFRESH TABLE \" + tableName);\n+    assertThatThrownBy(() -> sql(\"SELECT * FROM %s\", tableName))\n+        .hasMessageContaining(\"Split size must be > 0: 0\")\n+        .isInstanceOf(IllegalArgumentException.class);\n+  }\n+\n+  @TestTemplate\n   public void testSelectRewrite() {\n     List<Object[]> expected = ImmutableList.of(row(3L, \"c\", Float.NaN));\n \n@@ -90,29 +144,28 @@ public void testSelectRewrite() {\n         expected,\n         sql(\"SELECT * FROM %s where float = float('NaN')\", tableName));\n \n-    Assert.assertEquals(\"Should create only one scan\", 1, scanEventCount);\n-    Assert.assertEquals(\n-        \"Should push down expected filter\",\n-        \"(float IS NOT NULL AND is_nan(float))\",\n-        Spark3Util.describe(lastScanEvent.filter()));\n+    assertThat(scanEventCount).as(\"Should create only one scan\").isEqualTo(1);\n+    assertThat(Spark3Util.describe(lastScanEvent.filter()))\n+        .as(\"Should push down expected filter\")\n+        .isEqualTo(\"(float IS NOT NULL AND is_nan(float))\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testProjection() {\n     List<Object[]> expected = ImmutableList.of(row(1L), row(2L), row(3L));\n \n     assertEquals(\"Should return all expected rows\", expected, sql(\"SELECT id FROM %s\", tableName));\n \n-    Assert.assertEquals(\"Should create only one scan\", 1, scanEventCount);\n-    Assert.assertEquals(\n-        \"Should not push down a filter\", Expressions.alwaysTrue(), lastScanEvent.filter());\n-    Assert.assertEquals(\n-        \"Should project only the id column\",\n-        validationCatalog.loadTable(tableIdent).schema().select(\"id\").asStruct(),\n-        lastScanEvent.projection().asStruct());\n+    assertThat(scanEventCount).as(\"Should create only one scan\").isEqualTo(1);\n+    assertThat(lastScanEvent.filter())\n+        .as(\"Should not push down a filter\")\n+        .isEqualTo(Expressions.alwaysTrue());\n+    assertThat(lastScanEvent.projection().asStruct())\n+        .as(\"Should project only the id column\")\n+        .isEqualTo(validationCatalog.loadTable(tableIdent).schema().select(\"id\").asStruct());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpressionPushdown() {\n     List<Object[]> expected = ImmutableList.of(row(\"b\"));\n \n@@ -121,18 +174,17 @@ public void testExpressionPushdown() {\n         expected,\n         sql(\"SELECT data FROM %s WHERE id = 2\", tableName));\n \n-    Assert.assertEquals(\"Should create only one scan\", 1, scanEventCount);\n-    Assert.assertEquals(\n-        \"Should push down expected filter\",\n-        \"(id IS NOT NULL AND id = 2)\",\n-        Spark3Util.describe(lastScanEvent.filter()));\n-    Assert.assertEquals(\n-        \"Should project only id and data columns\",\n-        validationCatalog.loadTable(tableIdent).schema().select(\"id\", \"data\").asStruct(),\n-        lastScanEvent.projection().asStruct());\n+    assertThat(scanEventCount).as(\"Should create only one scan\").isEqualTo(1);\n+    assertThat(Spark3Util.describe(lastScanEvent.filter()))\n+        .as(\"Should push down expected filter\")\n+        .isEqualTo(\"(id IS NOT NULL AND id = 2)\");\n+    assertThat(lastScanEvent.projection().asStruct())\n+        .as(\"Should project only id and data columns\")\n+        .isEqualTo(\n+            validationCatalog.loadTable(tableIdent).schema().select(\"id\", \"data\").asStruct());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMetadataTables() {\n     assertEquals(\n         \"Snapshot metadata table\",\n@@ -140,7 +192,7 @@ public void testMetadataTables() {\n         sql(\"SELECT * FROM %s.snapshots\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSnapshotInTableName() {\n     // get the snapshot ID of the last write and get the current row set as expected\n     long snapshotId = validationCatalog.loadTable(tableIdent).currentSnapshot().snapshotId();\n@@ -165,7 +217,7 @@ public void testSnapshotInTableName() {\n     assertEquals(\"Snapshot at specific ID \" + snapshotId, expected, fromDF);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimestampInTableName() {\n     // get a timestamp just after the last write and get the current row set as expected\n     long snapshotTs = validationCatalog.loadTable(tableIdent).currentSnapshot().timestampMillis();\n@@ -191,7 +243,7 @@ public void testTimestampInTableName() {\n     assertEquals(\"Snapshot at timestamp \" + timestamp, expected, fromDF);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testVersionAsOf() {\n     // get the snapshot ID of the last write and get the current row set as expected\n     long snapshotId = validationCatalog.loadTable(tableIdent).currentSnapshot().snapshotId();\n@@ -221,7 +273,7 @@ public void testVersionAsOf() {\n     assertEquals(\"Snapshot at specific ID \" + snapshotId, expected, fromDF);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTagReference() {\n     Table table = validationCatalog.loadTable(tableIdent);\n     long snapshotId = table.currentSnapshot().snapshotId();\n@@ -252,7 +304,7 @@ public void testTagReference() {\n     assertEquals(\"Snapshot at specific tag reference name\", expected, fromDF);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUseSnapshotIdForTagReferenceAsOf() {\n     Table table = validationCatalog.loadTable(tableIdent);\n     long snapshotId1 = table.currentSnapshot().snapshotId();\n@@ -277,7 +329,65 @@ public void testUseSnapshotIdForTagReferenceAsOf() {\n     assertEquals(\"Snapshot at specific tag reference name\", actual, travelWithLongResult);\n   }\n \n-  @Test\n+  @TestTemplate\n+  public void readAndWriteWithBranchAfterSchemaChange() {\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    String branchName = \"test_branch\";\n+    table.manageSnapshots().createBranch(branchName, table.currentSnapshot().snapshotId()).commit();\n+\n+    List<Object[]> expected =\n+        Arrays.asList(row(1L, \"a\", 1.0f), row(2L, \"b\", 2.0f), row(3L, \"c\", Float.NaN));\n+    assertThat(sql(\"SELECT * FROM %s\", tableName)).containsExactlyElementsOf(expected);\n+\n+    // change schema on the table and add more data\n+    sql(\"ALTER TABLE %s DROP COLUMN float\", tableName);\n+    sql(\"ALTER TABLE %s ADD COLUMN new_col date\", tableName);\n+    sql(\n+        \"INSERT INTO %s VALUES (4, 'd', date('2024-04-04')), (5, 'e', date('2024-05-05'))\",\n+        tableName);\n+\n+    // time-travel query using snapshot id should return the snapshot's schema\n+    long branchSnapshotId = table.refs().get(branchName).snapshotId();\n+    assertThat(sql(\"SELECT * FROM %s VERSION AS OF %s\", tableName, branchSnapshotId))\n+        .containsExactlyElementsOf(expected);\n+\n+    // querying the head of the branch should return the table's schema\n+    assertThat(sql(\"SELECT * FROM %s VERSION AS OF '%s'\", tableName, branchName))\n+        .containsExactly(row(1L, \"a\", null), row(2L, \"b\", null), row(3L, \"c\", null));\n+\n+    if (!\"spark_catalog\".equals(catalogName)) {\n+      // querying the head of the branch using 'branch_' should return the table's schema\n+      assertThat(sql(\"SELECT * FROM %s.branch_%s\", tableName, branchName))\n+          .containsExactly(row(1L, \"a\", null), row(2L, \"b\", null), row(3L, \"c\", null));\n+    }\n+\n+    // writing to a branch uses the table's schema\n+    sql(\n+        \"INSERT INTO %s.branch_%s VALUES (6L, 'f', cast('2023-06-06' as date)), (7L, 'g', cast('2023-07-07' as date))\",\n+        tableName, branchName);\n+\n+    // querying the head of the branch returns the table's schema\n+    assertThat(sql(\"SELECT * FROM %s VERSION AS OF '%s'\", tableName, branchName))\n+        .containsExactlyInAnyOrder(\n+            row(1L, \"a\", null),\n+            row(2L, \"b\", null),\n+            row(3L, \"c\", null),\n+            row(6L, \"f\", java.sql.Date.valueOf(\"2023-06-06\")),\n+            row(7L, \"g\", java.sql.Date.valueOf(\"2023-07-07\")));\n+\n+    // using DataFrameReader with the 'branch' option should return the table's schema\n+    Dataset<Row> df =\n+        spark.read().format(\"iceberg\").option(SparkReadOptions.BRANCH, branchName).load(tableName);\n+    assertThat(rowsToJava(df.collectAsList()))\n+        .containsExactlyInAnyOrder(\n+            row(1L, \"a\", null),\n+            row(2L, \"b\", null),\n+            row(3L, \"c\", null),\n+            row(6L, \"f\", java.sql.Date.valueOf(\"2023-06-06\")),\n+            row(7L, \"g\", java.sql.Date.valueOf(\"2023-07-07\")));\n+  }\n+\n+  @TestTemplate\n   public void testBranchReference() {\n     Table table = validationCatalog.loadTable(tableIdent);\n     long snapshotId = table.currentSnapshot().snapshotId();\n@@ -313,14 +423,14 @@ public void testBranchReference() {\n     assertEquals(\"Snapshot at specific branch reference name\", expected, fromDF);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnknownReferenceAsOf() {\n     assertThatThrownBy(() -> sql(\"SELECT * FROM %s VERSION AS OF 'test_unknown'\", tableName))\n         .hasMessageContaining(\"Cannot find matching snapshot ID or reference name for version\")\n         .isInstanceOf(ValidationException.class);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimestampAsOf() {\n     long snapshotTs = validationCatalog.loadTable(tableIdent).currentSnapshot().timestampMillis();\n     long timestamp = waitUntilAfter(snapshotTs + 1000);\n@@ -367,7 +477,7 @@ public void testTimestampAsOf() {\n     assertEquals(\"Snapshot at timestamp \" + timestamp, expected, fromDF);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidTimeTravelBasedOnBothAsOfAndTableIdentifier() {\n     // get the snapshot ID of the last write\n     long snapshotId = validationCatalog.loadTable(tableIdent).currentSnapshot().snapshotId();\n@@ -422,7 +532,7 @@ public void testInvalidTimeTravelBasedOnBothAsOfAndTableIdentifier() {\n         .hasMessage(\"Cannot do time-travel based on both table identifier and AS OF\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidTimeTravelAgainstBranchIdentifierWithAsOf() {\n     long snapshotId = validationCatalog.loadTable(tableIdent).currentSnapshot().snapshotId();\n     validationCatalog.loadTable(tableIdent).manageSnapshots().createBranch(\"b1\").commit();\n@@ -442,7 +552,7 @@ public void testInvalidTimeTravelAgainstBranchIdentifierWithAsOf() {\n         .hasMessage(\"Cannot do time-travel based on both table identifier and AS OF\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSpecifySnapshotAndTimestamp() {\n     // get the snapshot ID of the last write\n     long snapshotId = validationCatalog.loadTable(tableIdent).currentSnapshot().snapshotId();\n@@ -470,7 +580,7 @@ public void testSpecifySnapshotAndTimestamp() {\n                 snapshotId, timestamp));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinaryInFilter() {\n     sql(\"CREATE TABLE %s (id bigint, binary binary) USING iceberg\", binaryTableName);\n     sql(\"INSERT INTO %s VALUES (1, X''), (2, X'1111'), (3, X'11')\", binaryTableName);\n@@ -482,7 +592,7 @@ public void testBinaryInFilter() {\n         sql(\"SELECT id, binary FROM %s where binary > X'11'\", binaryTableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComplexTypeFilter() {\n     String complexTypeTableName = tableName(\"complex_table\");\n     sql(\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestAggregatePushDown.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestAggregatePushDown.java\nindex 6e09252704a1..5ce56b4feca7 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestAggregatePushDown.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestAggregatePushDown.java\n@@ -27,6 +27,7 @@\n import java.util.List;\n import java.util.Locale;\n import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.exceptions.AlreadyExistsException;\n@@ -45,7 +46,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestAggregatePushDown extends CatalogTestBase {\n \n   @BeforeAll\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java\nindex 39aeacf68b9a..7fb1fda3364f 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java\n@@ -28,6 +28,7 @@\n import java.nio.file.Files;\n import java.util.UUID;\n import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -42,7 +43,9 @@\n import org.apache.spark.sql.connector.catalog.TableCatalog;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestCreateTable extends CatalogTestBase {\n \n   @AfterEach\n@@ -109,10 +112,8 @@ public void testCreateTable() {\n     assertThat(table.schema().asStruct())\n         .as(\"Should have the expected schema\")\n         .isEqualTo(expectedSchema);\n-    assertThat(table.spec().fields()).as(\"Should not be partitioned\").hasSize(0);\n-    assertThat(table.properties().get(TableProperties.DEFAULT_FILE_FORMAT))\n-        .as(\"Should not have the default format set\")\n-        .isNull();\n+    assertThat(table.spec().fields()).as(\"Should not be partitioned\").isEmpty();\n+    assertThat(table.properties()).doesNotContainKey(TableProperties.DEFAULT_FILE_FORMAT);\n   }\n \n   @TestTemplate\n@@ -134,7 +135,7 @@ public void testCreateTablePartitionedByUUID() {\n \n     sql(\"INSERT INTO %s VALUES('%s')\", tableName, uuid);\n \n-    assertThat(sql(\"SELECT uuid FROM %s\", tableName)).hasSize(1).element(0).isEqualTo(row(uuid));\n+    assertThat(sql(\"SELECT uuid FROM %s\", tableName)).singleElement().isEqualTo(row(uuid));\n   }\n \n   @TestTemplate\n@@ -172,10 +173,8 @@ public void testCreateTableUsingParquet() {\n     assertThat(table.schema().asStruct())\n         .as(\"Should have the expected schema\")\n         .isEqualTo(expectedSchema);\n-    assertThat(table.spec().fields()).as(\"Should not be partitioned\").hasSize(0);\n-    assertThat(table.properties().get(TableProperties.DEFAULT_FILE_FORMAT))\n-        .as(\"Should not have default format parquet\")\n-        .isEqualTo(\"parquet\");\n+    assertThat(table.spec().fields()).as(\"Should not be partitioned\").isEmpty();\n+    assertThat(table.properties()).containsEntry(TableProperties.DEFAULT_FILE_FORMAT, \"parquet\");\n \n     assertThatThrownBy(\n             () ->\n@@ -219,10 +218,7 @@ public void testCreateTablePartitionedBy() {\n             .day(\"created_at\")\n             .build();\n     assertThat(table.spec()).as(\"Should be partitioned correctly\").isEqualTo(expectedSpec);\n-\n-    assertThat(table.properties().get(TableProperties.DEFAULT_FILE_FORMAT))\n-        .as(\"Should not have the default format set\")\n-        .isNull();\n+    assertThat(table.properties()).doesNotContainKey(TableProperties.DEFAULT_FILE_FORMAT);\n   }\n \n   @TestTemplate\n@@ -247,10 +243,8 @@ public void testCreateTableColumnComments() {\n     assertThat(table.schema().asStruct())\n         .as(\"Should have the expected schema\")\n         .isEqualTo(expectedSchema);\n-    assertThat(table.spec().fields()).as(\"Should not be partitioned\").hasSize(0);\n-    assertThat(table.properties().get(TableProperties.DEFAULT_FILE_FORMAT))\n-        .as(\"Should not have the default format set\")\n-        .isNull();\n+    assertThat(table.spec().fields()).as(\"Should not be partitioned\").isEmpty();\n+    assertThat(table.properties()).doesNotContainKey(TableProperties.DEFAULT_FILE_FORMAT);\n   }\n \n   @TestTemplate\n@@ -276,13 +270,10 @@ public void testCreateTableComment() {\n     assertThat(table.schema().asStruct())\n         .as(\"Should have the expected schema\")\n         .isEqualTo(expectedSchema);\n-    assertThat(table.spec().fields()).as(\"Should not be partitioned\").hasSize(0);\n-    assertThat(table.properties().get(TableProperties.DEFAULT_FILE_FORMAT))\n-        .as(\"Should not have the default format set\")\n-        .isNull();\n-    assertThat(table.properties().get(TableCatalog.PROP_COMMENT))\n-        .as(\"Should have the table comment set in properties\")\n-        .isEqualTo(\"Table doc\");\n+    assertThat(table.spec().fields()).as(\"Should not be partitioned\").isEmpty();\n+    assertThat(table.properties())\n+        .doesNotContainKey(TableProperties.DEFAULT_FILE_FORMAT)\n+        .containsEntry(TableCatalog.PROP_COMMENT, \"Table doc\");\n   }\n \n   @TestTemplate\n@@ -317,10 +308,8 @@ public void testCreateTableLocation() throws Exception {\n     assertThat(table.schema().asStruct())\n         .as(\"Should have the expected schema\")\n         .isEqualTo(expectedSchema);\n-    assertThat(table.spec().fields()).as(\"Should not be partitioned\").hasSize(0);\n-    assertThat(table.properties().get(TableProperties.DEFAULT_FILE_FORMAT))\n-        .as(\"Should not have the default format set\")\n-        .isNull();\n+    assertThat(table.spec().fields()).as(\"Should not be partitioned\").isEmpty();\n+    assertThat(table.properties()).doesNotContainKey(TableProperties.DEFAULT_FILE_FORMAT);\n     assertThat(table.location()).as(\"Should have a custom table location\").isEqualTo(location);\n   }\n \n@@ -347,7 +336,7 @@ public void testCreateTableProperties() {\n     assertThat(table.schema().asStruct())\n         .as(\"Should have the expected schema\")\n         .isEqualTo(expectedSchema);\n-    assertThat(table.spec().fields()).as(\"Should not be partitioned\").hasSize(0);\n+    assertThat(table.spec().fields()).as(\"Should not be partitioned\").isEmpty();\n     assertThat(table.properties()).containsEntry(\"p1\", \"2\").containsEntry(\"p2\", \"x\");\n   }\n \n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java\nindex 4098a155be0d..1e04eba0b98a 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTableAsSelect.java\n@@ -24,6 +24,7 @@\n import static org.assertj.core.api.Assertions.assertThat;\n \n import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n@@ -34,7 +35,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestCreateTableAsSelect extends CatalogTestBase {\n \n   @Parameter(index = 3)\n@@ -92,8 +95,7 @@ public void testUnpartitionedCTAS() {\n     assertThat(ctasTable.schema().asStruct())\n         .as(\"Should have expected nullable schema\")\n         .isEqualTo(expectedSchema.asStruct());\n-\n-    assertThat(ctasTable.spec().fields()).as(\"Should be an unpartitioned table\").hasSize(0);\n+    assertThat(ctasTable.spec().fields()).as(\"Should be an unpartitioned table\").isEmpty();\n     assertEquals(\n         \"Should have rows matching the source table\",\n         sql(\"SELECT * FROM %s ORDER BY id\", sourceName),\n@@ -194,15 +196,10 @@ public void testRTAS() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n \n     assertThat(rtasTable.snapshots()).as(\"Table should have expected snapshots\").hasSize(2);\n-    assertThat(rtasTable.properties().get(\"prop1\"))\n-        .as(\"Should have updated table property\")\n-        .isEqualTo(\"newval1\");\n-    assertThat(rtasTable.properties().get(\"prop2\"))\n-        .as(\"Should have preserved table property\")\n-        .isEqualTo(\"val2\");\n-    assertThat(rtasTable.properties().get(\"prop3\"))\n-        .as(\"Should have new table property\")\n-        .isEqualTo(\"val3\");\n+    assertThat(rtasTable.properties())\n+        .containsEntry(\"prop1\", \"newval1\")\n+        .containsEntry(\"prop2\", \"val2\")\n+        .containsEntry(\"prop3\", \"val3\");\n   }\n \n   @TestTemplate\n@@ -272,7 +269,7 @@ public void testDataFrameV2Create() throws Exception {\n     assertThat(ctasTable.schema().asStruct())\n         .as(\"Should have expected nullable schema\")\n         .isEqualTo(expectedSchema.asStruct());\n-    assertThat(ctasTable.spec().fields()).as(\"Should be an unpartitioned table\").hasSize(0);\n+    assertThat(ctasTable.spec().fields()).as(\"Should be an unpartitioned table\").isEmpty();\n     assertEquals(\n         \"Should have rows matching the source table\",\n         sql(\"SELECT * FROM %s ORDER BY id\", sourceName),\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestDeleteFrom.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestDeleteFrom.java\nindex 7706c5aad4de..bd4a41593c34 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestDeleteFrom.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestDeleteFrom.java\n@@ -22,6 +22,7 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n@@ -32,7 +33,9 @@\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestDeleteFrom extends CatalogTestBase {\n   @AfterEach\n   public void removeTables() {\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestDropTable.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestDropTable.java\nindex ec8308e1c772..07faae52749b 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestDropTable.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestDropTable.java\n@@ -27,6 +27,7 @@\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n@@ -34,7 +35,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestDropTable extends CatalogTestBase {\n \n   @BeforeEach\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestNamespaceSQL.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestNamespaceSQL.java\nindex 0ba480692523..dd17e7c41763 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestNamespaceSQL.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestNamespaceSQL.java\n@@ -28,17 +28,18 @@\n import java.util.Set;\n import java.util.stream.Collectors;\n import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.exceptions.NamespaceNotEmptyException;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.SparkCatalogConfig;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestNamespaceSQL extends CatalogTestBase {\n   private static final Namespace NS = Namespace.of(\"db\");\n \n@@ -102,9 +103,12 @@ public void testDefaultNamespace() {\n \n     sql(\"USE %s\", catalogName);\n \n-    Object[] current = Iterables.getOnlyElement(sql(\"SHOW CURRENT NAMESPACE\"));\n-    assertThat(current[0]).as(\"Should use the current catalog\").isEqualTo(catalogName);\n-    assertThat(current[1]).as(\"Should use the configured default namespace\").isEqualTo(\"default\");\n+    assertThat(sql(\"SHOW CURRENT NAMESPACE\"))\n+        .singleElement()\n+        .satisfies(\n+            ns -> {\n+              assertThat(ns).containsExactly(catalogName, \"default\");\n+            });\n   }\n \n   @TestTemplate\n@@ -164,13 +168,16 @@ public void testListTables() {\n         .isTrue();\n \n     List<Object[]> rows = sql(\"SHOW TABLES IN %s\", fullNamespace);\n-    assertThat(rows).as(\"Should not list any tables\").hasSize(0);\n+    assertThat(rows).as(\"Should not list any tables\").isEmpty();\n \n     sql(\"CREATE TABLE %s.table (id bigint) USING iceberg\", fullNamespace);\n \n-    Object[] row = Iterables.getOnlyElement(sql(\"SHOW TABLES IN %s\", fullNamespace));\n-    assertThat(row[0]).as(\"Namespace should match\").isEqualTo(\"db\");\n-    assertThat(row[1]).as(\"Table name should match\").isEqualTo(\"table\");\n+    assertThat(sql(\"SHOW TABLES IN %s\", fullNamespace))\n+        .singleElement()\n+        .satisfies(\n+            row -> {\n+              assertThat(row).containsExactly(\"db\", \"table\", false);\n+            });\n   }\n \n   @TestTemplate\n@@ -188,26 +195,23 @@ public void testListNamespace() {\n     List<Object[]> namespaces = sql(\"SHOW NAMESPACES IN %s\", catalogName);\n \n     if (isHadoopCatalog) {\n-      assertThat(namespaces).as(\"Should have 1 namespace\").hasSize(1);\n-      Set<String> namespaceNames =\n-          namespaces.stream().map(arr -> arr[0].toString()).collect(Collectors.toSet());\n-      assertThat(namespaceNames)\n-          .as(\"Should have only db namespace\")\n-          .isEqualTo(ImmutableSet.of(\"db\"));\n+      assertThat(namespaces)\n+          .singleElement()\n+          .satisfies(\n+              ns -> {\n+                assertThat(ns).containsExactly(\"db\");\n+              });\n     } else {\n       assertThat(namespaces).as(\"Should have 2 namespaces\").hasSize(2);\n       Set<String> namespaceNames =\n           namespaces.stream().map(arr -> arr[0].toString()).collect(Collectors.toSet());\n       assertThat(namespaceNames)\n           .as(\"Should have default and db namespaces\")\n-          .isEqualTo(ImmutableSet.of(\"default\", \"db\"));\n+          .containsExactlyInAnyOrder(\"default\", \"db\");\n     }\n \n     List<Object[]> nestedNamespaces = sql(\"SHOW NAMESPACES IN %s\", fullNamespace);\n-\n-    Set<String> nestedNames =\n-        nestedNamespaces.stream().map(arr -> arr[0].toString()).collect(Collectors.toSet());\n-    assertThat(nestedNames).as(\"Should not have nested namespaces\").isEmpty();\n+    assertThat(nestedNamespaces).as(\"Should not have nested namespaces\").isEmpty();\n   }\n \n   @TestTemplate\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestRefreshTable.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestRefreshTable.java\nindex fe13d61db066..426c68447724 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestRefreshTable.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestRefreshTable.java\n@@ -21,6 +21,7 @@\n import java.util.List;\n import java.util.Set;\n import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.spark.CatalogTestBase;\n@@ -28,7 +29,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestRefreshTable extends CatalogTestBase {\n \n   @BeforeEach\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSelect.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSelect.java\nindex afc3256b7c9d..db5c6216a0cc 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSelect.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestSelect.java\n@@ -28,6 +28,7 @@\n import java.util.List;\n import java.util.concurrent.TimeUnit;\n import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.events.Listeners;\n@@ -44,7 +45,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestSelect extends CatalogTestBase {\n   private int scanEventCount = 0;\n   private ScanEvent lastScanEvent = null;\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12901",
    "pr_id": 12901,
    "issue_id": 12875,
    "repo": "apache/iceberg",
    "problem_statement": "Nessie should throw a NoSuchNamespaceException when listing a non-existing namespace\n### Feature Request / Improvement\n\nOnce this is implemented, `TestNesiseCatalog.testListNonExistingNamespace()` can be enabled again\n\n### Query engine\n\nNone\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [x] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 62,
    "test_files_count": 4,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "nessie/src/main/java/org/apache/iceberg/nessie/NessieIcebergClient.java",
      "nessie/src/test/java/org/apache/iceberg/nessie/TestBranchVisibility.java",
      "nessie/src/test/java/org/apache/iceberg/nessie/TestMultipleClients.java",
      "nessie/src/test/java/org/apache/iceberg/nessie/TestNessieCatalog.java",
      "nessie/src/test/java/org/apache/iceberg/nessie/TestNessieIcebergClient.java"
    ],
    "pr_changed_test_files": [
      "nessie/src/test/java/org/apache/iceberg/nessie/TestBranchVisibility.java",
      "nessie/src/test/java/org/apache/iceberg/nessie/TestMultipleClients.java",
      "nessie/src/test/java/org/apache/iceberg/nessie/TestNessieCatalog.java",
      "nessie/src/test/java/org/apache/iceberg/nessie/TestNessieIcebergClient.java"
    ],
    "base_commit": "97c0e136b8021058897cab7539e3ef89ce5a0341",
    "head_commit": "d4444f931f40ba6506c66dc5b6e251e6a0ef4590",
    "repo_url": "https://github.com/apache/iceberg/pull/12901",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12901",
    "dockerfile": "",
    "pr_merged_at": "2025-05-14T06:35:56.000Z",
    "patch": "diff --git a/nessie/src/main/java/org/apache/iceberg/nessie/NessieIcebergClient.java b/nessie/src/main/java/org/apache/iceberg/nessie/NessieIcebergClient.java\nindex f51aa62d02a6..d901ffd5572f 100644\n--- a/nessie/src/main/java/org/apache/iceberg/nessie/NessieIcebergClient.java\n+++ b/nessie/src/main/java/org/apache/iceberg/nessie/NessieIcebergClient.java\n@@ -270,6 +270,16 @@ public List<Namespace> listNamespaces(Namespace namespace) throws NoSuchNamespac\n       } else {\n         org.projectnessie.model.Namespace root =\n             org.projectnessie.model.Namespace.of(namespace.levels());\n+        Content existing =\n+            api.getContent()\n+                .reference(getReference())\n+                .key(root.toContentKey())\n+                .get()\n+                .get(root.toContentKey());\n+        if (existing == null) {\n+          throw new NoSuchNamespaceException(\"Namespace does not exist: %s\", namespace);\n+        }\n+\n         filter +=\n             String.format(\n                 Locale.ROOT,\n",
    "test_patch": "diff --git a/nessie/src/test/java/org/apache/iceberg/nessie/TestBranchVisibility.java b/nessie/src/test/java/org/apache/iceberg/nessie/TestBranchVisibility.java\nindex 00aa8458bf5a..4f1b87022f47 100644\n--- a/nessie/src/test/java/org/apache/iceberg/nessie/TestBranchVisibility.java\n+++ b/nessie/src/test/java/org/apache/iceberg/nessie/TestBranchVisibility.java\n@@ -36,6 +36,7 @@\n import org.apache.iceberg.avro.AvroSchemaUtil;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.Types;\n@@ -455,7 +456,9 @@ public void testWithRefAndHash() throws NessieConflictException, NessieNotFoundE\n     String hashBeforeNamespaceCreation = api.getReference().refName(testBranch).get().getHash();\n     Namespace namespaceA = Namespace.of(\"a\");\n     Namespace namespaceAB = Namespace.of(\"a\", \"b\");\n-    assertThat(nessieCatalog.listNamespaces(namespaceAB)).isEmpty();\n+    assertThatThrownBy(() -> nessieCatalog.listNamespaces(namespaceAB))\n+        .isInstanceOf(NoSuchNamespaceException.class)\n+        .hasMessage(\"Namespace does not exist: %s\", namespaceAB);\n \n     createMissingNamespaces(\n         nessieCatalog, Namespace.of(Arrays.copyOf(namespaceAB.levels(), namespaceAB.length() - 1)));\n@@ -465,7 +468,9 @@ public void testWithRefAndHash() throws NessieConflictException, NessieNotFoundE\n     assertThat(nessieCatalog.listTables(namespaceAB)).isEmpty();\n \n     NessieCatalog catalogAtHash1 = initCatalog(testBranch, hashBeforeNamespaceCreation);\n-    assertThat(catalogAtHash1.listNamespaces(namespaceAB)).isEmpty();\n+    assertThatThrownBy(() -> catalogAtHash1.listNamespaces(namespaceAB))\n+        .isInstanceOf(NoSuchNamespaceException.class)\n+        .hasMessage(\"Namespace does not exist: %s\", namespaceAB);\n     assertThat(catalogAtHash1.listTables(namespaceAB)).isEmpty();\n \n     TableIdentifier identifier = TableIdentifier.of(namespaceAB, \"table\");\n@@ -486,10 +491,10 @@ public void testWithRefAndHash() throws NessieConflictException, NessieNotFoundE\n     assertThat(catalogAtHash2.listTables(namespaceAB)).isEmpty();\n \n     // updates should be still possible here\n-    nessieCatalog = initCatalog(testBranch);\n+    NessieCatalog nessieCatalog2 = initCatalog(testBranch);\n     TableIdentifier identifier2 = TableIdentifier.of(namespaceAB, \"table2\");\n     nessieCatalog.createTable(identifier2, schema);\n-    assertThat(nessieCatalog.listTables(namespaceAB)).hasSize(2);\n+    assertThat(nessieCatalog2.listTables(namespaceAB)).hasSize(2);\n   }\n \n   @Test\n\ndiff --git a/nessie/src/test/java/org/apache/iceberg/nessie/TestMultipleClients.java b/nessie/src/test/java/org/apache/iceberg/nessie/TestMultipleClients.java\nindex 49b721d0e5f4..883dc9d2f0ce 100644\n--- a/nessie/src/test/java/org/apache/iceberg/nessie/TestMultipleClients.java\n+++ b/nessie/src/test/java/org/apache/iceberg/nessie/TestMultipleClients.java\n@@ -76,9 +76,15 @@ public void testListNamespaces() throws NessieConflictException, NessieNotFoundE\n     assertThat(catalog.listNamespaces()).isEmpty();\n     assertThat(anotherCatalog.listNamespaces()).isEmpty();\n \n-    // listing a non-existent namespace should return empty\n-    assertThat(catalog.listNamespaces(Namespace.of(\"db1\"))).isEmpty();\n-    assertThat(anotherCatalog.listNamespaces(Namespace.of(\"db1\"))).isEmpty();\n+    // listing a non-existent namespace should throw an NoSuchNamespaceExists exception\n+    Namespace namespaceDb1 = Namespace.of(\"db1\");\n+    assertThatThrownBy(() -> catalog.listNamespaces(namespaceDb1))\n+        .isInstanceOf(NoSuchNamespaceException.class)\n+        .hasMessage(\"Namespace does not exist: %s\", namespaceDb1);\n+\n+    assertThatThrownBy(() -> anotherCatalog.listNamespaces(namespaceDb1))\n+        .isInstanceOf(NoSuchNamespaceException.class)\n+        .hasMessage(\"Namespace does not exist: %s\", namespaceDb1);\n \n     catalog.createNamespace(Namespace.of(\"db1\"), Collections.emptyMap());\n \n\ndiff --git a/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieCatalog.java b/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieCatalog.java\nindex fbd9e8447b87..5123809a04a9 100644\n--- a/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieCatalog.java\n+++ b/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieCatalog.java\n@@ -190,11 +190,4 @@ public void testWarehouseLocationWithTrailingSlash() {\n                 + \"/\"\n                 + TABLE.name());\n   }\n-\n-  @Test\n-  @Override\n-  @Disabled(\"Nessie currently returns an empty list instead of throwing a NoSuchNamespaceException\")\n-  public void testListNonExistingNamespace() {\n-    super.testListNonExistingNamespace();\n-  }\n }\n\ndiff --git a/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieIcebergClient.java b/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieIcebergClient.java\nindex b1f844c6d991..de68656c89ab 100644\n--- a/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieIcebergClient.java\n+++ b/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieIcebergClient.java\n@@ -98,7 +98,7 @@ public void testWithReferenceAfterRecreatingBranch()\n     // just create a new commit on the branch and then delete & re-create it\n     Namespace namespace = Namespace.of(\"a\");\n     client.createNamespace(namespace, ImmutableMap.of());\n-    assertThat(client.listNamespaces(namespace)).isNotNull();\n+    assertThat(client.listNamespaces(namespace)).isEmpty();\n     client\n         .getApi()\n         .deleteBranch()\n@@ -123,7 +123,8 @@ public void testCreateNamespace() throws NessieConflictException, NessieNotFound\n     NessieIcebergClient client = new NessieIcebergClient(api, branch, null, catalogOptions);\n \n     client.createNamespace(Namespace.of(\"a\"), Map.of());\n-    assertThat(client.listNamespaces(Namespace.of(\"a\"))).isNotNull();\n+    assertThat(client.listNamespaces(Namespace.of(\"a\"))).isEmpty();\n+    assertThat(client.listNamespaces(Namespace.empty())).containsOnly(Namespace.of(\"a\"));\n \n     List<LogResponse.LogEntry> entries = api.getCommitLog().refName(branch).get().getLogEntries();\n     assertThat(entries)\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12887",
    "pr_id": 12887,
    "issue_id": 12870,
    "repo": "apache/iceberg",
    "problem_statement": "Disallow creation of invalid PartitionSpec\n### Bug\n\nThe specification says in relationship to [partitioning](https://iceberg.apache.org/spec/#partitioning):\n\n>  The source columns, selected by ids, must be a primitive type and cannot be contained in a map or list, but may be nested in a struct.\n\nThe java implementation provides the necessary safety checks around the \"primitive type\" part of this statement, but does not provide safety checks for the \"contained in a map or list\" part.\n\n```java\nfinal Schema schema =\n    new Schema(\n        NestedField.required(\n            2, \"MyList\", Types.ListType.ofRequired(1, Types.IntegerType.get())));\n// Does not currently fail, but ideally should\nPartitionSpec.builderFor(schema).identity(\"MyList.element\").build();\n```\n\nI've got a branch with changes to [TestPartitionSpecValidation.java](https://github.com/apache/iceberg/compare/main...devinrsmith:iceberg:partition-spec-leniency-example#diff-a101be1d61ec7b3b95b6b2925b42d9ef0c77aa97c27a881eb5e9f39086bfa388) that further demonstrates the current behavior.\n\n### Query engine\n\nNone\n\n### Willingness to contribute\n\n- [x] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [ ] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 177,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "api/src/main/java/org/apache/iceberg/PartitionSpec.java",
      "api/src/test/java/org/apache/iceberg/TestPartitionSpecValidation.java"
    ],
    "pr_changed_test_files": [
      "api/src/test/java/org/apache/iceberg/TestPartitionSpecValidation.java"
    ],
    "base_commit": "d7f8e5400519f84cd2af82d7769713b24e916809",
    "head_commit": "44d328168688d6d8465b9774d359092537e24057",
    "repo_url": "https://github.com/apache/iceberg/pull/12887",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12887",
    "dockerfile": "",
    "pr_merged_at": "2025-05-07T07:05:42.000Z",
    "patch": "diff --git a/api/src/main/java/org/apache/iceberg/PartitionSpec.java b/api/src/main/java/org/apache/iceberg/PartitionSpec.java\nindex 9b74893f1831..80985dcb79a9 100644\n--- a/api/src/main/java/org/apache/iceberg/PartitionSpec.java\n+++ b/api/src/main/java/org/apache/iceberg/PartitionSpec.java\n@@ -40,6 +40,7 @@\n import org.apache.iceberg.transforms.Transforms;\n import org.apache.iceberg.transforms.UnknownTransform;\n import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.types.Types.StructType;\n \n@@ -624,6 +625,7 @@ PartitionSpec buildUnchecked() {\n   }\n \n   static void checkCompatibility(PartitionSpec spec, Schema schema) {\n+    final Map<Integer, Integer> parents = TypeUtil.indexParents(schema.asStruct());\n     for (PartitionField field : spec.fields) {\n       Type sourceType = schema.findType(field.sourceId());\n       Transform<?, ?> transform = field.transform();\n@@ -644,6 +646,15 @@ static void checkCompatibility(PartitionSpec spec, Schema schema) {\n             \"Invalid source type %s for transform: %s\",\n             sourceType,\n             transform);\n+        // The only valid parent types for a PartitionField are StructTypes. This must be checked\n+        // recursively.\n+        Integer parentId = parents.get(field.sourceId());\n+        while (parentId != null) {\n+          Type parentType = schema.findType(parentId);\n+          ValidationException.check(\n+              parentType.isStructType(), \"Invalid partition field parent: %s\", parentType);\n+          parentId = parents.get(parentId);\n+        }\n       }\n     }\n   }\n",
    "test_patch": "diff --git a/api/src/test/java/org/apache/iceberg/TestPartitionSpecValidation.java b/api/src/test/java/org/apache/iceberg/TestPartitionSpecValidation.java\nindex ee71d39bb2db..b8e16a9ee45e 100644\n--- a/api/src/test/java/org/apache/iceberg/TestPartitionSpecValidation.java\n+++ b/api/src/test/java/org/apache/iceberg/TestPartitionSpecValidation.java\n@@ -340,4 +340,105 @@ private static Object[][] unsupportedFieldsProvider() {\n       {10, \"unknown_partition1\", \"Invalid source type unknown for transform: bucket[5]\"}\n     };\n   }\n+\n+  @Test\n+  void testSourceIdNotFound() {\n+    assertThatThrownBy(\n+            () ->\n+                PartitionSpec.builderFor(SCHEMA)\n+                    .add(99, 1000, \"Test\", Transforms.identity())\n+                    .build())\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessage(\"Cannot find source column for partition field: 1000: Test: identity(99)\");\n+  }\n+\n+  @Test\n+  void testPartitionFieldInStruct() {\n+    final Schema schema =\n+        new Schema(\n+            NestedField.required(SCHEMA.highestFieldId() + 1, \"MyStruct\", SCHEMA.asStruct()));\n+    PartitionSpec.builderFor(schema).identity(\"MyStruct.id\").build();\n+  }\n+\n+  @Test\n+  void testPartitionFieldInStructInStruct() {\n+    final Schema schema =\n+        new Schema(\n+            NestedField.optional(\n+                SCHEMA.highestFieldId() + 2,\n+                \"Outer\",\n+                Types.StructType.of(\n+                    NestedField.required(\n+                        SCHEMA.highestFieldId() + 1, \"Inner\", SCHEMA.asStruct()))));\n+    PartitionSpec.builderFor(schema).identity(\"Outer.Inner.id\").build();\n+  }\n+\n+  @Test\n+  void testPartitionFieldInList() {\n+    final Schema schema =\n+        new Schema(\n+            NestedField.required(\n+                2, \"MyList\", Types.ListType.ofRequired(1, Types.IntegerType.get())));\n+    assertThatThrownBy(() -> PartitionSpec.builderFor(schema).identity(\"MyList.element\").build())\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessage(\"Invalid partition field parent: list<int>\");\n+  }\n+\n+  @Test\n+  void testPartitionFieldInStructInList() {\n+    final Schema schema =\n+        new Schema(\n+            NestedField.required(\n+                3,\n+                \"MyList\",\n+                Types.ListType.ofRequired(\n+                    2,\n+                    Types.StructType.of(NestedField.optional(1, \"Foo\", Types.IntegerType.get())))));\n+    assertThatThrownBy(\n+            () -> PartitionSpec.builderFor(schema).identity(\"MyList.element.Foo\").build())\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessage(\"Invalid partition field parent: list<struct<1: Foo: optional int>>\");\n+  }\n+\n+  @Test\n+  void testPartitionFieldInMap() {\n+    final Schema schema =\n+        new Schema(\n+            NestedField.required(\n+                3,\n+                \"MyMap\",\n+                Types.MapType.ofRequired(1, 2, Types.IntegerType.get(), Types.IntegerType.get())));\n+\n+    assertThatThrownBy(() -> PartitionSpec.builderFor(schema).identity(\"MyMap.key\").build())\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessage(\"Invalid partition field parent: map<int, int>\");\n+\n+    assertThatThrownBy(() -> PartitionSpec.builderFor(schema).identity(\"MyMap.value\").build())\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessage(\"Invalid partition field parent: map<int, int>\");\n+  }\n+\n+  @Test\n+  void testPartitionFieldInStructInMap() {\n+    final Schema schema =\n+        new Schema(\n+            NestedField.required(\n+                5,\n+                \"MyMap\",\n+                Types.MapType.ofRequired(\n+                    3,\n+                    4,\n+                    Types.StructType.of(NestedField.optional(1, \"Foo\", Types.IntegerType.get())),\n+                    Types.StructType.of(NestedField.optional(2, \"Bar\", Types.IntegerType.get())))));\n+\n+    assertThatThrownBy(() -> PartitionSpec.builderFor(schema).identity(\"MyMap.key.Foo\").build())\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessage(\n+            \"Invalid partition field parent: map<struct<1: Foo: optional int>, struct<2: Bar: optional int>>\");\n+\n+    assertThatThrownBy(() -> PartitionSpec.builderFor(schema).identity(\"MyMap.value.Bar\").build())\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessage(\n+            \"Invalid partition field parent: map<struct<1: Foo: optional int>, struct<2: Bar: optional int>>\");\n+  }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12861",
    "pr_id": 12861,
    "issue_id": 12717,
    "repo": "apache/iceberg",
    "problem_statement": "Adding deleteFile method to existing RowDelta operation\n### Proposed Change\n\nSome engines are able to not only produce new data file during row-level change to a table, but can also eliminate entire data files files in the same pending update. Alternative is to use separate pending change and commit delete files in separate snapshot, but having delete files in the same pending update and snapshot feels like a natural extension.\n\n### Proposal document\n\n_No response_\n\n### Specifications\n\n- [ ] Table\n- [ ] View\n- [ ] REST\n- [ ] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 81,
    "test_files_count": 1,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "api/src/main/java/org/apache/iceberg/RowDelta.java",
      "core/src/main/java/org/apache/iceberg/BaseRowDelta.java",
      "core/src/test/java/org/apache/iceberg/TestRowDelta.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/TestRowDelta.java"
    ],
    "base_commit": "ab92d6e66b61195a8e9845d3eb592f3e67ae67e1",
    "head_commit": "aae6764abc9eda4616994094c8b56a4070aa982f",
    "repo_url": "https://github.com/apache/iceberg/pull/12861",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12861",
    "dockerfile": "",
    "pr_merged_at": "2025-05-06T16:09:35.000Z",
    "patch": "diff --git a/api/src/main/java/org/apache/iceberg/RowDelta.java b/api/src/main/java/org/apache/iceberg/RowDelta.java\nindex a5e3fa477ba9..4644579be55e 100644\n--- a/api/src/main/java/org/apache/iceberg/RowDelta.java\n+++ b/api/src/main/java/org/apache/iceberg/RowDelta.java\n@@ -46,6 +46,17 @@ public interface RowDelta extends SnapshotUpdate<RowDelta> {\n    */\n   RowDelta addDeletes(DeleteFile deletes);\n \n+  /**\n+   * Delete a {@link DataFile} from the table.\n+   *\n+   * @param file a data file\n+   * @return this for method chaining\n+   */\n+  default RowDelta deleteFile(DataFile file) {\n+    throw new UnsupportedOperationException(\n+        getClass().getName() + \" does not implement deleteFile\");\n+  }\n+\n   /**\n    * Removes a rewritten {@link DeleteFile} from the table.\n    *\n\ndiff --git a/core/src/main/java/org/apache/iceberg/BaseRowDelta.java b/core/src/main/java/org/apache/iceberg/BaseRowDelta.java\nindex 372fc5367f08..0f0d136eb839 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseRowDelta.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseRowDelta.java\n@@ -18,15 +18,20 @@\n  */\n package org.apache.iceberg;\n \n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.expressions.Expression;\n import org.apache.iceberg.expressions.Expressions;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.util.CharSequenceSet;\n+import org.apache.iceberg.util.DataFileSet;\n import org.apache.iceberg.util.SnapshotUtil;\n \n class BaseRowDelta extends MergingSnapshotProducer<RowDelta> implements RowDelta {\n   private Long startingSnapshotId = null; // check all versions by default\n   private final CharSequenceSet referencedDataFiles = CharSequenceSet.empty();\n+  private final DataFileSet deletedDataFiles = DataFileSet.create();\n   private boolean validateDeletes = false;\n   private Expression conflictDetectionFilter = Expressions.alwaysTrue();\n   private boolean validateNewDataFiles = false;\n@@ -62,6 +67,13 @@ public RowDelta addDeletes(DeleteFile deletes) {\n     return this;\n   }\n \n+  @Override\n+  public RowDelta deleteFile(DataFile file) {\n+    deletedDataFiles.add(file);\n+    delete(file);\n+    return this;\n+  }\n+\n   @Override\n   public RowDelta removeDeletes(DeleteFile deletes) {\n     delete(deletes);\n@@ -132,15 +144,47 @@ protected void validate(TableMetadata base, Snapshot parent) {\n             parent);\n       }\n \n+      if (validateDeletes) {\n+        failMissingDeletePaths();\n+      }\n+\n       if (validateNewDataFiles) {\n         validateAddedDataFiles(base, startingSnapshotId, conflictDetectionFilter, parent);\n       }\n \n       if (validateNewDeleteFiles) {\n+        // validate that explicitly deleted files have not had added deletes\n+        if (!deletedDataFiles.isEmpty()) {\n+          validateNoNewDeletesForDataFiles(\n+              base, startingSnapshotId, conflictDetectionFilter, deletedDataFiles, parent);\n+        }\n+\n+        // validate that previous deletes do not conflict with added deletes\n         validateNoNewDeleteFiles(base, startingSnapshotId, conflictDetectionFilter, parent);\n       }\n \n+      validateNoConflictingFileAndPositionDeletes();\n+\n       validateAddedDVs(base, startingSnapshotId, conflictDetectionFilter, parent);\n     }\n   }\n+\n+  /**\n+   * Validates that the data files removed in this commit do not overlap with data files with delete\n+   * files added\n+   */\n+  @SuppressWarnings(\"CollectionUndefinedEquality\")\n+  private void validateNoConflictingFileAndPositionDeletes() {\n+    List<CharSequence> deletedFileWithNewDVs =\n+        deletedDataFiles.stream()\n+            .map(DataFile::path)\n+            .filter(referencedDataFiles::contains)\n+            .collect(Collectors.toList());\n+\n+    if (!deletedFileWithNewDVs.isEmpty()) {\n+      throw new ValidationException(\n+          \"Cannot delete data files %s that are referenced by new delete files\",\n+          deletedFileWithNewDVs);\n+    }\n+  }\n }\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/TestRowDelta.java b/core/src/test/java/org/apache/iceberg/TestRowDelta.java\nindex cbb4db476678..a42b90fd9aea 100644\n--- a/core/src/test/java/org/apache/iceberg/TestRowDelta.java\n+++ b/core/src/test/java/org/apache/iceberg/TestRowDelta.java\n@@ -289,6 +289,147 @@ public void testValidateDataFilesExistFromSnapshot() {\n         statuses(Status.ADDED));\n   }\n \n+  @TestTemplate\n+  public void testFileDeleteAndRowDelete() {\n+    commit(table, table.newAppend().appendFile(FILE_A).appendFile(FILE_B), branch);\n+    long initialCommit = latestSnapshot(table, branch).snapshotId();\n+\n+    commit(\n+        table,\n+        table\n+            .newRowDelta()\n+            .addDeletes(fileADeletes())\n+            .addRows(FILE_A2)\n+            .deleteFile(FILE_B)\n+            .validateFromSnapshot(initialCommit)\n+            .validateDataFilesExist(ImmutableList.of(FILE_A.location())),\n+        branch);\n+\n+    Snapshot snap = latestSnapshot(table, branch);\n+    assertThat(snap.sequenceNumber()).isEqualTo(2);\n+    assertThat(table.ops().current().lastSequenceNumber()).isEqualTo(2);\n+\n+    assertThat(snap.dataManifests(table.io())).hasSize(2);\n+    // manifest with FILE_A2 added\n+    validateManifest(\n+        snap.dataManifests(table.io()).get(0),\n+        dataSeqs(2L),\n+        fileSeqs(2L),\n+        ids(snap.snapshotId()),\n+        files(FILE_A2),\n+        statuses(Status.ADDED));\n+\n+    // manifest with FILE_A deleted\n+    validateManifest(\n+        snap.dataManifests(table.io()).get(1),\n+        dataSeqs(1L, 1L),\n+        fileSeqs(1L, 1L),\n+        ids(initialCommit, snap.snapshotId()),\n+        files(FILE_A, FILE_B),\n+        statuses(Status.EXISTING, Status.DELETED));\n+\n+    assertThat(snap.deleteManifests(table.io())).hasSize(1);\n+    validateDeleteManifest(\n+        snap.deleteManifests(table.io()).get(0),\n+        dataSeqs(2L),\n+        fileSeqs(2L),\n+        ids(snap.snapshotId()),\n+        files(fileADeletes()),\n+        statuses(Status.ADDED));\n+  }\n+\n+  @TestTemplate\n+  public void testValidateFileDeleteAndRowDelete() {\n+    commit(table, table.newAppend().appendFile(FILE_A).appendFile(FILE_B), branch);\n+    long initialCommit = latestSnapshot(table, branch).snapshotId();\n+\n+    commit(\n+        table,\n+        table\n+            .newRowDelta()\n+            .addDeletes(fileBDeletes())\n+            .validateFromSnapshot(initialCommit)\n+            .validateDataFilesExist(ImmutableList.of(FILE_A.location())),\n+        branch);\n+\n+    assertThatThrownBy(\n+            () -> {\n+              commit(\n+                  table,\n+                  table\n+                      .newRowDelta()\n+                      .addDeletes(fileADeletes())\n+                      .addRows(FILE_A2)\n+                      .deleteFile(FILE_B)\n+                      .validateFromSnapshot(initialCommit)\n+                      .validateNoConflictingDeleteFiles()\n+                      .validateDataFilesExist(ImmutableList.of(FILE_A.location())),\n+                  branch);\n+            })\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessageContaining(\"found new delete for replaced data file: \" + FILE_B);\n+  }\n+\n+  @TestTemplate\n+  public void testValidateFileDeleteAndRowDeleteSameFile() {\n+    commit(table, table.newAppend().appendFile(FILE_A), branch);\n+    long initialCommit = latestSnapshot(table, branch).snapshotId();\n+\n+    // test adding a delete vector to a deleted file\n+    assertThatThrownBy(\n+            () -> {\n+              commit(\n+                  table,\n+                  table\n+                      .newRowDelta()\n+                      .addDeletes(fileADeletes())\n+                      .deleteFile(FILE_A)\n+                      .validateFromSnapshot(initialCommit)\n+                      .validateDeletedFiles()\n+                      .validateDataFilesExist(ImmutableList.of(FILE_A.location())),\n+                  branch);\n+            })\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessageContaining(\"Cannot delete data files\")\n+        .hasMessageContaining(FILE_A.location());\n+  }\n+\n+  @TestTemplate\n+  public void testValidateDeleteFile() {\n+    commit(table, table.newAppend().appendFile(FILE_B), branch);\n+    long initialCommit = latestSnapshot(table, branch).snapshotId();\n+\n+    // Remove a file which does not exist\n+    assertThatThrownBy(\n+            () -> {\n+              commit(\n+                  table,\n+                  table\n+                      .newRowDelta()\n+                      .deleteFile(FILE_A)\n+                      .validateFromSnapshot(initialCommit)\n+                      .validateDeletedFiles(),\n+                  branch);\n+            })\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessageContaining(\"Missing required files to delete:\")\n+        .hasMessageContaining(FILE_A.location());\n+\n+    // Should succeed if validation is ignored\n+    commit(\n+        table, table.newRowDelta().deleteFile(FILE_A).validateFromSnapshot(initialCommit), branch);\n+\n+    // Commit should be a no-op\n+    Snapshot snap = latestSnapshot(table, branch);\n+    validateManifest(\n+        snap.dataManifests(table.io()).get(0),\n+        dataSeqs(1L),\n+        fileSeqs(1L),\n+        ids(initialCommit),\n+        files(FILE_B),\n+        statuses(Status.ADDED));\n+  }\n+\n   @TestTemplate\n   public void testValidateDataFilesExistRewrite() {\n     commit(table, table.newAppend().appendFile(FILE_A).appendFile(FILE_B), branch);\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12855",
    "pr_id": 12855,
    "issue_id": 11239,
    "repo": "apache/iceberg",
    "problem_statement": "BaseDeleteLoader may ignore delete records for binary columns\n### Apache Iceberg version\n\n1.6.1 (latest release)\n\n### Query engine\n\nNone\n\n### Please describe the bug üêû\n\nIn case of a binary column, Iceberg Equality Delete loader, `BaseDeleteLoader` reuses the underlying ByteBuffer used to read the delete records. In some situations it leads to overwriting previously read records from the delete file and in turn produces duplicates in the output.\r\n\r\nUnfortunately, I can't share the exact code to reproduce the bug, but here is the table that contains the issue - [9a264a21-cc2f-4f13-8e7d-4d899a31ca2f 2.zip](https://github.com/user-attachments/files/17196281/9a264a21-cc2f-4f13-8e7d-4d899a31ca2f.2.zip). A read from that table will contain duplicates as the equality delete reader will miss one of the 2 records present in the delete file.\r\n\r\nIn principle, it should be easy to reproduce with a table having a binary column and doing an overwrite on it. Unfortunately, I am new to Iceberg and couldn't find working instructions on how to set up a local environment and do an overwrite using equality deletes. If someone helps me with a local setup, I will be happy to provide a more actionable example.\r\n\r\nHere are the pointers in the code to show how/why the overwrite happens:\r\n\r\n* Parquet reader configuration for delete files set up to reuse the `ByteBuffer`: https://github.com/apache/iceberg/blob/09370ddbc39fc3920fb8cbd3dff11b377dd37e40/data/src/main/java/org/apache/iceberg/data/BaseDeleteLoader.java#L197-L203\r\n\r\n* `ParquetValueReaders` that reuses the same ByteByffer:\r\n\r\nhttps://github.com/apache/iceberg/blob/09370ddbc39fc3920fb8cbd3dff11b377dd37e40/parquet/src/main/java/org/apache/iceberg/parquet/ParquetValueReaders.java#L366-L380\r\n\r\n* And finally, `InternalRecordWrapper` copies the pointer to that ByteBuffer (instead of its content) leading to the overwrite:\r\nhttps://github.com/apache/iceberg/blob/09370ddbc39fc3920fb8cbd3dff11b377dd37e40/data/src/main/java/org/apache/iceberg/data/InternalRecordWrapper.java#L76-L78\r\n* Reading subsequent records from the delete file overwrites previously read records if the newer records fit into the buffer. \r\n\r\n\r\n\n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [X] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 365,
    "test_files_count": 3,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/data/GenericRecord.java",
      "data/src/test/java/org/apache/iceberg/data/DeleteReadTests.java",
      "data/src/test/java/org/apache/iceberg/data/TestGenericReaderDeletes.java",
      "data/src/test/java/org/apache/iceberg/data/TestGenericRecord.java"
    ],
    "pr_changed_test_files": [
      "data/src/test/java/org/apache/iceberg/data/DeleteReadTests.java",
      "data/src/test/java/org/apache/iceberg/data/TestGenericReaderDeletes.java",
      "data/src/test/java/org/apache/iceberg/data/TestGenericRecord.java"
    ],
    "base_commit": "97c0e136b8021058897cab7539e3ef89ce5a0341",
    "head_commit": "6b5b4a00edbf233ca58cf7e219d82e16298c05e8",
    "repo_url": "https://github.com/apache/iceberg/pull/12855",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12855",
    "dockerfile": "",
    "pr_merged_at": "2025-05-19T17:01:40.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/data/GenericRecord.java b/core/src/main/java/org/apache/iceberg/data/GenericRecord.java\nindex 6fcd4bdd6223..6173a29ec5e8 100644\n--- a/core/src/main/java/org/apache/iceberg/data/GenericRecord.java\n+++ b/core/src/main/java/org/apache/iceberg/data/GenericRecord.java\n@@ -20,6 +20,7 @@\n \n import com.github.benmanes.caffeine.cache.Caffeine;\n import com.github.benmanes.caffeine.cache.LoadingCache;\n+import java.nio.ByteBuffer;\n import java.util.Arrays;\n import java.util.List;\n import java.util.Map;\n@@ -30,6 +31,7 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.types.Types.StructType;\n+import org.apache.iceberg.util.ByteBuffers;\n \n public class GenericRecord implements Record, StructLike {\n   private static final LoadingCache<StructType, Map<String, Integer>> NAME_MAP_CACHE =\n@@ -68,10 +70,23 @@ private GenericRecord(StructType struct) {\n   private GenericRecord(GenericRecord toCopy) {\n     this.struct = toCopy.struct;\n     this.size = toCopy.size;\n-    this.values = Arrays.copyOf(toCopy.values, toCopy.values.length);\n+    this.values = new Object[toCopy.values.length];\n+    for (int i = 0; i < this.values.length; i++) {\n+      this.values[i] = deepCopyValue(toCopy.values[i]);\n+    }\n     this.nameToPos = toCopy.nameToPos;\n   }\n \n+  private Object deepCopyValue(Object value) {\n+    if (value instanceof ByteBuffer) {\n+      return ByteBuffers.copy((ByteBuffer) value);\n+    } else if (value instanceof GenericRecord) {\n+      return ((GenericRecord) value).copy();\n+    } else {\n+      return value;\n+    }\n+  }\n+\n   private GenericRecord(GenericRecord toCopy, Map<String, Object> overwrite) {\n     this.struct = toCopy.struct;\n     this.size = toCopy.size;\n",
    "test_patch": "diff --git a/data/src/test/java/org/apache/iceberg/data/DeleteReadTests.java b/data/src/test/java/org/apache/iceberg/data/DeleteReadTests.java\nindex 501929bbcae7..4b59b376efba 100644\n--- a/data/src/test/java/org/apache/iceberg/data/DeleteReadTests.java\n+++ b/data/src/test/java/org/apache/iceberg/data/DeleteReadTests.java\n@@ -18,11 +18,14 @@\n  */\n package org.apache.iceberg.data;\n \n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.ByteBuffer;\n import java.nio.file.Path;\n import java.time.LocalDate;\n import java.util.List;\n@@ -37,6 +40,8 @@\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TestHelpers.Row;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.types.Types;\n@@ -55,14 +60,19 @@ public abstract class DeleteReadTests {\n   // Schema passed to create tables\n   public static final Schema SCHEMA =\n       new Schema(\n-          Types.NestedField.required(1, \"id\", Types.IntegerType.get()),\n-          Types.NestedField.required(2, \"data\", Types.StringType.get()));\n+          required(1, \"id\", Types.IntegerType.get()),\n+          required(2, \"data\", Types.StringType.get()),\n+          optional(3, \"binaryData\", Types.BinaryType.get()),\n+          optional(\n+              4,\n+              \"structData\",\n+              Types.StructType.of(optional(100, \"structInnerData\", Types.StringType.get()))));\n \n   public static final Schema DATE_SCHEMA =\n       new Schema(\n-          Types.NestedField.required(1, \"dt\", Types.DateType.get()),\n-          Types.NestedField.required(2, \"data\", Types.StringType.get()),\n-          Types.NestedField.required(3, \"id\", Types.IntegerType.get()));\n+          required(1, \"dt\", Types.DateType.get()),\n+          required(2, \"data\", Types.StringType.get()),\n+          required(3, \"id\", Types.IntegerType.get()));\n \n   // Partition spec used to create tables\n   public static final PartitionSpec SPEC =\n@@ -586,6 +596,91 @@ public void testEqualityDeleteByNull() throws IOException {\n     checkDeleteCount(1L);\n   }\n \n+  @TestTemplate\n+  public void testEqualityDeleteBinaryColumn() throws IOException {\n+    writeOptionalTestDataFile();\n+\n+    Record binaryDataDelete = GenericRecord.create(table.schema().select(\"binaryData\"));\n+    List<Record> equalityDeletes =\n+        Lists.newArrayList(\n+            binaryDataDelete.copy(\"binaryData\", ByteBuffer.wrap((\"binaryData_0\").getBytes())),\n+            binaryDataDelete.copy(\"binaryData\", ByteBuffer.wrap((\"binaryData_1\").getBytes())),\n+            binaryDataDelete.copy(\"binaryData\", ByteBuffer.wrap((\"binaryData_2\").getBytes())));\n+    StructLikeSet expected = rowSetWithoutIds(table, records, 200, 201, 202);\n+    testEqualityDeletes(equalityDeletes, expected);\n+  }\n+\n+  @TestTemplate\n+  public void testEqualityDeleteStructColumn() throws IOException {\n+    writeOptionalTestDataFile();\n+\n+    Record structDataDelete = GenericRecord.create(table.schema().select(\"structData\"));\n+    Record structRecord0 =\n+        GenericRecord.create(table.schema().findType(\"structData\").asStructType());\n+    Record structRecord1 =\n+        GenericRecord.create(table.schema().findType(\"structData\").asStructType());\n+    Record structRecord2 =\n+        GenericRecord.create(table.schema().findType(\"structData\").asStructType());\n+    structRecord0.setField(\"structInnerData\", \"structInnerData_0\");\n+    structRecord1.setField(\"structInnerData\", \"structInnerData_1\");\n+    structRecord2.setField(\"structInnerData\", \"structInnerData_2\");\n+    List<Record> equalityDeletes =\n+        Lists.newArrayList(\n+            structDataDelete.copy(\"structData\", structRecord0),\n+            structDataDelete.copy(\"structData\", structRecord1),\n+            structDataDelete.copy(\"structData\", structRecord2));\n+    StructLikeSet expected = rowSetWithoutIds(table, records, 200, 201, 202);\n+    testEqualityDeletes(equalityDeletes, expected);\n+  }\n+\n+  private void testEqualityDeletes(List<Record> equalityDeletes, StructLikeSet expected)\n+      throws IOException {\n+    Preconditions.checkArgument(\n+        !equalityDeletes.isEmpty(), \"Expected equality deletes in testEqualityDeletes\");\n+    DeleteFile eqDeletes =\n+        FileHelpers.writeDeleteFile(\n+            table,\n+            Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n+            Row.of(0),\n+            equalityDeletes,\n+            equalityDeletes.get(0).struct().asSchema());\n+    table.newRowDelta().addDeletes(eqDeletes).commit();\n+\n+    StructLikeSet actual = rowSet(tableName, table, \"*\");\n+    assertThat(actual).as(\"Table should contain expected rows\").isEqualTo(expected);\n+    checkDeleteCount(equalityDeletes.size());\n+  }\n+\n+  private void writeOptionalTestDataFile() throws IOException {\n+    Record record = GenericRecord.create(table.schema());\n+    List<Record> recordsWithOptionalColumns = Lists.newArrayList();\n+    for (int i = 0; i < 10; i++) {\n+      Record structRecord =\n+          GenericRecord.create(table.schema().findType(\"structData\").asStructType());\n+      structRecord.setField(\"structInnerData\", \"structInnerData_\" + i);\n+      recordsWithOptionalColumns.add(\n+          record.copy(\n+              ImmutableMap.of(\n+                  \"id\",\n+                  i + 200,\n+                  \"data\",\n+                  \"testData\",\n+                  \"binaryData\",\n+                  ByteBuffer.wrap((\"binaryData_\" + i).getBytes()),\n+                  \"structData\",\n+                  structRecord)));\n+    }\n+    DataFile binaryDataFile =\n+        FileHelpers.writeDataFile(\n+            table,\n+            Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n+            Row.of(0),\n+            recordsWithOptionalColumns);\n+\n+    table.newAppend().appendFile(binaryDataFile).commit();\n+    records.addAll(recordsWithOptionalColumns);\n+  }\n+\n   private StructLikeSet selectColumns(StructLikeSet rows, String... columns) {\n     Schema projection = table.schema().select(columns);\n     StructLikeSet set = StructLikeSet.create(projection.asStruct());\n\ndiff --git a/data/src/test/java/org/apache/iceberg/data/TestGenericReaderDeletes.java b/data/src/test/java/org/apache/iceberg/data/TestGenericReaderDeletes.java\nindex b15f5b70720b..959010021393 100644\n--- a/data/src/test/java/org/apache/iceberg/data/TestGenericReaderDeletes.java\n+++ b/data/src/test/java/org/apache/iceberg/data/TestGenericReaderDeletes.java\n@@ -27,6 +27,7 @@\n import org.apache.iceberg.TestTables;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.StructLikeSet;\n import org.junit.jupiter.api.extension.ExtendWith;\n import org.junit.jupiter.api.io.TempDir;\n@@ -47,12 +48,13 @@ protected void dropTable(String name) {\n \n   @Override\n   public StructLikeSet rowSet(String name, Table table, String... columns) throws IOException {\n-    StructLikeSet set = StructLikeSet.create(table.schema().asStruct());\n+    Types.StructType schema = table.schema().select(columns).asStruct();\n+    StructLikeSet set = StructLikeSet.create(schema);\n     try (CloseableIterable<Record> reader = IcebergGenerics.read(table).select(columns).build()) {\n       Iterables.addAll(\n           set,\n           CloseableIterable.transform(\n-              reader, record -> new InternalRecordWrapper(table.schema().asStruct()).wrap(record)));\n+              reader, record -> new InternalRecordWrapper(schema).wrap(record)));\n     }\n     return set;\n   }\n\ndiff --git a/data/src/test/java/org/apache/iceberg/data/TestGenericRecord.java b/data/src/test/java/org/apache/iceberg/data/TestGenericRecord.java\nindex cf28c20efdc9..e158ba12c542 100644\n--- a/data/src/test/java/org/apache/iceberg/data/TestGenericRecord.java\n+++ b/data/src/test/java/org/apache/iceberg/data/TestGenericRecord.java\n@@ -19,9 +19,11 @@\n package org.apache.iceberg.data;\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import java.nio.ByteBuffer;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.types.Types;\n import org.junit.jupiter.api.Test;\n@@ -58,4 +60,27 @@ public void testGetIncorrectClassInstance() {\n         .isInstanceOf(IllegalStateException.class)\n         .hasMessage(\"Not an instance of java.lang.CharSequence: 10\");\n   }\n+\n+  @Test\n+  public void testDeepCopyValues() {\n+    Schema schema =\n+        new Schema(\n+            optional(0, \"binaryData\", Types.BinaryType.get()),\n+            optional(\n+                1,\n+                \"structData\",\n+                Types.StructType.of(required(100, \"structInnerData\", Types.StringType.get()))));\n+\n+    GenericRecord original = GenericRecord.create(schema);\n+    original.setField(\"binaryData\", ByteBuffer.wrap(\"binaryData_0\".getBytes()));\n+    Record structRecord = GenericRecord.create(schema.findType(\"structData\").asStructType());\n+    structRecord.setField(\"structInnerData\", \"structInnerData_1\");\n+    original.setField(\"structData\", structRecord);\n+\n+    GenericRecord copy = original.copy();\n+    for (Types.NestedField field : schema.columns()) {\n+      assertThat(copy.getField(field.name())).isEqualTo(original.getField(field.name()));\n+      assertThat(copy.getField(field.name())).isNotSameAs(original.getField(field.name()));\n+    }\n+  }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12853",
    "pr_id": 12853,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 16,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/TaskCheckHelper.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/TestDataFileSerialization.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/TestFileIOSerialization.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/TestHadoopMetricsContextSerialization.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/TestManifestFileSerialization.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/TestTableSerialization.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestComputeTableStatsAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSnapshotTableAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/functions/TestSparkFunctions.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/TaskCheckHelper.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/TestFileIOSerialization.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestComputeTableStatsAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/TaskCheckHelper.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/TestDataFileSerialization.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/TestFileIOSerialization.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/TestHadoopMetricsContextSerialization.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/TestManifestFileSerialization.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/TestTableSerialization.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestComputeTableStatsAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSnapshotTableAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/functions/TestSparkFunctions.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/TaskCheckHelper.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/TestFileIOSerialization.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestComputeTableStatsAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java"
    ],
    "base_commit": "edc7f2a5400897499b8e9b63a2e75bafe6be5808",
    "head_commit": "1cb1e2288b499b7815438c9f0203bcced68509e3",
    "repo_url": "https://github.com/apache/iceberg/pull/12853",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12853",
    "dockerfile": "",
    "pr_merged_at": "2025-04-22T10:03:15.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/TaskCheckHelper.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/TaskCheckHelper.java\nindex b241b8ed6363..e4979512a65f 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/TaskCheckHelper.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/TaskCheckHelper.java\n@@ -18,10 +18,11 @@\n  */\n package org.apache.iceberg;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import java.util.Comparator;\n import java.util.List;\n import java.util.stream.Collectors;\n-import org.junit.Assert;\n \n public final class TaskCheckHelper {\n   private TaskCheckHelper() {}\n@@ -31,8 +32,9 @@ public static void assertEquals(\n     List<FileScanTask> expectedTasks = getFileScanTasksInFilePathOrder(expected);\n     List<FileScanTask> actualTasks = getFileScanTasksInFilePathOrder(actual);\n \n-    Assert.assertEquals(\n-        \"The number of file scan tasks should match\", expectedTasks.size(), actualTasks.size());\n+    assertThat(actualTasks)\n+        .as(\"The number of file scan tasks should match\")\n+        .hasSameSizeAs(expectedTasks);\n \n     for (int i = 0; i < expectedTasks.size(); i++) {\n       FileScanTask expectedTask = expectedTasks.get(i);\n@@ -45,61 +47,55 @@ public static void assertEquals(FileScanTask expected, FileScanTask actual) {\n     assertEquals(expected.file(), actual.file());\n \n     // PartitionSpec implements its own equals method\n-    Assert.assertEquals(\"PartitionSpec doesn't match\", expected.spec(), actual.spec());\n+    assertThat(actual.spec()).as(\"PartitionSpec doesn't match\").isEqualTo(expected.spec());\n \n-    Assert.assertEquals(\"starting position doesn't match\", expected.start(), actual.start());\n+    assertThat(actual.start()).as(\"starting position doesn't match\").isEqualTo(expected.start());\n \n-    Assert.assertEquals(\n-        \"the number of bytes to scan doesn't match\", expected.start(), actual.start());\n+    assertThat(actual.start())\n+        .as(\"the number of bytes to scan doesn't match\")\n+        .isEqualTo(expected.start());\n \n     // simplify comparison on residual expression via comparing toString\n-    Assert.assertEquals(\n-        \"Residual expression doesn't match\",\n-        expected.residual().toString(),\n-        actual.residual().toString());\n+    assertThat(actual.residual())\n+        .asString()\n+        .as(\"Residual expression doesn't match\")\n+        .isEqualTo(expected.residual().toString());\n   }\n \n   public static void assertEquals(DataFile expected, DataFile actual) {\n-    Assert.assertEquals(\n-        \"Should match the serialized record path\", expected.location(), actual.location());\n-    Assert.assertEquals(\n-        \"Should match the serialized record format\", expected.format(), actual.format());\n-    Assert.assertEquals(\n-        \"Should match the serialized record partition\",\n-        expected.partition().get(0, Object.class),\n-        actual.partition().get(0, Object.class));\n-    Assert.assertEquals(\n-        \"Should match the serialized record count\", expected.recordCount(), actual.recordCount());\n-    Assert.assertEquals(\n-        \"Should match the serialized record size\",\n-        expected.fileSizeInBytes(),\n-        actual.fileSizeInBytes());\n-    Assert.assertEquals(\n-        \"Should match the serialized record value counts\",\n-        expected.valueCounts(),\n-        actual.valueCounts());\n-    Assert.assertEquals(\n-        \"Should match the serialized record null value counts\",\n-        expected.nullValueCounts(),\n-        actual.nullValueCounts());\n-    Assert.assertEquals(\n-        \"Should match the serialized record lower bounds\",\n-        expected.lowerBounds(),\n-        actual.lowerBounds());\n-    Assert.assertEquals(\n-        \"Should match the serialized record upper bounds\",\n-        expected.upperBounds(),\n-        actual.upperBounds());\n-    Assert.assertEquals(\n-        \"Should match the serialized record key metadata\",\n-        expected.keyMetadata(),\n-        actual.keyMetadata());\n-    Assert.assertEquals(\n-        \"Should match the serialized record offsets\",\n-        expected.splitOffsets(),\n-        actual.splitOffsets());\n-    Assert.assertEquals(\n-        \"Should match the serialized record offsets\", expected.keyMetadata(), actual.keyMetadata());\n+    assertThat(actual.location())\n+        .as(\"Should match the serialized record path\")\n+        .isEqualTo(expected.location());\n+    assertThat(actual.format())\n+        .as(\"Should match the serialized record format\")\n+        .isEqualTo(expected.format());\n+    assertThat(actual.partition().get(0, Object.class))\n+        .as(\"Should match the serialized record partition\")\n+        .isEqualTo(expected.partition().get(0, Object.class));\n+    assertThat(actual.recordCount())\n+        .as(\"Should match the serialized record count\")\n+        .isEqualTo(expected.recordCount());\n+    assertThat(actual.fileSizeInBytes())\n+        .as(\"Should match the serialized record size\")\n+        .isEqualTo(expected.fileSizeInBytes());\n+    assertThat(actual.valueCounts())\n+        .as(\"Should match the serialized record value counts\")\n+        .isEqualTo(expected.valueCounts());\n+    assertThat(actual.nullValueCounts())\n+        .as(\"Should match the serialized record null value counts\")\n+        .isEqualTo(expected.nullValueCounts());\n+    assertThat(actual.lowerBounds())\n+        .as(\"Should match the serialized record lower bounds\")\n+        .isEqualTo(expected.lowerBounds());\n+    assertThat(actual.upperBounds())\n+        .as(\"Should match the serialized record upper bounds\")\n+        .isEqualTo(expected.upperBounds());\n+    assertThat(actual.keyMetadata())\n+        .as(\"Should match the serialized record key metadata\")\n+        .isEqualTo(expected.keyMetadata());\n+    assertThat(actual.splitOffsets())\n+        .as(\"Should match the serialized record offsets\")\n+        .isEqualTo(expected.splitOffsets());\n   }\n \n   private static List<FileScanTask> getFileScanTasksInFilePathOrder(\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestDataFileSerialization.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestDataFileSerialization.java\nindex 2a7d3d4c331f..57c4dc7cdf23 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestDataFileSerialization.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestDataFileSerialization.java\n@@ -36,6 +36,7 @@\n import java.io.ObjectOutputStream;\n import java.nio.ByteBuffer;\n import java.nio.ByteOrder;\n+import java.nio.file.Path;\n import java.util.Map;\n import java.util.UUID;\n import org.apache.iceberg.io.FileAppender;\n@@ -49,10 +50,8 @@\n import org.apache.spark.SparkConf;\n import org.apache.spark.serializer.KryoSerializer;\n import org.apache.spark.sql.catalyst.InternalRow;\n-import org.junit.Assert;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n public class TestDataFileSerialization {\n \n@@ -102,12 +101,12 @@ public class TestDataFileSerialization {\n           .withSortOrder(SortOrder.unsorted())\n           .build();\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n   @Test\n   public void testDataFileKryoSerialization() throws Exception {\n-    File data = temp.newFile();\n-    Assert.assertTrue(data.delete());\n+    File data = File.createTempFile(\"junit\", null, temp.toFile());\n+    assertThat(data.delete()).isTrue();\n     Kryo kryo = new KryoSerializer(new SparkConf()).newKryo();\n \n     try (Output out = new Output(new FileOutputStream(data))) {\n@@ -146,7 +145,7 @@ public void testDataFileJavaSerialization() throws Exception {\n   public void testParquetWriterSplitOffsets() throws IOException {\n     Iterable<InternalRow> records = RandomData.generateSpark(DATE_SCHEMA, 1, 33L);\n     File parquetFile =\n-        new File(temp.getRoot(), FileFormat.PARQUET.addExtension(UUID.randomUUID().toString()));\n+        new File(temp.toFile(), FileFormat.PARQUET.addExtension(UUID.randomUUID().toString()));\n     FileAppender<InternalRow> writer =\n         Parquet.write(Files.localOutput(parquetFile))\n             .schema(DATE_SCHEMA)\n@@ -161,7 +160,7 @@ public void testParquetWriterSplitOffsets() throws IOException {\n     }\n \n     Kryo kryo = new KryoSerializer(new SparkConf()).newKryo();\n-    File dataFile = temp.newFile();\n+    File dataFile = File.createTempFile(\"junit\", null, temp.toFile());\n     try (Output out = new Output(new FileOutputStream(dataFile))) {\n       kryo.writeClassAndObject(out, writer.splitOffsets());\n     }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestFileIOSerialization.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestFileIOSerialization.java\nindex c6f491ece5ad..cdd2443cc0e0 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestFileIOSerialization.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestFileIOSerialization.java\n@@ -20,9 +20,12 @@\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n import java.util.Map;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.hadoop.HadoopFileIO;\n@@ -32,11 +35,9 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.spark.source.SerializableTableWithSize;\n import org.apache.iceberg.types.Types;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n public class TestFileIOSerialization {\n \n@@ -60,15 +61,15 @@ public class TestFileIOSerialization {\n     CONF.set(\"k2\", \"v2\");\n   }\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n   private Table table;\n \n-  @Before\n+  @BeforeEach\n   public void initTable() throws IOException {\n     Map<String, String> props = ImmutableMap.of(\"k1\", \"v1\", \"k2\", \"v2\");\n \n-    File tableLocation = temp.newFolder();\n-    Assert.assertTrue(tableLocation.delete());\n+    File tableLocation = Files.createTempDirectory(temp, \"junit\").toFile();\n+    assertThat(tableLocation.delete()).isTrue();\n \n     this.table = TABLES.create(SCHEMA, SPEC, SORT_ORDER, props, tableLocation.toString());\n   }\n@@ -82,9 +83,7 @@ public void testHadoopFileIOKryoSerialization() throws IOException {\n     FileIO deserializedIO = KryoHelpers.roundTripSerialize(serializableTable.io());\n     Configuration actualConf = ((HadoopFileIO) deserializedIO).conf();\n \n-    Assert.assertEquals(\"Conf pairs must match\", toMap(expectedConf), toMap(actualConf));\n-    Assert.assertEquals(\"Conf values must be present\", \"v1\", actualConf.get(\"k1\"));\n-    Assert.assertEquals(\"Conf values must be present\", \"v2\", actualConf.get(\"k2\"));\n+    assertThat(actualConf).containsExactlyInAnyOrderElementsOf(expectedConf);\n   }\n \n   @Test\n@@ -96,9 +95,7 @@ public void testHadoopFileIOJavaSerialization() throws IOException, ClassNotFoun\n     FileIO deserializedIO = TestHelpers.roundTripSerialize(serializableTable.io());\n     Configuration actualConf = ((HadoopFileIO) deserializedIO).conf();\n \n-    Assert.assertEquals(\"Conf pairs must match\", toMap(expectedConf), toMap(actualConf));\n-    Assert.assertEquals(\"Conf values must be present\", \"v1\", actualConf.get(\"k1\"));\n-    Assert.assertEquals(\"Conf values must be present\", \"v2\", actualConf.get(\"k2\"));\n+    assertThat(actualConf).containsExactlyInAnyOrderElementsOf(expectedConf);\n   }\n \n   private Map<String, String> toMap(Configuration conf) {\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestHadoopMetricsContextSerialization.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestHadoopMetricsContextSerialization.java\nindex 92d233e129e2..a4643d7a087b 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestHadoopMetricsContextSerialization.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestHadoopMetricsContextSerialization.java\n@@ -23,7 +23,7 @@\n import org.apache.iceberg.io.FileIOMetricsContext;\n import org.apache.iceberg.metrics.MetricsContext;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n public class TestHadoopMetricsContextSerialization {\n \n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestManifestFileSerialization.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestManifestFileSerialization.java\nindex a42781f95282..1e09917d0305 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestManifestFileSerialization.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestManifestFileSerialization.java\n@@ -35,6 +35,7 @@\n import java.io.ObjectOutputStream;\n import java.nio.ByteBuffer;\n import java.nio.ByteOrder;\n+import java.nio.file.Path;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.ManifestFile.PartitionFieldSummary;\n import org.apache.iceberg.hadoop.HadoopFileIO;\n@@ -44,10 +45,8 @@\n import org.apache.iceberg.types.Types;\n import org.apache.spark.SparkConf;\n import org.apache.spark.serializer.KryoSerializer;\n-import org.junit.Assert;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n public class TestManifestFileSerialization {\n \n@@ -99,12 +98,12 @@ public class TestManifestFileSerialization {\n \n   private static final FileIO FILE_IO = new HadoopFileIO(new Configuration());\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n \n   @Test\n   public void testManifestFileKryoSerialization() throws IOException {\n-    File data = temp.newFile();\n-    Assert.assertTrue(data.delete());\n+    File data = File.createTempFile(\"junit\", null, temp.toFile());\n+    assertThat(data.delete()).isTrue();\n \n     Kryo kryo = new KryoSerializer(new SparkConf()).newKryo();\n \n@@ -148,55 +147,60 @@ public void testManifestFileJavaSerialization() throws Exception {\n   }\n \n   private void checkManifestFile(ManifestFile expected, ManifestFile actual) {\n-    Assert.assertEquals(\"Path must match\", expected.path(), actual.path());\n-    Assert.assertEquals(\"Length must match\", expected.length(), actual.length());\n-    Assert.assertEquals(\"Spec id must match\", expected.partitionSpecId(), actual.partitionSpecId());\n-    Assert.assertEquals(\"Snapshot id must match\", expected.snapshotId(), actual.snapshotId());\n-    Assert.assertEquals(\n-        \"Added files flag must match\", expected.hasAddedFiles(), actual.hasAddedFiles());\n-    Assert.assertEquals(\n-        \"Added files count must match\", expected.addedFilesCount(), actual.addedFilesCount());\n-    Assert.assertEquals(\n-        \"Added rows count must match\", expected.addedRowsCount(), actual.addedRowsCount());\n-    Assert.assertEquals(\n-        \"Existing files flag must match\", expected.hasExistingFiles(), actual.hasExistingFiles());\n-    Assert.assertEquals(\n-        \"Existing files count must match\",\n-        expected.existingFilesCount(),\n-        actual.existingFilesCount());\n-    Assert.assertEquals(\n-        \"Existing rows count must match\", expected.existingRowsCount(), actual.existingRowsCount());\n-    Assert.assertEquals(\n-        \"Deleted files flag must match\", expected.hasDeletedFiles(), actual.hasDeletedFiles());\n-    Assert.assertEquals(\n-        \"Deleted files count must match\", expected.deletedFilesCount(), actual.deletedFilesCount());\n-    Assert.assertEquals(\n-        \"Deleted rows count must match\", expected.deletedRowsCount(), actual.deletedRowsCount());\n+    assertThat(actual.path()).as(\"Path must match\").isEqualTo(expected.path());\n+    assertThat(actual.length()).as(\"Length must match\").isEqualTo(expected.length());\n+    assertThat(actual.partitionSpecId())\n+        .as(\"Spec id must match\")\n+        .isEqualTo(expected.partitionSpecId());\n+    assertThat(actual.snapshotId()).as(\"Snapshot id must match\").isEqualTo(expected.snapshotId());\n+    assertThat(actual.hasAddedFiles())\n+        .as(\"Added files flag must match\")\n+        .isEqualTo(expected.hasAddedFiles());\n+    assertThat(actual.addedFilesCount())\n+        .as(\"Added files count must match\")\n+        .isEqualTo(expected.addedFilesCount());\n+    assertThat(actual.addedRowsCount())\n+        .as(\"Added rows count must match\")\n+        .isEqualTo(expected.addedRowsCount());\n+    assertThat(actual.hasExistingFiles())\n+        .as(\"Existing files flag must match\")\n+        .isEqualTo(expected.hasExistingFiles());\n+    assertThat(actual.existingFilesCount())\n+        .as(\"Existing files count must match\")\n+        .isEqualTo(expected.existingFilesCount());\n+    assertThat(actual.existingRowsCount())\n+        .as(\"Existing rows count must match\")\n+        .isEqualTo(expected.existingRowsCount());\n+    assertThat(actual.hasDeletedFiles())\n+        .as(\"Deleted files flag must match\")\n+        .isEqualTo(expected.hasDeletedFiles());\n+    assertThat(actual.deletedFilesCount())\n+        .as(\"Deleted files count must match\")\n+        .isEqualTo(expected.deletedFilesCount());\n+    assertThat(actual.deletedRowsCount())\n+        .as(\"Deleted rows count must match\")\n+        .isEqualTo(expected.deletedRowsCount());\n \n     PartitionFieldSummary expectedPartition = expected.partitions().get(0);\n     PartitionFieldSummary actualPartition = actual.partitions().get(0);\n \n-    Assert.assertEquals(\n-        \"Null flag in partition must match\",\n-        expectedPartition.containsNull(),\n-        actualPartition.containsNull());\n-    Assert.assertEquals(\n-        \"NaN flag in partition must match\",\n-        expectedPartition.containsNaN(),\n-        actualPartition.containsNaN());\n-    Assert.assertEquals(\n-        \"Lower bounds in partition must match\",\n-        expectedPartition.lowerBound(),\n-        actualPartition.lowerBound());\n-    Assert.assertEquals(\n-        \"Upper bounds in partition must match\",\n-        expectedPartition.upperBound(),\n-        actualPartition.upperBound());\n+    assertThat(actualPartition.containsNull())\n+        .as(\"Null flag in partition must match\")\n+        .isEqualTo(expectedPartition.containsNull());\n+    assertThat(actualPartition.containsNaN())\n+        .as(\"NaN flag in partition must match\")\n+        .isEqualTo(expectedPartition.containsNaN());\n+    assertThat(actualPartition.lowerBound())\n+        .as(\"Lower bounds in partition must match\")\n+        .isEqualTo(expectedPartition.lowerBound());\n+    assertThat(actualPartition.upperBound())\n+        .as(\"Upper bounds in partition must match\")\n+        .isEqualTo(expectedPartition.upperBound());\n   }\n \n   private ManifestFile writeManifest(DataFile... files) throws IOException {\n-    File manifestFile = temp.newFile(\"input.m0.avro\");\n-    Assert.assertTrue(manifestFile.delete());\n+    File manifestFile = File.createTempFile(\"input.m0\", \".avro\", temp.toFile());\n+    assertThat(manifestFile.delete()).isTrue();\n     OutputFile outputFile = FILE_IO.newOutputFile(manifestFile.getCanonicalPath());\n \n     ManifestWriter<DataFile> writer = ManifestFiles.write(SPEC, outputFile);\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java\nindex b9e4b04953d5..4fdbc862ee8c 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java\n@@ -32,6 +32,7 @@\n import java.io.ObjectInputStream;\n import java.io.ObjectOutputStream;\n import java.nio.file.Files;\n+import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n import org.apache.hadoop.conf.Configuration;\n@@ -40,20 +41,18 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n-import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.spark.source.ThreeColumnRecord;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.SparkConf;\n import org.apache.spark.serializer.KryoSerializer;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n-public class TestScanTaskSerialization extends SparkTestBase {\n+public class TestScanTaskSerialization extends TestBase {\n \n   private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n   private static final Schema SCHEMA =\n@@ -62,13 +61,13 @@ public class TestScanTaskSerialization extends SparkTestBase {\n           optional(2, \"c2\", Types.StringType.get()),\n           optional(3, \"c3\", Types.StringType.get()));\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n+  @TempDir private File tableDir;\n \n   private String tableLocation = null;\n \n-  @Before\n+  @BeforeEach\n   public void setupTableLocation() throws Exception {\n-    File tableDir = temp.newFolder();\n     this.tableLocation = tableDir.toURI().toString();\n   }\n \n@@ -76,8 +75,8 @@ public void setupTableLocation() throws Exception {\n   public void testBaseCombinedScanTaskKryoSerialization() throws Exception {\n     BaseCombinedScanTask scanTask = prepareBaseCombinedScanTaskForSerDeTest();\n \n-    File data = temp.newFile();\n-    Assert.assertTrue(data.delete());\n+    File data = File.createTempFile(\"junit\", null, temp.toFile());\n+    assertThat(data.delete()).isTrue();\n     Kryo kryo = new KryoSerializer(new SparkConf()).newKryo();\n \n     try (Output out = new Output(new FileOutputStream(data))) {\n@@ -117,10 +116,10 @@ public void testBaseCombinedScanTaskJavaSerialization() throws Exception {\n   public void testBaseScanTaskGroupKryoSerialization() throws Exception {\n     BaseScanTaskGroup<FileScanTask> taskGroup = prepareBaseScanTaskGroupForSerDeTest();\n \n-    Assert.assertFalse(\"Task group can't be empty\", taskGroup.tasks().isEmpty());\n+    assertThat(taskGroup.tasks()).as(\"Task group can't be empty\").isNotEmpty();\n \n-    File data = temp.newFile();\n-    Assert.assertTrue(data.delete());\n+    File data = File.createTempFile(\"junit\", null, temp.toFile());\n+    assertThat(data.delete()).isTrue();\n     Kryo kryo = new KryoSerializer(new SparkConf()).newKryo();\n \n     try (Output out = new Output(Files.newOutputStream(data.toPath()))) {\n@@ -139,7 +138,7 @@ public void testBaseScanTaskGroupKryoSerialization() throws Exception {\n   public void testBaseScanTaskGroupJavaSerialization() throws Exception {\n     BaseScanTaskGroup<FileScanTask> taskGroup = prepareBaseScanTaskGroupForSerDeTest();\n \n-    Assert.assertFalse(\"Task group can't be empty\", taskGroup.tasks().isEmpty());\n+    assertThat(taskGroup.tasks()).as(\"Task group can't be empty\").isNotEmpty();\n \n     ByteArrayOutputStream bytes = new ByteArrayOutputStream();\n     try (ObjectOutputStream out = new ObjectOutputStream(bytes)) {\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestTableSerialization.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestTableSerialization.java\nindex ebab094cbe84..fd6dfd07b568 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestTableSerialization.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/TestTableSerialization.java\n@@ -20,6 +20,7 @@\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.mockito.Mockito.never;\n import static org.mockito.Mockito.spy;\n import static org.mockito.Mockito.times;\n@@ -28,6 +29,9 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n import java.util.List;\n import java.util.Map;\n import org.apache.iceberg.hadoop.HadoopTables;\n@@ -36,29 +40,22 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.spark.source.SerializableTableWithSize;\n import org.apache.iceberg.types.Types;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-\n-@RunWith(Parameterized.class)\n-public class TestTableSerialization {\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n \n-  public TestTableSerialization(String isObjectStoreEnabled) {\n-    this.isObjectStoreEnabled = isObjectStoreEnabled;\n-  }\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestTableSerialization {\n \n-  @Parameterized.Parameters(name = \"isObjectStoreEnabled = {0}\")\n-  public static Object[] parameters() {\n-    return new Object[] {\"true\", \"false\"};\n+  @Parameters(name = \"isObjectStoreEnabled = {0}\")\n+  public static List<String> parameters() {\n+    return Arrays.asList(\"true\", \"false\");\n   }\n \n   private static final HadoopTables TABLES = new HadoopTables();\n \n-  private final String isObjectStoreEnabled;\n+  @Parameter private String isObjectStoreEnabled;\n \n   private static final Schema SCHEMA =\n       new Schema(\n@@ -72,21 +69,21 @@ public static Object[] parameters() {\n \n   private static final SortOrder SORT_ORDER = SortOrder.builderFor(SCHEMA).asc(\"id\").build();\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n   private Table table;\n \n-  @Before\n+  @BeforeEach\n   public void initTable() throws IOException {\n     Map<String, String> props =\n         ImmutableMap.of(\"k1\", \"v1\", TableProperties.OBJECT_STORE_ENABLED, isObjectStoreEnabled);\n \n-    File tableLocation = temp.newFolder();\n-    Assert.assertTrue(tableLocation.delete());\n+    File tableLocation = Files.createTempDirectory(temp, \"junit\").toFile();\n+    assertThat(tableLocation.delete()).isTrue();\n \n     this.table = TABLES.create(SCHEMA, SPEC, SORT_ORDER, props, tableLocation.toString());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCloseSerializableTableKryoSerialization() throws Exception {\n     for (Table tbl : tables()) {\n       Table spyTable = spy(tbl);\n@@ -107,7 +104,7 @@ public void testCloseSerializableTableKryoSerialization() throws Exception {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCloseSerializableTableJavaSerialization() throws Exception {\n     for (Table tbl : tables()) {\n       Table spyTable = spy(tbl);\n@@ -128,14 +125,14 @@ public void testCloseSerializableTableJavaSerialization() throws Exception {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSerializableTableKryoSerialization() throws IOException {\n     Table serializableTable = SerializableTableWithSize.copyOf(table);\n     TestHelpers.assertSerializedAndLoadedMetadata(\n         table, KryoHelpers.roundTripSerialize(serializableTable));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSerializableMetadataTableKryoSerialization() throws IOException {\n     for (MetadataTableType type : MetadataTableType.values()) {\n       TableOperations ops = ((HasTableOperations) table).operations();\n@@ -148,7 +145,7 @@ public void testSerializableMetadataTableKryoSerialization() throws IOException\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSerializableTransactionTableKryoSerialization() throws IOException {\n     Transaction txn = table.newTransaction();\n \n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestComputeTableStatsAction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestComputeTableStatsAction.java\nindex 26d53832a490..d9d83f7caf5d 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestComputeTableStatsAction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestComputeTableStatsAction.java\n@@ -27,10 +27,9 @@\n \n import java.io.IOException;\n import java.util.List;\n-import java.util.Map;\n-import org.apache.iceberg.BlobMetadata;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.Files;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.StatisticsFile;\n@@ -41,8 +40,8 @@\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.Spark3Util;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.iceberg.spark.SparkWriteOptions;\n import org.apache.iceberg.spark.data.RandomData;\n@@ -56,10 +55,12 @@\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n import org.apache.spark.sql.types.StructType;\n-import org.junit.After;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestComputeTableStatsAction extends SparkCatalogTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestComputeTableStatsAction extends CatalogTestBase {\n \n   private static final Types.StructType LEAF_STRUCT_TYPE =\n       Types.StructType.of(\n@@ -77,12 +78,12 @@ public class TestComputeTableStatsAction extends SparkCatalogTestBase {\n           required(4, \"nestedStructCol\", NESTED_STRUCT_TYPE),\n           required(5, \"stringCol\", Types.StringType.get()));\n \n-  public TestComputeTableStatsAction(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n+  @AfterEach\n+  public void removeTable() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testLoadingTableDirectly() {\n     sql(\"CREATE TABLE %s (id int, data string) USING iceberg\", tableName);\n     sql(\"INSERT into %s values(1, 'abcd')\", tableName);\n@@ -92,11 +93,11 @@ public void testLoadingTableDirectly() {\n     SparkActions actions = SparkActions.get();\n     ComputeTableStats.Result results = actions.computeTableStats(table).execute();\n     StatisticsFile statisticsFile = results.statisticsFile();\n-    assertThat(statisticsFile.fileSizeInBytes()).isNotEqualTo(0);\n-    assertThat(statisticsFile.blobMetadata().size()).isEqualTo(2);\n+    assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n+    assertThat(statisticsFile.blobMetadata()).hasSize(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsAction() throws NoSuchTableException, ParseException {\n     sql(\"CREATE TABLE %s (id int, data string) USING iceberg\", tableName);\n     Table table = Spark3Util.loadIcebergTable(spark, tableName);\n@@ -122,18 +123,23 @@ public void testComputeTableStatsAction() throws NoSuchTableException, ParseExce\n     assertThat(results).isNotNull();\n \n     List<StatisticsFile> statisticsFiles = table.statisticsFiles();\n-    assertThat(statisticsFiles.size()).isEqualTo(1);\n-\n-    StatisticsFile statisticsFile = statisticsFiles.get(0);\n-    assertThat(statisticsFile.fileSizeInBytes()).isNotEqualTo(0);\n-    assertThat(statisticsFile.blobMetadata().size()).isEqualTo(2);\n-\n-    BlobMetadata blobMetadata = statisticsFile.blobMetadata().get(0);\n-    assertThat(blobMetadata.properties().get(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY))\n-        .isEqualTo(String.valueOf(4));\n+    assertThat(statisticsFiles)\n+        .singleElement()\n+        .satisfies(\n+            statisticsFile -> {\n+              assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n+              assertThat(statisticsFile.blobMetadata())\n+                  .hasSize(2)\n+                  .element(0)\n+                  .satisfies(\n+                      blobMetadata -> {\n+                        assertThat(blobMetadata.properties())\n+                            .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"4\");\n+                      });\n+            });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsActionWithoutExplicitColumns()\n       throws NoSuchTableException, ParseException {\n     sql(\"CREATE TABLE %s (id int, data string) USING iceberg\", tableName);\n@@ -154,29 +160,26 @@ public void testComputeTableStatsActionWithoutExplicitColumns()\n     ComputeTableStats.Result results = actions.computeTableStats(table).execute();\n     assertThat(results).isNotNull();\n \n-    assertThat(table.statisticsFiles().size()).isEqualTo(1);\n-    StatisticsFile statisticsFile = table.statisticsFiles().get(0);\n-    assertThat(statisticsFile.blobMetadata().size()).isEqualTo(2);\n-    assertThat(statisticsFile.fileSizeInBytes()).isNotEqualTo(0);\n-    assertThat(\n-            Long.parseLong(\n-                statisticsFile\n-                    .blobMetadata()\n-                    .get(0)\n-                    .properties()\n-                    .get(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY)))\n-        .isEqualTo(4);\n-    assertThat(\n-            Long.parseLong(\n-                statisticsFile\n-                    .blobMetadata()\n-                    .get(1)\n-                    .properties()\n-                    .get(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY)))\n-        .isEqualTo(4);\n+    assertThat(table.statisticsFiles())\n+        .singleElement()\n+        .satisfies(\n+            statisticsFile -> {\n+              assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n+              assertThat(statisticsFile.blobMetadata())\n+                  .hasSize(2)\n+                  .satisfiesExactly(\n+                      blobMetadata -> {\n+                        assertThat(blobMetadata.properties())\n+                            .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"4\");\n+                      },\n+                      blobMetadata -> {\n+                        assertThat(blobMetadata.properties())\n+                            .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"4\");\n+                      });\n+            });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsForInvalidColumns() throws NoSuchTableException, ParseException {\n     sql(\"CREATE TABLE %s (id int, data string) USING iceberg\", tableName);\n     // Append data to create snapshot\n@@ -188,7 +191,7 @@ public void testComputeTableStatsForInvalidColumns() throws NoSuchTableException\n         .hasMessageContaining(\"Can't find column id1 in table\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsWithNoSnapshots() throws NoSuchTableException, ParseException {\n     sql(\"CREATE TABLE %s (id int, data string) USING iceberg\", tableName);\n     Table table = Spark3Util.loadIcebergTable(spark, tableName);\n@@ -197,7 +200,7 @@ public void testComputeTableStatsWithNoSnapshots() throws NoSuchTableException,\n     assertThat(result.statisticsFile()).isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsWithNullValues() throws NoSuchTableException, ParseException {\n     sql(\"CREATE TABLE %s (id int, data string) USING iceberg\", tableName);\n     List<SimpleRecord> records =\n@@ -217,19 +220,22 @@ public void testComputeTableStatsWithNullValues() throws NoSuchTableException, P\n     ComputeTableStats.Result results = actions.computeTableStats(table).columns(\"data\").execute();\n     assertThat(results).isNotNull();\n \n-    List<StatisticsFile> statisticsFiles = table.statisticsFiles();\n-    assertThat(statisticsFiles.size()).isEqualTo(1);\n-\n-    StatisticsFile statisticsFile = statisticsFiles.get(0);\n-    assertThat(statisticsFile.fileSizeInBytes()).isNotEqualTo(0);\n-    assertThat(statisticsFile.blobMetadata().size()).isEqualTo(1);\n-\n-    BlobMetadata blobMetadata = statisticsFile.blobMetadata().get(0);\n-    assertThat(blobMetadata.properties().get(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY))\n-        .isEqualTo(String.valueOf(4));\n+    assertThat(table.statisticsFiles())\n+        .singleElement()\n+        .satisfies(\n+            statisticsFile -> {\n+              assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n+              assertThat(statisticsFile.blobMetadata())\n+                  .singleElement()\n+                  .satisfies(\n+                      blobMetadata -> {\n+                        assertThat(blobMetadata.properties())\n+                            .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"4\");\n+                      });\n+            });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsWithSnapshotHavingDifferentSchemas()\n       throws NoSuchTableException, ParseException {\n     SparkActions actions = SparkActions.get();\n@@ -260,7 +266,7 @@ public void testComputeTableStatsWithSnapshotHavingDifferentSchemas()\n         .hasMessageContaining(\"Can't find column data in table\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsWhenSnapshotIdNotSpecified()\n       throws NoSuchTableException, ParseException {\n     sql(\"CREATE TABLE %s (id int, data string) USING iceberg\", tableName);\n@@ -272,19 +278,22 @@ public void testComputeTableStatsWhenSnapshotIdNotSpecified()\n \n     assertThat(results).isNotNull();\n \n-    List<StatisticsFile> statisticsFiles = table.statisticsFiles();\n-    assertThat(statisticsFiles.size()).isEqualTo(1);\n-\n-    StatisticsFile statisticsFile = statisticsFiles.get(0);\n-    assertThat(statisticsFile.fileSizeInBytes()).isNotEqualTo(0);\n-    assertThat(statisticsFile.blobMetadata().size()).isEqualTo(1);\n-\n-    BlobMetadata blobMetadata = statisticsFile.blobMetadata().get(0);\n-    assertThat(blobMetadata.properties().get(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY))\n-        .isEqualTo(String.valueOf(1));\n+    assertThat(table.statisticsFiles())\n+        .singleElement()\n+        .satisfies(\n+            statisticsFile -> {\n+              assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n+              assertThat(statisticsFile.blobMetadata())\n+                  .singleElement()\n+                  .satisfies(\n+                      blobMetadata -> {\n+                        assertThat(blobMetadata.properties())\n+                            .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"1\");\n+                      });\n+            });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsWithNestedSchema()\n       throws NoSuchTableException, ParseException, IOException {\n     List<Record> records = Lists.newArrayList(createNestedRecord());\n@@ -294,8 +303,7 @@ public void testComputeTableStatsWithNestedSchema()\n             SCHEMA_WITH_NESTED_COLUMN,\n             PartitionSpec.unpartitioned(),\n             ImmutableMap.of());\n-    DataFile dataFile =\n-        FileHelpers.writeDataFile(table, Files.localOutput(temp.newFile()), records);\n+    DataFile dataFile = FileHelpers.writeDataFile(table, Files.localOutput(temp.toFile()), records);\n     table.newAppend().appendFile(dataFile).commit();\n \n     Table tbl = Spark3Util.loadIcebergTable(spark, tableName);\n@@ -303,21 +311,22 @@ public void testComputeTableStatsWithNestedSchema()\n     actions.computeTableStats(tbl).execute();\n \n     tbl.refresh();\n-    List<StatisticsFile> statisticsFiles = tbl.statisticsFiles();\n-    assertThat(statisticsFiles.size()).isEqualTo(1);\n-    StatisticsFile statisticsFile = statisticsFiles.get(0);\n-    assertThat(statisticsFile.fileSizeInBytes()).isNotEqualTo(0);\n-    assertThat(statisticsFile.blobMetadata().size()).isEqualTo(1);\n+    assertThat(tbl.statisticsFiles())\n+        .singleElement()\n+        .satisfies(\n+            statisticsFile -> {\n+              assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n+              assertThat(statisticsFile.blobMetadata()).hasSize(1);\n+            });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsWithNoComputableColumns() throws IOException {\n     List<Record> records = Lists.newArrayList(createNestedRecord());\n     Table table =\n         validationCatalog.createTable(\n             tableIdent, NESTED_SCHEMA, PartitionSpec.unpartitioned(), ImmutableMap.of());\n-    DataFile dataFile =\n-        FileHelpers.writeDataFile(table, Files.localOutput(temp.newFile()), records);\n+    DataFile dataFile = FileHelpers.writeDataFile(table, Files.localOutput(temp.toFile()), records);\n     table.newAppend().appendFile(dataFile).commit();\n \n     table.refresh();\n@@ -327,48 +336,48 @@ public void testComputeTableStatsWithNoComputableColumns() throws IOException {\n         .hasMessageContaining(\"No columns found to compute stats\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsOnByteColumn() throws NoSuchTableException, ParseException {\n     testComputeTableStats(\"byte_col\", \"TINYINT\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsOnShortColumn() throws NoSuchTableException, ParseException {\n     testComputeTableStats(\"short_col\", \"SMALLINT\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsOnIntColumn() throws NoSuchTableException, ParseException {\n     testComputeTableStats(\"int_col\", \"INT\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsOnLongColumn() throws NoSuchTableException, ParseException {\n     testComputeTableStats(\"long_col\", \"BIGINT\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsOnTimestampColumn() throws NoSuchTableException, ParseException {\n     testComputeTableStats(\"timestamp_col\", \"TIMESTAMP\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsOnTimestampNtzColumn()\n       throws NoSuchTableException, ParseException {\n     testComputeTableStats(\"timestamp_col\", \"TIMESTAMP_NTZ\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsOnDateColumn() throws NoSuchTableException, ParseException {\n     testComputeTableStats(\"date_col\", \"DATE\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsOnDecimalColumn() throws NoSuchTableException, ParseException {\n     testComputeTableStats(\"decimal_col\", \"DECIMAL(20, 2)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testComputeTableStatsOnBinaryColumn() throws NoSuchTableException, ParseException {\n     testComputeTableStats(\"binary_col\", \"BINARY\");\n   }\n@@ -387,16 +396,19 @@ public void testComputeTableStats(String columnName, String type)\n         actions.computeTableStats(table).columns(columnName).execute();\n     assertThat(results).isNotNull();\n \n-    List<StatisticsFile> statisticsFiles = table.statisticsFiles();\n-    assertThat(statisticsFiles.size()).isEqualTo(1);\n-\n-    StatisticsFile statisticsFile = statisticsFiles.get(0);\n-    assertThat(statisticsFile.fileSizeInBytes()).isNotEqualTo(0);\n-    assertThat(statisticsFile.blobMetadata().size()).isEqualTo(1);\n-\n-    BlobMetadata blobMetadata = statisticsFile.blobMetadata().get(0);\n-    assertThat(blobMetadata.properties().get(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY))\n-        .isNotNull();\n+    assertThat(table.statisticsFiles())\n+        .singleElement()\n+        .satisfies(\n+            statisticsFile -> {\n+              assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n+              assertThat(statisticsFile.blobMetadata())\n+                  .singleElement()\n+                  .satisfies(\n+                      blobMetadata -> {\n+                        assertThat(blobMetadata.properties())\n+                            .containsKey(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY);\n+                      });\n+            });\n   }\n \n   private GenericRecord createNestedRecord() {\n@@ -422,9 +434,4 @@ private void append(String table, Dataset<Row> df) throws NoSuchTableException {\n     // fanout writes are enabled as write-time clustering is not supported without Spark extensions\n     df.coalesce(1).writeTo(table).option(SparkWriteOptions.FANOUT_ENABLED, \"true\").append();\n   }\n-\n-  @After\n-  public void removeTable() {\n-    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n-  }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java\nindex 06f118be0c6f..dd751499df30 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java\n@@ -18,10 +18,14 @@\n  */\n package org.apache.iceberg.spark.actions;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n+\n import java.io.File;\n import java.io.FilenameFilter;\n import java.io.IOException;\n import java.net.URI;\n+import java.nio.file.Files;\n import java.nio.file.Paths;\n import java.util.Arrays;\n import java.util.Collections;\n@@ -31,6 +35,9 @@\n import org.apache.commons.io.FileUtils;\n import org.apache.commons.io.filefilter.TrueFileFilter;\n import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n@@ -40,9 +47,9 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.spark.Spark3Util;\n import org.apache.iceberg.spark.SparkCatalog;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.iceberg.spark.source.SimpleRecord;\n import org.apache.iceberg.spark.source.SparkTable;\n@@ -68,20 +75,18 @@\n import org.apache.spark.sql.types.Metadata;\n import org.apache.spark.sql.types.StructField;\n import org.apache.spark.sql.types.StructType;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n import scala.Option;\n import scala.Some;\n import scala.collection.JavaConverters;\n import scala.collection.Seq;\n \n-public class TestCreateActions extends SparkCatalogTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestCreateActions extends CatalogTestBase {\n   private static final String CREATE_PARTITIONED_PARQUET =\n       \"CREATE TABLE %s (id INT, data STRING) \" + \"using parquet PARTITIONED BY (id) LOCATION '%s'\";\n   private static final String CREATE_PARQUET =\n@@ -94,70 +99,72 @@ public class TestCreateActions extends SparkCatalogTestBase {\n \n   private static final String NAMESPACE = \"default\";\n \n-  @Parameterized.Parameters(name = \"Catalog Name {0} - Options {2}\")\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}, type = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n       new Object[] {\n         \"spark_catalog\",\n         SparkSessionCatalog.class.getName(),\n         ImmutableMap.of(\n-            \"type\", \"hive\",\n-            \"default-namespace\", \"default\",\n-            \"parquet-enabled\", \"true\",\n+            \"type\",\n+            \"hive\",\n+            \"default-namespace\",\n+            \"default\",\n+            \"parquet-enabled\",\n+            \"true\",\n             \"cache-enabled\",\n-                \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n-            )\n+            \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+            ),\n+        \"hive\"\n       },\n       new Object[] {\n         \"spark_catalog\",\n         SparkSessionCatalog.class.getName(),\n         ImmutableMap.of(\n-            \"type\", \"hadoop\",\n-            \"default-namespace\", \"default\",\n-            \"parquet-enabled\", \"true\",\n+            \"type\",\n+            \"hadoop\",\n+            \"default-namespace\",\n+            \"default\",\n+            \"parquet-enabled\",\n+            \"true\",\n             \"cache-enabled\",\n-                \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n-            )\n+            \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n+            ),\n+        \"hadoop\"\n       },\n       new Object[] {\n         \"testhive\",\n         SparkCatalog.class.getName(),\n         ImmutableMap.of(\n             \"type\", \"hive\",\n-            \"default-namespace\", \"default\")\n+            \"default-namespace\", \"default\"),\n+        \"hive\"\n       },\n       new Object[] {\n         \"testhadoop\",\n         SparkCatalog.class.getName(),\n         ImmutableMap.of(\n             \"type\", \"hadoop\",\n-            \"default-namespace\", \"default\")\n+            \"default-namespace\", \"default\"),\n+        \"hadoop\"\n       }\n     };\n   }\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n-\n   private final String baseTableName = \"baseTable\";\n-  private File tableDir;\n+  @TempDir private File tableDir;\n   private String tableLocation;\n-  private final String type;\n-  private final TableCatalog catalog;\n \n-  public TestCreateActions(String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-    this.catalog = (TableCatalog) spark.sessionState().catalogManager().catalog(catalogName);\n-    this.type = config.get(\"type\");\n-  }\n+  @Parameter(index = 3)\n+  private String type;\n \n-  @Before\n+  private TableCatalog catalog;\n+\n+  @BeforeEach\n   public void before() {\n-    try {\n-      this.tableDir = temp.newFolder();\n-    } catch (IOException e) {\n-      throw new RuntimeException(e);\n-    }\n+    super.before();\n     this.tableLocation = tableDir.toURI().toString();\n+    this.catalog = (TableCatalog) spark.sessionState().catalogManager().catalog(catalogName);\n \n     spark.conf().set(\"hive.exec.dynamic.partition\", \"true\");\n     spark.conf().set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\");\n@@ -179,7 +186,7 @@ public void before() {\n         .saveAsTable(baseTableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void after() throws IOException {\n     // Drop the hive table.\n     spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", baseTableName));\n@@ -190,25 +197,27 @@ public void after() throws IOException {\n     spark.conf().unset(\"spark.sql.catalog.spark_catalog.cache-enabled\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMigratePartitioned() throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n-    Assume.assumeTrue(\n-        \"Can only migrate from Spark Session Catalog\", catalog.name().equals(\"spark_catalog\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n+    assumeThat(catalog.name())\n+        .as(\"Can only migrate from Spark Session Catalog\")\n+        .isEqualTo(\"spark_catalog\");\n     String source = sourceName(\"test_migrate_partitioned_table\");\n     String dest = source;\n     createSourceTable(CREATE_PARTITIONED_PARQUET, source);\n     assertMigratedFileCount(SparkActions.get().migrateTable(source), source, dest);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedTableWithUnRecoveredPartitions() throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n-    Assume.assumeTrue(\n-        \"Can only migrate from Spark Session Catalog\", catalog.name().equals(\"spark_catalog\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n+    assumeThat(catalog.name())\n+        .as(\"Can only migrate from Spark Session Catalog\")\n+        .isEqualTo(\"spark_catalog\");\n     String source = sourceName(\"test_unrecovered_partitions\");\n     String dest = source;\n-    File location = temp.newFolder();\n+    File location = Files.createTempDirectory(temp, \"junit\").toFile();\n     sql(CREATE_PARTITIONED_PARQUET, source, location);\n \n     // Data generation and partition addition\n@@ -224,15 +233,16 @@ public void testPartitionedTableWithUnRecoveredPartitions() throws Exception {\n     assertMigratedFileCount(SparkActions.get().migrateTable(source), source, dest);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedTableWithCustomPartitions() throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n-    Assume.assumeTrue(\n-        \"Can only migrate from Spark Session Catalog\", catalog.name().equals(\"spark_catalog\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n+    assumeThat(catalog.name())\n+        .as(\"Can only migrate from Spark Session Catalog\")\n+        .isEqualTo(\"spark_catalog\");\n     String source = sourceName(\"test_custom_parts\");\n     String dest = source;\n-    File tblLocation = temp.newFolder();\n-    File partitionDataLoc = temp.newFolder();\n+    File tblLocation = Files.createTempDirectory(temp, \"junit\").toFile();\n+    File partitionDataLoc = Files.createTempDirectory(temp, \"junit\").toFile();\n \n     // Data generation and partition addition\n     spark.sql(String.format(CREATE_PARTITIONED_PARQUET, source, tblLocation));\n@@ -248,11 +258,12 @@ public void testPartitionedTableWithCustomPartitions() throws Exception {\n     assertMigratedFileCount(SparkActions.get().migrateTable(source), source, dest);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddColumnOnMigratedTableAtEnd() throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n-    Assume.assumeTrue(\n-        \"Can only migrate from Spark Session Catalog\", catalog.name().equals(\"spark_catalog\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n+    assumeThat(catalog.name())\n+        .as(\"Can only migrate from Spark Session Catalog\")\n+        .isEqualTo(\"spark_catalog\");\n     String source = sourceName(\"test_add_column_migrated_table\");\n     String dest = source;\n     createSourceTable(CREATE_PARQUET, source);\n@@ -269,30 +280,31 @@ public void testAddColumnOnMigratedTableAtEnd() throws Exception {\n     String newCol1 = \"newCol1\";\n     sparkTable.table().updateSchema().addColumn(newCol1, Types.IntegerType.get()).commit();\n     Schema afterSchema = table.schema();\n-    Assert.assertNull(beforeSchema.findField(newCol1));\n-    Assert.assertNotNull(afterSchema.findField(newCol1));\n+    assertThat(beforeSchema.findField(newCol1)).isNull();\n+    assertThat(afterSchema.findField(newCol1)).isNotNull();\n \n     // reads should succeed without any exceptions\n     List<Object[]> results1 = sql(\"select * from %s order by id\", dest);\n-    Assert.assertFalse(results1.isEmpty());\n+    assertThat(results1).isNotEmpty();\n     assertEquals(\"Output must match\", results1, expected1);\n \n     String newCol2 = \"newCol2\";\n     sql(\"ALTER TABLE %s ADD COLUMN %s INT\", dest, newCol2);\n     StructType schema = spark.table(dest).schema();\n-    Assert.assertTrue(Arrays.asList(schema.fieldNames()).contains(newCol2));\n+    assertThat(schema.fieldNames()).contains(newCol2);\n \n     // reads should succeed without any exceptions\n     List<Object[]> results2 = sql(\"select * from %s order by id\", dest);\n-    Assert.assertFalse(results2.isEmpty());\n+    assertThat(results2).isNotEmpty();\n     assertEquals(\"Output must match\", results2, expected2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddColumnOnMigratedTableAtMiddle() throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n-    Assume.assumeTrue(\n-        \"Can only migrate from Spark Session Catalog\", catalog.name().equals(\"spark_catalog\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n+    assumeThat(catalog.name())\n+        .as(\"Can only migrate from Spark Session Catalog\")\n+        .isEqualTo(\"spark_catalog\");\n     String source = sourceName(\"test_add_column_migrated_table_middle\");\n     String dest = source;\n     createSourceTable(CREATE_PARQUET, source);\n@@ -313,26 +325,26 @@ public void testAddColumnOnMigratedTableAtMiddle() throws Exception {\n         .moveAfter(newCol1, \"id\")\n         .commit();\n     Schema afterSchema = table.schema();\n-    Assert.assertNull(beforeSchema.findField(newCol1));\n-    Assert.assertNotNull(afterSchema.findField(newCol1));\n+    assertThat(beforeSchema.findField(newCol1)).isNull();\n+    assertThat(afterSchema.findField(newCol1)).isNotNull();\n \n     // reads should succeed\n     List<Object[]> results = sql(\"select * from %s order by id\", dest);\n-    Assert.assertFalse(results.isEmpty());\n+    assertThat(results).isNotEmpty();\n     assertEquals(\"Output must match\", results, expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void removeColumnsAtEnd() throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n-    Assume.assumeTrue(\n-        \"Can only migrate from Spark Session Catalog\", catalog.name().equals(\"spark_catalog\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n+    assumeThat(catalog.name())\n+        .as(\"Can only migrate from Spark Session Catalog\")\n+        .isEqualTo(\"spark_catalog\");\n     String source = sourceName(\"test_remove_column_migrated_table\");\n     String dest = source;\n \n     String colName1 = \"newCol1\";\n     String colName2 = \"newCol2\";\n-    File location = temp.newFolder();\n     spark\n         .range(10)\n         .selectExpr(\"cast(id as INT)\", \"CAST(id as INT) \" + colName1, \"CAST(id as INT) \" + colName2)\n@@ -351,29 +363,30 @@ public void removeColumnsAtEnd() throws Exception {\n     Schema beforeSchema = table.schema();\n     sparkTable.table().updateSchema().deleteColumn(colName1).commit();\n     Schema afterSchema = table.schema();\n-    Assert.assertNotNull(beforeSchema.findField(colName1));\n-    Assert.assertNull(afterSchema.findField(colName1));\n+    assertThat(beforeSchema.findField(colName1)).isNotNull();\n+    assertThat(afterSchema.findField(colName1)).isNull();\n \n     // reads should succeed without any exceptions\n     List<Object[]> results1 = sql(\"select * from %s order by id\", dest);\n-    Assert.assertFalse(results1.isEmpty());\n+    assertThat(results1).isNotEmpty();\n     assertEquals(\"Output must match\", expected1, results1);\n \n     sql(\"ALTER TABLE %s DROP COLUMN %s\", dest, colName2);\n     StructType schema = spark.table(dest).schema();\n-    Assert.assertFalse(Arrays.asList(schema.fieldNames()).contains(colName2));\n+    assertThat(schema.fieldNames()).doesNotContain(colName2);\n \n     // reads should succeed without any exceptions\n     List<Object[]> results2 = sql(\"select * from %s order by id\", dest);\n-    Assert.assertFalse(results2.isEmpty());\n+    assertThat(results2).isNotEmpty();\n     assertEquals(\"Output must match\", expected2, results2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void removeColumnFromMiddle() throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n-    Assume.assumeTrue(\n-        \"Can only migrate from Spark Session Catalog\", catalog.name().equals(\"spark_catalog\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n+    assumeThat(catalog.name())\n+        .as(\"Can only migrate from Spark Session Catalog\")\n+        .isEqualTo(\"spark_catalog\");\n     String source = sourceName(\"test_remove_column_migrated_table_from_middle\");\n     String dest = source;\n     String dropColumnName = \"col1\";\n@@ -393,31 +406,32 @@ public void removeColumnFromMiddle() throws Exception {\n     // drop column\n     sql(\"ALTER TABLE %s DROP COLUMN %s\", dest, \"col1\");\n     StructType schema = spark.table(dest).schema();\n-    Assert.assertFalse(Arrays.asList(schema.fieldNames()).contains(dropColumnName));\n+    assertThat(schema.fieldNames()).doesNotContain(dropColumnName);\n \n     // reads should return same output as that of non-iceberg table\n     List<Object[]> results = sql(\"select * from %s order by id\", dest);\n-    Assert.assertFalse(results.isEmpty());\n+    assertThat(results).isNotEmpty();\n     assertEquals(\"Output must match\", expected, results);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMigrateUnpartitioned() throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n-    Assume.assumeTrue(\n-        \"Can only migrate from Spark Session Catalog\", catalog.name().equals(\"spark_catalog\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n+    assumeThat(catalog.name())\n+        .as(\"Can only migrate from Spark Session Catalog\")\n+        .isEqualTo(\"spark_catalog\");\n     String source = sourceName(\"test_migrate_unpartitioned_table\");\n     String dest = source;\n     createSourceTable(CREATE_PARQUET, source);\n     assertMigratedFileCount(SparkActions.get().migrateTable(source), source, dest);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSnapshotPartitioned() throws Exception {\n-    Assume.assumeTrue(\n-        \"Cannot snapshot with arbitrary location in a hadoop based catalog\",\n-        !type.equals(\"hadoop\"));\n-    File location = temp.newFolder();\n+    assumeThat(type)\n+        .as(\"Cannot snapshot with arbitrary location in a hadoop based catalog\")\n+        .isNotEqualTo(\"hadoop\");\n+    File location = Files.createTempDirectory(temp, \"junit\").toFile();\n     String source = sourceName(\"test_snapshot_partitioned_table\");\n     String dest = destName(\"iceberg_snapshot_partitioned\");\n     createSourceTable(CREATE_PARTITIONED_PARQUET, source);\n@@ -428,12 +442,12 @@ public void testSnapshotPartitioned() throws Exception {\n     assertIsolatedSnapshot(source, dest);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSnapshotUnpartitioned() throws Exception {\n-    Assume.assumeTrue(\n-        \"Cannot snapshot with arbitrary location in a hadoop based catalog\",\n-        !type.equals(\"hadoop\"));\n-    File location = temp.newFolder();\n+    assumeThat(type)\n+        .as(\"Cannot snapshot with arbitrary location in a hadoop based catalog\")\n+        .isNotEqualTo(\"hadoop\");\n+    File location = Files.createTempDirectory(temp, \"junit\").toFile();\n     String source = sourceName(\"test_snapshot_unpartitioned_table\");\n     String dest = destName(\"iceberg_snapshot_unpartitioned\");\n     createSourceTable(CREATE_PARQUET, source);\n@@ -444,12 +458,12 @@ public void testSnapshotUnpartitioned() throws Exception {\n     assertIsolatedSnapshot(source, dest);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSnapshotHiveTable() throws Exception {\n-    Assume.assumeTrue(\n-        \"Cannot snapshot with arbitrary location in a hadoop based catalog\",\n-        !type.equals(\"hadoop\"));\n-    File location = temp.newFolder();\n+    assumeThat(type)\n+        .as(\"Cannot snapshot with arbitrary location in a hadoop based catalog\")\n+        .isNotEqualTo(\"hadoop\");\n+    File location = Files.createTempDirectory(temp, \"junit\").toFile();\n     String source = sourceName(\"snapshot_hive_table\");\n     String dest = destName(\"iceberg_snapshot_hive_table\");\n     createSourceTable(CREATE_HIVE_EXTERNAL_PARQUET, source);\n@@ -460,19 +474,19 @@ public void testSnapshotHiveTable() throws Exception {\n     assertIsolatedSnapshot(source, dest);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMigrateHiveTable() throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n     String source = sourceName(\"migrate_hive_table\");\n     String dest = source;\n     createSourceTable(CREATE_HIVE_EXTERNAL_PARQUET, source);\n     assertMigratedFileCount(SparkActions.get().migrateTable(source), source, dest);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSnapshotManagedHiveTable() throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n-    File location = temp.newFolder();\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n+    File location = Files.createTempDirectory(temp, \"junit\").toFile();\n     String source = sourceName(\"snapshot_managed_hive_table\");\n     String dest = destName(\"iceberg_snapshot_managed_hive_table\");\n     createSourceTable(CREATE_HIVE_PARQUET, source);\n@@ -483,10 +497,10 @@ public void testSnapshotManagedHiveTable() throws Exception {\n     assertIsolatedSnapshot(source, dest);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMigrateManagedHiveTable() throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n-    File location = temp.newFolder();\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n+    File location = Files.createTempDirectory(temp, \"junit\").toFile();\n     String source = sourceName(\"migrate_managed_hive_table\");\n     String dest = destName(\"iceberg_migrate_managed_hive_table\");\n     createSourceTable(CREATE_HIVE_PARQUET, source);\n@@ -496,7 +510,7 @@ public void testMigrateManagedHiveTable() throws Exception {\n         dest);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testProperties() throws Exception {\n     String source = sourceName(\"test_properties_table\");\n     String dest = destName(\"iceberg_properties\");\n@@ -520,18 +534,10 @@ public void testProperties() throws Exception {\n     expectedProps.putAll(props);\n     expectedProps.put(\"dogs\", \"sundance\");\n \n-    for (Map.Entry<String, String> entry : expectedProps.entrySet()) {\n-      Assert.assertTrue(\n-          \"Created table missing property \" + entry.getKey(),\n-          table.properties().containsKey(entry.getKey()));\n-      Assert.assertEquals(\n-          \"Property value is not the expected value\",\n-          entry.getValue(),\n-          table.properties().get(entry.getKey()));\n-    }\n+    assertThat(table.properties()).containsAllEntriesOf(expectedProps);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkTableReservedProperties() throws Exception {\n     String destTableName = \"iceberg_reserved_properties\";\n     String source = sourceName(\"test_reserved_properties_table\");\n@@ -546,29 +552,23 @@ public void testSparkTableReservedProperties() throws Exception {\n \n     String[] keys = {\"provider\", \"format\", \"current-snapshot-id\", \"location\", \"sort-order\"};\n \n-    for (String entry : keys) {\n-      Assert.assertTrue(\n-          \"Created table missing reserved property \" + entry,\n-          table.properties().containsKey(entry));\n-    }\n+    assertThat(table.properties())\n+        .as(\"Created table missing reserved properties\")\n+        .containsKeys(keys);\n \n-    Assert.assertEquals(\"Unexpected provider\", \"iceberg\", table.properties().get(\"provider\"));\n-    Assert.assertEquals(\"Unexpected format\", \"iceberg/parquet\", table.properties().get(\"format\"));\n-    Assert.assertNotEquals(\n-        \"No current-snapshot-id found\", \"none\", table.properties().get(\"current-snapshot-id\"));\n-    Assert.assertTrue(\n-        \"Location isn't correct\", table.properties().get(\"location\").endsWith(destTableName));\n+    assertThat(table.properties())\n+        .containsEntry(\"provider\", \"iceberg\")\n+        .containsEntry(\"format\", \"iceberg/parquet\")\n+        .hasEntrySatisfying(\"current-snapshot-id\", id -> assertThat(id).isNotEqualTo(\"none\"))\n+        .hasEntrySatisfying(\"location\", loc -> assertThat(loc).endsWith(destTableName));\n \n-    Assert.assertEquals(\"Unexpected format-version\", \"1\", table.properties().get(\"format-version\"));\n+    assertThat(table.properties()).containsEntry(\"format-version\", \"1\");\n     table.table().updateProperties().set(\"format-version\", \"2\").commit();\n-    Assert.assertEquals(\"Unexpected format-version\", \"2\", table.properties().get(\"format-version\"));\n+    assertThat(table.properties()).containsEntry(\"format-version\", \"2\");\n \n-    Assert.assertEquals(\n-        \"Sort-order isn't correct\",\n-        \"id ASC NULLS FIRST, data DESC NULLS LAST\",\n-        table.properties().get(\"sort-order\"));\n-    Assert.assertNull(\n-        \"Identifier fields should be null\", table.properties().get(\"identifier-fields\"));\n+    assertThat(table.properties())\n+        .containsEntry(\"sort-order\", \"id ASC NULLS FIRST, data DESC NULLS LAST\")\n+        .doesNotContainKey(\"identifier-fields\");\n \n     table\n         .table()\n@@ -577,11 +577,10 @@ public void testSparkTableReservedProperties() throws Exception {\n         .requireColumn(\"id\")\n         .setIdentifierFields(\"id\")\n         .commit();\n-    Assert.assertEquals(\n-        \"Identifier fields aren't correct\", \"[id]\", table.properties().get(\"identifier-fields\"));\n+    assertThat(table.properties()).containsEntry(\"identifier-fields\", \"[id]\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSnapshotDefaultLocation() throws Exception {\n     String source = sourceName(\"test_snapshot_default\");\n     String dest = destName(\"iceberg_snapshot_default\");\n@@ -590,13 +589,14 @@ public void testSnapshotDefaultLocation() throws Exception {\n     assertIsolatedSnapshot(source, dest);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void schemaEvolutionTestWithSparkAPI() throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n-    Assume.assumeTrue(\n-        \"Can only migrate from Spark Session Catalog\", catalog.name().equals(\"spark_catalog\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n+    assumeThat(catalog.name())\n+        .as(\"Can only migrate from Spark Session Catalog\")\n+        .isEqualTo(\"spark_catalog\");\n \n-    File location = temp.newFolder();\n+    File location = Files.createTempDirectory(temp, \"junit\").toFile();\n     String tblName = sourceName(\"schema_evolution_test\");\n \n     // Data generation and partition addition\n@@ -644,11 +644,12 @@ public void schemaEvolutionTestWithSparkAPI() throws Exception {\n     assertEquals(\"Output must match\", expectedAfterAddColumn, afterMigarteAfterAddResults);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void schemaEvolutionTestWithSparkSQL() throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n-    Assume.assumeTrue(\n-        \"Can only migrate from Spark Session Catalog\", catalog.name().equals(\"spark_catalog\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n+    assumeThat(catalog.name())\n+        .as(\"Can only migrate from Spark Session Catalog\")\n+        .isEqualTo(\"spark_catalog\");\n     String tblName = sourceName(\"schema_evolution_test_sql\");\n \n     // Data generation and partition addition\n@@ -691,54 +692,54 @@ public void schemaEvolutionTestWithSparkSQL() throws Exception {\n     assertEquals(\"Output must match\", expectedAfterAddColumn, afterMigarteAfterAddResults);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHiveStyleThreeLevelList() throws Exception {\n     threeLevelList(true);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testThreeLevelList() throws Exception {\n     threeLevelList(false);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHiveStyleThreeLevelListWithNestedStruct() throws Exception {\n     threeLevelListWithNestedStruct(true);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testThreeLevelListWithNestedStruct() throws Exception {\n     threeLevelListWithNestedStruct(false);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHiveStyleThreeLevelLists() throws Exception {\n     threeLevelLists(true);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testThreeLevelLists() throws Exception {\n     threeLevelLists(false);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHiveStyleStructOfThreeLevelLists() throws Exception {\n     structOfThreeLevelLists(true);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testStructOfThreeLevelLists() throws Exception {\n     structOfThreeLevelLists(false);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTwoLevelList() throws IOException {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n \n     spark.conf().set(\"spark.sql.parquet.writeLegacyFormat\", true);\n \n     String tableName = sourceName(\"testTwoLevelList\");\n-    File location = temp.newFolder();\n+    File location = Files.createTempDirectory(temp, \"junit\").toFile();\n \n     StructType sparkSchema =\n         new StructType(\n@@ -798,7 +799,7 @@ public boolean accept(File dir, String name) {\n             HadoopInputFile.fromPath(\n                 new Path(parquetFile.getPath()), spark.sessionState().newHadoopConf()));\n     MessageType schema = pqReader.getFooter().getFileMetaData().getSchema();\n-    Assert.assertEquals(MessageTypeParser.parseMessageType(expectedParquetSchema), schema);\n+    assertThat(schema).isEqualTo(MessageTypeParser.parseMessageType(expectedParquetSchema));\n \n     // create sql table on top of it\n     sql(\n@@ -813,17 +814,17 @@ public boolean accept(File dir, String name) {\n \n     // check migrated table is returning expected result\n     List<Object[]> results = sql(\"SELECT * FROM %s\", tableName);\n-    Assert.assertFalse(results.isEmpty());\n+    assertThat(results).isNotEmpty();\n     assertEquals(\"Output must match\", expected, results);\n   }\n \n   private void threeLevelList(boolean useLegacyMode) throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n \n     spark.conf().set(\"spark.sql.parquet.writeLegacyFormat\", useLegacyMode);\n \n     String tableName = sourceName(String.format(\"threeLevelList_%s\", useLegacyMode));\n-    File location = temp.newFolder();\n+    File location = Files.createTempDirectory(temp, \"junit\").toFile();\n     sql(\n         \"CREATE TABLE %s (col1 ARRAY<STRUCT<col2 INT>>)\" + \" STORED AS parquet\" + \" LOCATION '%s'\",\n         tableName, location);\n@@ -837,18 +838,18 @@ private void threeLevelList(boolean useLegacyMode) throws Exception {\n \n     // check migrated table is returning expected result\n     List<Object[]> results = sql(\"SELECT * FROM %s\", tableName);\n-    Assert.assertFalse(results.isEmpty());\n+    assertThat(results).isNotEmpty();\n     assertEquals(\"Output must match\", expected, results);\n   }\n \n   private void threeLevelListWithNestedStruct(boolean useLegacyMode) throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n \n     spark.conf().set(\"spark.sql.parquet.writeLegacyFormat\", useLegacyMode);\n \n     String tableName =\n         sourceName(String.format(\"threeLevelListWithNestedStruct_%s\", useLegacyMode));\n-    File location = temp.newFolder();\n+    File location = Files.createTempDirectory(temp, \"junit\").toFile();\n     sql(\n         \"CREATE TABLE %s (col1 ARRAY<STRUCT<col2 STRUCT<col3 INT>>>)\"\n             + \" STORED AS parquet\"\n@@ -864,17 +865,17 @@ private void threeLevelListWithNestedStruct(boolean useLegacyMode) throws Except\n \n     // check migrated table is returning expected result\n     List<Object[]> results = sql(\"SELECT * FROM %s\", tableName);\n-    Assert.assertFalse(results.isEmpty());\n+    assertThat(results).isNotEmpty();\n     assertEquals(\"Output must match\", expected, results);\n   }\n \n   private void threeLevelLists(boolean useLegacyMode) throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n \n     spark.conf().set(\"spark.sql.parquet.writeLegacyFormat\", useLegacyMode);\n \n     String tableName = sourceName(String.format(\"threeLevelLists_%s\", useLegacyMode));\n-    File location = temp.newFolder();\n+    File location = Files.createTempDirectory(temp, \"junit\").toFile();\n     sql(\n         \"CREATE TABLE %s (col1 ARRAY<STRUCT<col2 INT>>, col3 ARRAY<STRUCT<col4 INT>>)\"\n             + \" STORED AS parquet\"\n@@ -893,17 +894,17 @@ private void threeLevelLists(boolean useLegacyMode) throws Exception {\n \n     // check migrated table is returning expected result\n     List<Object[]> results = sql(\"SELECT * FROM %s\", tableName);\n-    Assert.assertFalse(results.isEmpty());\n+    assertThat(results).isNotEmpty();\n     assertEquals(\"Output must match\", expected, results);\n   }\n \n   private void structOfThreeLevelLists(boolean useLegacyMode) throws Exception {\n-    Assume.assumeTrue(\"Cannot migrate to a hadoop based catalog\", !type.equals(\"hadoop\"));\n+    assumeThat(type).as(\"Cannot migrate to a hadoop based catalog\").isNotEqualTo(\"hadoop\");\n \n     spark.conf().set(\"spark.sql.parquet.writeLegacyFormat\", useLegacyMode);\n \n     String tableName = sourceName(String.format(\"structOfThreeLevelLists_%s\", useLegacyMode));\n-    File location = temp.newFolder();\n+    File location = Files.createTempDirectory(temp, \"junit\").toFile();\n     sql(\n         \"CREATE TABLE %s (col1 STRUCT<col2 ARRAY<STRUCT<col3 INT>>>)\"\n             + \" STORED AS parquet\"\n@@ -919,7 +920,7 @@ private void structOfThreeLevelLists(boolean useLegacyMode) throws Exception {\n \n     // check migrated table is returning expected result\n     List<Object[]> results = sql(\"SELECT * FROM %s\", tableName);\n-    Assert.assertFalse(results.isEmpty());\n+    assertThat(results).isNotEmpty();\n     assertEquals(\"Output must match\", expected, results);\n   }\n \n@@ -940,7 +941,7 @@ private CatalogTable loadSessionTable(String name)\n \n   private void createSourceTable(String createStatement, String tableName)\n       throws IOException, NoSuchTableException, NoSuchDatabaseException, ParseException {\n-    File location = temp.newFolder();\n+    File location = Files.createTempDirectory(temp, \"junit\").toFile();\n     spark.sql(String.format(createStatement, tableName, location));\n     CatalogTable table = loadSessionTable(tableName);\n     Seq<String> partitionColumns = table.partitionColumnNames();\n@@ -961,8 +962,9 @@ private void assertMigratedFileCount(MigrateTable migrateAction, String source,\n     long expectedFiles = expectedFilesCount(source);\n     MigrateTable.Result migratedFiles = migrateAction.execute();\n     validateTables(source, dest);\n-    Assert.assertEquals(\n-        \"Expected number of migrated files\", expectedFiles, migratedFiles.migratedDataFilesCount());\n+    assertThat(migratedFiles.migratedDataFilesCount())\n+        .as(\"Expected number of migrated files\")\n+        .isEqualTo(expectedFiles);\n   }\n \n   // Counts the number of files in the source table, makes sure the same files exist in the\n@@ -972,33 +974,36 @@ private void assertSnapshotFileCount(SnapshotTable snapshotTable, String source,\n     long expectedFiles = expectedFilesCount(source);\n     SnapshotTable.Result snapshotTableResult = snapshotTable.execute();\n     validateTables(source, dest);\n-    Assert.assertEquals(\n-        \"Expected number of imported snapshot files\",\n-        expectedFiles,\n-        snapshotTableResult.importedDataFilesCount());\n+    assertThat(snapshotTableResult.importedDataFilesCount())\n+        .as(\"Expected number of imported snapshot files\")\n+        .isEqualTo(expectedFiles);\n   }\n \n   private void validateTables(String source, String dest)\n       throws NoSuchTableException, ParseException {\n     List<Row> expected = spark.table(source).collectAsList();\n     SparkTable destTable = loadTable(dest);\n-    Assert.assertEquals(\n-        \"Provider should be iceberg\",\n-        \"iceberg\",\n-        destTable.properties().get(TableCatalog.PROP_PROVIDER));\n+    assertThat(destTable.properties()).containsEntry(TableCatalog.PROP_PROVIDER, \"iceberg\");\n     List<Row> actual = spark.table(dest).collectAsList();\n-    Assert.assertTrue(\n-        String.format(\n-            \"Rows in migrated table did not match\\nExpected :%s rows \\nFound    :%s\",\n-            expected, actual),\n-        expected.containsAll(actual) && actual.containsAll(expected));\n+    assertThat(actual)\n+        .as(\n+            String.format(\n+                \"Rows in migrated table did not match\\nExpected :%s rows \\nFound    :%s\",\n+                expected, actual))\n+        .containsAll(expected);\n+    assertThat(expected)\n+        .as(\n+            String.format(\n+                \"Rows in migrated table did not match\\nExpected :%s rows \\nFound    :%s\",\n+                expected, actual))\n+        .containsAll(actual);\n   }\n \n   private long expectedFilesCount(String source)\n       throws NoSuchDatabaseException, NoSuchTableException, ParseException {\n     CatalogTable sourceTable = loadSessionTable(source);\n     List<URI> uris;\n-    if (sourceTable.partitionColumnNames().size() == 0) {\n+    if (sourceTable.partitionColumnNames().isEmpty()) {\n       uris = Lists.newArrayList();\n       uris.add(sourceTable.location());\n     } else {\n@@ -1032,14 +1037,15 @@ private void assertIsolatedSnapshot(String source, String dest) {\n     df.write().format(\"iceberg\").mode(\"append\").saveAsTable(dest);\n \n     List<Row> result = spark.sql(String.format(\"SELECT * FROM %s\", source)).collectAsList();\n-    Assert.assertEquals(\n-        \"No additional rows should be added to the original table\", expected.size(), result.size());\n+    assertThat(result)\n+        .as(\"No additional rows should be added to the original table\")\n+        .hasSameSizeAs(expected);\n \n     List<Row> snapshot =\n         spark\n             .sql(String.format(\"SELECT * FROM %s WHERE id = 4 AND data = 'd'\", dest))\n             .collectAsList();\n-    Assert.assertEquals(\"Added row not found in snapshot\", 1, snapshot.size());\n+    assertThat(snapshot).as(\"Added row not found in snapshot\").hasSize(1);\n   }\n \n   private String sourceName(String source) {\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSnapshotTableAction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSnapshotTableAction.java\nindex 8e6358f51bcd..d9c42a07b853 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSnapshotTableAction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSnapshotTableAction.java\n@@ -18,32 +18,31 @@\n  */\n package org.apache.iceberg.spark.actions;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import java.io.IOException;\n-import java.util.Map;\n+import java.nio.file.Files;\n import java.util.concurrent.Executors;\n import java.util.concurrent.atomic.AtomicInteger;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.spark.CatalogTestBase;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSnapshotTableAction extends SparkCatalogTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSnapshotTableAction extends CatalogTestBase {\n   private static final String SOURCE_NAME = \"spark_catalog.default.source\";\n \n-  public TestSnapshotTableAction(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS %s PURGE\", SOURCE_NAME);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSnapshotWithParallelTasks() throws IOException {\n-    String location = temp.newFolder().toURI().toString();\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         SOURCE_NAME, location);\n@@ -64,6 +63,6 @@ public void testSnapshotWithParallelTasks() throws IOException {\n                   return thread;\n                 }))\n         .execute();\n-    Assert.assertEquals(snapshotThreadsIndex.get(), 2);\n+    assertThat(snapshotThreadsIndex.get()).isEqualTo(2);\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/functions/TestSparkFunctions.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/functions/TestSparkFunctions.java\nindex 5d1f44897d0c..38ce0d4d95f1 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/functions/TestSparkFunctions.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/functions/TestSparkFunctions.java\n@@ -24,7 +24,7 @@\n import org.apache.spark.sql.connector.catalog.functions.UnboundFunction;\n import org.apache.spark.sql.types.DataTypes;\n import org.apache.spark.sql.types.DecimalType;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n public class TestSparkFunctions {\n \n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/TaskCheckHelper.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/TaskCheckHelper.java\nindex bcd00eb6f4e5..e4979512a65f 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/TaskCheckHelper.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/TaskCheckHelper.java\n@@ -56,7 +56,8 @@ public static void assertEquals(FileScanTask expected, FileScanTask actual) {\n         .isEqualTo(expected.start());\n \n     // simplify comparison on residual expression via comparing toString\n-    assertThat(actual.residual().toString())\n+    assertThat(actual.residual())\n+        .asString()\n         .as(\"Residual expression doesn't match\")\n         .isEqualTo(expected.residual().toString());\n   }\n@@ -95,9 +96,6 @@ public static void assertEquals(DataFile expected, DataFile actual) {\n     assertThat(actual.splitOffsets())\n         .as(\"Should match the serialized record offsets\")\n         .isEqualTo(expected.splitOffsets());\n-    assertThat(actual.keyMetadata())\n-        .as(\"Should match the serialized record offsets\")\n-        .isEqualTo(expected.keyMetadata());\n   }\n \n   private static List<FileScanTask> getFileScanTasksInFilePathOrder(\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestFileIOSerialization.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestFileIOSerialization.java\nindex bfdfa8deca06..cdd2443cc0e0 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestFileIOSerialization.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestFileIOSerialization.java\n@@ -83,9 +83,7 @@ public void testHadoopFileIOKryoSerialization() throws IOException {\n     FileIO deserializedIO = KryoHelpers.roundTripSerialize(serializableTable.io());\n     Configuration actualConf = ((HadoopFileIO) deserializedIO).conf();\n \n-    assertThat(toMap(actualConf)).as(\"Conf pairs must match\").isEqualTo(toMap(expectedConf));\n-    assertThat(actualConf.get(\"k1\")).as(\"Conf values must be present\").isEqualTo(\"v1\");\n-    assertThat(actualConf.get(\"k2\")).as(\"Conf values must be present\").isEqualTo(\"v2\");\n+    assertThat(actualConf).containsExactlyInAnyOrderElementsOf(expectedConf);\n   }\n \n   @Test\n@@ -97,9 +95,7 @@ public void testHadoopFileIOJavaSerialization() throws IOException, ClassNotFoun\n     FileIO deserializedIO = TestHelpers.roundTripSerialize(serializableTable.io());\n     Configuration actualConf = ((HadoopFileIO) deserializedIO).conf();\n \n-    assertThat(toMap(actualConf)).as(\"Conf pairs must match\").isEqualTo(toMap(expectedConf));\n-    assertThat(actualConf.get(\"k1\")).as(\"Conf values must be present\").isEqualTo(\"v1\");\n-    assertThat(actualConf.get(\"k2\")).as(\"Conf values must be present\").isEqualTo(\"v2\");\n+    assertThat(actualConf).containsExactlyInAnyOrderElementsOf(expectedConf);\n   }\n \n   private Map<String, String> toMap(Configuration conf) {\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java\nindex 4fdbc862ee8c..66ed837eafb5 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestScanTaskSerialization.java\n@@ -67,7 +67,7 @@ public class TestScanTaskSerialization extends TestBase {\n   private String tableLocation = null;\n \n   @BeforeEach\n-  public void setupTableLocation() throws Exception {\n+  public void setupTableLocation() {\n     this.tableLocation = tableDir.toURI().toString();\n   }\n \n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestComputeTableStatsAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestComputeTableStatsAction.java\nindex 057ef231ca1d..dd4d5b4f7c49 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestComputeTableStatsAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestComputeTableStatsAction.java\n@@ -27,9 +27,9 @@\n \n import java.io.IOException;\n import java.util.List;\n-import org.apache.iceberg.BlobMetadata;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.Files;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.StatisticsFile;\n@@ -57,9 +57,10 @@\n import org.apache.spark.sql.types.StructType;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestComputeTableStatsAction extends CatalogTestBase {\n-\n   private static final Types.StructType LEAF_STRUCT_TYPE =\n       Types.StructType.of(\n           optional(1, \"leafLongCol\", Types.LongType.get()),\n@@ -76,6 +77,11 @@ public class TestComputeTableStatsAction extends CatalogTestBase {\n           required(4, \"nestedStructCol\", NESTED_STRUCT_TYPE),\n           required(5, \"stringCol\", Types.StringType.get()));\n \n+  @AfterEach\n+  public void removeTable() {\n+    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n+  }\n+\n   @TestTemplate\n   public void testLoadingTableDirectly() {\n     sql(\"CREATE TABLE %s (id int, data string) USING iceberg\", tableName);\n@@ -116,15 +122,20 @@ public void testComputeTableStatsAction() throws NoSuchTableException, ParseExce\n     assertThat(results).isNotNull();\n \n     List<StatisticsFile> statisticsFiles = table.statisticsFiles();\n-    assertThat(statisticsFiles).hasSize(1);\n-\n-    StatisticsFile statisticsFile = statisticsFiles.get(0);\n-    assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n-    assertThat(statisticsFile.blobMetadata()).hasSize(2);\n-\n-    BlobMetadata blobMetadata = statisticsFile.blobMetadata().get(0);\n-    assertThat(blobMetadata.properties())\n-        .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"4\");\n+    assertThat(statisticsFiles)\n+        .singleElement()\n+        .satisfies(\n+            statisticsFile -> {\n+              assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n+              assertThat(statisticsFile.blobMetadata())\n+                  .hasSize(2)\n+                  .element(0)\n+                  .satisfies(\n+                      blobMetadata -> {\n+                        assertThat(blobMetadata.properties())\n+                            .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"4\");\n+                      });\n+            });\n   }\n \n   @TestTemplate\n@@ -148,14 +159,23 @@ public void testComputeTableStatsActionWithoutExplicitColumns()\n     ComputeTableStats.Result results = actions.computeTableStats(table).execute();\n     assertThat(results).isNotNull();\n \n-    assertThat(table.statisticsFiles()).hasSize(1);\n-    StatisticsFile statisticsFile = table.statisticsFiles().get(0);\n-    assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n-    assertThat(statisticsFile.blobMetadata()).hasSize(2);\n-    assertThat(statisticsFile.blobMetadata().get(0).properties())\n-        .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"4\");\n-    assertThat(statisticsFile.blobMetadata().get(1).properties())\n-        .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"4\");\n+    assertThat(table.statisticsFiles())\n+        .singleElement()\n+        .satisfies(\n+            statisticsFile -> {\n+              assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n+              assertThat(statisticsFile.blobMetadata())\n+                  .hasSize(2)\n+                  .satisfiesExactly(\n+                      blobMetadata -> {\n+                        assertThat(blobMetadata.properties())\n+                            .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"4\");\n+                      },\n+                      blobMetadata -> {\n+                        assertThat(blobMetadata.properties())\n+                            .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"4\");\n+                      });\n+            });\n   }\n \n   @TestTemplate\n@@ -199,15 +219,19 @@ public void testComputeTableStatsWithNullValues() throws NoSuchTableException, P\n     ComputeTableStats.Result results = actions.computeTableStats(table).columns(\"data\").execute();\n     assertThat(results).isNotNull();\n \n-    List<StatisticsFile> statisticsFiles = table.statisticsFiles();\n-    assertThat(statisticsFiles).hasSize(1);\n-\n-    StatisticsFile statisticsFile = statisticsFiles.get(0);\n-    assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n-    assertThat(statisticsFile.blobMetadata()).hasSize(1);\n-\n-    assertThat(statisticsFile.blobMetadata().get(0).properties())\n-        .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"4\");\n+    assertThat(table.statisticsFiles())\n+        .singleElement()\n+        .satisfies(\n+            statisticsFile -> {\n+              assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n+              assertThat(statisticsFile.blobMetadata())\n+                  .singleElement()\n+                  .satisfies(\n+                      blobMetadata -> {\n+                        assertThat(blobMetadata.properties())\n+                            .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"4\");\n+                      });\n+            });\n   }\n \n   @TestTemplate\n@@ -253,15 +277,19 @@ public void testComputeTableStatsWhenSnapshotIdNotSpecified()\n \n     assertThat(results).isNotNull();\n \n-    List<StatisticsFile> statisticsFiles = table.statisticsFiles();\n-    assertThat(statisticsFiles).hasSize(1);\n-\n-    StatisticsFile statisticsFile = statisticsFiles.get(0);\n-    assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n-    assertThat(statisticsFile.blobMetadata()).hasSize(1);\n-\n-    assertThat(statisticsFile.blobMetadata().get(0).properties())\n-        .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"1\");\n+    assertThat(table.statisticsFiles())\n+        .singleElement()\n+        .satisfies(\n+            statisticsFile -> {\n+              assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n+              assertThat(statisticsFile.blobMetadata())\n+                  .singleElement()\n+                  .satisfies(\n+                      blobMetadata -> {\n+                        assertThat(blobMetadata.properties())\n+                            .containsEntry(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY, \"1\");\n+                      });\n+            });\n   }\n \n   @TestTemplate\n@@ -282,11 +310,13 @@ public void testComputeTableStatsWithNestedSchema()\n     actions.computeTableStats(tbl).execute();\n \n     tbl.refresh();\n-    List<StatisticsFile> statisticsFiles = tbl.statisticsFiles();\n-    assertThat(statisticsFiles).hasSize(1);\n-    StatisticsFile statisticsFile = statisticsFiles.get(0);\n-    assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n-    assertThat(statisticsFile.blobMetadata()).hasSize(1);\n+    assertThat(tbl.statisticsFiles())\n+        .singleElement()\n+        .satisfies(\n+            statisticsFile -> {\n+              assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n+              assertThat(statisticsFile.blobMetadata()).hasSize(1);\n+            });\n   }\n \n   @TestTemplate\n@@ -365,15 +395,19 @@ public void testComputeTableStats(String columnName, String type)\n         actions.computeTableStats(table).columns(columnName).execute();\n     assertThat(results).isNotNull();\n \n-    List<StatisticsFile> statisticsFiles = table.statisticsFiles();\n-    assertThat(statisticsFiles).hasSize(1);\n-\n-    StatisticsFile statisticsFile = statisticsFiles.get(0);\n-    assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n-    assertThat(statisticsFile.blobMetadata()).hasSize(1);\n-\n-    assertThat(statisticsFile.blobMetadata().get(0).properties())\n-        .containsKey(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY);\n+    assertThat(table.statisticsFiles())\n+        .singleElement()\n+        .satisfies(\n+            statisticsFile -> {\n+              assertThat(statisticsFile.fileSizeInBytes()).isGreaterThan(0);\n+              assertThat(statisticsFile.blobMetadata())\n+                  .singleElement()\n+                  .satisfies(\n+                      blobMetadata -> {\n+                        assertThat(blobMetadata.properties())\n+                            .containsKey(APACHE_DATASKETCHES_THETA_V1_NDV_PROPERTY);\n+                      });\n+            });\n   }\n \n   private GenericRecord createNestedRecord() {\n@@ -399,9 +433,4 @@ private void append(String table, Dataset<Row> df) throws NoSuchTableException {\n     // fanout writes are enabled as write-time clustering is not supported without Spark extensions\n     df.coalesce(1).writeTo(table).option(SparkWriteOptions.FANOUT_ENABLED, \"true\").append();\n   }\n-\n-  @AfterEach\n-  public void removeTable() {\n-    sql(\"DROP TABLE IF EXISTS %s\", tableName);\n-  }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java\nindex 6954903b4102..eb89b0a23274 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestCreateActions.java\n@@ -36,6 +36,7 @@\n import org.apache.commons.io.filefilter.TrueFileFilter;\n import org.apache.hadoop.fs.Path;\n import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -77,12 +78,14 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n import org.junit.jupiter.api.io.TempDir;\n import scala.Option;\n import scala.Some;\n import scala.collection.JavaConverters;\n import scala.collection.Seq;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestCreateActions extends CatalogTestBase {\n   private static final String CREATE_PARTITIONED_PARQUET =\n       \"CREATE TABLE %s (id INT, data STRING) \" + \"using parquet PARTITIONED BY (id) LOCATION '%s'\";\n@@ -337,7 +340,6 @@ public void removeColumnsAtEnd() throws Exception {\n \n     String colName1 = \"newCol1\";\n     String colName2 = \"newCol2\";\n-    File location = Files.createTempDirectory(temp, \"junit\").toFile();\n     spark\n         .range(10)\n         .selectExpr(\"cast(id as INT)\", \"CAST(id as INT) \" + colName1, \"CAST(id as INT) \" + colName2)\n@@ -527,11 +529,7 @@ public void testProperties() throws Exception {\n     expectedProps.putAll(props);\n     expectedProps.put(\"dogs\", \"sundance\");\n \n-    for (Map.Entry<String, String> entry : expectedProps.entrySet()) {\n-      assertThat(table.properties())\n-          .as(\"Property value is not the expected value\")\n-          .containsEntry(entry.getKey(), entry.getValue());\n-    }\n+    assertThat(table.properties()).containsAllEntriesOf(expectedProps);\n   }\n \n   @TestTemplate\n@@ -549,37 +547,23 @@ public void testSparkTableReservedProperties() throws Exception {\n \n     String[] keys = {\"provider\", \"format\", \"current-snapshot-id\", \"location\", \"sort-order\"};\n \n-    for (String entry : keys) {\n-      assertThat(table.properties())\n-          .as(\"Created table missing reserved property \" + entry)\n-          .containsKey(entry);\n-    }\n+    assertThat(table.properties())\n+        .as(\"Created table missing reserved properties\")\n+        .containsKeys(keys);\n+\n+    assertThat(table.properties())\n+        .containsEntry(\"provider\", \"iceberg\")\n+        .containsEntry(\"format\", \"iceberg/parquet\")\n+        .hasEntrySatisfying(\"current-snapshot-id\", id -> assertThat(id).isNotEqualTo(\"none\"))\n+        .hasEntrySatisfying(\"location\", loc -> assertThat(loc).endsWith(destTableName));\n \n-    assertThat(table.properties().get(\"provider\")).as(\"Unexpected provider\").isEqualTo(\"iceberg\");\n-    assertThat(table.properties().get(\"format\"))\n-        .as(\"Unexpected provider\")\n-        .isEqualTo(\"iceberg/parquet\");\n-    assertThat(table.properties().get(\"current-snapshot-id\"))\n-        .as(\"No current-snapshot-id found\")\n-        .isNotEqualTo(\"none\");\n-    assertThat(table.properties().get(\"location\"))\n-        .as(\"Location isn't correct\")\n-        .endsWith(destTableName);\n-\n-    assertThat(table.properties().get(\"format-version\"))\n-        .as(\"Unexpected format-version\")\n-        .isEqualTo(\"1\");\n+    assertThat(table.properties()).containsEntry(\"format-version\", \"1\");\n     table.table().updateProperties().set(\"format-version\", \"2\").commit();\n-    assertThat(table.properties().get(\"format-version\"))\n-        .as(\"Unexpected format-version\")\n-        .isEqualTo(\"2\");\n+    assertThat(table.properties()).containsEntry(\"format-version\", \"2\");\n \n-    assertThat(table.properties().get(\"sort-order\"))\n-        .as(\"Sort-order isn't correct\")\n-        .isEqualTo(\"id ASC NULLS FIRST, data DESC NULLS LAST\");\n-    assertThat(table.properties().get(\"identifier-fields\"))\n-        .as(\"Identifier fields should be null\")\n-        .isNull();\n+    assertThat(table.properties())\n+        .containsEntry(\"sort-order\", \"id ASC NULLS FIRST, data DESC NULLS LAST\")\n+        .doesNotContainKey(\"identifier-fields\");\n \n     table\n         .table()\n@@ -588,9 +572,7 @@ public void testSparkTableReservedProperties() throws Exception {\n         .requireColumn(\"id\")\n         .setIdentifierFields(\"id\")\n         .commit();\n-    assertThat(table.properties().get(\"identifier-fields\"))\n-        .as(\"Identifier fields aren't correct\")\n-        .isEqualTo(\"[id]\");\n+    assertThat(table.properties()).containsEntry(\"identifier-fields\", \"[id]\");\n   }\n \n   @TestTemplate\n@@ -995,9 +977,7 @@ private void validateTables(String source, String dest)\n       throws NoSuchTableException, ParseException {\n     List<Row> expected = spark.table(source).collectAsList();\n     SparkTable destTable = loadTable(dest);\n-    assertThat(destTable.properties().get(TableCatalog.PROP_PROVIDER))\n-        .as(\"Provider should be iceberg\")\n-        .isEqualTo(\"iceberg\");\n+    assertThat(destTable.properties()).containsEntry(TableCatalog.PROP_PROVIDER, \"iceberg\");\n     List<Row> actual = spark.table(dest).collectAsList();\n     assertThat(actual)\n         .as(\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12839",
    "pr_id": 12839,
    "issue_id": 9955,
    "repo": "apache/iceberg",
    "problem_statement": "Is there any way on Flink to read newly appended data only (NOT in current Iceberg table snapshot)?\n### Query engine\r\n\r\nFlink\r\n\r\n### Question\r\n\r\nWhen I ingest from iceberg table to iceberg table with Flink, I want to start sink table ingestion with newly added records from current snapshot, like `consumer.override.auto.offset.reset: \"latest\"` in Kafka source..!\r\n\r\nBut, there is no option out of `connector.iceberg.starting-strategy` available, for my use case.\r\n- https://iceberg.apache.org/docs/latest/flink-configuration/#read-options\r\n\r\nIs there any way to read newly appended data only (NOT in current Iceberg table snapshot)?",
    "issue_word_count": 99,
    "test_files_count": 2,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "docs/docs/flink-configuration.md",
      "flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/StreamingStartingStrategy.java",
      "flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/enumerator/ContinuousSplitPlannerImpl.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/enumerator/TestContinuousSplitPlannerImpl.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/enumerator/TestContinuousSplitPlannerImplStartStrategy.java"
    ],
    "pr_changed_test_files": [
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/enumerator/TestContinuousSplitPlannerImpl.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/enumerator/TestContinuousSplitPlannerImplStartStrategy.java"
    ],
    "base_commit": "3a29199e73f2e9ae0f8f92a1a0732a338c66aa0d",
    "head_commit": "a264288cf19776687da3fc9e215d4c0c1fd141a1",
    "repo_url": "https://github.com/apache/iceberg/pull/12839",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12839",
    "dockerfile": "",
    "pr_merged_at": "2025-04-21T17:37:32.000Z",
    "patch": "diff --git a/docs/docs/flink-configuration.md b/docs/docs/flink-configuration.md\nindex 1ac16d7fc3e9..8b13d0c2ae4f 100644\n--- a/docs/docs/flink-configuration.md\n+++ b/docs/docs/flink-configuration.md\n@@ -107,7 +107,7 @@ env.getConfig()\n | snapshot-id                   | N/A                                             | N/A                          | null                             | For time travel in batch mode. Read data from the specified snapshot-id.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n | case-sensitive                | connector.iceberg.case-sensitive                | N/A                          | false                            | If true, match column name in a case sensitive way.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n | as-of-timestamp               | N/A                                             | N/A                          | null                             | For time travel in batch mode. Read data from the most recent snapshot as of the given time in milliseconds.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n-| starting-strategy             | connector.iceberg.starting-strategy             | N/A                          | INCREMENTAL_FROM_LATEST_SNAPSHOT | Starting strategy for streaming execution. TABLE_SCAN_THEN_INCREMENTAL: Do a regular table scan then switch to the incremental mode. The incremental mode starts from the current snapshot exclusive. INCREMENTAL_FROM_LATEST_SNAPSHOT: Start incremental mode from the latest snapshot inclusive. If it is an empty map, all future append snapshots should be discovered. INCREMENTAL_FROM_EARLIEST_SNAPSHOT: Start incremental mode from the earliest snapshot inclusive. If it is an empty map, all future append snapshots should be discovered. INCREMENTAL_FROM_SNAPSHOT_ID: Start incremental mode from a snapshot with a specific id inclusive. INCREMENTAL_FROM_SNAPSHOT_TIMESTAMP: Start incremental mode from a snapshot with a specific timestamp inclusive. If the timestamp is between two snapshots, it should start from the snapshot after the timestamp. Just for FIP27 Source. |\n+| starting-strategy             | connector.iceberg.starting-strategy             | N/A                          | INCREMENTAL_FROM_LATEST_SNAPSHOT | Starting strategy for streaming execution. TABLE_SCAN_THEN_INCREMENTAL: Do a regular table scan then switch to the incremental mode. The incremental mode starts from the current snapshot exclusive. INCREMENTAL_FROM_LATEST_SNAPSHOT: Start incremental mode from the latest snapshot inclusive. If it is an empty table, all future append snapshots should be discovered. INCREMENTAL_FROM_LATEST_SNAPSHOT_EXCLUSIVE: Start incremental mode from the latest snapshot exclusive. If it is an empty table, all future append snapshots should be discovered. INCREMENTAL_FROM_EARLIEST_SNAPSHOT: Start incremental mode from the earliest snapshot inclusive. If it is an empty table, all future append snapshots should be discovered. INCREMENTAL_FROM_SNAPSHOT_ID: Start incremental mode from a snapshot with a specific id inclusive. INCREMENTAL_FROM_SNAPSHOT_TIMESTAMP: Start incremental mode from a snapshot with a specific timestamp inclusive. If the timestamp is between two snapshots, it should start from the snapshot after the timestamp. Just for FIP27 Source. |\n | start-snapshot-timestamp      | N/A                                             | N/A                          | null                             | Start to read data from the most recent snapshot as of the given time in milliseconds.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n | start-snapshot-id             | N/A                                             | N/A                          | null                             | Start to read data from the specified snapshot-id.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n | end-snapshot-id               | N/A                                             | N/A                          | The latest snapshot id           | Specifies the end snapshot.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/StreamingStartingStrategy.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/StreamingStartingStrategy.java\nindex 11707bf82a0f..fbeaace20934 100644\n--- a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/StreamingStartingStrategy.java\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/StreamingStartingStrategy.java\n@@ -30,14 +30,21 @@ public enum StreamingStartingStrategy {\n   /**\n    * Start incremental mode from the latest snapshot inclusive.\n    *\n-   * <p>If it is an empty map, all future append snapshots should be discovered.\n+   * <p>If it is an empty table, all future append snapshots should be discovered.\n    */\n   INCREMENTAL_FROM_LATEST_SNAPSHOT,\n \n+  /**\n+   * Start incremental mode from the latest snapshot exclusive.\n+   *\n+   * <p>If it is an empty table, all future append snapshots should be discovered.\n+   */\n+  INCREMENTAL_FROM_LATEST_SNAPSHOT_EXCLUSIVE,\n+\n   /**\n    * Start incremental mode from the earliest snapshot inclusive.\n    *\n-   * <p>If it is an empty map, all future append snapshots should be discovered.\n+   * <p>If it is an empty table, all future append snapshots should be discovered.\n    */\n   INCREMENTAL_FROM_EARLIEST_SNAPSHOT,\n \n\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/enumerator/ContinuousSplitPlannerImpl.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/enumerator/ContinuousSplitPlannerImpl.java\nindex 9c99d442174f..50a3919c1380 100644\n--- a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/enumerator/ContinuousSplitPlannerImpl.java\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/enumerator/ContinuousSplitPlannerImpl.java\n@@ -165,7 +165,7 @@ private ContinuousEnumerationResult discoverInitialSplits() {\n         \"Get starting snapshot id {} based on strategy {}\",\n         startSnapshot.snapshotId(),\n         scanContext.streamingStartingStrategy());\n-    List<IcebergSourceSplit> splits;\n+    List<IcebergSourceSplit> splits = Collections.emptyList();\n     IcebergEnumeratorPosition toPosition;\n     if (scanContext.streamingStartingStrategy()\n         == StreamingStartingStrategy.TABLE_SCAN_THEN_INCREMENTAL) {\n@@ -180,10 +180,17 @@ private ContinuousEnumerationResult discoverInitialSplits() {\n       // For TABLE_SCAN_THEN_INCREMENTAL, incremental mode starts exclusive from the startSnapshot\n       toPosition =\n           IcebergEnumeratorPosition.of(startSnapshot.snapshotId(), startSnapshot.timestampMillis());\n+    } else if (scanContext.streamingStartingStrategy()\n+        == StreamingStartingStrategy.INCREMENTAL_FROM_LATEST_SNAPSHOT_EXCLUSIVE) {\n+      toPosition =\n+          IcebergEnumeratorPosition.of(startSnapshot.snapshotId(), startSnapshot.timestampMillis());\n+      LOG.info(\n+          \"Start incremental scan with start snapshot (exclusive): id = {}, timestamp = {}\",\n+          startSnapshot.snapshotId(),\n+          startSnapshot.timestampMillis());\n     } else {\n       // For all other modes, starting snapshot should be consumed inclusively.\n       // Use parentId to achieve the inclusive behavior. It is fine if parentId is null.\n-      splits = Collections.emptyList();\n       Long parentSnapshotId = startSnapshot.parentId();\n       if (parentSnapshotId != null) {\n         Snapshot parentSnapshot = table.snapshot(parentSnapshotId);\n@@ -216,6 +223,7 @@ static Optional<Snapshot> startSnapshot(Table table, ScanContext scanContext) {\n     switch (scanContext.streamingStartingStrategy()) {\n       case TABLE_SCAN_THEN_INCREMENTAL:\n       case INCREMENTAL_FROM_LATEST_SNAPSHOT:\n+      case INCREMENTAL_FROM_LATEST_SNAPSHOT_EXCLUSIVE:\n         return Optional.ofNullable(table.currentSnapshot());\n       case INCREMENTAL_FROM_EARLIEST_SNAPSHOT:\n         return Optional.ofNullable(SnapshotUtil.oldestAncestor(table));\n",
    "test_patch": "diff --git a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/enumerator/TestContinuousSplitPlannerImpl.java b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/enumerator/TestContinuousSplitPlannerImpl.java\nindex f66b9e302924..9a4bfa03e28b 100644\n--- a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/enumerator/TestContinuousSplitPlannerImpl.java\n+++ b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/enumerator/TestContinuousSplitPlannerImpl.java\n@@ -44,6 +44,8 @@\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.extension.RegisterExtension;\n import org.junit.jupiter.api.io.TempDir;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.EnumSource;\n \n public class TestContinuousSplitPlannerImpl {\n   @TempDir protected Path temporaryFolder;\n@@ -173,13 +175,14 @@ public void testTableScanThenIncrementalWithNonEmptyTable() throws Exception {\n     }\n   }\n \n-  @Test\n-  public void testIncrementalFromLatestSnapshotWithEmptyTable() throws Exception {\n+  @ParameterizedTest\n+  @EnumSource(\n+      value = StreamingStartingStrategy.class,\n+      names = {\"INCREMENTAL_FROM_LATEST_SNAPSHOT\", \"INCREMENTAL_FROM_LATEST_SNAPSHOT_EXCLUSIVE\"})\n+  public void testIncrementalFromLatestSnapshotWithEmptyTable(\n+      StreamingStartingStrategy startingStrategy) throws Exception {\n     ScanContext scanContext =\n-        ScanContext.builder()\n-            .startingStrategy(StreamingStartingStrategy.INCREMENTAL_FROM_LATEST_SNAPSHOT)\n-            .splitSize(1L)\n-            .build();\n+        ScanContext.builder().startingStrategy(startingStrategy).splitSize(1L).build();\n     ContinuousSplitPlannerImpl splitPlanner =\n         new ContinuousSplitPlannerImpl(TABLE_RESOURCE.tableLoader().clone(), scanContext, null);\n \n@@ -256,6 +259,44 @@ public void testIncrementalFromLatestSnapshotWithNonEmptyTable() throws Exceptio\n     }\n   }\n \n+  @Test\n+  public void testIncrementalFromLatestSnapshotExclusiveWithNonEmptyTable() throws Exception {\n+    appendTwoSnapshots();\n+\n+    ScanContext scanContext =\n+        ScanContext.builder()\n+            .startingStrategy(StreamingStartingStrategy.INCREMENTAL_FROM_LATEST_SNAPSHOT_EXCLUSIVE)\n+            .build();\n+    ContinuousSplitPlannerImpl splitPlanner =\n+        new ContinuousSplitPlannerImpl(TABLE_RESOURCE.tableLoader().clone(), scanContext, null);\n+\n+    ContinuousEnumerationResult initialResult = splitPlanner.planSplits(null);\n+    assertThat(initialResult.splits()).isEmpty();\n+    assertThat(initialResult.fromPosition()).isNull();\n+    // For exclusive behavior, the initial result should point to snapshot2\n+    assertThat(initialResult.toPosition().snapshotId().longValue())\n+        .isEqualTo(snapshot2.snapshotId());\n+    assertThat(initialResult.toPosition().snapshotTimestampMs().longValue())\n+        .isEqualTo(snapshot2.timestampMillis());\n+\n+    // Then the next incremental scan shall discover no files\n+    ContinuousEnumerationResult secondResult = splitPlanner.planSplits(initialResult.toPosition());\n+    assertThat(initialResult.splits()).isEmpty();\n+    assertThat(secondResult.fromPosition().snapshotId().longValue())\n+        .isEqualTo(snapshot2.snapshotId());\n+    assertThat(secondResult.fromPosition().snapshotTimestampMs().longValue())\n+        .isEqualTo(snapshot2.timestampMillis());\n+    assertThat(secondResult.toPosition().snapshotId().longValue())\n+        .isEqualTo(snapshot2.snapshotId());\n+    assertThat(secondResult.toPosition().snapshotTimestampMs().longValue())\n+        .isEqualTo(snapshot2.timestampMillis());\n+\n+    IcebergEnumeratorPosition lastPosition = secondResult.toPosition();\n+    for (int i = 0; i < 3; ++i) {\n+      lastPosition = verifyOneCycle(splitPlanner, lastPosition).lastPosition;\n+    }\n+  }\n+\n   @Test\n   public void testIncrementalFromEarliestSnapshotWithEmptyTable() throws Exception {\n     ScanContext scanContext =\n\ndiff --git a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/enumerator/TestContinuousSplitPlannerImplStartStrategy.java b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/enumerator/TestContinuousSplitPlannerImplStartStrategy.java\nindex b2185675340f..9b59e85d2afb 100644\n--- a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/enumerator/TestContinuousSplitPlannerImplStartStrategy.java\n+++ b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/enumerator/TestContinuousSplitPlannerImplStartStrategy.java\n@@ -37,6 +37,8 @@\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.extension.RegisterExtension;\n import org.junit.jupiter.api.io.TempDir;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.EnumSource;\n \n public class TestContinuousSplitPlannerImplStartStrategy {\n   private static final FileFormat FILE_FORMAT = FileFormat.PARQUET;\n@@ -88,13 +90,14 @@ public void testTableScanThenIncrementalStrategy() throws IOException {\n     assertThat(startSnapshot.snapshotId()).isEqualTo(snapshot3.snapshotId());\n   }\n \n-  @Test\n-  public void testForLatestSnapshotStrategy() throws IOException {\n+  @ParameterizedTest\n+  @EnumSource(\n+      value = StreamingStartingStrategy.class,\n+      names = {\"INCREMENTAL_FROM_LATEST_SNAPSHOT\", \"INCREMENTAL_FROM_LATEST_SNAPSHOT_EXCLUSIVE\"})\n+  public void testForLatestSnapshotStrategyWithEmptyTable(\n+      StreamingStartingStrategy startingStrategy) throws IOException {\n     ScanContext scanContext =\n-        ScanContext.builder()\n-            .streaming(true)\n-            .startingStrategy(StreamingStartingStrategy.INCREMENTAL_FROM_LATEST_SNAPSHOT)\n-            .build();\n+        ScanContext.builder().streaming(true).startingStrategy(startingStrategy).build();\n \n     assertThat(ContinuousSplitPlannerImpl.startSnapshot(TABLE_RESOURCE.table(), scanContext))\n         .isNotPresent();\n@@ -105,6 +108,22 @@ public void testForLatestSnapshotStrategy() throws IOException {\n     assertThat(startSnapshot.snapshotId()).isEqualTo(snapshot3.snapshotId());\n   }\n \n+  @ParameterizedTest\n+  @EnumSource(\n+      value = StreamingStartingStrategy.class,\n+      names = {\"INCREMENTAL_FROM_LATEST_SNAPSHOT\", \"INCREMENTAL_FROM_LATEST_SNAPSHOT_EXCLUSIVE\"})\n+  public void testForLatestSnapshotStrategyWithNonEmptyTable(\n+      StreamingStartingStrategy startingStrategy) throws IOException {\n+    appendThreeSnapshots();\n+\n+    ScanContext scanContext =\n+        ScanContext.builder().streaming(true).startingStrategy(startingStrategy).build();\n+\n+    Snapshot startSnapshot =\n+        ContinuousSplitPlannerImpl.startSnapshot(TABLE_RESOURCE.table(), scanContext).get();\n+    assertThat(startSnapshot.snapshotId()).isEqualTo(snapshot3.snapshotId());\n+  }\n+\n   @Test\n   public void testForEarliestSnapshotStrategy() throws IOException {\n     ScanContext scanContext =\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12822",
    "pr_id": 12822,
    "issue_id": 12817,
    "repo": "apache/iceberg",
    "problem_statement": "Add checkstyle rule to enforce using assumeThat() instead of assumeTrue()\n### Feature Request / Improvement\n\nWe should add a checkstyle rule to enforce using AssertJ's `assumeThat()` instead of using JUnit's `assumeTrue()` in tests\n\n### Query engine\n\nNone\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [ ] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 70,
    "test_files_count": 2,
    "non_test_files_count": 1,
    "pr_changed_files": [
      ".baseline/checkstyle/checkstyle.xml",
      "core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java",
      "core/src/test/java/org/apache/iceberg/util/TestEnvironmentUtil.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java",
      "core/src/test/java/org/apache/iceberg/util/TestEnvironmentUtil.java"
    ],
    "base_commit": "04bb3eed3451522a4ee53daf38b198853844d815",
    "head_commit": "d34d17c5f73d9ac5bf6963ed8e064f323472722e",
    "repo_url": "https://github.com/apache/iceberg/pull/12822",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12822",
    "dockerfile": "",
    "pr_merged_at": "2025-04-22T10:02:05.000Z",
    "patch": "diff --git a/.baseline/checkstyle/checkstyle.xml b/.baseline/checkstyle/checkstyle.xml\nindex 0cb18e4e09ea..139d26fedb3a 100644\n--- a/.baseline/checkstyle/checkstyle.xml\n+++ b/.baseline/checkstyle/checkstyle.xml\n@@ -439,6 +439,11 @@\n             <property name=\"illegalClasses\" value=\"org.junit.rules.ExpectedException\"/>\n             <message key=\"import.illegal\" value=\"Prefer using Assertions.assertThatThrownBy(...).isInstanceOf(...) instead.\"/>\n         </module>\n+        <module name=\"IllegalImport\">\n+            <property name=\"id\" value=\"BanJUnit5AssumptionsUsage\"/>\n+            <property name=\"illegalPkgs\" value=\"org.junit.jupiter.api.Assumptions\"/>\n+            <message key=\"import.illegal\" value=\"Prefer using Assertions.assumeThat(...).isTrue() instead.\"/>\n+        </module>\n         <module name=\"IllegalImport\">\n             <property name=\"id\" value=\"BanHamcrestUsage\"/>\n             <property name=\"illegalPkgs\" value=\"org.hamcrest\"/>\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java b/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\nindex fdcb28144a9b..eddae2a276b7 100644\n--- a/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\n+++ b/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\n@@ -79,7 +79,6 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.CharSequenceSet;\n-import org.junit.jupiter.api.Assumptions;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.params.ParameterizedTest;\n import org.junit.jupiter.params.provider.ValueSource;\n@@ -222,7 +221,7 @@ public void testCreateExistingNamespace() {\n \n   @Test\n   public void testCreateNamespaceWithProperties() {\n-    Assumptions.assumeTrue(supportsNamespaceProperties());\n+    assumeThat(supportsNamespaceProperties()).isTrue();\n \n     C catalog = catalog();\n \n@@ -258,7 +257,7 @@ public void testLoadNamespaceMetadata() {\n \n   @Test\n   public void testSetNamespaceProperties() {\n-    Assumptions.assumeTrue(supportsNamespaceProperties());\n+    assumeThat(supportsNamespaceProperties()).isTrue();\n \n     C catalog = catalog();\n \n@@ -275,7 +274,7 @@ public void testSetNamespaceProperties() {\n \n   @Test\n   public void testUpdateNamespaceProperties() {\n-    Assumptions.assumeTrue(supportsNamespaceProperties());\n+    assumeThat(supportsNamespaceProperties()).isTrue();\n \n     C catalog = catalog();\n \n@@ -301,7 +300,7 @@ public void testUpdateNamespaceProperties() {\n \n   @Test\n   public void testUpdateAndSetNamespaceProperties() {\n-    Assumptions.assumeTrue(supportsNamespaceProperties());\n+    assumeThat(supportsNamespaceProperties()).isTrue();\n \n     C catalog = catalog();\n \n@@ -328,7 +327,7 @@ public void testUpdateAndSetNamespaceProperties() {\n \n   @Test\n   public void testSetNamespacePropertiesNamespaceDoesNotExist() {\n-    Assumptions.assumeTrue(supportsNamespaceProperties());\n+    assumeThat(supportsNamespaceProperties()).isTrue();\n \n     C catalog = catalog();\n \n@@ -339,7 +338,7 @@ public void testSetNamespacePropertiesNamespaceDoesNotExist() {\n \n   @Test\n   public void testRemoveNamespaceProperties() {\n-    Assumptions.assumeTrue(supportsNamespaceProperties());\n+    assumeThat(supportsNamespaceProperties()).isTrue();\n \n     C catalog = catalog();\n \n@@ -360,7 +359,7 @@ public void testRemoveNamespaceProperties() {\n \n   @Test\n   public void testRemoveNamespacePropertiesNamespaceDoesNotExist() {\n-    Assumptions.assumeTrue(supportsNamespaceProperties());\n+    assumeThat(supportsNamespaceProperties()).isTrue();\n \n     C catalog = catalog();\n \n@@ -448,8 +447,9 @@ public void testListNamespaces() {\n \n   @Test\n   public void testListNestedNamespaces() {\n-    Assumptions.assumeTrue(\n-        supportsNestedNamespaces(), \"Only valid when the catalog supports nested namespaces\");\n+    assumeThat(supportsNestedNamespaces())\n+        .as(\"Only valid when the catalog supports nested namespaces\")\n+        .isTrue();\n \n     C catalog = catalog();\n \n@@ -494,7 +494,7 @@ public void testListNestedNamespaces() {\n \n   @Test\n   public void testNamespaceWithSlash() {\n-    Assumptions.assumeTrue(supportsNamesWithSlashes());\n+    assumeThat(supportsNamesWithSlashes()).isTrue();\n \n     C catalog = catalog();\n \n@@ -515,7 +515,7 @@ public void testNamespaceWithSlash() {\n \n   @Test\n   public void testNamespaceWithDot() {\n-    Assumptions.assumeTrue(supportsNamesWithDot());\n+    assumeThat(supportsNamesWithDot()).isTrue();\n \n     C catalog = catalog();\n \n@@ -564,7 +564,7 @@ public void testBasicCreateTable() {\n \n   @Test\n   public void testTableNameWithSlash() {\n-    Assumptions.assumeTrue(supportsNamesWithSlashes());\n+    assumeThat(supportsNamesWithSlashes()).isTrue();\n \n     C catalog = catalog();\n \n@@ -590,7 +590,7 @@ public void testTableNameWithSlash() {\n \n   @Test\n   public void testTableNameWithDot() {\n-    Assumptions.assumeTrue(supportsNamesWithDot());\n+    assumeThat(supportsNamesWithDot()).isTrue();\n \n     C catalog = catalog();\n \n@@ -1340,9 +1340,9 @@ public void testUUIDValidation() {\n \n   @Test\n   public void testUpdateTableSchemaServerSideRetry() {\n-    Assumptions.assumeTrue(\n-        supportsServerSideRetry(),\n-        \"Schema update recovery is only supported with server-side retry\");\n+    assumeThat(supportsServerSideRetry())\n+        .as(\"Schema update recovery is only supported with server-side retry\")\n+        .isTrue();\n     C catalog = catalog();\n \n     if (requiresNamespaceCreate()) {\n@@ -1478,8 +1478,9 @@ public void testUpdateTableSpec() {\n \n   @Test\n   public void testUpdateTableSpecServerSideRetry() {\n-    Assumptions.assumeTrue(\n-        supportsServerSideRetry(), \"Spec update recovery is only supported with server-side retry\");\n+    assumeThat(supportsServerSideRetry())\n+        .as(\"Spec update recovery is only supported with server-side retry\")\n+        .isTrue();\n     C catalog = catalog();\n \n     if (requiresNamespaceCreate()) {\n@@ -1746,9 +1747,9 @@ public void testUpdateTableSortOrder() {\n \n   @Test\n   public void testUpdateTableSortOrderServerSideRetry() {\n-    Assumptions.assumeTrue(\n-        supportsServerSideRetry(),\n-        \"Sort order update recovery is only supported with server-side retry\");\n+    assumeThat(supportsServerSideRetry())\n+        .as(\"Sort order update recovery is only supported with server-side retry\")\n+        .isTrue();\n     C catalog = catalog();\n \n     if (requiresNamespaceCreate()) {\n@@ -2405,9 +2406,9 @@ public void testCompleteCreateOrReplaceTransactionReplace() {\n \n   @Test\n   public void testCreateOrReplaceTransactionConcurrentCreate() {\n-    Assumptions.assumeTrue(\n-        supportsServerSideRetry(),\n-        \"Conversion to replace transaction is not supported by REST catalog\");\n+    assumeThat(supportsServerSideRetry())\n+        .as(\"Conversion to replace transaction is not supported by REST catalog\")\n+        .isTrue();\n \n     C catalog = catalog();\n \n@@ -2742,7 +2743,7 @@ public void testConcurrentReplaceTransactionSchema2() {\n \n   @Test\n   public void testConcurrentReplaceTransactionSchemaConflict() {\n-    Assumptions.assumeTrue(supportsServerSideRetry(), \"Schema conflicts are detected server-side\");\n+    assumeThat(supportsServerSideRetry()).as(\"Schema conflicts are detected server-side\").isTrue();\n \n     C catalog = catalog();\n \n@@ -2862,7 +2863,7 @@ public void testConcurrentReplaceTransactionPartitionSpec2() {\n \n   @Test\n   public void testConcurrentReplaceTransactionPartitionSpecConflict() {\n-    Assumptions.assumeTrue(supportsServerSideRetry(), \"Spec conflicts are detected server-side\");\n+    assumeThat(supportsServerSideRetry()).as(\"Spec conflicts are detected server-side\").isTrue();\n     C catalog = catalog();\n \n     if (requiresNamespaceCreate()) {\n@@ -3074,7 +3075,7 @@ public void testMetadataFileLocationsRemovalAfterCommit() {\n \n   @Test\n   public void tableCreationWithoutNamespace() {\n-    Assumptions.assumeTrue(requiresNamespaceCreate());\n+    assumeThat(requiresNamespaceCreate()).isTrue();\n \n     assertThatThrownBy(\n             () ->\n\ndiff --git a/core/src/test/java/org/apache/iceberg/util/TestEnvironmentUtil.java b/core/src/test/java/org/apache/iceberg/util/TestEnvironmentUtil.java\nindex d9a573b80941..0596f4b5d3bd 100644\n--- a/core/src/test/java/org/apache/iceberg/util/TestEnvironmentUtil.java\n+++ b/core/src/test/java/org/apache/iceberg/util/TestEnvironmentUtil.java\n@@ -19,19 +19,18 @@\n package org.apache.iceberg.util;\n \n import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.util.Map;\n import java.util.Optional;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.junit.jupiter.api.Assumptions;\n import org.junit.jupiter.api.Test;\n \n class TestEnvironmentUtil {\n   @Test\n   public void testEnvironmentSubstitution() {\n     Optional<Map.Entry<String, String>> envEntry = System.getenv().entrySet().stream().findFirst();\n-    Assumptions.assumeTrue(\n-        envEntry.isPresent(), \"Expecting at least one env. variable to be present\");\n+    assumeThat(envEntry).as(\"Expecting at least one env. variable to be present\").isPresent();\n     Map<String, String> resolvedProps =\n         EnvironmentUtil.resolveAll(ImmutableMap.of(\"env-test\", \"env:\" + envEntry.get().getKey()));\n     assertThat(resolvedProps)\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12821",
    "pr_id": 12821,
    "issue_id": 12809,
    "repo": "apache/iceberg",
    "problem_statement": "View's VersionLog uses wrong timestamp if ViewVersions are reused\n### Apache Iceberg version\n\n1.8.1 (latest release)\n\n### Query engine\n\nSpark\n\n### Please describe the bug üêû\n\nLets assume the following changes:\n1. ViewVersion 1 is added and set active\n2. View Version 2 is added and set active\n3. View Version 1 is set active.\n\nThis resulted in the following `version_log`:\n```json\n    \"version-log\": [\n        {\n            \"version-id\": 1,\n            \"timestamp-ms\": 1744716416168\n        },\n        {\n            \"version-id\": 2,\n            \"timestamp-ms\": 1744716449754\n        },\n        {\n            \"version-id\": 1,\n            \"timestamp-ms\": 1744716416168\n        }\n    ],\n```\n\nNote that the last and first entries have the same timestamp, namely the timestamp of the initial ViewVersion creation.\n\nI believe this is undesired in a history, where we are interested when a certain change became active. It makes sense to use the exact timestamp of the ViewVersion if it was added in the same set of changes, but re-enabling a previously used view version (maybe years ago) should not add a history for this past timestamp.\n\n\nFix in Rust: https://github.com/apache/iceberg-rust/pull/1218\n\nThe problematic line in Java is here:\nhttps://github.com/apache/iceberg/blob/cc4fe4cc50043ccba89700f7948090ff87a5baee/core/src/main/java/org/apache/iceberg/view/ViewMetadata.java#L254\n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 235,
    "test_files_count": 2,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/view/ViewMetadata.java",
      "core/src/test/java/org/apache/iceberg/view/TestViewMetadata.java",
      "core/src/test/java/org/apache/iceberg/view/TestViewMetadataParser.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/view/TestViewMetadata.java",
      "core/src/test/java/org/apache/iceberg/view/TestViewMetadataParser.java"
    ],
    "base_commit": "013d09e473faada1a3b21c155964ec4d1254d69d",
    "head_commit": "262c6b009da675333a87b17838be70e969aa064a",
    "repo_url": "https://github.com/apache/iceberg/pull/12821",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12821",
    "dockerfile": "",
    "pr_merged_at": "2025-04-25T07:39:42.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/view/ViewMetadata.java b/core/src/main/java/org/apache/iceberg/view/ViewMetadata.java\nindex 05c6ef375085..506eef7b546a 100644\n--- a/core/src/main/java/org/apache/iceberg/view/ViewMetadata.java\n+++ b/core/src/main/java/org/apache/iceberg/view/ViewMetadata.java\n@@ -249,9 +249,17 @@ public Builder setCurrentVersionId(int newVersionId) {\n         changes.add(new MetadataUpdate.SetCurrentViewVersion(newVersionId));\n       }\n \n+      // Use the timestamp from the view version if it was added in current set of changes.\n+      // Otherwise, use the current system time. This handles cases where the view version\n+      // was set as current in the past and is being re-activated.\n+      boolean versionAddedInThisChange =\n+          changes(MetadataUpdate.AddViewVersion.class)\n+              .anyMatch(added -> added.viewVersion().versionId() == newVersionId);\n+\n       this.historyEntry =\n           ImmutableViewHistoryEntry.builder()\n-              .timestampMillis(version.timestampMillis())\n+              .timestampMillis(\n+                  versionAddedInThisChange ? version.timestampMillis() : System.currentTimeMillis())\n               .versionId(version.versionId())\n               .build();\n \n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/view/TestViewMetadata.java b/core/src/test/java/org/apache/iceberg/view/TestViewMetadata.java\nindex 3a713d061b0e..e7fd6dccfb2a 100644\n--- a/core/src/test/java/org/apache/iceberg/view/TestViewMetadata.java\n+++ b/core/src/test/java/org/apache/iceberg/view/TestViewMetadata.java\n@@ -42,9 +42,13 @@ private ViewVersion newViewVersion(int id, String sql) {\n   }\n \n   private ViewVersion newViewVersion(int id, int schemaId, String sql) {\n+    return newViewVersion(id, schemaId, System.currentTimeMillis(), sql);\n+  }\n+\n+  private ViewVersion newViewVersion(int id, int schemaId, long timestampMillis, String sql) {\n     return ImmutableViewVersion.builder()\n         .versionId(id)\n-        .timestampMillis(System.currentTimeMillis())\n+        .timestampMillis(timestampMillis)\n         .defaultCatalog(\"prod\")\n         .defaultNamespace(Namespace.of(\"default\"))\n         .putSummary(\"user\", \"some-user\")\n@@ -396,6 +400,70 @@ public void viewVersionHistoryIsCorrectlyRetained() {\n         .hasMessage(\"Cannot set current version to unknown version: 1\");\n   }\n \n+  @Test\n+  public void versionHistoryEntryMaintainCorrectTimeline() {\n+    ViewVersion viewVersionOne = newViewVersion(1, 0, 1000, \"select * from ns.tbl\");\n+    ViewVersion viewVersionTwo = newViewVersion(2, 0, 2000, \"select count(*) from ns.tbl\");\n+    ViewVersion viewVersionThree =\n+        newViewVersion(3, 0, 3000, \"select count(*) as count from ns.tbl\");\n+\n+    ViewMetadata viewMetadata =\n+        ViewMetadata.builder()\n+            .setLocation(\"location\")\n+            .addSchema(new Schema(Types.NestedField.required(1, \"x\", Types.LongType.get())))\n+            .addVersion(viewVersionOne)\n+            .addVersion(viewVersionTwo)\n+            .setCurrentVersionId(1)\n+            .build();\n+\n+    // setting an existing view version as the new current should update the timestamp in the\n+    // history\n+    ViewMetadata updated = ViewMetadata.buildFrom(viewMetadata).setCurrentVersionId(2).build();\n+\n+    List<ViewHistoryEntry> history = updated.history();\n+    assertThat(history)\n+        .hasSize(2)\n+        .element(0)\n+        .isEqualTo(ImmutableViewHistoryEntry.builder().versionId(1).timestampMillis(1000).build());\n+    assertThat(history)\n+        .element(1)\n+        .satisfies(\n+            v -> {\n+              assertThat(v.versionId()).isEqualTo(2);\n+              assertThat(v.timestampMillis())\n+                  .isGreaterThan(3000)\n+                  .isLessThanOrEqualTo(System.currentTimeMillis());\n+            });\n+\n+    // adding a new view version and setting it as current should use the view version's timestamp\n+    // in the history (which has been set to a fixed value for testing)\n+    updated =\n+        ViewMetadata.buildFrom(updated).addVersion(viewVersionThree).setCurrentVersionId(3).build();\n+    List<ViewHistoryEntry> historyTwo = updated.history();\n+    assertThat(historyTwo)\n+        .hasSize(3)\n+        .containsAll(history)\n+        .element(2)\n+        .isEqualTo(ImmutableViewHistoryEntry.builder().versionId(3).timestampMillis(3000).build());\n+\n+    // setting an older view version as the new current (aka doing a rollback) should update the\n+    // timestamp in the history\n+    ViewMetadata reactiveOldViewVersion =\n+        ViewMetadata.buildFrom(updated).setCurrentVersionId(1).build();\n+    List<ViewHistoryEntry> historyThree = reactiveOldViewVersion.history();\n+    assertThat(historyThree)\n+        .hasSize(4)\n+        .containsAll(historyTwo)\n+        .element(3)\n+        .satisfies(\n+            v -> {\n+              assertThat(v.versionId()).isEqualTo(1);\n+              assertThat(v.timestampMillis())\n+                  .isGreaterThan(3000)\n+                  .isLessThanOrEqualTo(System.currentTimeMillis());\n+            });\n+  }\n+\n   @Test\n   public void versionsAddedInCurrentBuildAreRetained() {\n     ViewVersion v1 = newViewVersion(1, \"select 1 as count\");\n\ndiff --git a/core/src/test/java/org/apache/iceberg/view/TestViewMetadataParser.java b/core/src/test/java/org/apache/iceberg/view/TestViewMetadataParser.java\nindex 7784fdc4ed04..8f72596e266d 100644\n--- a/core/src/test/java/org/apache/iceberg/view/TestViewMetadataParser.java\n+++ b/core/src/test/java/org/apache/iceberg/view/TestViewMetadataParser.java\n@@ -106,7 +106,6 @@ public void readAndWriteValidViewMetadata() throws Exception {\n                     .assignUUID(\"fa6506c3-7681-40c8-86dc-e36561f83385\")\n                     .addSchema(TEST_SCHEMA)\n                     .addVersion(version1)\n-                    .addVersion(version2)\n                     .setLocation(\"s3://bucket/test/location\")\n                     .setProperties(\n                         ImmutableMap.of(\n@@ -114,6 +113,7 @@ public void readAndWriteValidViewMetadata() throws Exception {\n                     .setCurrentVersionId(1)\n                     .upgradeFormatVersion(1)\n                     .build())\n+            .addVersion(version2)\n             .setCurrentVersionId(2)\n             .build();\n \n@@ -222,7 +222,6 @@ public void viewMetadataWithMetadataLocation() throws Exception {\n                             .assignUUID(\"fa6506c3-7681-40c8-86dc-e36561f83385\")\n                             .addSchema(TEST_SCHEMA)\n                             .addVersion(version1)\n-                            .addVersion(version2)\n                             .setLocation(\"s3://bucket/test/location\")\n                             .setProperties(\n                                 ImmutableMap.of(\n@@ -233,6 +232,7 @@ public void viewMetadataWithMetadataLocation() throws Exception {\n                             .setCurrentVersionId(1)\n                             .upgradeFormatVersion(1)\n                             .build())\n+                    .addVersion(version2)\n                     .setCurrentVersionId(2)\n                     .build())\n             .setMetadataLocation(metadataLocation)\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12813",
    "pr_id": 12813,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 5,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSparkExecutorCache.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestViews.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSparkExecutorCache.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestViews.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java"
    ],
    "base_commit": "2d63ec4917c97687061bd202b58eb8e9ab7c5144",
    "head_commit": "9a2af957d5030c68f1ea5188e0132a8eadc3a358",
    "repo_url": "https://github.com/apache/iceberg/pull/12813",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12813",
    "dockerfile": "",
    "pr_merged_at": "2025-04-16T14:51:46.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java\ndeleted file mode 100644\nindex 4f137f5b8dac..000000000000\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkExtensionsTestBase.java\n+++ /dev/null\n@@ -1,71 +0,0 @@\n-/*\n- * Licensed to the Apache Software Foundation (ASF) under one\n- * or more contributor license agreements.  See the NOTICE file\n- * distributed with this work for additional information\n- * regarding copyright ownership.  The ASF licenses this file\n- * to you under the Apache License, Version 2.0 (the\n- * \"License\"); you may not use this file except in compliance\n- * with the License.  You may obtain a copy of the License at\n- *\n- *   http://www.apache.org/licenses/LICENSE-2.0\n- *\n- * Unless required by applicable law or agreed to in writing,\n- * software distributed under the License is distributed on an\n- * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n- * KIND, either express or implied.  See the License for the\n- * specific language governing permissions and limitations\n- * under the License.\n- */\n-package org.apache.iceberg.spark.extensions;\n-\n-import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.METASTOREURIS;\n-\n-import java.util.Map;\n-import java.util.Random;\n-import java.util.concurrent.ThreadLocalRandom;\n-import org.apache.iceberg.CatalogUtil;\n-import org.apache.iceberg.hive.HiveCatalog;\n-import org.apache.iceberg.hive.TestHiveMetastore;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n-import org.apache.iceberg.spark.SparkTestBase;\n-import org.apache.spark.sql.SparkSession;\n-import org.apache.spark.sql.internal.SQLConf;\n-import org.junit.BeforeClass;\n-\n-public abstract class SparkExtensionsTestBase extends SparkCatalogTestBase {\n-\n-  private static final Random RANDOM = ThreadLocalRandom.current();\n-\n-  public SparkExtensionsTestBase(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @BeforeClass\n-  public static void startMetastoreAndSpark() {\n-    SparkTestBase.metastore = new TestHiveMetastore();\n-    metastore.start();\n-    SparkTestBase.hiveConf = metastore.hiveConf();\n-\n-    SparkTestBase.spark =\n-        SparkSession.builder()\n-            .master(\"local[2]\")\n-            .config(\"spark.testing\", \"true\")\n-            .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n-            .config(\"spark.sql.extensions\", IcebergSparkSessionExtensions.class.getName())\n-            .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n-            .config(\"spark.sql.shuffle.partitions\", \"4\")\n-            .config(\"spark.sql.hive.metastorePartitionPruningFallbackOnException\", \"true\")\n-            .config(\"spark.sql.legacy.respectNullabilityInTextDatasetConversion\", \"true\")\n-            .config(\n-                SQLConf.ADAPTIVE_EXECUTION_ENABLED().key(), String.valueOf(RANDOM.nextBoolean()))\n-            .enableHiveSupport()\n-            .getOrCreate();\n-\n-    SparkTestBase.catalog =\n-        (HiveCatalog)\n-            CatalogUtil.loadCatalog(\n-                HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n-  }\n-}\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java\nindex ae82cf596147..4a8b5cfc3e70 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java\n@@ -20,52 +20,47 @@\n \n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assertions.atIndex;\n \n+import java.nio.file.Path;\n import java.util.List;\n-import java.util.Map;\n import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RewriteTablePathUtil;\n import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableUtil;\n import org.apache.spark.sql.AnalysisException;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-\n-public class TestRewriteTablePathProcedure extends SparkExtensionsTestBase {\n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n-\n-  public String staging = null;\n-  public String targetTableDir = null;\n-\n-  public TestRewriteTablePathProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Before\n-  public void setupTableLocation() throws Exception {\n-    this.staging = temp.newFolder(\"staging\").toURI().toString();\n-    this.targetTableDir = temp.newFolder(\"targetTable\").toURI().toString();\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRewriteTablePathProcedure extends ExtensionsTestBase {\n+  @TempDir private Path staging;\n+  @TempDir private Path targetTableDir;\n+\n+  @BeforeEach\n+  public void setupTableLocation() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteTablePathWithPositionalArgument() {\n+    String location = targetTableDir.toFile().toURI().toString();\n     Table table = validationCatalog.loadTable(tableIdent);\n-    String metadataJson =\n-        (((HasTableOperations) table).operations()).current().metadataFileLocation();\n+    String metadataJson = TableUtil.metadataFileLocation(table);\n \n     List<Object[]> result =\n         sql(\n             \"CALL %s.system.rewrite_table_path('%s', '%s', '%s')\",\n-            catalogName, tableIdent, table.location(), targetTableDir);\n+            catalogName, tableIdent, table.location(), location);\n     assertThat(result).hasSize(1);\n     assertThat(result.get(0)[0])\n         .as(\"Should return correct latest version\")\n@@ -78,18 +73,18 @@ public void testRewriteTablePathWithPositionalArgument() {\n     checkFileListLocationCount((String) result.get(0)[1], 1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteTablePathWithNamedArgument() {\n     Table table = validationCatalog.loadTable(tableIdent);\n-    String v0Metadata =\n-        RewriteTablePathUtil.fileName(\n-            (((HasTableOperations) table).operations()).current().metadataFileLocation());\n+    String v0Metadata = RewriteTablePathUtil.fileName(TableUtil.metadataFileLocation(table));\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n     String v1Metadata =\n         RewriteTablePathUtil.fileName(\n             (((HasTableOperations) table).operations()).refresh().metadataFileLocation());\n \n-    String expectedFileListLocation = staging + \"file-list\";\n+    String targetLocation = targetTableDir.toFile().toURI().toString();\n+    String stagingLocation = staging.toFile().toURI().toString();\n+    String expectedFileListLocation = stagingLocation + \"file-list\";\n \n     List<Object[]> result =\n         sql(\n@@ -102,21 +97,24 @@ public void testRewriteTablePathWithNamedArgument() {\n                 + \"staging_location => '%s')\",\n             catalogName,\n             tableIdent,\n-            this.targetTableDir,\n+            targetLocation,\n             table.location(),\n             v1Metadata,\n             v0Metadata,\n-            this.staging);\n-    assertThat(result).hasSize(1);\n-    assertThat(result.get(0)[0]).as(\"Should return correct latest version\").isEqualTo(v1Metadata);\n-    assertThat(result.get(0)[1])\n-        .as(\"Should return correct file_list_location\")\n-        .isEqualTo(expectedFileListLocation);\n+            stagingLocation);\n+    assertThat(result)\n+        .singleElement()\n+        .satisfies(\n+            objects -> {\n+              assertThat(objects).contains(v1Metadata, atIndex(0));\n+              assertThat(objects).contains(expectedFileListLocation, atIndex(1));\n+            });\n     checkFileListLocationCount((String) result.get(0)[1], 4);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testProcedureWithInvalidInput() {\n+    String targetLocation = targetTableDir.toFile().toURI().toString();\n \n     assertThatThrownBy(\n             () -> sql(\"CALL %s.system.rewrite_table_path('%s')\", catalogName, tableIdent))\n@@ -126,21 +124,19 @@ public void testProcedureWithInvalidInput() {\n             () ->\n                 sql(\n                     \"CALL %s.system.rewrite_table_path('%s','%s')\",\n-                    catalogName, tableIdent, this.targetTableDir))\n+                    catalogName, tableIdent, targetLocation))\n         .isInstanceOf(AnalysisException.class)\n         .hasMessageContaining(\"Missing required parameters: [target_prefix]\");\n     assertThatThrownBy(\n             () ->\n                 sql(\n                     \"CALL %s.system.rewrite_table_path('%s', '%s','%s')\",\n-                    catalogName, \"notExists\", this.targetTableDir, this.targetTableDir))\n+                    catalogName, \"notExists\", targetLocation, targetLocation))\n         .isInstanceOf(RuntimeException.class)\n         .hasMessageContaining(\"Couldn't load table\");\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    String v0Metadata =\n-        RewriteTablePathUtil.fileName(\n-            (((HasTableOperations) table).operations()).current().metadataFileLocation());\n+    String v0Metadata = RewriteTablePathUtil.fileName(TableUtil.metadataFileLocation(table));\n     assertThatThrownBy(\n             () ->\n                 sql(\n@@ -149,11 +145,7 @@ public void testProcedureWithInvalidInput() {\n                         + \"source_prefix => '%s', \"\n                         + \"target_prefix => '%s', \"\n                         + \"start_version => '%s')\",\n-                    catalogName,\n-                    tableIdent,\n-                    table.location(),\n-                    this.targetTableDir,\n-                    \"v20.metadata.json\"))\n+                    catalogName, tableIdent, table.location(), targetLocation, \"v20.metadata.json\"))\n         .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\n             \"Cannot find provided version file %s in metadata log.\", \"v20.metadata.json\");\n@@ -169,7 +161,7 @@ public void testProcedureWithInvalidInput() {\n                     catalogName,\n                     tableIdent,\n                     table.location(),\n-                    this.targetTableDir,\n+                    targetLocation,\n                     v0Metadata,\n                     \"v11.metadata.json\"))\n         .isInstanceOf(IllegalArgumentException.class)\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSparkExecutorCache.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSparkExecutorCache.java\nindex ed0685735941..382f0ff74e89 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSparkExecutorCache.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSparkExecutorCache.java\n@@ -38,6 +38,8 @@\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.Files;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -67,12 +69,13 @@\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.storage.memory.MemoryStore;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.Test;\n-import org.junit.runners.Parameterized.Parameters;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSparkExecutorCache extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSparkExecutorCache extends ExtensionsTestBase {\n \n   @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n   public static Object[][] parameters() {\n@@ -99,31 +102,26 @@ public static Object[][] parameters() {\n   private String targetTableName;\n   private TableIdentifier targetTableIdent;\n \n-  public TestSparkExecutorCache(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Before\n+  @BeforeEach\n   public void configureTargetTableName() {\n     String name = \"target_exec_cache_\" + JOB_COUNTER.incrementAndGet();\n     this.targetTableName = tableName(name);\n     this.targetTableIdent = TableIdentifier.of(Namespace.of(\"default\"), name);\n   }\n \n-  @After\n+  @AfterEach\n   public void releaseResources() {\n     sql(\"DROP TABLE IF EXISTS %s\", targetTableName);\n     sql(\"DROP TABLE IF EXISTS %s\", UPDATES_VIEW_NAME);\n     INPUT_FILES.clear();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteDelete() throws Exception {\n     checkDelete(COPY_ON_WRITE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadDelete() throws Exception {\n     checkDelete(MERGE_ON_READ);\n   }\n@@ -148,12 +146,12 @@ private void checkDelete(RowLevelOperationMode mode) throws Exception {\n         sql(\"SELECT * FROM %s ORDER BY id ASC\", targetTableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteUpdate() throws Exception {\n     checkUpdate(COPY_ON_WRITE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadUpdate() throws Exception {\n     checkUpdate(MERGE_ON_READ);\n   }\n@@ -181,12 +179,12 @@ private void checkUpdate(RowLevelOperationMode mode) throws Exception {\n         sql(\"SELECT * FROM %s ORDER BY id ASC\", targetTableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteMerge() throws Exception {\n     checkMerge(COPY_ON_WRITE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadMerge() throws Exception {\n     checkMerge(MERGE_ON_READ);\n   }\n@@ -282,13 +280,13 @@ private DeleteFile writeEqDeletes(Table table, String col, Object... values) thr\n       deletes.add(delete.copy(col, value));\n     }\n \n-    OutputFile out = Files.localOutput(temp.newFile(\"eq-deletes-\" + UUID.randomUUID()));\n+    OutputFile out = Files.localOutput(new File(temp.toFile(), \"eq-deletes-\" + UUID.randomUUID()));\n     return FileHelpers.writeDeleteFile(table, out, null, deletes, deleteSchema);\n   }\n \n   private Pair<DeleteFile, CharSequenceSet> writePosDeletes(\n       Table table, List<Pair<CharSequence, Long>> deletes) throws IOException {\n-    OutputFile out = Files.localOutput(temp.newFile(\"pos-deletes-\" + UUID.randomUUID()));\n+    OutputFile out = Files.localOutput(new File(temp.toFile(), \"pos-deletes-\" + UUID.randomUUID()));\n     return FileHelpers.writeDeleteFile(table, out, null, deletes);\n   }\n \n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestViews.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestViews.java\nindex bcb0991d1d2e..7f2f4fd1f8f6 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestViews.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestViews.java\n@@ -23,16 +23,17 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n import static org.assertj.core.api.Assumptions.assumeThat;\n \n-import java.io.IOException;\n+import java.nio.file.Paths;\n import java.util.List;\n import java.util.Locale;\n-import java.util.Map;\n import java.util.Random;\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n import org.apache.iceberg.CatalogProperties;\n import org.apache.iceberg.CatalogUtil;\n import org.apache.iceberg.IcebergBuild;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.catalog.Namespace;\n@@ -58,18 +59,21 @@\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.catalyst.catalog.SessionCatalog;\n import org.assertj.core.api.InstanceOfAssertFactories;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.Test;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestViews extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestViews extends ExtensionsTestBase {\n   private static final Namespace NAMESPACE = Namespace.of(\"default\");\n   private static final String SPARK_CATALOG = \"spark_catalog\";\n   private final String tableName = \"table\";\n \n-  @Before\n+  @BeforeEach\n+  @Override\n   public void before() {\n+    super.before();\n     spark.conf().set(\"spark.sql.defaultCatalog\", catalogName);\n     sql(\"USE %s\", catalogName);\n     sql(\"CREATE NAMESPACE IF NOT EXISTS %s\", NAMESPACE);\n@@ -79,13 +83,17 @@ public void before() {\n     sql(\"USE %s.%s\", catalogName, NAMESPACE);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"USE %s\", catalogName);\n     sql(\"DROP TABLE IF EXISTS %s.%s\", NAMESPACE, tableName);\n+\n+    // reset spark session catalog\n+    spark.sessionState().catalogManager().reset();\n+    spark.conf().unset(\"spark.sql.catalog.spark_catalog\");\n   }\n \n-  @Parameterized.Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n       {\n@@ -98,17 +106,13 @@ public static Object[][] parameters() {\n         SparkCatalogConfig.SPARK_SESSION_WITH_VIEWS.implementation(),\n         ImmutableMap.builder()\n             .putAll(SparkCatalogConfig.SPARK_SESSION_WITH_VIEWS.properties())\n-            .put(CatalogProperties.URI, REST_SERVER_RULE.uri())\n+            .put(CatalogProperties.URI, restCatalog.properties().get(CatalogProperties.URI))\n             .build()\n       }\n     };\n   }\n \n-  public TestViews(String catalog, String implementation, Map<String, String> properties) {\n-    super(catalog, implementation, properties);\n-  }\n-\n-  @Test\n+  @TestTemplate\n   public void readFromView() throws NoSuchTableException {\n     insertRows(10);\n     String viewName = viewName(\"simpleView\");\n@@ -134,7 +138,7 @@ public void readFromView() throws NoSuchTableException {\n         .containsExactlyInAnyOrderElementsOf(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromTrinoView() throws NoSuchTableException {\n     insertRows(10);\n     String viewName = viewName(\"trinoView\");\n@@ -159,7 +163,7 @@ public void readFromTrinoView() throws NoSuchTableException {\n         .containsExactlyInAnyOrderElementsOf(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromMultipleViews() throws NoSuchTableException {\n     insertRows(6);\n     String viewName = viewName(\"firstView\");\n@@ -192,7 +196,7 @@ public void readFromMultipleViews() throws NoSuchTableException {\n         .containsExactlyInAnyOrder(row(4), row(5), row(6));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromViewUsingNonExistingTable() throws NoSuchTableException {\n     insertRows(10);\n     String viewName = viewName(\"viewWithNonExistingTable\");\n@@ -216,7 +220,7 @@ public void readFromViewUsingNonExistingTable() throws NoSuchTableException {\n                 catalogName, NAMESPACE));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromViewUsingNonExistingTableColumn() throws NoSuchTableException {\n     insertRows(10);\n     String viewName = viewName(\"viewWithNonExistingColumn\");\n@@ -238,7 +242,7 @@ public void readFromViewUsingNonExistingTableColumn() throws NoSuchTableExceptio\n             \"A column or function parameter with name `non_existing` cannot be resolved\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromViewUsingInvalidSQL() throws NoSuchTableException {\n     insertRows(10);\n     String viewName = viewName(\"viewWithInvalidSQL\");\n@@ -260,7 +264,7 @@ public void readFromViewUsingInvalidSQL() throws NoSuchTableException {\n             String.format(\"Invalid view text: invalid SQL. The view %s\", viewName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromViewWithStaleSchema() throws NoSuchTableException {\n     insertRows(10);\n     String viewName = viewName(\"staleView\");\n@@ -286,7 +290,7 @@ public void readFromViewWithStaleSchema() throws NoSuchTableException {\n         .hasMessageContaining(\"A column or function parameter with name `data` cannot be resolved\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromViewHiddenByTempView() throws NoSuchTableException {\n     insertRows(10);\n     String viewName = viewName(\"viewHiddenByTempView\");\n@@ -313,7 +317,7 @@ public void readFromViewHiddenByTempView() throws NoSuchTableException {\n         .containsExactlyInAnyOrderElementsOf(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromViewWithGlobalTempView() throws NoSuchTableException {\n     insertRows(10);\n     String viewName = viewName(\"viewWithGlobalTempView\");\n@@ -343,7 +347,7 @@ public void readFromViewWithGlobalTempView() throws NoSuchTableException {\n             IntStream.rangeClosed(6, 10).mapToObj(this::row).collect(Collectors.toList()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromViewReferencingAnotherView() throws NoSuchTableException {\n     insertRows(10);\n     String firstView = viewName(\"viewBeingReferencedInAnotherView\");\n@@ -374,7 +378,7 @@ public void readFromViewReferencingAnotherView() throws NoSuchTableException {\n         .containsExactly(row(5));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromViewReferencingTempView() throws NoSuchTableException {\n     insertRows(10);\n     String tempView = viewName(\"tempViewBeingReferencedInAnotherView\");\n@@ -410,7 +414,7 @@ public void readFromViewReferencingTempView() throws NoSuchTableException {\n         .hasMessageContaining(\"cannot be found\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromViewReferencingAnotherViewHiddenByTempView() throws NoSuchTableException {\n     insertRows(10);\n     String innerViewName = viewName(\"inner_view\");\n@@ -458,7 +462,7 @@ public void readFromViewReferencingAnotherViewHiddenByTempView() throws NoSuchTa\n         .containsExactlyInAnyOrderElementsOf(expectedViewRows);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromViewReferencingGlobalTempView() throws NoSuchTableException {\n     insertRows(10);\n     String globalTempView = viewName(\"globalTempViewBeingReferenced\");\n@@ -496,7 +500,7 @@ public void readFromViewReferencingGlobalTempView() throws NoSuchTableException\n         .hasMessageContaining(\"cannot be found\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromViewReferencingTempFunction() throws NoSuchTableException {\n     insertRows(10);\n     String viewName = viewName(\"viewReferencingTempFunction\");\n@@ -537,7 +541,7 @@ public void readFromViewReferencingTempFunction() throws NoSuchTableException {\n         .hasMessageStartingWith(expectedErrorMsg);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromViewWithCTE() throws NoSuchTableException {\n     insertRows(10);\n     String viewName = viewName(\"viewWithCTE\");\n@@ -560,7 +564,7 @@ public void readFromViewWithCTE() throws NoSuchTableException {\n     assertThat(sql(\"SELECT * FROM %s\", viewName)).hasSize(1).containsExactly(row(10, 1L));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void readFromViewWithGroupByOrdinal() throws NoSuchTableException {\n     insertRows(3);\n     insertRows(2);\n@@ -582,7 +586,7 @@ public void readFromViewWithGroupByOrdinal() throws NoSuchTableException {\n         .containsExactlyInAnyOrder(row(1, 2L), row(2, 2L), row(3, 1L));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithGroupByOrdinal() throws NoSuchTableException {\n     insertRows(3);\n     insertRows(2);\n@@ -594,7 +598,7 @@ public void createViewWithGroupByOrdinal() throws NoSuchTableException {\n         .containsExactlyInAnyOrder(row(1, 2L), row(2, 2L), row(3, 1L));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void rewriteFunctionIdentifier() {\n     assumeThat(catalogName)\n         .as(\"system namespace doesn't exist in SparkSessionCatalog\")\n@@ -623,7 +627,7 @@ public void rewriteFunctionIdentifier() {\n         .containsExactly(row(IcebergBuild.version()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void builtinFunctionIdentifierNotRewritten() {\n     String viewName = viewName(\"builtinFunctionIdentifierNotRewritten\");\n     String sql = \"SELECT trim('  abc   ') AS result\";\n@@ -642,7 +646,7 @@ public void builtinFunctionIdentifierNotRewritten() {\n     assertThat(sql(\"SELECT * FROM %s\", viewName)).hasSize(1).containsExactly(row(\"abc\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void rewriteFunctionIdentifierWithNamespace() {\n     assumeThat(catalogName)\n         .as(\"system namespace doesn't exist in SparkSessionCatalog\")\n@@ -671,7 +675,7 @@ public void rewriteFunctionIdentifierWithNamespace() {\n         .containsExactly(row(50, \"a\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void fullFunctionIdentifier() {\n     assumeThat(catalogName)\n         .as(\"system namespace doesn't exist in SparkSessionCatalog\")\n@@ -698,7 +702,7 @@ public void fullFunctionIdentifier() {\n         .containsExactly(row(50, \"a\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void fullFunctionIdentifierNotRewrittenLoadFailure() {\n     String viewName = viewName(\"fullFunctionIdentifierNotRewrittenLoadFailure\");\n     String sql = \"SELECT spark_catalog.system.bucket(100, 'a') AS bucket_result, 'a' AS value\";\n@@ -743,7 +747,7 @@ private Catalog tableCatalog() {\n     return Spark3Util.loadIcebergCatalog(spark, catalogName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void renameView() throws NoSuchTableException {\n     insertRows(10);\n     String viewName = viewName(\"originalView\");\n@@ -769,7 +773,7 @@ public void renameView() throws NoSuchTableException {\n         .containsExactlyInAnyOrderElementsOf(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void renameViewHiddenByTempView() throws NoSuchTableException {\n     insertRows(10);\n     String viewName = viewName(\"originalView\");\n@@ -808,7 +812,7 @@ public void renameViewHiddenByTempView() throws NoSuchTableException {\n     assertThat(viewCatalog.viewExists(TableIdentifier.of(NAMESPACE, renamedView))).isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void renameViewToDifferentTargetCatalog() {\n     String viewName = viewName(\"originalView\");\n     String renamedView = viewName(\"renamedView\");\n@@ -839,14 +843,14 @@ public void renameViewToDifferentTargetCatalog() {\n             \"Cannot move view between catalogs: from=%s and to=%s\", catalogName, targetCatalog);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void renameNonExistingView() {\n     assertThatThrownBy(() -> sql(\"ALTER VIEW non_existing RENAME TO target\"))\n         .isInstanceOf(AnalysisException.class)\n         .hasMessageContaining(\"The table or view `non_existing` cannot be found\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void renameViewTargetAlreadyExistsAsView() {\n     String viewName = viewName(\"renameViewSource\");\n     String target = viewName(\"renameViewTarget\");\n@@ -876,7 +880,7 @@ public void renameViewTargetAlreadyExistsAsView() {\n             String.format(\"Cannot create view default.%s because it already exists\", target));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void renameViewTargetAlreadyExistsAsTable() {\n     String viewName = viewName(\"renameViewSource\");\n     String target = viewName(\"renameViewTarget\");\n@@ -901,7 +905,7 @@ public void renameViewTargetAlreadyExistsAsTable() {\n             String.format(\"Cannot create view default.%s because it already exists\", target));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void dropView() {\n     String viewName = viewName(\"viewToBeDropped\");\n     String sql = String.format(\"SELECT id FROM %s\", tableName);\n@@ -923,14 +927,14 @@ public void dropView() {\n     assertThat(viewCatalog.viewExists(identifier)).isFalse();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void dropNonExistingView() {\n     assertThatThrownBy(() -> sql(\"DROP VIEW non_existing\"))\n         .isInstanceOf(AnalysisException.class)\n         .hasMessageContaining(\"The view %s.%s cannot be found\", NAMESPACE, \"non_existing\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void dropViewIfExists() {\n     String viewName = viewName(\"viewToBeDropped\");\n     String sql = String.format(\"SELECT id FROM %s\", tableName);\n@@ -955,7 +959,7 @@ public void dropViewIfExists() {\n   }\n \n   /** The purpose of this test is mainly to make sure that normal view deletion isn't messed up */\n-  @Test\n+  @TestTemplate\n   public void dropGlobalTempView() {\n     String globalTempView = viewName(\"globalViewToBeDropped\");\n     sql(\"CREATE GLOBAL TEMPORARY VIEW %s AS SELECT id FROM %s\", globalTempView, tableName);\n@@ -966,7 +970,7 @@ public void dropGlobalTempView() {\n   }\n \n   /** The purpose of this test is mainly to make sure that normal view deletion isn't messed up */\n-  @Test\n+  @TestTemplate\n   public void dropTempView() {\n     String tempView = viewName(\"tempViewToBeDropped\");\n     sql(\"CREATE TEMPORARY VIEW %s AS SELECT id FROM %s\", tempView, tableName);\n@@ -984,7 +988,7 @@ private String viewName(String viewName) {\n     return viewName + new Random().nextInt(1000000);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewIfNotExists() {\n     String viewName = viewName(\"viewThatAlreadyExists\");\n     sql(\"CREATE VIEW %s AS SELECT id FROM %s\", viewName, tableName);\n@@ -1001,7 +1005,7 @@ public void createViewIfNotExists() {\n             () -> sql(\"CREATE VIEW IF NOT EXISTS %s AS SELECT id FROM %s\", viewName, tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createOrReplaceView() throws NoSuchTableException {\n     insertRows(6);\n     String viewName = viewName(\"simpleView\");\n@@ -1017,14 +1021,14 @@ public void createOrReplaceView() throws NoSuchTableException {\n         .containsExactlyInAnyOrder(row(4), row(5), row(6));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithInvalidSQL() {\n     assertThatThrownBy(() -> sql(\"CREATE VIEW simpleViewWithInvalidSQL AS invalid SQL\"))\n         .isInstanceOf(AnalysisException.class)\n         .hasMessageContaining(\"Syntax error\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewReferencingTempView() throws NoSuchTableException {\n     insertRows(10);\n     String tempView = viewName(\"temporaryViewBeingReferencedInAnotherView\");\n@@ -1043,7 +1047,7 @@ public void createViewReferencingTempView() throws NoSuchTableException {\n         .hasMessageContaining(tempView);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewReferencingGlobalTempView() throws NoSuchTableException {\n     insertRows(10);\n     String globalTempView = viewName(\"globalTemporaryViewBeingReferenced\");\n@@ -1067,7 +1071,7 @@ public void createViewReferencingGlobalTempView() throws NoSuchTableException {\n         .hasMessageContaining(String.format(\"%s.%s\", \"global_temp\", globalTempView));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewReferencingTempFunction() {\n     String viewName = viewName(\"viewReferencingTemporaryFunction\");\n     String functionName = viewName(\"test_avg_func\");\n@@ -1086,7 +1090,7 @@ public void createViewReferencingTempFunction() {\n         .hasMessageContaining(functionName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewReferencingQualifiedTempFunction() {\n     String viewName = viewName(\"viewReferencingTemporaryFunction\");\n     String functionName = viewName(\"test_avg_func_qualified\");\n@@ -1117,7 +1121,7 @@ public void createViewReferencingQualifiedTempFunction() {\n         .hasMessageContaining(String.format(\"`%s`.`%s`\", NAMESPACE, functionName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewUsingNonExistingTable() {\n     assertThatThrownBy(\n             () -> sql(\"CREATE VIEW viewWithNonExistingTable AS SELECT id FROM non_existing\"))\n@@ -1125,7 +1129,7 @@ public void createViewUsingNonExistingTable() {\n         .hasMessageContaining(\"The table or view `non_existing` cannot be found\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithMismatchedColumnCounts() {\n     String viewName = viewName(\"viewWithMismatchedColumnCounts\");\n \n@@ -1148,7 +1152,7 @@ public void createViewWithMismatchedColumnCounts() {\n         .hasMessageContaining(\"Data columns: id, data\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithColumnAliases() throws NoSuchTableException {\n     insertRows(6);\n     String viewName = viewName(\"viewWithColumnAliases\");\n@@ -1184,7 +1188,7 @@ public void createViewWithColumnAliases() throws NoSuchTableException {\n         .containsExactlyInAnyOrder(row(1), row(2), row(3));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithDuplicateColumnNames() {\n     assertThatThrownBy(\n             () ->\n@@ -1195,7 +1199,7 @@ public void createViewWithDuplicateColumnNames() {\n         .hasMessageContaining(\"The column `new_id` already exists\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithDuplicateQueryColumnNames() throws NoSuchTableException {\n     insertRows(3);\n     String viewName = viewName(\"viewWithDuplicateQueryColumnNames\");\n@@ -1213,7 +1217,7 @@ public void createViewWithDuplicateQueryColumnNames() throws NoSuchTableExceptio\n         .containsExactlyInAnyOrder(row(1, 1), row(2, 2), row(3, 3));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithCTE() throws NoSuchTableException {\n     insertRows(10);\n     String viewName = viewName(\"simpleViewWithCTE\");\n@@ -1228,7 +1232,7 @@ public void createViewWithCTE() throws NoSuchTableException {\n     assertThat(sql(\"SELECT * FROM %s\", viewName)).hasSize(1).containsExactly(row(10, 1L));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithConflictingNamesForCTEAndTempView() throws NoSuchTableException {\n     insertRows(10);\n     String viewName = viewName(\"viewWithConflictingNamesForCTEAndTempView\");\n@@ -1247,7 +1251,7 @@ public void createViewWithConflictingNamesForCTEAndTempView() throws NoSuchTable\n     assertThat(sql(\"SELECT * FROM %s\", viewName)).hasSize(1).containsExactly(row(10, 1L));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithCTEReferencingTempView() {\n     String viewName = viewName(\"viewWithCTEReferencingTempView\");\n     String tempViewInCTE = viewName(\"tempViewInCTE\");\n@@ -1267,7 +1271,7 @@ public void createViewWithCTEReferencingTempView() {\n         .hasMessageContaining(tempViewInCTE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithCTEReferencingTempFunction() {\n     String viewName = viewName(\"viewWithCTEReferencingTempFunction\");\n     String functionName = viewName(\"avg_function_in_cte\");\n@@ -1289,7 +1293,7 @@ public void createViewWithCTEReferencingTempFunction() {\n         .hasMessageContaining(functionName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithNonExistingQueryColumn() {\n     assertThatThrownBy(\n             () ->\n@@ -1301,7 +1305,7 @@ public void createViewWithNonExistingQueryColumn() {\n             \"A column or function parameter with name `non_existing` cannot be resolved\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithSubqueryExpressionUsingTempView() {\n     String viewName = viewName(\"viewWithSubqueryExpression\");\n     String tempView = viewName(\"simpleTempView\");\n@@ -1318,7 +1322,7 @@ public void createViewWithSubqueryExpressionUsingTempView() {\n         .hasMessageContaining(tempView);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithSubqueryExpressionUsingGlobalTempView() {\n     String viewName = viewName(\"simpleViewWithSubqueryExpression\");\n     String globalTempView = viewName(\"simpleGlobalTempView\");\n@@ -1339,7 +1343,7 @@ public void createViewWithSubqueryExpressionUsingGlobalTempView() {\n         .hasMessageContaining(String.format(\"%s.%s\", \"global_temp\", globalTempView));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithSubqueryExpressionUsingTempFunction() {\n     String viewName = viewName(\"viewWithSubqueryExpression\");\n     String functionName = viewName(\"avg_function_in_subquery\");\n@@ -1360,7 +1364,7 @@ public void createViewWithSubqueryExpressionUsingTempFunction() {\n         .hasMessageContaining(functionName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithSubqueryExpressionInFilterThatIsRewritten()\n       throws NoSuchTableException {\n     insertRows(5);\n@@ -1387,7 +1391,7 @@ public void createViewWithSubqueryExpressionInFilterThatIsRewritten()\n         .containsExactly(row(5));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithSubqueryExpressionInQueryThatIsRewritten() throws NoSuchTableException {\n     insertRows(3);\n     String viewName = viewName(\"viewWithSubqueryExpression\");\n@@ -1414,7 +1418,7 @@ public void createViewWithSubqueryExpressionInQueryThatIsRewritten() throws NoSu\n         .containsExactly(row(3), row(3), row(3));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void describeView() {\n     String viewName = viewName(\"describeView\");\n \n@@ -1423,7 +1427,7 @@ public void describeView() {\n         .containsExactly(row(\"id\", \"int\", \"\"), row(\"data\", \"string\", \"\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void describeExtendedView() {\n     String viewName = viewName(\"describeExtendedView\");\n     String sql = String.format(\"SELECT id, data FROM %s WHERE id <= 3\", tableName);\n@@ -1449,7 +1453,7 @@ public void describeExtendedView() {\n                 \"\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createAndDescribeViewInDefaultNamespace() {\n     String viewName = viewName(\"createViewInDefaultNamespace\");\n     String sql = String.format(\"SELECT id, data FROM %s WHERE id <= 3\", tableName);\n@@ -1479,7 +1483,7 @@ public void createAndDescribeViewInDefaultNamespace() {\n                 \"\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createAndDescribeViewWithoutCurrentNamespace() {\n     String viewName = viewName(\"createViewWithoutCurrentNamespace\");\n     Namespace namespace = Namespace.of(\"test_namespace\");\n@@ -1511,7 +1515,7 @@ public void createAndDescribeViewWithoutCurrentNamespace() {\n                 \"\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void showViewProperties() {\n     String viewName = viewName(\"showViewProps\");\n \n@@ -1522,7 +1526,7 @@ public void showViewProperties() {\n         .contains(row(\"key1\", \"val1\"), row(\"key2\", \"val2\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void showViewPropertiesByKey() {\n     String viewName = viewName(\"showViewPropsByKey\");\n \n@@ -1541,7 +1545,7 @@ public void showViewPropertiesByKey() {\n                     catalogName, NAMESPACE, viewName)));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void showViews() throws NoSuchTableException {\n     insertRows(6);\n     String sql = String.format(\"SELECT * from %s\", tableName);\n@@ -1598,7 +1602,7 @@ public void showViews() throws NoSuchTableException {\n     assertThat(sql(\"SHOW VIEWS IN default\")).contains(tempView);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void showViewsWithCurrentNamespace() {\n     String namespaceOne = \"show_views_ns1\";\n     String namespaceTwo = \"show_views_ns2\";\n@@ -1629,7 +1633,7 @@ public void showViewsWithCurrentNamespace() {\n     assertThat(sql(\"SHOW VIEWS LIKE 'viewTwo*'\")).contains(v2).doesNotContain(v1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void showCreateSimpleView() {\n     String viewName = viewName(\"showCreateSimpleView\");\n     String sql = String.format(\"SELECT id, data FROM %s WHERE id <= 3\", tableName);\n@@ -1651,7 +1655,7 @@ public void showCreateSimpleView() {\n     assertThat(sql(\"SHOW CREATE TABLE %s\", viewName)).containsExactly(row(expected));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void showCreateComplexView() {\n     String viewName = viewName(\"showCreateComplexView\");\n     String sql = String.format(\"SELECT id, data FROM %s WHERE id <= 3\", tableName);\n@@ -1679,7 +1683,7 @@ public void showCreateComplexView() {\n     assertThat(sql(\"SHOW CREATE TABLE %s\", viewName)).containsExactly(row(expected));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void alterViewSetProperties() {\n     String viewName = viewName(\"viewWithSetProperties\");\n \n@@ -1701,7 +1705,7 @@ public void alterViewSetProperties() {\n         .containsEntry(\"comment\", \"view comment\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void alterViewSetReservedProperties() {\n     String viewName = viewName(\"viewWithSetReservedProperties\");\n \n@@ -1732,7 +1736,7 @@ public void alterViewSetReservedProperties() {\n         .hasMessageContaining(\"Cannot set reserved property: 'spark.query-column-names'\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void alterViewUnsetProperties() {\n     String viewName = viewName(\"viewWithUnsetProperties\");\n     sql(\"CREATE VIEW %s AS SELECT id FROM %s WHERE id <= 3\", viewName, tableName);\n@@ -1753,7 +1757,7 @@ public void alterViewUnsetProperties() {\n         .containsEntry(\"comment\", \"view comment\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void alterViewUnsetUnknownProperty() {\n     String viewName = viewName(\"viewWithUnsetUnknownProp\");\n     sql(\"CREATE VIEW %s AS SELECT id FROM %s WHERE id <= 3\", viewName, tableName);\n@@ -1767,7 +1771,7 @@ public void alterViewUnsetUnknownProperty() {\n             () -> sql(\"ALTER VIEW %s UNSET TBLPROPERTIES IF EXISTS ('unknown-key')\", viewName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void alterViewUnsetReservedProperties() {\n     String viewName = viewName(\"viewWithUnsetReservedProperties\");\n \n@@ -1803,7 +1807,7 @@ public void alterViewUnsetReservedProperties() {\n         .hasMessageContaining(\"Cannot unset reserved property: 'spark.query-column-names'\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createOrReplaceViewWithColumnAliases() throws NoSuchTableException {\n     insertRows(6);\n     String viewName = viewName(\"viewWithColumnAliases\");\n@@ -1849,7 +1853,7 @@ public void createOrReplaceViewWithColumnAliases() throws NoSuchTableException {\n     assertThat(second.doc()).isEqualTo(\"new ID\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void alterViewIsNotSupported() throws NoSuchTableException {\n     insertRows(6);\n     String viewName = viewName(\"alteredView\");\n@@ -1867,7 +1871,7 @@ public void alterViewIsNotSupported() throws NoSuchTableException {\n             \"ALTER VIEW <viewName> AS is not supported. Use CREATE OR REPLACE VIEW instead\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createOrReplaceViewKeepsViewHistory() {\n     String viewName = viewName(\"viewWithHistoryAfterReplace\");\n     String sql = String.format(\"SELECT id, data FROM %s WHERE id <= 3\", tableName);\n@@ -1906,7 +1910,7 @@ public void createOrReplaceViewKeepsViewHistory() {\n                 .asStruct());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void replacingTrinoViewShouldFail() {\n     String viewName = viewName(\"trinoView\");\n     String sql = String.format(\"SELECT id FROM %s\", tableName);\n@@ -1929,7 +1933,7 @@ public void replacingTrinoViewShouldFail() {\n                 + \"New dialects: [spark]\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void replacingTrinoAndSparkViewShouldFail() {\n     String viewName = viewName(\"trinoAndSparkView\");\n     String sql = String.format(\"SELECT id FROM %s\", tableName);\n@@ -1953,7 +1957,7 @@ public void replacingTrinoAndSparkViewShouldFail() {\n                 + \"New dialects: [spark]\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void replacingViewWithDialectDropAllowed() {\n     String viewName = viewName(\"trinoView\");\n     String sql = String.format(\"SELECT id FROM %s\", tableName);\n@@ -2002,7 +2006,7 @@ public void replacingViewWithDialectDropAllowed() {\n         .isEqualTo(ImmutableSQLViewRepresentation.builder().dialect(\"spark\").sql(sql).build());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithRecursiveCycle() {\n     String viewOne = viewName(\"viewOne\");\n     String viewTwo = viewName(\"viewTwo\");\n@@ -2021,7 +2025,7 @@ public void createViewWithRecursiveCycle() {\n             String.format(\"Recursive cycle in view detected: %s (cycle: %s)\", view1, cycle));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithRecursiveCycleToV1View() {\n     assumeThat(catalogName).isNotEqualTo(SPARK_CATALOG);\n     String viewOne = viewName(\"view_one\");\n@@ -2043,7 +2047,7 @@ public void createViewWithRecursiveCycleToV1View() {\n             String.format(\"Recursive cycle in view detected: %s (cycle: %s)\", view1, cycle));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithRecursiveCycleInCTE() {\n     String viewOne = viewName(\"viewOne\");\n     String viewTwo = viewName(\"viewTwo\");\n@@ -2068,7 +2072,7 @@ public void createViewWithRecursiveCycleInCTE() {\n             String.format(\"Recursive cycle in view detected: %s (cycle: %s)\", view1, cycle));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createViewWithRecursiveCycleInSubqueryExpression() {\n     String viewOne = viewName(\"viewOne\");\n     String viewTwo = viewName(\"viewTwo\");\n@@ -2090,10 +2094,11 @@ public void createViewWithRecursiveCycleInSubqueryExpression() {\n             String.format(\"Recursive cycle in view detected: %s (cycle: %s)\", view1, cycle));\n   }\n \n-  @Test\n-  public void createViewWithCustomMetadataLocation() throws IOException {\n+  @TestTemplate\n+  public void createViewWithCustomMetadataLocation() {\n     String viewName = viewName(\"v\");\n-    String customMetadataLocation = temp.newFolder(\"custom-metadata-location\").toString();\n+    String customMetadataLocation =\n+        Paths.get(temp.toUri().toString(), \"custom-metadata-location\").toString();\n     sql(\n         \"CREATE VIEW %s TBLPROPERTIES ('%s'='%s') AS SELECT * FROM %s\",\n         viewName, ViewProperties.WRITE_METADATA_LOCATION, customMetadataLocation, tableName);\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java\nindex 7bbfc29e0a83..4a8b5cfc3e70 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java\n@@ -20,6 +20,7 @@\n \n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assertions.atIndex;\n \n import java.nio.file.Path;\n import java.util.List;\n@@ -101,11 +102,13 @@ public void testRewriteTablePathWithNamedArgument() {\n             v1Metadata,\n             v0Metadata,\n             stagingLocation);\n-    assertThat(result).hasSize(1);\n-    assertThat(result.get(0)[0]).as(\"Should return correct latest version\").isEqualTo(v1Metadata);\n-    assertThat(result.get(0)[1])\n-        .as(\"Should return correct file_list_location\")\n-        .isEqualTo(expectedFileListLocation);\n+    assertThat(result)\n+        .singleElement()\n+        .satisfies(\n+            objects -> {\n+              assertThat(objects).contains(v1Metadata, atIndex(0));\n+              assertThat(objects).contains(expectedFileListLocation, atIndex(1));\n+            });\n     checkFileListLocationCount((String) result.get(0)[1], 4);\n   }\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12807",
    "pr_id": 12807,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 23,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "spark/v3.4/spark-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSqlExtensionsAstBuilder.scala",
      "spark/v3.4/spark-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/SetWriteDistributionAndOrderingExec.scala",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAncestorsOfProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestChangelogTable.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestConflictValidation.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMetaColumnProjectionWithStageScan.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRequiredDistributionAndOrdering.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetWriteDistributionAndOrdering.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestStoragePartitionedJoinsInRowLevelOperations.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSystemFunctionPushDownDQL.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSystemFunctionPushDownInRowLevelOperations.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestWriteAborts.java",
      "spark/v3.4/spark/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/SetWriteDistributionAndOrdering.scala",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestChangelogTable.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetWriteDistributionAndOrdering.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAncestorsOfProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestChangelogTable.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestConflictValidation.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMetaColumnProjectionWithStageScan.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRequiredDistributionAndOrdering.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetWriteDistributionAndOrdering.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestStoragePartitionedJoinsInRowLevelOperations.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSystemFunctionPushDownDQL.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSystemFunctionPushDownInRowLevelOperations.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestWriteAborts.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestChangelogTable.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetWriteDistributionAndOrdering.java"
    ],
    "base_commit": "414b7bc11531647054f333400a74be23982b6270",
    "head_commit": "2b1ed0d50944c14a98babf6b2892f6acaabc0003",
    "repo_url": "https://github.com/apache/iceberg/pull/12807",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12807",
    "dockerfile": "",
    "pr_merged_at": "2025-04-16T08:58:56.000Z",
    "patch": "diff --git a/spark/v3.4/spark-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSqlExtensionsAstBuilder.scala b/spark/v3.4/spark-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSqlExtensionsAstBuilder.scala\nindex 2e438de2b8cd..6b1cc41da04c 100644\n--- a/spark/v3.4/spark-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSqlExtensionsAstBuilder.scala\n+++ b/spark/v3.4/spark-extensions/src/main/scala/org/apache/spark/sql/catalyst/parser/extensions/IcebergSqlExtensionsAstBuilder.scala\n@@ -226,11 +226,13 @@ class IcebergSqlExtensionsAstBuilder(delegate: ParserInterface) extends IcebergS\n     }\n \n     val distributionMode = if (distributionSpec != null) {\n-      DistributionMode.HASH\n-    } else if (orderingSpec.UNORDERED != null || orderingSpec.LOCALLY != null) {\n-      DistributionMode.NONE\n+      Some(DistributionMode.HASH)\n+    } else if (orderingSpec.UNORDERED != null) {\n+      Some(DistributionMode.NONE)\n+    } else if (orderingSpec.LOCALLY() != null) {\n+      None\n     } else {\n-      DistributionMode.RANGE\n+      Some(DistributionMode.RANGE)\n     }\n \n     val ordering = if (orderingSpec != null && orderingSpec.order != null) {\n\ndiff --git a/spark/v3.4/spark-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/SetWriteDistributionAndOrderingExec.scala b/spark/v3.4/spark-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/SetWriteDistributionAndOrderingExec.scala\nindex feecc0235076..c9004ddc5bda 100644\n--- a/spark/v3.4/spark-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/SetWriteDistributionAndOrderingExec.scala\n+++ b/spark/v3.4/spark-extensions/src/main/scala/org/apache/spark/sql/execution/datasources/v2/SetWriteDistributionAndOrderingExec.scala\n@@ -35,7 +35,7 @@ import org.apache.spark.sql.connector.catalog.TableCatalog\n case class SetWriteDistributionAndOrderingExec(\n     catalog: TableCatalog,\n     ident: Identifier,\n-    distributionMode: DistributionMode,\n+    distributionMode: Option[DistributionMode],\n     sortOrder: Seq[(Term, SortDirection, NullOrder)]) extends LeafV2CommandExec {\n \n   import CatalogV2Implicits._\n@@ -56,9 +56,11 @@ case class SetWriteDistributionAndOrderingExec(\n         }\n         orderBuilder.commit()\n \n-        txn.updateProperties()\n-          .set(WRITE_DISTRIBUTION_MODE, distributionMode.modeName())\n-          .commit()\n+        distributionMode.foreach { mode =>\n+          txn.updateProperties()\n+            .set(WRITE_DISTRIBUTION_MODE, mode.modeName())\n+            .commit()\n+        }\n \n         txn.commitTransaction()\n \n\ndiff --git a/spark/v3.4/spark/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/SetWriteDistributionAndOrdering.scala b/spark/v3.4/spark/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/SetWriteDistributionAndOrdering.scala\nindex 0a0234cdfe34..7b599eb3da1d 100644\n--- a/spark/v3.4/spark/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/SetWriteDistributionAndOrdering.scala\n+++ b/spark/v3.4/spark/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/SetWriteDistributionAndOrdering.scala\n@@ -28,7 +28,7 @@ import org.apache.spark.sql.connector.catalog.CatalogV2Implicits\n \n case class SetWriteDistributionAndOrdering(\n     table: Seq[String],\n-    distributionMode: DistributionMode,\n+    distributionMode: Option[DistributionMode],\n     sortOrder: Seq[(Term, SortDirection, NullOrder)]) extends LeafCommand {\n \n   import CatalogV2Implicits._\n",
    "test_patch": "diff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAncestorsOfProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAncestorsOfProcedure.java\nindex 1aa3e17fce02..4a3a158dea52 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAncestorsOfProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAncestorsOfProcedure.java\n@@ -21,26 +21,23 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.spark.sql.AnalysisException;\n-import org.junit.After;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestAncestorsOfProcedure extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestAncestorsOfProcedure extends ExtensionsTestBase {\n \n-  public TestAncestorsOfProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAncestorOfUsingEmptyArgs() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -61,7 +58,7 @@ public void testAncestorOfUsingEmptyArgs() {\n         output);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAncestorOfUsingSnapshotId() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -85,7 +82,7 @@ public void testAncestorOfUsingSnapshotId() {\n         sql(\"CALL %s.system.ancestors_of('%s', %dL)\", catalogName, tableIdent, preSnapshotId));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAncestorOfWithRollBack() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n@@ -129,7 +126,7 @@ public void testAncestorOfWithRollBack() {\n         sql(\"CALL %s.system.ancestors_of('%s', %dL)\", catalogName, tableIdent, thirdSnapshotId));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAncestorOfUsingNamedArgs() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -146,7 +143,7 @@ public void testAncestorOfUsingNamedArgs() {\n             catalogName, firstSnapshotId, tableIdent));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidAncestorOfCases() {\n     assertThatThrownBy(() -> sql(\"CALL %s.system.ancestors_of()\", catalogName))\n         .isInstanceOf(AnalysisException.class)\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java\nindex a26f58074f14..58d054bd0560 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java\n@@ -18,13 +18,14 @@\n  */\n package org.apache.iceberg.spark.extensions;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static scala.collection.JavaConverters.seqAsJavaList;\n \n import java.math.BigDecimal;\n import java.sql.Timestamp;\n import java.time.Instant;\n import java.util.List;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.spark.sql.SparkSession;\n import org.apache.spark.sql.catalyst.expressions.Expression;\n@@ -39,22 +40,16 @@\n import org.apache.spark.sql.catalyst.plans.logical.PositionalArgument;\n import org.apache.spark.sql.types.DataType;\n import org.apache.spark.sql.types.DataTypes;\n-import org.junit.AfterClass;\n-import org.junit.Assert;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import scala.collection.JavaConverters;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.Test;\n \n public class TestCallStatementParser {\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n-\n   private static SparkSession spark = null;\n   private static ParserInterface parser = null;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void startSpark() {\n     TestCallStatementParser.spark =\n         SparkSession.builder()\n@@ -65,7 +60,7 @@ public static void startSpark() {\n     TestCallStatementParser.parser = spark.sessionState().sqlParser();\n   }\n \n-  @AfterClass\n+  @AfterAll\n   public static void stopSpark() {\n     SparkSession currentSpark = TestCallStatementParser.spark;\n     TestCallStatementParser.spark = null;\n@@ -81,8 +76,8 @@ public void testDelegateUnsupportedProcedure() {\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n-              Assert.assertEquals(\"PARSE_SYNTAX_ERROR\", parseException.getErrorClass());\n-              Assert.assertEquals(\"'CALL'\", parseException.getMessageParameters().get(\"error\"));\n+              assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n+              assertThat(parseException.getMessageParameters()).containsEntry(\"error\", \"'CALL'\");\n             });\n   }\n \n@@ -90,10 +85,8 @@ public void testDelegateUnsupportedProcedure() {\n   public void testCallWithBackticks() throws ParseException {\n     CallStatement call =\n         (CallStatement) parser.parsePlan(\"CALL cat.`system`.`rollback_to_snapshot`()\");\n-    Assert.assertEquals(\n-        ImmutableList.of(\"cat\", \"system\", \"rollback_to_snapshot\"),\n-        JavaConverters.seqAsJavaList(call.name()));\n-    Assert.assertEquals(0, call.args().size());\n+    assertThat(seqAsJavaList(call.name())).containsExactly(\"cat\", \"system\", \"rollback_to_snapshot\");\n+    assertThat(seqAsJavaList(call.args())).isEmpty();\n   }\n \n   @Test\n@@ -102,11 +95,9 @@ public void testCallWithPositionalArgs() throws ParseException {\n         (CallStatement)\n             parser.parsePlan(\n                 \"CALL c.system.rollback_to_snapshot(1, '2', 3L, true, 1.0D, 9.0e1, 900e-1BD)\");\n-    Assert.assertEquals(\n-        ImmutableList.of(\"c\", \"system\", \"rollback_to_snapshot\"),\n-        JavaConverters.seqAsJavaList(call.name()));\n+    assertThat(seqAsJavaList(call.name())).containsExactly(\"c\", \"system\", \"rollback_to_snapshot\");\n \n-    Assert.assertEquals(7, call.args().size());\n+    assertThat(seqAsJavaList(call.args())).hasSize(7);\n \n     checkArg(call, 0, 1, DataTypes.IntegerType);\n     checkArg(call, 1, \"2\", DataTypes.StringType);\n@@ -123,11 +114,9 @@ public void testCallWithNamedArgs() throws ParseException {\n         (CallStatement)\n             parser.parsePlan(\n                 \"CALL cat.system.rollback_to_snapshot(c1 => 1, c2 => '2', c3 => true)\");\n-    Assert.assertEquals(\n-        ImmutableList.of(\"cat\", \"system\", \"rollback_to_snapshot\"),\n-        JavaConverters.seqAsJavaList(call.name()));\n+    assertThat(seqAsJavaList(call.name())).containsExactly(\"cat\", \"system\", \"rollback_to_snapshot\");\n \n-    Assert.assertEquals(3, call.args().size());\n+    assertThat(seqAsJavaList(call.args())).hasSize(3);\n \n     checkArg(call, 0, \"c1\", 1, DataTypes.IntegerType);\n     checkArg(call, 1, \"c2\", \"2\", DataTypes.StringType);\n@@ -138,11 +127,9 @@ public void testCallWithNamedArgs() throws ParseException {\n   public void testCallWithMixedArgs() throws ParseException {\n     CallStatement call =\n         (CallStatement) parser.parsePlan(\"CALL cat.system.rollback_to_snapshot(c1 => 1, '2')\");\n-    Assert.assertEquals(\n-        ImmutableList.of(\"cat\", \"system\", \"rollback_to_snapshot\"),\n-        JavaConverters.seqAsJavaList(call.name()));\n+    assertThat(seqAsJavaList(call.name())).containsExactly(\"cat\", \"system\", \"rollback_to_snapshot\");\n \n-    Assert.assertEquals(2, call.args().size());\n+    assertThat(seqAsJavaList(call.args())).hasSize(2);\n \n     checkArg(call, 0, \"c1\", 1, DataTypes.IntegerType);\n     checkArg(call, 1, \"2\", DataTypes.StringType);\n@@ -154,11 +141,9 @@ public void testCallWithTimestampArg() throws ParseException {\n         (CallStatement)\n             parser.parsePlan(\n                 \"CALL cat.system.rollback_to_snapshot(TIMESTAMP '2017-02-03T10:37:30.00Z')\");\n-    Assert.assertEquals(\n-        ImmutableList.of(\"cat\", \"system\", \"rollback_to_snapshot\"),\n-        JavaConverters.seqAsJavaList(call.name()));\n+    assertThat(seqAsJavaList(call.name())).containsExactly(\"cat\", \"system\", \"rollback_to_snapshot\");\n \n-    Assert.assertEquals(1, call.args().size());\n+    assertThat(seqAsJavaList(call.args())).hasSize(1);\n \n     checkArg(\n         call, 0, Timestamp.from(Instant.parse(\"2017-02-03T10:37:30.00Z\")), DataTypes.TimestampType);\n@@ -169,11 +154,9 @@ public void testCallWithVarSubstitution() throws ParseException {\n     CallStatement call =\n         (CallStatement)\n             parser.parsePlan(\"CALL cat.system.rollback_to_snapshot('${spark.extra.prop}')\");\n-    Assert.assertEquals(\n-        ImmutableList.of(\"cat\", \"system\", \"rollback_to_snapshot\"),\n-        JavaConverters.seqAsJavaList(call.name()));\n+    assertThat(seqAsJavaList(call.name())).containsExactly(\"cat\", \"system\", \"rollback_to_snapshot\");\n \n-    Assert.assertEquals(1, call.args().size());\n+    assertThat(seqAsJavaList(call.args())).hasSize(1);\n \n     checkArg(call, 0, \"value\", DataTypes.StringType);\n   }\n@@ -202,11 +185,8 @@ public void testCallStripsComments() throws ParseException {\n                 + \"cat.system.rollback_to_snapshot('${spark.extra.prop}')\");\n     for (String sqlText : callStatementsWithComments) {\n       CallStatement call = (CallStatement) parser.parsePlan(sqlText);\n-      Assert.assertEquals(\n-          ImmutableList.of(\"cat\", \"system\", \"rollback_to_snapshot\"),\n-          JavaConverters.seqAsJavaList(call.name()));\n-\n-      Assert.assertEquals(1, call.args().size());\n+      assertThat(seqAsJavaList(call.name()))\n+          .containsExactly(\"cat\", \"system\", \"rollback_to_snapshot\");\n \n       checkArg(call, 0, \"value\", DataTypes.StringType);\n     }\n@@ -226,7 +206,7 @@ private void checkArg(\n \n     if (expectedName != null) {\n       NamedArgument arg = checkCast(call.args().apply(index), NamedArgument.class);\n-      Assert.assertEquals(expectedName, arg.name());\n+      assertThat(arg.name()).isEqualTo(expectedName);\n     } else {\n       CallArgument arg = call.args().apply(index);\n       checkCast(arg, PositionalArgument.class);\n@@ -234,8 +214,8 @@ private void checkArg(\n \n     Expression expectedExpr = toSparkLiteral(expectedValue, expectedType);\n     Expression actualExpr = call.args().apply(index).expr();\n-    Assert.assertEquals(\"Arg types must match\", expectedExpr.dataType(), actualExpr.dataType());\n-    Assert.assertEquals(\"Arg must match\", expectedExpr, actualExpr);\n+    assertThat(actualExpr.dataType()).as(\"Arg types must match\").isEqualTo(expectedExpr.dataType());\n+    assertThat(actualExpr).as(\"Arg must match\").isEqualTo(expectedExpr);\n   }\n \n   private Literal toSparkLiteral(Object value, DataType dataType) {\n@@ -243,8 +223,7 @@ private Literal toSparkLiteral(Object value, DataType dataType) {\n   }\n \n   private <T> T checkCast(Object value, Class<T> expectedClass) {\n-    Assert.assertTrue(\n-        \"Expected instance of \" + expectedClass.getName(), expectedClass.isInstance(value));\n+    assertThat(value).isInstanceOf(expectedClass);\n     return expectedClass.cast(value);\n   }\n }\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestChangelogTable.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestChangelogTable.java\nindex 01523180b01f..99606b1f6363 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestChangelogTable.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestChangelogTable.java\n@@ -21,59 +21,56 @@\n import static org.apache.iceberg.TableProperties.FORMAT_VERSION;\n import static org.apache.iceberg.TableProperties.MANIFEST_MERGE_ENABLED;\n import static org.apache.iceberg.TableProperties.MANIFEST_MIN_MERGE_COUNT;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n-import java.util.Map;\n import org.apache.iceberg.DataOperations;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.spark.SparkCatalogConfig;\n import org.apache.iceberg.spark.SparkReadOptions;\n import org.apache.iceberg.spark.source.SparkChangelogTable;\n import org.apache.spark.sql.DataFrameReader;\n import org.apache.spark.sql.Row;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n-import org.junit.runners.Parameterized.Parameters;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestChangelogTable extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestChangelogTable extends ExtensionsTestBase {\n \n-  @Parameters(name = \"formatVersion = {0}, catalogName = {1}, implementation = {2}, config = {3}\")\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}, formatVersion = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n       {\n-        1,\n         SparkCatalogConfig.SPARK.catalogName(),\n         SparkCatalogConfig.SPARK.implementation(),\n-        SparkCatalogConfig.SPARK.properties()\n+        SparkCatalogConfig.SPARK.properties(),\n+        1\n       },\n       {\n-        2,\n         SparkCatalogConfig.HIVE.catalogName(),\n         SparkCatalogConfig.HIVE.implementation(),\n-        SparkCatalogConfig.HIVE.properties()\n+        SparkCatalogConfig.HIVE.properties(),\n+        2\n       }\n     };\n   }\n \n-  private final int formatVersion;\n+  @Parameter(index = 3)\n+  private int formatVersion;\n \n-  public TestChangelogTable(\n-      int formatVersion, String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-    this.formatVersion = formatVersion;\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDataFilters() {\n     createTableWithDefaultRows();\n \n@@ -97,7 +94,7 @@ public void testDataFilters() {\n         sql(\"SELECT * FROM %s.changes WHERE id = 3 ORDER BY _change_ordinal, id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOverwrites() {\n     createTableWithDefaultRows();\n \n@@ -119,7 +116,7 @@ public void testOverwrites() {\n         changelogRecords(snap2, snap3));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testQueryWithTimeRange() {\n     createTable();\n \n@@ -189,7 +186,7 @@ public void testQueryWithTimeRange() {\n         changelogRecords(rightAfterSnap2, snap3.timestampMillis() - 1));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimeRangeValidation() {\n     createTableWithDefaultRows();\n \n@@ -207,7 +204,7 @@ public void testTimeRangeValidation() {\n         .hasMessage(\"Cannot set start-timestamp to be greater than end-timestamp for changelogs\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMetadataDeletes() {\n     createTableWithDefaultRows();\n \n@@ -220,7 +217,7 @@ public void testMetadataDeletes() {\n     table.refresh();\n \n     Snapshot snap3 = table.currentSnapshot();\n-    Assert.assertEquals(\"Operation must match\", DataOperations.DELETE, snap3.operation());\n+    assertThat(snap3.operation()).as(\"Operation must match\").isEqualTo(DataOperations.DELETE);\n \n     assertEquals(\n         \"Rows should match\",\n@@ -228,7 +225,7 @@ public void testMetadataDeletes() {\n         changelogRecords(snap2, snap3));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExistingEntriesInNewDataManifestsAreIgnored() {\n     sql(\n         \"CREATE TABLE %s (id INT, data STRING) \"\n@@ -252,7 +249,7 @@ public void testExistingEntriesInNewDataManifestsAreIgnored() {\n     table.refresh();\n \n     Snapshot snap2 = table.currentSnapshot();\n-    Assert.assertEquals(\"Manifest number must match\", 1, snap2.dataManifests(table.io()).size());\n+    assertThat(snap2.dataManifests(table.io())).as(\"Manifest number must match\").hasSize(1);\n \n     assertEquals(\n         \"Rows should match\",\n@@ -260,14 +257,14 @@ public void testExistingEntriesInNewDataManifestsAreIgnored() {\n         changelogRecords(snap1, snap2));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testManifestRewritesAreIgnored() {\n     createTableWithDefaultRows();\n \n     sql(\"CALL %s.system.rewrite_manifests('%s')\", catalogName, tableIdent);\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Num snapshots must match\", 3, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Num snapshots must match\").hasSize(3);\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -275,7 +272,7 @@ public void testManifestRewritesAreIgnored() {\n         sql(\"SELECT id, _change_type FROM %s.changes ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMetadataColumns() {\n     createTableWithDefaultRows();\n     List<Object[]> rows =\n@@ -284,7 +281,7 @@ public void testMetadataColumns() {\n             tableName);\n \n     String file1 = rows.get(0)[1].toString();\n-    Assert.assertTrue(file1.startsWith(\"file:/\"));\n+    assertThat(file1).startsWith(\"file:/\");\n     String file2 = rows.get(1)[1].toString();\n \n     assertEquals(\n@@ -294,7 +291,7 @@ public void testMetadataColumns() {\n         rows);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testQueryWithRollback() {\n     createTable();\n \n@@ -312,7 +309,7 @@ public void testQueryWithRollback() {\n         \"CALL %s.system.rollback_to_snapshot('%s', %d)\",\n         catalogName, tableIdent, snap1.snapshotId());\n     table.refresh();\n-    Assert.assertEquals(\"Snapshot should match after rollback\", table.currentSnapshot(), snap1);\n+    assertThat(table.currentSnapshot()).isEqualTo(snap1);\n \n     sql(\"INSERT OVERWRITE %s VALUES (-2, 'a')\", tableName);\n     table.refresh();\n@@ -350,7 +347,7 @@ public void testQueryWithRollback() {\n         \"CALL %s.system.set_current_snapshot('%s', %d)\",\n         catalogName, tableIdent, snap2.snapshotId());\n     table.refresh();\n-    Assert.assertEquals(\"Snapshot should match after reset\", table.currentSnapshot(), snap2);\n+    assertThat(table.currentSnapshot()).isEqualTo(snap2);\n     assertEquals(\n         \"Should have expected changed rows from snapshot 2 only since snapshot 3 is on a different branch.\",\n         ImmutableList.of(row(2, \"b\", \"INSERT\", 0, snap2.snapshotId())),\n@@ -412,4 +409,46 @@ private List<Row> collect(DataFrameReader reader) {\n         .orderBy(\"_change_ordinal\", \"_commit_snapshot_id\", \"_change_type\", \"id\")\n         .collectAsList();\n   }\n+\n+  @TestTemplate\n+  public void testChangelogViewOutsideTimeRange() {\n+    createTableWithDefaultRows();\n+\n+    // Insert new records\n+    sql(\"INSERT INTO %s VALUES (3, 'c')\", tableName);\n+    sql(\"INSERT INTO %s VALUES (4, 'd')\", tableName);\n+\n+    // Small delay to ensure our timestamps are different\n+    try {\n+      Thread.sleep(100);\n+    } catch (InterruptedException e) {\n+      throw new RuntimeException(\"Test interrupted\", e);\n+    }\n+\n+    long startTime = System.currentTimeMillis();\n+    long endTime = startTime + 1000; // 1 second window\n+\n+    // Create changelog view for a time window after our inserts\n+    sql(\n+        \"CALL %s.system.create_changelog_view(\"\n+            + \"  table => '%s', \"\n+            + \"  options => map(\"\n+            + \"    'start-timestamp', '%d',\"\n+            + \"    'end-timestamp', '%d'\"\n+            + \"  ),\"\n+            + \"  changelog_view => 'test_changelog_view'\"\n+            + \")\",\n+        catalogName, tableName, startTime, endTime);\n+\n+    // Query the changelog view\n+    List<Object[]> results =\n+        sql(\n+            \"SELECT * FROM test_changelog_view WHERE _change_type IN ('INSERT', 'DELETE') ORDER BY _change_ordinal\");\n+\n+    // Verify no changes are returned since our window is after the inserts\n+    assertThat(results).as(\"Num records must be zero\").isEmpty();\n+\n+    // Clean up the changelog view\n+    sql(\"DROP VIEW IF EXISTS test_changelog_view\");\n+  }\n }\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\nindex 550a763a33e0..82e87c45f11d 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\n@@ -19,10 +19,11 @@\n package org.apache.iceberg.spark.extensions;\n \n import static org.apache.iceberg.TableProperties.WRITE_AUDIT_PUBLISH_ENABLED;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.exceptions.ValidationException;\n@@ -32,23 +33,19 @@\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestCherrypickSnapshotProcedure extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestCherrypickSnapshotProcedure extends ExtensionsTestBase {\n \n-  public TestCherrypickSnapshotProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCherrypickSnapshotUsingPositionalArgs() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"ALTER TABLE %s SET TBLPROPERTIES ('%s' 'true')\", tableName, WRITE_AUDIT_PUBLISH_ENABLED);\n@@ -85,7 +82,7 @@ public void testCherrypickSnapshotUsingPositionalArgs() {\n         sql(\"SELECT * FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCherrypickSnapshotUsingNamedArgs() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"ALTER TABLE %s SET TBLPROPERTIES ('%s' 'true')\", tableName, WRITE_AUDIT_PUBLISH_ENABLED);\n@@ -122,7 +119,7 @@ public void testCherrypickSnapshotUsingNamedArgs() {\n         sql(\"SELECT * FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCherrypickSnapshotRefreshesRelationCache() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"ALTER TABLE %s SET TBLPROPERTIES ('%s' 'true')\", tableName, WRITE_AUDIT_PUBLISH_ENABLED);\n@@ -158,7 +155,7 @@ public void testCherrypickSnapshotRefreshesRelationCache() {\n     sql(\"UNCACHE TABLE tmp\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCherrypickInvalidSnapshot() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -168,7 +165,7 @@ public void testCherrypickInvalidSnapshot() {\n         .hasMessage(\"Cannot cherry-pick unknown snapshot ID: -1\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidCherrypickSnapshotCases() {\n     assertThatThrownBy(\n             () -> sql(\"CALL %s.system.cherrypick_snapshot('n', table => 't', 1L)\", catalogName))\n@@ -181,8 +178,8 @@ public void testInvalidCherrypickSnapshotCases() {\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n-              Assert.assertEquals(\"PARSE_SYNTAX_ERROR\", parseException.getErrorClass());\n-              Assert.assertEquals(\"'CALL'\", parseException.getMessageParameters().get(\"error\"));\n+              assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n+              assertThat(parseException.getMessageParameters()).containsEntry(\"error\", \"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.cherrypick_snapshot('t')\", catalogName))\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestConflictValidation.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestConflictValidation.java\nindex b4366d93c73b..b5ba7eec1b01 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestConflictValidation.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestConflictValidation.java\n@@ -21,8 +21,8 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n-import java.util.Map;\n import org.apache.iceberg.IsolationLevel;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n@@ -31,18 +31,15 @@\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.functions;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestConflictValidation extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestConflictValidation extends ExtensionsTestBase {\n \n-  public TestConflictValidation(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Before\n+  @BeforeEach\n   public void createTables() {\n     sql(\n         \"CREATE TABLE %s (id int, data string) USING iceberg \"\n@@ -54,12 +51,12 @@ public void createTables() {\n     sql(\"INSERT INTO %s VALUES (1, 'a'), (2, 'b'), (3, 'c')\", tableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOverwriteFilterSerializableIsolation() throws Exception {\n     Table table = validationCatalog.loadTable(tableIdent);\n     long snapshotId = table.currentSnapshot().snapshotId();\n@@ -91,7 +88,7 @@ public void testOverwriteFilterSerializableIsolation() throws Exception {\n         .overwrite(functions.col(\"id\").equalTo(1));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOverwriteFilterSerializableIsolation2() throws Exception {\n     List<SimpleRecord> records =\n         Lists.newArrayList(new SimpleRecord(1, \"a\"), new SimpleRecord(1, \"b\"));\n@@ -128,7 +125,7 @@ public void testOverwriteFilterSerializableIsolation2() throws Exception {\n         .overwrite(functions.col(\"id\").equalTo(1));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOverwriteFilterSerializableIsolation3() throws Exception {\n     Table table = validationCatalog.loadTable(tableIdent);\n     long snapshotId = table.currentSnapshot().snapshotId();\n@@ -162,7 +159,7 @@ public void testOverwriteFilterSerializableIsolation3() throws Exception {\n         .overwrite(functions.col(\"id\").equalTo(1));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOverwriteFilterNoSnapshotIdValidation() throws Exception {\n     Table table = validationCatalog.loadTable(tableIdent);\n \n@@ -193,7 +190,7 @@ public void testOverwriteFilterNoSnapshotIdValidation() throws Exception {\n         .overwrite(functions.col(\"id\").equalTo(1));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOverwriteFilterSnapshotIsolation() throws Exception {\n     List<SimpleRecord> records =\n         Lists.newArrayList(new SimpleRecord(1, \"a\"), new SimpleRecord(1, \"b\"));\n@@ -230,7 +227,7 @@ public void testOverwriteFilterSnapshotIsolation() throws Exception {\n         .overwrite(functions.col(\"id\").equalTo(1));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOverwriteFilterSnapshotIsolation2() throws Exception {\n     Table table = validationCatalog.loadTable(tableIdent);\n     long snapshotId = table.currentSnapshot().snapshotId();\n@@ -247,7 +244,7 @@ public void testOverwriteFilterSnapshotIsolation2() throws Exception {\n         .overwrite(functions.col(\"id\").equalTo(1));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOverwritePartitionSerializableIsolation() throws Exception {\n     Table table = validationCatalog.loadTable(tableIdent);\n     final long snapshotId = table.currentSnapshot().snapshotId();\n@@ -279,7 +276,7 @@ public void testOverwritePartitionSerializableIsolation() throws Exception {\n         .overwritePartitions();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOverwritePartitionSnapshotIsolation() throws Exception {\n     List<SimpleRecord> records =\n         Lists.newArrayList(new SimpleRecord(1, \"a\"), new SimpleRecord(1, \"b\"));\n@@ -314,7 +311,7 @@ public void testOverwritePartitionSnapshotIsolation() throws Exception {\n         .overwritePartitions();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOverwritePartitionSnapshotIsolation2() throws Exception {\n     Table table = validationCatalog.loadTable(tableIdent);\n     final long snapshotId = table.currentSnapshot().snapshotId();\n@@ -348,7 +345,7 @@ public void testOverwritePartitionSnapshotIsolation2() throws Exception {\n         .overwritePartitions();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOverwritePartitionSnapshotIsolation3() throws Exception {\n     Table table = validationCatalog.loadTable(tableIdent);\n     final long snapshotId = table.currentSnapshot().snapshotId();\n@@ -365,7 +362,7 @@ public void testOverwritePartitionSnapshotIsolation3() throws Exception {\n         .overwritePartitions();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOverwritePartitionNoSnapshotIdValidation() throws Exception {\n     Table table = validationCatalog.loadTable(tableIdent);\n \n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java\nindex 3eecc567ea37..4d061d764e87 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java\n@@ -18,34 +18,31 @@\n  */\n package org.apache.iceberg.spark.extensions;\n \n-import static org.junit.Assert.assertThrows;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.Arrays;\n import java.util.List;\n-import java.util.Map;\n import java.util.stream.Collectors;\n import org.apache.iceberg.ChangelogOperation;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.spark.SparkReadOptions;\n import org.apache.spark.sql.types.StructField;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestCreateChangelogViewProcedure extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestCreateChangelogViewProcedure extends ExtensionsTestBase {\n   private static final String DELETE = ChangelogOperation.DELETE.name();\n   private static final String INSERT = ChangelogOperation.INSERT.name();\n   private static final String UPDATE_BEFORE = ChangelogOperation.UPDATE_BEFORE.name();\n   private static final String UPDATE_AFTER = ChangelogOperation.UPDATE_AFTER.name();\n \n-  public TestCreateChangelogViewProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n@@ -65,7 +62,7 @@ private void createTableWithIdentifierField() {\n     sql(\"ALTER TABLE %s SET IDENTIFIER FIELDS id\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCustomizedViewName() {\n     createTableWithTwoColumns();\n     sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n@@ -95,10 +92,10 @@ public void testCustomizedViewName() {\n         \"cdc_view\");\n \n     long rowCount = sql(\"select * from %s\", \"cdc_view\").stream().count();\n-    Assert.assertEquals(2, rowCount);\n+    assertThat(rowCount).isEqualTo(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNonStandardColumnNames() {\n     sql(\"CREATE TABLE %s (`the id` INT, `the.data` STRING) USING iceberg\", tableName);\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD `the.data`\", tableName);\n@@ -133,14 +130,14 @@ public void testNonStandardColumnNames() {\n     var fieldNames =\n         Arrays.stream(df.schema().fields()).map(StructField::name).collect(Collectors.toList());\n \n-    Assert.assertEquals(\n-        \"Result Schema should match\",\n-        List.of(\"the id\", \"the.data\", \"_change_type\", \"_change_ordinal\", \"_commit_snapshot_id\"),\n-        fieldNames);\n-    Assert.assertEquals(\"Result Row Count should match\", 2, df.collectAsList().size());\n+    assertThat(fieldNames)\n+        .containsExactly(\n+            \"the id\", \"the.data\", \"_change_type\", \"_change_ordinal\", \"_commit_snapshot_id\");\n+\n+    assertThat(df.collectAsList()).hasSize(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoSnapshotIdInput() {\n     createTableWithTwoColumns();\n     sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n@@ -171,7 +168,7 @@ public void testNoSnapshotIdInput() {\n         sql(\"select * from %s order by _change_ordinal, id\", viewName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOnlyStartSnapshotIdInput() {\n     createTableWithTwoColumns();\n     sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n@@ -200,7 +197,7 @@ public void testOnlyStartSnapshotIdInput() {\n         sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOnlyEndTimestampIdInput() {\n     createTableWithTwoColumns();\n     sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n@@ -225,7 +222,7 @@ public void testOnlyEndTimestampIdInput() {\n         sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimestampsBasedQuery() {\n     createTableWithTwoColumns();\n     long beginning = System.currentTimeMillis();\n@@ -285,7 +282,7 @@ public void testTimestampsBasedQuery() {\n         sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOnlyStartTimestampInput() {\n     createTableWithTwoColumns();\n     long beginning = System.currentTimeMillis();\n@@ -331,7 +328,7 @@ public void testOnlyStartTimestampInput() {\n         sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOnlyEndTimestampInput() {\n     createTableWithTwoColumns();\n \n@@ -358,7 +355,7 @@ public void testOnlyEndTimestampInput() {\n         sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testStartTimeStampEndSnapshotId() {\n     createTableWithTwoColumns();\n \n@@ -395,7 +392,7 @@ public void testStartTimeStampEndSnapshotId() {\n         sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testStartSnapshotIdEndTimestamp() {\n     createTableWithTwoColumns();\n \n@@ -428,7 +425,7 @@ public void testStartSnapshotIdEndTimestamp() {\n         sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdate() {\n     createTableWithTwoColumns();\n     sql(\"ALTER TABLE %s DROP PARTITION FIELD data\", tableName);\n@@ -459,7 +456,7 @@ public void testUpdate() {\n         sql(\"select * from %s order by _change_ordinal, id, data\", viewName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithIdentifierField() {\n     createTableWithIdentifierField();\n \n@@ -487,7 +484,7 @@ public void testUpdateWithIdentifierField() {\n         sql(\"select * from %s order by _change_ordinal, id, data\", viewName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithFilter() {\n     createTableWithTwoColumns();\n     sql(\"ALTER TABLE %s DROP PARTITION FIELD data\", tableName);\n@@ -519,7 +516,7 @@ public void testUpdateWithFilter() {\n         sql(\"select * from %s where id != 3 order by _change_ordinal, id, data\", viewName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithMultipleIdentifierColumns() {\n     createTableWithThreeColumns();\n \n@@ -551,7 +548,7 @@ public void testUpdateWithMultipleIdentifierColumns() {\n         sql(\"select * from %s order by _change_ordinal, id, data\", viewName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveCarryOvers() {\n     createTableWithThreeColumns();\n \n@@ -585,7 +582,7 @@ public void testRemoveCarryOvers() {\n         sql(\"select * from %s order by _change_ordinal, id, data\", viewName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveCarryOversWithoutUpdatedRows() {\n     createTableWithThreeColumns();\n \n@@ -617,7 +614,7 @@ public void testRemoveCarryOversWithoutUpdatedRows() {\n         sql(\"select * from %s order by _change_ordinal, id, data\", viewName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNetChangesWithRemoveCarryOvers() {\n     // partitioned by id\n     createTableWithThreeColumns();\n@@ -670,15 +667,15 @@ public void testNetChangesWithRemoveCarryOvers() {\n         sql(\"select * from %s order by _change_ordinal, data\", viewName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNetChangesWithComputeUpdates() {\n     createTableWithTwoColumns();\n-    assertThrows(\n-        \"Should fail because net_changes is not supported with computing updates\",\n-        IllegalArgumentException.class,\n-        () ->\n-            sql(\n-                \"CALL %s.system.create_changelog_view(table => '%s', identifier_columns => array('id'), net_changes => true)\",\n-                catalogName, tableName));\n+    assertThatThrownBy(\n+            () ->\n+                sql(\n+                    \"CALL %s.system.create_changelog_view(table => '%s', identifier_columns => array('id'), net_changes => true)\",\n+                    catalogName, tableName))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"Not support net changes with update images\");\n   }\n }\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMetaColumnProjectionWithStageScan.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMetaColumnProjectionWithStageScan.java\nindex 17ed003f6a67..b783a006ef73 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMetaColumnProjectionWithStageScan.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMetaColumnProjectionWithStageScan.java\n@@ -21,8 +21,9 @@\n import static org.assertj.core.api.Assertions.assertThat;\n \n import java.util.List;\n-import java.util.Map;\n import java.util.UUID;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.ScanTask;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.io.CloseableIterable;\n@@ -36,18 +37,14 @@\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Encoders;\n import org.apache.spark.sql.Row;\n-import org.junit.After;\n-import org.junit.Test;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestMetaColumnProjectionWithStageScan extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestMetaColumnProjectionWithStageScan extends ExtensionsTestBase {\n \n-  public TestMetaColumnProjectionWithStageScan(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Parameterized.Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n       {\n@@ -58,7 +55,7 @@ public static Object[][] parameters() {\n     };\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n@@ -69,7 +66,7 @@ private <T extends ScanTask> void stageTask(\n     taskSetManager.stageTasks(tab, fileSetID, Lists.newArrayList(tasks));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReadStageTableMeta() throws Exception {\n     sql(\n         \"CREATE TABLE %s (id bigint, data string) USING iceberg TBLPROPERTIES\"\n@@ -104,7 +101,7 @@ public void testReadStageTableMeta() throws Exception {\n               .option(SparkReadOptions.SCAN_TASK_SET_ID, fileSetID)\n               .load(tableLocation);\n \n-      assertThat(scanDF2.columns().length).isEqualTo(2);\n+      assertThat(scanDF2.columns()).hasSize(2);\n     }\n \n     try (CloseableIterable<ScanTask> tasks = table.newBatchScan().planFiles()) {\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRequiredDistributionAndOrdering.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRequiredDistributionAndOrdering.java\nindex 6e5671bb1870..fe1c38482bed 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRequiredDistributionAndOrdering.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRequiredDistributionAndOrdering.java\n@@ -22,29 +22,26 @@\n \n import java.math.BigDecimal;\n import java.util.List;\n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.spark.SparkWriteOptions;\n import org.apache.iceberg.spark.source.ThreeColumnRecord;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n-import org.junit.After;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestRequiredDistributionAndOrdering extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRequiredDistributionAndOrdering extends ExtensionsTestBase {\n \n-  public TestRequiredDistributionAndOrdering(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void dropTestTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultLocalSortWithBucketTransforms() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 STRING, c3 STRING) \"\n@@ -73,7 +70,7 @@ public void testDefaultLocalSortWithBucketTransforms() throws NoSuchTableExcepti\n         sql(\"SELECT count(*) FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionColumnsArePrependedForRangeDistribution() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 STRING, c3 STRING) \"\n@@ -104,7 +101,7 @@ public void testPartitionColumnsArePrependedForRangeDistribution() throws NoSuch\n         sql(\"SELECT count(*) FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortOrderIncludesPartitionColumns() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 STRING, c3 STRING) \"\n@@ -135,7 +132,7 @@ public void testSortOrderIncludesPartitionColumns() throws NoSuchTableException\n         sql(\"SELECT count(*) FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHashDistributionOnBucketedColumn() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 STRING, c3 STRING) \"\n@@ -166,7 +163,7 @@ public void testHashDistributionOnBucketedColumn() throws NoSuchTableException {\n         sql(\"SELECT count(*) FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDisabledDistributionAndOrdering() {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 STRING, c3 STRING) \"\n@@ -201,7 +198,7 @@ public void testDisabledDistributionAndOrdering() {\n                 + \"and by partition within each spec. Either cluster the incoming records or switch to fanout writers.\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultSortOnDecimalBucketedColumn() {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 DECIMAL(20, 2)) \"\n@@ -220,7 +217,7 @@ public void testDefaultSortOnDecimalBucketedColumn() {\n     assertEquals(\"Rows must match\", expected, sql(\"SELECT * FROM %s ORDER BY c1\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultSortOnStringBucketedColumn() {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 STRING) \"\n@@ -235,7 +232,7 @@ public void testDefaultSortOnStringBucketedColumn() {\n     assertEquals(\"Rows must match\", expected, sql(\"SELECT * FROM %s ORDER BY c1\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultSortOnBinaryBucketedColumn() {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 Binary) \"\n@@ -252,7 +249,7 @@ public void testDefaultSortOnBinaryBucketedColumn() {\n     assertEquals(\"Rows must match\", expected, sql(\"SELECT * FROM %s ORDER BY c1\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultSortOnDecimalTruncatedColumn() {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 DECIMAL(20, 2)) \"\n@@ -268,7 +265,7 @@ public void testDefaultSortOnDecimalTruncatedColumn() {\n     assertEquals(\"Rows must match\", expected, sql(\"SELECT * FROM %s ORDER BY c1\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultSortOnLongTruncatedColumn() {\n     sql(\n         \"CREATE TABLE %s (c1 INT, c2 BIGINT) \"\n@@ -283,7 +280,7 @@ public void testDefaultSortOnLongTruncatedColumn() {\n     assertEquals(\"Rows must match\", expected, sql(\"SELECT * FROM %s ORDER BY c1\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRangeDistributionWithQuotedColumnNames() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (`c.1` INT, c2 STRING, c3 STRING) \"\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\nindex 6bae280a134a..5d56abacab9c 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n@@ -18,10 +18,12 @@\n  */\n package org.apache.iceberg.spark.extensions;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.util.List;\n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.exceptions.ValidationException;\n@@ -30,24 +32,19 @@\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestRollbackToSnapshotProcedure extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRollbackToSnapshotProcedure extends ExtensionsTestBase {\n \n-  public TestRollbackToSnapshotProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRollbackToSnapshotUsingPositionalArgs() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -82,7 +79,7 @@ public void testRollbackToSnapshotUsingPositionalArgs() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRollbackToSnapshotUsingNamedArgs() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -117,7 +114,7 @@ public void testRollbackToSnapshotUsingNamedArgs() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRollbackToSnapshotRefreshesRelationCache() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -157,7 +154,7 @@ public void testRollbackToSnapshotRefreshesRelationCache() {\n     sql(\"UNCACHE TABLE tmp\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRollbackToSnapshotWithQuotedIdentifiers() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -202,9 +199,9 @@ public void testRollbackToSnapshotWithQuotedIdentifiers() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRollbackToSnapshotWithoutExplicitCatalog() {\n-    Assume.assumeTrue(\"Working only with the session catalog\", \"spark_catalog\".equals(catalogName));\n+    assumeThat(catalogName).as(\"Working only with the session catalog\").isEqualTo(\"spark_catalog\");\n \n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -238,7 +235,7 @@ public void testRollbackToSnapshotWithoutExplicitCatalog() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRollbackToInvalidSnapshot() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -248,7 +245,7 @@ public void testRollbackToInvalidSnapshot() {\n         .hasMessage(\"Cannot roll back to unknown snapshot id: -1\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidRollbackToSnapshotCases() {\n     assertThatThrownBy(\n             () ->\n@@ -264,8 +261,8 @@ public void testInvalidRollbackToSnapshotCases() {\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n-              Assert.assertEquals(\"PARSE_SYNTAX_ERROR\", parseException.getErrorClass());\n-              Assert.assertEquals(\"'CALL'\", parseException.getMessageParameters().get(\"error\"));\n+              assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n+              assertThat(parseException.getMessageParameters()).containsEntry(\"error\", \"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.rollback_to_snapshot('t')\", catalogName))\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\nindex b624eb2534f7..528c873ffa35 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\n@@ -18,13 +18,15 @@\n  */\n package org.apache.iceberg.spark.extensions;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.sql.Timestamp;\n import java.time.Instant;\n import java.time.LocalDateTime;\n import java.util.List;\n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n@@ -32,24 +34,19 @@\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestRollbackToTimestampProcedure extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRollbackToTimestampProcedure extends ExtensionsTestBase {\n \n-  public TestRollbackToTimestampProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRollbackToTimestampUsingPositionalArgs() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -87,7 +84,7 @@ public void testRollbackToTimestampUsingPositionalArgs() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRollbackToTimestampUsingNamedArgs() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -125,7 +122,7 @@ public void testRollbackToTimestampUsingNamedArgs() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRollbackToTimestampRefreshesRelationCache() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -168,7 +165,7 @@ public void testRollbackToTimestampRefreshesRelationCache() {\n     sql(\"UNCACHE TABLE tmp\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRollbackToTimestampWithQuotedIdentifiers() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -214,9 +211,9 @@ public void testRollbackToTimestampWithQuotedIdentifiers() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRollbackToTimestampWithoutExplicitCatalog() {\n-    Assume.assumeTrue(\"Working only with the session catalog\", \"spark_catalog\".equals(catalogName));\n+    assumeThat(catalogName).as(\"Working only with the session catalog\").isEqualTo(\"spark_catalog\");\n \n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -255,7 +252,7 @@ public void testRollbackToTimestampWithoutExplicitCatalog() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRollbackToTimestampBeforeOrEqualToOldestSnapshot() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -288,7 +285,7 @@ public void testRollbackToTimestampBeforeOrEqualToOldestSnapshot() {\n             exactFirstSnapshot.toInstant().toEpochMilli());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidRollbackToTimestampCases() {\n     String timestamp = \"TIMESTAMP '2007-12-03T10:15:30'\";\n \n@@ -307,8 +304,8 @@ public void testInvalidRollbackToTimestampCases() {\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n-              Assert.assertEquals(\"PARSE_SYNTAX_ERROR\", parseException.getErrorClass());\n-              Assert.assertEquals(\"'CALL'\", parseException.getMessageParameters().get(\"error\"));\n+              assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n+              assertThat(parseException.getMessageParameters()).containsEntry(\"error\", \"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.rollback_to_timestamp('t')\", catalogName))\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\nindex e593d1262f63..0b78d914c34f 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\n@@ -19,10 +19,12 @@\n package org.apache.iceberg.spark.extensions;\n \n import static org.apache.iceberg.TableProperties.WRITE_AUDIT_PUBLISH_ENABLED;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.util.List;\n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.exceptions.ValidationException;\n@@ -30,24 +32,19 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.spark.sql.AnalysisException;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSetCurrentSnapshotProcedure extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSetCurrentSnapshotProcedure extends ExtensionsTestBase {\n \n-  public TestSetCurrentSnapshotProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetCurrentSnapshotUsingPositionalArgs() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -82,7 +79,7 @@ public void testSetCurrentSnapshotUsingPositionalArgs() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetCurrentSnapshotUsingNamedArgs() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -117,7 +114,7 @@ public void testSetCurrentSnapshotUsingNamedArgs() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetCurrentSnapshotWap() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"ALTER TABLE %s SET TBLPROPERTIES ('%s' 'true')\", tableName, WRITE_AUDIT_PUBLISH_ENABLED);\n@@ -150,9 +147,9 @@ public void testSetCurrentSnapshotWap() {\n         sql(\"SELECT * FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void tesSetCurrentSnapshotWithoutExplicitCatalog() {\n-    Assume.assumeTrue(\"Working only with the session catalog\", \"spark_catalog\".equals(catalogName));\n+    assumeThat(catalogName).as(\"Working only with the session catalog\").isEqualTo(\"spark_catalog\");\n \n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -186,7 +183,7 @@ public void tesSetCurrentSnapshotWithoutExplicitCatalog() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetCurrentSnapshotToInvalidSnapshot() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -196,7 +193,7 @@ public void testSetCurrentSnapshotToInvalidSnapshot() {\n         .hasMessage(\"Cannot roll back to unknown snapshot id: -1\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidRollbackToSnapshotCases() {\n     assertThatThrownBy(\n             () ->\n@@ -212,8 +209,8 @@ public void testInvalidRollbackToSnapshotCases() {\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n-              Assert.assertEquals(\"PARSE_SYNTAX_ERROR\", parseException.getErrorClass());\n-              Assert.assertEquals(\"'CALL'\", parseException.getMessageParameters().get(\"error\"));\n+              assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n+              assertThat(parseException.getMessageParameters()).containsEntry(\"error\", \"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.set_current_snapshot('t')\", catalogName))\n@@ -250,7 +247,7 @@ public void testInvalidRollbackToSnapshotCases() {\n         .hasMessage(\"Either snapshot_id or ref must be provided, not both\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetCurrentSnapshotToRef() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetWriteDistributionAndOrdering.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetWriteDistributionAndOrdering.java\nindex 4eaef55b693f..2bdeb197126a 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetWriteDistributionAndOrdering.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetWriteDistributionAndOrdering.java\n@@ -19,44 +19,42 @@\n package org.apache.iceberg.spark.extensions;\n \n import static org.apache.iceberg.expressions.Expressions.bucket;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n-import java.util.Map;\n import org.apache.iceberg.NullOrder;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.SortOrder;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.spark.sql.internal.SQLConf;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n-\n-public class TestSetWriteDistributionAndOrdering extends SparkExtensionsTestBase {\n-  public TestSetWriteDistributionAndOrdering(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSetWriteDistributionAndOrdering extends ExtensionsTestBase {\n \n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetWriteOrderByColumn() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, category string, ts timestamp, data string) USING iceberg\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unsorted\", table.sortOrder().isUnsorted());\n+    assertThat(table.sortOrder().isUnsorted()).as(\"Table should start unsorted\").isTrue();\n \n     sql(\"ALTER TABLE %s WRITE ORDERED BY category, id\", tableName);\n \n     table.refresh();\n \n     String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    Assert.assertEquals(\"Distribution mode must match\", \"range\", distributionMode);\n+    assertThat(distributionMode).as(\"Distribution mode must match\").isEqualTo(\"range\");\n \n     SortOrder expected =\n         SortOrder.builderFor(table.schema())\n@@ -64,16 +62,16 @@ public void testSetWriteOrderByColumn() {\n             .asc(\"category\", NullOrder.NULLS_FIRST)\n             .asc(\"id\", NullOrder.NULLS_FIRST)\n             .build();\n-    Assert.assertEquals(\"Should have expected order\", expected, table.sortOrder());\n+    assertThat(table.sortOrder()).as(\"Should have expected order\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetWriteOrderWithCaseSensitiveColumnNames() {\n     sql(\n         \"CREATE TABLE %s (Id bigint NOT NULL, Category string, ts timestamp, data string) USING iceberg\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unsorted\", table.sortOrder().isUnsorted());\n+    assertThat(table.sortOrder().isUnsorted()).as(\"Table should start unsorted\").isTrue();\n     sql(\"SET %s=true\", SQLConf.CASE_SENSITIVE().key());\n     assertThatThrownBy(\n             () -> {\n@@ -87,23 +85,22 @@ public void testSetWriteOrderWithCaseSensitiveColumnNames() {\n     table = validationCatalog.loadTable(tableIdent);\n     SortOrder expected =\n         SortOrder.builderFor(table.schema()).withOrderId(1).asc(\"Category\").asc(\"Id\").build();\n-    Assert.assertEquals(\"Should have expected order\", expected, table.sortOrder());\n+    assertThat(table.sortOrder()).as(\"Should have expected order\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetWriteOrderByColumnWithDirection() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, category string, ts timestamp, data string) USING iceberg\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unsorted\", table.sortOrder().isUnsorted());\n+    assertThat(table.sortOrder().isUnsorted()).as(\"Table should start unsorted\").isTrue();\n \n     sql(\"ALTER TABLE %s WRITE ORDERED BY category ASC, id DESC\", tableName);\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    Assert.assertEquals(\"Distribution mode must match\", \"range\", distributionMode);\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"range\");\n \n     SortOrder expected =\n         SortOrder.builderFor(table.schema())\n@@ -111,23 +108,22 @@ public void testSetWriteOrderByColumnWithDirection() {\n             .asc(\"category\", NullOrder.NULLS_FIRST)\n             .desc(\"id\", NullOrder.NULLS_LAST)\n             .build();\n-    Assert.assertEquals(\"Should have expected order\", expected, table.sortOrder());\n+    assertThat(table.sortOrder()).as(\"Should have expected order\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetWriteOrderByColumnWithDirectionAndNullOrder() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, category string, ts timestamp, data string) USING iceberg\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unsorted\", table.sortOrder().isUnsorted());\n+    assertThat(table.sortOrder().isUnsorted()).as(\"Table should start unsorted\").isTrue();\n \n     sql(\"ALTER TABLE %s WRITE ORDERED BY category ASC NULLS LAST, id DESC NULLS FIRST\", tableName);\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    Assert.assertEquals(\"Distribution mode must match\", \"range\", distributionMode);\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"range\");\n \n     SortOrder expected =\n         SortOrder.builderFor(table.schema())\n@@ -135,23 +131,22 @@ public void testSetWriteOrderByColumnWithDirectionAndNullOrder() {\n             .asc(\"category\", NullOrder.NULLS_LAST)\n             .desc(\"id\", NullOrder.NULLS_FIRST)\n             .build();\n-    Assert.assertEquals(\"Should have expected order\", expected, table.sortOrder());\n+    assertThat(table.sortOrder()).as(\"Should have expected order\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetWriteOrderByTransform() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, category string, ts timestamp, data string) USING iceberg\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unsorted\", table.sortOrder().isUnsorted());\n+    assertThat(table.sortOrder().isUnsorted()).isTrue();\n \n     sql(\"ALTER TABLE %s WRITE ORDERED BY category DESC, bucket(16, id), id\", tableName);\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    Assert.assertEquals(\"Distribution mode must match\", \"range\", distributionMode);\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"range\");\n \n     SortOrder expected =\n         SortOrder.builderFor(table.schema())\n@@ -160,50 +155,47 @@ public void testSetWriteOrderByTransform() {\n             .asc(bucket(\"id\", 16))\n             .asc(\"id\")\n             .build();\n-    Assert.assertEquals(\"Should have expected order\", expected, table.sortOrder());\n+    assertThat(table.sortOrder()).as(\"Should have expected order\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetWriteUnordered() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, category string, ts timestamp, data string) USING iceberg\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unsorted\", table.sortOrder().isUnsorted());\n+    assertThat(table.sortOrder().isUnsorted()).as(\"Table should start unsorted\").isTrue();\n \n     sql(\"ALTER TABLE %s WRITE ORDERED BY category DESC, bucket(16, id), id\", tableName);\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    Assert.assertEquals(\"Distribution mode must match\", \"range\", distributionMode);\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"range\");\n \n-    Assert.assertNotEquals(\"Table must be sorted\", SortOrder.unsorted(), table.sortOrder());\n+    assertThat(table.sortOrder()).as(\"Table must be sorted\").isNotEqualTo(SortOrder.unsorted());\n \n     sql(\"ALTER TABLE %s WRITE UNORDERED\", tableName);\n \n     table.refresh();\n \n-    String newDistributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    Assert.assertEquals(\"New distribution mode must match\", \"none\", newDistributionMode);\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"none\");\n \n-    Assert.assertEquals(\"New sort order must match\", SortOrder.unsorted(), table.sortOrder());\n+    assertThat(table.sortOrder()).as(\"New sort order must match\").isEqualTo(SortOrder.unsorted());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetWriteLocallyOrdered() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, category string, ts timestamp, data string) USING iceberg\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unsorted\", table.sortOrder().isUnsorted());\n+    assertThat(table.sortOrder().isUnsorted()).as(\"Table should start unsorted\").isTrue();\n \n     sql(\"ALTER TABLE %s WRITE LOCALLY ORDERED BY category DESC, bucket(16, id), id\", tableName);\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    Assert.assertEquals(\"Distribution mode must match\", \"none\", distributionMode);\n+    assertThat(table.properties()).doesNotContainKey(TableProperties.WRITE_DISTRIBUTION_MODE);\n \n     SortOrder expected =\n         SortOrder.builderFor(table.schema())\n@@ -212,117 +204,136 @@ public void testSetWriteLocallyOrdered() {\n             .asc(bucket(\"id\", 16))\n             .asc(\"id\")\n             .build();\n-    Assert.assertEquals(\"Sort order must match\", expected, table.sortOrder());\n+    assertThat(table.sortOrder()).as(\"Sort order must match\").isEqualTo(expected);\n+  }\n+\n+  @TestTemplate\n+  public void testSetWriteLocallyOrderedToPartitionedTable() {\n+    sql(\n+        \"CREATE TABLE %s (id bigint NOT NULL, category string) USING iceberg PARTITIONED BY (id)\",\n+        tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    assertThat(table.sortOrder().isUnsorted()).as(\"Table should start unsorted\").isTrue();\n+\n+    sql(\"ALTER TABLE %s WRITE LOCALLY ORDERED BY category DESC\", tableName);\n+\n+    table.refresh();\n+\n+    assertThat(table.properties()).doesNotContainKey(TableProperties.WRITE_DISTRIBUTION_MODE);\n+\n+    SortOrder expected =\n+        SortOrder.builderFor(table.schema()).withOrderId(1).desc(\"category\").build();\n+    assertThat(table.sortOrder()).as(\"Sort order must match\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetWriteDistributedByWithSort() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, category string) USING iceberg PARTITIONED BY (category)\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unsorted\", table.sortOrder().isUnsorted());\n+    assertThat(table.sortOrder().isUnsorted()).as(\"Table should start unsorted\").isTrue();\n \n     sql(\"ALTER TABLE %s WRITE DISTRIBUTED BY PARTITION ORDERED BY id\", tableName);\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    Assert.assertEquals(\"Distribution mode must match\", \"hash\", distributionMode);\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"hash\");\n \n     SortOrder expected = SortOrder.builderFor(table.schema()).withOrderId(1).asc(\"id\").build();\n-    Assert.assertEquals(\"Sort order must match\", expected, table.sortOrder());\n+    assertThat(table.sortOrder()).as(\"Sort order must match\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetWriteDistributedByWithLocalSort() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, category string) USING iceberg PARTITIONED BY (category)\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unsorted\", table.sortOrder().isUnsorted());\n+    assertThat(table.sortOrder().isUnsorted()).as(\"Table should start unsorted\").isTrue();\n \n     sql(\"ALTER TABLE %s WRITE DISTRIBUTED BY PARTITION LOCALLY ORDERED BY id\", tableName);\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    Assert.assertEquals(\"Distribution mode must match\", \"hash\", distributionMode);\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"hash\");\n \n     SortOrder expected = SortOrder.builderFor(table.schema()).withOrderId(1).asc(\"id\").build();\n-    Assert.assertEquals(\"Sort order must match\", expected, table.sortOrder());\n+    assertThat(table.sortOrder()).as(\"Sort order must match\").isEqualTo(expected);\n+\n+    sql(\"ALTER TABLE %s WRITE LOCALLY ORDERED BY id\", tableName);\n+\n+    table.refresh();\n+\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"hash\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetWriteDistributedByAndUnordered() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, category string) USING iceberg PARTITIONED BY (category)\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unsorted\", table.sortOrder().isUnsorted());\n+    assertThat(table.sortOrder().isUnsorted()).as(\"Table should start unsorted\").isTrue();\n \n     sql(\"ALTER TABLE %s WRITE DISTRIBUTED BY PARTITION UNORDERED\", tableName);\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    Assert.assertEquals(\"Distribution mode must match\", \"hash\", distributionMode);\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"hash\");\n \n-    Assert.assertEquals(\"Sort order must match\", SortOrder.unsorted(), table.sortOrder());\n+    assertThat(table.sortOrder()).as(\"Sort order must match\").isEqualTo(SortOrder.unsorted());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetWriteDistributedByOnly() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, category string) USING iceberg PARTITIONED BY (category)\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unsorted\", table.sortOrder().isUnsorted());\n+    assertThat(table.sortOrder().isUnsorted()).as(\"Table should start unsorted\").isTrue();\n \n     sql(\"ALTER TABLE %s WRITE DISTRIBUTED BY PARTITION UNORDERED\", tableName);\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    Assert.assertEquals(\"Distribution mode must match\", \"hash\", distributionMode);\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"hash\");\n \n-    Assert.assertEquals(\"Sort order must match\", SortOrder.unsorted(), table.sortOrder());\n+    assertThat(table.sortOrder()).as(\"Sort order must match\").isEqualTo(SortOrder.unsorted());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetWriteDistributedAndUnorderedInverted() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, category string) USING iceberg PARTITIONED BY (category)\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unsorted\", table.sortOrder().isUnsorted());\n+    assertThat(table.sortOrder().isUnsorted()).as(\"Table should start unsorted\").isTrue();\n \n     sql(\"ALTER TABLE %s WRITE UNORDERED DISTRIBUTED BY PARTITION\", tableName);\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    Assert.assertEquals(\"Distribution mode must match\", \"hash\", distributionMode);\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"hash\");\n \n-    Assert.assertEquals(\"Sort order must match\", SortOrder.unsorted(), table.sortOrder());\n+    assertThat(table.sortOrder()).as(\"Sort order must match\").isEqualTo(SortOrder.unsorted());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetWriteDistributedAndLocallyOrderedInverted() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, category string) USING iceberg PARTITIONED BY (category)\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unsorted\", table.sortOrder().isUnsorted());\n+    assertThat(table.sortOrder().isUnsorted()).as(\"Table should start unsorted\").isTrue();\n \n     sql(\"ALTER TABLE %s WRITE ORDERED BY id DISTRIBUTED BY PARTITION\", tableName);\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    Assert.assertEquals(\"Distribution mode must match\", \"hash\", distributionMode);\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"hash\");\n \n     SortOrder expected = SortOrder.builderFor(table.schema()).withOrderId(1).asc(\"id\").build();\n-    Assert.assertEquals(\"Sort order must match\", expected, table.sortOrder());\n+    assertThat(table.sortOrder()).as(\"Sort order must match\").isEqualTo(expected);\n   }\n }\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestStoragePartitionedJoinsInRowLevelOperations.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestStoragePartitionedJoinsInRowLevelOperations.java\nindex 7b22fffb0bfa..8f811a53c64c 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestStoragePartitionedJoinsInRowLevelOperations.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestStoragePartitionedJoinsInRowLevelOperations.java\n@@ -24,6 +24,8 @@\n \n import java.util.Map;\n import org.apache.commons.lang3.StringUtils;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n@@ -32,12 +34,12 @@\n import org.apache.iceberg.spark.SparkSQLProperties;\n import org.apache.spark.sql.execution.SparkPlan;\n import org.apache.spark.sql.internal.SQLConf;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestStoragePartitionedJoinsInRowLevelOperations extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestStoragePartitionedJoinsInRowLevelOperations extends ExtensionsTestBase {\n \n   private static final String OTHER_TABLE_NAME = \"other_table\";\n \n@@ -68,7 +70,7 @@ public class TestStoragePartitionedJoinsInRowLevelOperations extends SparkExtens\n           SparkSQLProperties.PRESERVE_DATA_GROUPING,\n           \"true\");\n \n-  @Parameterized.Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n       {\n@@ -79,23 +81,18 @@ public static Object[][] parameters() {\n     };\n   }\n \n-  public TestStoragePartitionedJoinsInRowLevelOperations(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS %s\", tableName(OTHER_TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteDeleteWithoutShuffles() {\n     checkDelete(COPY_ON_WRITE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadDeleteWithoutShuffles() {\n     checkDelete(MERGE_ON_READ);\n   }\n@@ -153,12 +150,12 @@ private void checkDelete(RowLevelOperationMode mode) {\n         sql(\"SELECT * FROM %s ORDER BY id, salary\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteUpdateWithoutShuffles() {\n     checkUpdate(COPY_ON_WRITE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadUpdateWithoutShuffles() {\n     checkUpdate(MERGE_ON_READ);\n   }\n@@ -217,22 +214,22 @@ private void checkUpdate(RowLevelOperationMode mode) {\n         sql(\"SELECT * FROM %s ORDER BY id, salary\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteMergeWithoutShuffles() {\n     checkMerge(COPY_ON_WRITE, false /* with ON predicate */);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteMergeWithoutShufflesWithPredicate() {\n     checkMerge(COPY_ON_WRITE, true /* with ON predicate */);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadMergeWithoutShuffles() {\n     checkMerge(MERGE_ON_READ, false /* with ON predicate */);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadMergeWithoutShufflesWithPredicate() {\n     checkMerge(MERGE_ON_READ, true /* with ON predicate */);\n   }\n@@ -284,7 +281,7 @@ private void checkMerge(RowLevelOperationMode mode, boolean withPredicate) {\n           String planAsString = plan.toString();\n           if (mode == COPY_ON_WRITE) {\n             int actualNumShuffles = StringUtils.countMatches(planAsString, \"Exchange\");\n-            Assert.assertEquals(\"Should be 1 shuffle with SPJ\", 1, actualNumShuffles);\n+            assertThat(actualNumShuffles).as(\"Should be 1 shuffle with SPJ\").isEqualTo(1);\n             assertThat(planAsString).contains(\"Exchange hashpartitioning(_file\");\n           } else {\n             assertThat(planAsString).doesNotContain(\"Exchange\");\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSystemFunctionPushDownDQL.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSystemFunctionPushDownDQL.java\nindex a63a3b65820a..f6102bab69b0 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSystemFunctionPushDownDQL.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSystemFunctionPushDownDQL.java\n@@ -40,7 +40,8 @@\n import static org.assertj.core.api.Assertions.assertThat;\n \n import java.util.List;\n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.expressions.ExpressionUtil;\n import org.apache.iceberg.spark.SparkCatalogConfig;\n import org.apache.iceberg.spark.source.PlanUtils;\n@@ -50,18 +51,15 @@\n import org.apache.spark.sql.catalyst.expressions.Expression;\n import org.apache.spark.sql.catalyst.expressions.objects.StaticInvoke;\n import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.Test;\n-import org.junit.runners.Parameterized;\n-\n-public class TestSystemFunctionPushDownDQL extends SparkExtensionsTestBase {\n-  public TestSystemFunctionPushDownDQL(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSystemFunctionPushDownDQL extends ExtensionsTestBase {\n \n-  @Parameterized.Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n       {\n@@ -72,23 +70,24 @@ public static Object[][] parameters() {\n     };\n   }\n \n-  @Before\n+  @BeforeEach\n   public void before() {\n+    super.before();\n     sql(\"USE %s\", catalogName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testYearsFunctionOnUnpartitionedTable() {\n     createUnpartitionedTable(spark, tableName);\n     testYearsFunction(false);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testYearsFunctionOnPartitionedTable() {\n     createPartitionedTable(spark, tableName, \"years(ts)\");\n     testYearsFunction(true);\n@@ -107,16 +106,16 @@ private void testYearsFunction(boolean partitioned) {\n     checkPushedFilters(optimizedPlan, equal(year(\"ts\"), targetYears));\n \n     List<Object[]> actual = rowsToJava(df.collectAsList());\n-    assertThat(actual.size()).isEqualTo(5);\n+    assertThat(actual).hasSize(5);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMonthsFunctionOnUnpartitionedTable() {\n     createUnpartitionedTable(spark, tableName);\n     testMonthsFunction(false);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMonthsFunctionOnPartitionedTable() {\n     createPartitionedTable(spark, tableName, \"months(ts)\");\n     testMonthsFunction(true);\n@@ -135,16 +134,16 @@ private void testMonthsFunction(boolean partitioned) {\n     checkPushedFilters(optimizedPlan, greaterThan(month(\"ts\"), targetMonths));\n \n     List<Object[]> actual = rowsToJava(df.collectAsList());\n-    assertThat(actual.size()).isEqualTo(5);\n+    assertThat(actual).hasSize(5);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDaysFunctionOnUnpartitionedTable() {\n     createUnpartitionedTable(spark, tableName);\n     testDaysFunction(false);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDaysFunctionOnPartitionedTable() {\n     createPartitionedTable(spark, tableName, \"days(ts)\");\n     testDaysFunction(true);\n@@ -165,16 +164,16 @@ private void testDaysFunction(boolean partitioned) {\n     checkPushedFilters(optimizedPlan, lessThan(day(\"ts\"), targetDays));\n \n     List<Object[]> actual = rowsToJava(df.collectAsList());\n-    assertThat(actual.size()).isEqualTo(5);\n+    assertThat(actual).hasSize(5);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHoursFunctionOnUnpartitionedTable() {\n     createUnpartitionedTable(spark, tableName);\n     testHoursFunction(false);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHoursFunctionOnPartitionedTable() {\n     createPartitionedTable(spark, tableName, \"hours(ts)\");\n     testHoursFunction(true);\n@@ -193,16 +192,16 @@ private void testHoursFunction(boolean partitioned) {\n     checkPushedFilters(optimizedPlan, greaterThanOrEqual(hour(\"ts\"), targetHours));\n \n     List<Object[]> actual = rowsToJava(df.collectAsList());\n-    assertThat(actual.size()).isEqualTo(8);\n+    assertThat(actual).hasSize(8);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBucketLongFunctionOnUnpartitionedTable() {\n     createUnpartitionedTable(spark, tableName);\n     testBucketLongFunction(false);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBucketLongFunctionOnPartitionedTable() {\n     createPartitionedTable(spark, tableName, \"bucket(5, id)\");\n     testBucketLongFunction(true);\n@@ -221,16 +220,16 @@ private void testBucketLongFunction(boolean partitioned) {\n     checkPushedFilters(optimizedPlan, lessThanOrEqual(bucket(\"id\", 5), target));\n \n     List<Object[]> actual = rowsToJava(df.collectAsList());\n-    assertThat(actual.size()).isEqualTo(5);\n+    assertThat(actual).hasSize(5);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBucketStringFunctionOnUnpartitionedTable() {\n     createUnpartitionedTable(spark, tableName);\n     testBucketStringFunction(false);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBucketStringFunctionOnPartitionedTable() {\n     createPartitionedTable(spark, tableName, \"bucket(5, data)\");\n     testBucketStringFunction(true);\n@@ -249,16 +248,16 @@ private void testBucketStringFunction(boolean partitioned) {\n     checkPushedFilters(optimizedPlan, notEqual(bucket(\"data\", 5), target));\n \n     List<Object[]> actual = rowsToJava(df.collectAsList());\n-    assertThat(actual.size()).isEqualTo(8);\n+    assertThat(actual).hasSize(8);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTruncateFunctionOnUnpartitionedTable() {\n     createUnpartitionedTable(spark, tableName);\n     testTruncateFunction(false);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTruncateFunctionOnPartitionedTable() {\n     createPartitionedTable(spark, tableName, \"truncate(4, data)\");\n     testTruncateFunction(true);\n@@ -278,7 +277,7 @@ private void testTruncateFunction(boolean partitioned) {\n     checkPushedFilters(optimizedPlan, equal(truncate(\"data\", 4), target));\n \n     List<Object[]> actual = rowsToJava(df.collectAsList());\n-    assertThat(actual.size()).isEqualTo(5);\n+    assertThat(actual).hasSize(5);\n   }\n \n   private void checkExpressions(\n@@ -295,7 +294,7 @@ private void checkExpressions(\n     if (partitioned) {\n       assertThat(applyExpressions).isEmpty();\n     } else {\n-      assertThat(applyExpressions.size()).isEqualTo(1);\n+      assertThat(applyExpressions).hasSize(1);\n       ApplyFunctionExpression expression = (ApplyFunctionExpression) applyExpressions.get(0);\n       assertThat(expression.name()).isEqualTo(expectedFunctionName);\n     }\n@@ -305,7 +304,7 @@ private void checkPushedFilters(\n       LogicalPlan optimizedPlan, org.apache.iceberg.expressions.Expression expected) {\n     List<org.apache.iceberg.expressions.Expression> pushedFilters =\n         PlanUtils.collectPushDownFilters(optimizedPlan);\n-    assertThat(pushedFilters.size()).isEqualTo(1);\n+    assertThat(pushedFilters).hasSize(1);\n     org.apache.iceberg.expressions.Expression actual = pushedFilters.get(0);\n     assertThat(ExpressionUtil.equivalent(expected, actual, STRUCT, true))\n         .as(\"Pushed filter should match\")\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSystemFunctionPushDownInRowLevelOperations.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSystemFunctionPushDownInRowLevelOperations.java\nindex 49119d319c40..5a0a04edba3d 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSystemFunctionPushDownInRowLevelOperations.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSystemFunctionPushDownInRowLevelOperations.java\n@@ -23,9 +23,10 @@\n import static org.assertj.core.api.Assertions.assertThat;\n \n import java.util.List;\n-import java.util.Map;\n import org.apache.iceberg.DistributionMode;\n import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n@@ -38,12 +39,13 @@\n import org.apache.spark.sql.catalyst.expressions.objects.StaticInvoke;\n import org.apache.spark.sql.execution.CommandResultExec;\n import org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.Test;\n-import org.junit.runners.Parameterized.Parameters;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestSystemFunctionPushDownInRowLevelOperations extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSystemFunctionPushDownInRowLevelOperations extends ExtensionsTestBase {\n \n   private static final String CHANGES_TABLE_NAME = \"changes\";\n \n@@ -58,161 +60,156 @@ public static Object[][] parameters() {\n     };\n   }\n \n-  public TestSystemFunctionPushDownInRowLevelOperations(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Before\n+  @BeforeEach\n   public void beforeEach() {\n     sql(\"USE %s\", catalogName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s PURGE\", tableName);\n     sql(\"DROP TABLE IF EXISTS %s PURGE\", tableName(CHANGES_TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteDeleteBucketTransformInPredicate() {\n     initTable(\"bucket(4, dep)\");\n     checkDelete(COPY_ON_WRITE, \"system.bucket(4, dep) IN (2, 3)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadDeleteBucketTransformInPredicate() {\n     initTable(\"bucket(4, dep)\");\n     checkDelete(MERGE_ON_READ, \"system.bucket(4, dep) IN (2, 3)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteDeleteBucketTransformEqPredicate() {\n     initTable(\"bucket(4, dep)\");\n     checkDelete(COPY_ON_WRITE, \"system.bucket(4, dep) = 2\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadDeleteBucketTransformEqPredicate() {\n     initTable(\"bucket(4, dep)\");\n     checkDelete(MERGE_ON_READ, \"system.bucket(4, dep) = 2\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteDeleteYearsTransform() {\n     initTable(\"years(ts)\");\n     checkDelete(COPY_ON_WRITE, \"system.years(ts) > 30\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadDeleteYearsTransform() {\n     initTable(\"years(ts)\");\n     checkDelete(MERGE_ON_READ, \"system.years(ts) <= 30\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteDeleteMonthsTransform() {\n     initTable(\"months(ts)\");\n     checkDelete(COPY_ON_WRITE, \"system.months(ts) <= 250\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadDeleteMonthsTransform() {\n     initTable(\"months(ts)\");\n     checkDelete(MERGE_ON_READ, \"system.months(ts) > 250\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteDeleteDaysTransform() {\n     initTable(\"days(ts)\");\n     checkDelete(COPY_ON_WRITE, \"system.days(ts) <= date('2000-01-03 00:00:00')\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadDeleteDaysTransform() {\n     initTable(\"days(ts)\");\n     checkDelete(MERGE_ON_READ, \"system.days(ts) > date('2000-01-03 00:00:00')\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteDeleteHoursTransform() {\n     initTable(\"hours(ts)\");\n     checkDelete(COPY_ON_WRITE, \"system.hours(ts) <= 100000\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadDeleteHoursTransform() {\n     initTable(\"hours(ts)\");\n     checkDelete(MERGE_ON_READ, \"system.hours(ts) > 100000\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteDeleteTruncateTransform() {\n     initTable(\"truncate(1, dep)\");\n     checkDelete(COPY_ON_WRITE, \"system.truncate(1, dep) = 'i'\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadDeleteTruncateTransform() {\n     initTable(\"truncate(1, dep)\");\n     checkDelete(MERGE_ON_READ, \"system.truncate(1, dep) = 'i'\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteUpdateBucketTransform() {\n     initTable(\"bucket(4, dep)\");\n     checkUpdate(COPY_ON_WRITE, \"system.bucket(4, dep) IN (2, 3)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadUpdateBucketTransform() {\n     initTable(\"bucket(4, dep)\");\n     checkUpdate(MERGE_ON_READ, \"system.bucket(4, dep) = 2\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteUpdateYearsTransform() {\n     initTable(\"years(ts)\");\n     checkUpdate(COPY_ON_WRITE, \"system.years(ts) > 30\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadUpdateYearsTransform() {\n     initTable(\"years(ts)\");\n     checkUpdate(MERGE_ON_READ, \"system.years(ts) <= 30\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteMergeBucketTransform() {\n     initTable(\"bucket(4, dep)\");\n     checkMerge(COPY_ON_WRITE, \"system.bucket(4, dep) IN (2, 3)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadMergeBucketTransform() {\n     initTable(\"bucket(4, dep)\");\n     checkMerge(MERGE_ON_READ, \"system.bucket(4, dep) = 2\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteMergeYearsTransform() {\n     initTable(\"years(ts)\");\n     checkMerge(COPY_ON_WRITE, \"system.years(ts) > 30\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadMergeYearsTransform() {\n     initTable(\"years(ts)\");\n     checkMerge(MERGE_ON_READ, \"system.years(ts) <= 30\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCopyOnWriteMergeTruncateTransform() {\n     initTable(\"truncate(1, dep)\");\n     checkMerge(COPY_ON_WRITE, \"system.truncate(1, dep) = 'i'\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeOnReadMergeTruncateTransform() {\n     initTable(\"truncate(1, dep)\");\n     checkMerge(MERGE_ON_READ, \"system.truncate(1, dep) = 'i'\");\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestWriteAborts.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestWriteAborts.java\nindex a58dcb5d5eea..66aeda92119b 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestWriteAborts.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestWriteAborts.java\n@@ -20,10 +20,13 @@\n \n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import java.nio.file.Files;\n import java.util.List;\n import java.util.Map;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.hadoop.HadoopFileIO;\n import org.apache.iceberg.io.BulkDeletionFailureException;\n@@ -39,15 +42,14 @@\n import org.apache.spark.SparkException;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n-import org.junit.After;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestWriteAborts extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestWriteAborts extends ExtensionsTestBase {\n \n-  @Parameterized.Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n       {\n@@ -75,20 +77,14 @@ public static Object[][] parameters() {\n     };\n   }\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n-\n-  public TestWriteAborts(String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBatchAppend() throws Exception {\n-    String dataLocation = temp.newFolder().toString();\n+    String dataLocation = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n \n     sql(\n         \"CREATE TABLE %s (id INT, data STRING) \"\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java\nindex 54e100727380..ecf9e6f8a59d 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java\n@@ -77,7 +77,7 @@ public void testDelegateUnsupportedProcedure() {\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n               assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n-              assertThat(parseException.getMessageParameters().get(\"error\")).isEqualTo(\"'CALL'\");\n+              assertThat(parseException.getMessageParameters()).containsEntry(\"error\", \"'CALL'\");\n             });\n   }\n \n@@ -87,7 +87,7 @@ public void testCallWithBackticks() throws ParseException {\n         (CallStatement) parser.parsePlan(\"CALL cat.`system`.`rollback_to_snapshot`()\");\n     assertThat(seqAsJavaList(call.name())).containsExactly(\"cat\", \"system\", \"rollback_to_snapshot\");\n \n-    assertThat(seqAsJavaList(call.args())).hasSize(0);\n+    assertThat(seqAsJavaList(call.args())).isEmpty();\n   }\n \n   @Test\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestChangelogTable.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestChangelogTable.java\nindex d4930250cb10..5491f01e942d 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestChangelogTable.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestChangelogTable.java\n@@ -417,12 +417,6 @@ public void testChangelogViewOutsideTimeRange() {\n     sql(\"INSERT INTO %s VALUES (3, 'c')\", tableName);\n     sql(\"INSERT INTO %s VALUES (4, 'd')\", tableName);\n \n-    Table table = validationCatalog.loadTable(tableIdent);\n-    Snapshot insertSnapshot = table.currentSnapshot();\n-\n-    // Get timestamp after inserts but before our changelog window\n-    long beforeWindowTime = System.currentTimeMillis();\n-\n     // Small delay to ensure our timestamps are different\n     try {\n       Thread.sleep(100);\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\nindex 4c89f9eb0bcc..82e87c45f11d 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\n@@ -19,10 +19,11 @@\n package org.apache.iceberg.spark.extensions;\n \n import static org.apache.iceberg.TableProperties.WRITE_AUDIT_PUBLISH_ENABLED;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n-import static org.assertj.core.api.AssertionsForClassTypes.assertThat;\n \n import java.util.List;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.exceptions.ValidationException;\n@@ -34,7 +35,9 @@\n import org.apache.spark.sql.catalyst.parser.ParseException;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestCherrypickSnapshotProcedure extends ExtensionsTestBase {\n \n   @AfterEach\n@@ -176,7 +179,7 @@ public void testInvalidCherrypickSnapshotCases() {\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n               assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n-              assertThat(parseException.getMessageParameters().get(\"error\")).isEqualTo(\"'CALL'\");\n+              assertThat(parseException.getMessageParameters()).containsEntry(\"error\", \"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.cherrypick_snapshot('t')\", catalogName))\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\nindex d9c1172f9cd2..572347971d8c 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n@@ -18,8 +18,8 @@\n  */\n package org.apache.iceberg.spark.extensions;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n-import static org.assertj.core.api.AssertionsForClassTypes.assertThat;\n import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.util.List;\n@@ -261,7 +261,7 @@ public void testInvalidRollbackToSnapshotCases() {\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n               assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n-              assertThat(parseException.getMessageParameters().get(\"error\")).isEqualTo(\"'CALL'\");\n+              assertThat(parseException.getMessageParameters()).containsEntry(\"error\", \"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.rollback_to_snapshot('t')\", catalogName))\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\nindex 8e75ba2548a3..528c873ffa35 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\n@@ -18,14 +18,15 @@\n  */\n package org.apache.iceberg.spark.extensions;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n-import static org.assertj.core.api.AssertionsForClassTypes.assertThat;\n import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.sql.Timestamp;\n import java.time.Instant;\n import java.time.LocalDateTime;\n import java.util.List;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n@@ -35,7 +36,9 @@\n import org.apache.spark.sql.catalyst.parser.ParseException;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestRollbackToTimestampProcedure extends ExtensionsTestBase {\n \n   @AfterEach\n@@ -302,7 +305,7 @@ public void testInvalidRollbackToTimestampCases() {\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n               assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n-              assertThat(parseException.getMessageParameters().get(\"error\")).isEqualTo(\"'CALL'\");\n+              assertThat(parseException.getMessageParameters()).containsEntry(\"error\", \"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.rollback_to_timestamp('t')\", catalogName))\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\nindex 50a8760e8a41..0b78d914c34f 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\n@@ -19,8 +19,8 @@\n package org.apache.iceberg.spark.extensions;\n \n import static org.apache.iceberg.TableProperties.WRITE_AUDIT_PUBLISH_ENABLED;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n-import static org.assertj.core.api.AssertionsForClassTypes.assertThat;\n import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.util.List;\n@@ -210,7 +210,7 @@ public void testInvalidRollbackToSnapshotCases() {\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n               assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n-              assertThat(parseException.getMessageParameters().get(\"error\")).isEqualTo(\"'CALL'\");\n+              assertThat(parseException.getMessageParameters()).containsEntry(\"error\", \"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.set_current_snapshot('t')\", catalogName))\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetWriteDistributionAndOrdering.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetWriteDistributionAndOrdering.java\nindex b8547772da67..2bdeb197126a 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetWriteDistributionAndOrdering.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetWriteDistributionAndOrdering.java\n@@ -100,8 +100,7 @@ public void testSetWriteOrderByColumnWithDirection() {\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    assertThat(distributionMode).as(\"Distribution mode must match\").isEqualTo(\"range\");\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"range\");\n \n     SortOrder expected =\n         SortOrder.builderFor(table.schema())\n@@ -124,8 +123,7 @@ public void testSetWriteOrderByColumnWithDirectionAndNullOrder() {\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    assertThat(distributionMode).as(\"Distribution mode must match\").isEqualTo(\"range\");\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"range\");\n \n     SortOrder expected =\n         SortOrder.builderFor(table.schema())\n@@ -148,8 +146,7 @@ public void testSetWriteOrderByTransform() {\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    assertThat(distributionMode).as(\"Distribution mode must match\").isEqualTo(\"range\");\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"range\");\n \n     SortOrder expected =\n         SortOrder.builderFor(table.schema())\n@@ -173,8 +170,7 @@ public void testSetWriteUnordered() {\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    assertThat(distributionMode).as(\"Distribution mode must match\").isEqualTo(\"range\");\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"range\");\n \n     assertThat(table.sortOrder()).as(\"Table must be sorted\").isNotEqualTo(SortOrder.unsorted());\n \n@@ -182,8 +178,7 @@ public void testSetWriteUnordered() {\n \n     table.refresh();\n \n-    String newDistributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    assertThat(newDistributionMode).as(\"New distribution mode must match\").isEqualTo(\"none\");\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"none\");\n \n     assertThat(table.sortOrder()).as(\"New sort order must match\").isEqualTo(SortOrder.unsorted());\n   }\n@@ -200,7 +195,7 @@ public void testSetWriteLocallyOrdered() {\n \n     table.refresh();\n \n-    assertThat(table.properties().containsKey(TableProperties.WRITE_DISTRIBUTION_MODE)).isFalse();\n+    assertThat(table.properties()).doesNotContainKey(TableProperties.WRITE_DISTRIBUTION_MODE);\n \n     SortOrder expected =\n         SortOrder.builderFor(table.schema())\n@@ -224,7 +219,7 @@ public void testSetWriteLocallyOrderedToPartitionedTable() {\n \n     table.refresh();\n \n-    assertThat(table.properties().containsKey(TableProperties.WRITE_DISTRIBUTION_MODE)).isFalse();\n+    assertThat(table.properties()).doesNotContainKey(TableProperties.WRITE_DISTRIBUTION_MODE);\n \n     SortOrder expected =\n         SortOrder.builderFor(table.schema()).withOrderId(1).desc(\"category\").build();\n@@ -243,8 +238,7 @@ public void testSetWriteDistributedByWithSort() {\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    assertThat(distributionMode).as(\"Distribution mode must match\").isEqualTo(\"hash\");\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"hash\");\n \n     SortOrder expected = SortOrder.builderFor(table.schema()).withOrderId(1).asc(\"id\").build();\n     assertThat(table.sortOrder()).as(\"Sort order must match\").isEqualTo(expected);\n@@ -262,8 +256,7 @@ public void testSetWriteDistributedByWithLocalSort() {\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    assertThat(distributionMode).as(\"Distribution mode must match\").isEqualTo(\"hash\");\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"hash\");\n \n     SortOrder expected = SortOrder.builderFor(table.schema()).withOrderId(1).asc(\"id\").build();\n     assertThat(table.sortOrder()).as(\"Sort order must match\").isEqualTo(expected);\n@@ -272,8 +265,7 @@ public void testSetWriteDistributedByWithLocalSort() {\n \n     table.refresh();\n \n-    String newDistributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    assertThat(newDistributionMode).as(\"Distribution mode must match\").isEqualTo(distributionMode);\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"hash\");\n   }\n \n   @TestTemplate\n@@ -288,8 +280,7 @@ public void testSetWriteDistributedByAndUnordered() {\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    assertThat(distributionMode).as(\"Distribution mode must match\").isEqualTo(\"hash\");\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"hash\");\n \n     assertThat(table.sortOrder()).as(\"Sort order must match\").isEqualTo(SortOrder.unsorted());\n   }\n@@ -306,8 +297,7 @@ public void testSetWriteDistributedByOnly() {\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    assertThat(distributionMode).as(\"Distribution mode must match\").isEqualTo(\"hash\");\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"hash\");\n \n     assertThat(table.sortOrder()).as(\"Sort order must match\").isEqualTo(SortOrder.unsorted());\n   }\n@@ -324,8 +314,7 @@ public void testSetWriteDistributedAndUnorderedInverted() {\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    assertThat(distributionMode).as(\"Distribution mode must match\").isEqualTo(\"hash\");\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"hash\");\n \n     assertThat(table.sortOrder()).as(\"Sort order must match\").isEqualTo(SortOrder.unsorted());\n   }\n@@ -342,8 +331,7 @@ public void testSetWriteDistributedAndLocallyOrderedInverted() {\n \n     table.refresh();\n \n-    String distributionMode = table.properties().get(TableProperties.WRITE_DISTRIBUTION_MODE);\n-    assertThat(distributionMode).as(\"Distribution mode must match\").isEqualTo(\"hash\");\n+    assertThat(table.properties()).containsEntry(TableProperties.WRITE_DISTRIBUTION_MODE, \"hash\");\n \n     SortOrder expected = SortOrder.builderFor(table.schema()).withOrderId(1).asc(\"id\").build();\n     assertThat(table.sortOrder()).as(\"Sort order must match\").isEqualTo(expected);\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12766",
    "pr_id": 12766,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 17,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTablePartitionFields.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTableSchema.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestBranchDDL.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestComputeTableStatsProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRegisterTableProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestReplaceBranch.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestTagDDL.java",
      "spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/procedures/FastForwardBranchProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTablePartitionFields.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTableSchema.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestBranchDDL.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRegisterTableProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestTagDDL.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTablePartitionFields.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTableSchema.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestBranchDDL.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestComputeTableStatsProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRegisterTableProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestReplaceBranch.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestTagDDL.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTablePartitionFields.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTableSchema.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestBranchDDL.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRegisterTableProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestTagDDL.java"
    ],
    "base_commit": "55b5371e9feb04973335b569e4f24b15c4f30570",
    "head_commit": "ec35162aad2551080124295980986a974c96e1a4",
    "repo_url": "https://github.com/apache/iceberg/pull/12766",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12766",
    "dockerfile": "",
    "pr_merged_at": "2025-04-14T08:54:23.000Z",
    "patch": "diff --git a/spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/procedures/FastForwardBranchProcedure.java b/spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/procedures/FastForwardBranchProcedure.java\nindex 459cc01c469b..11ea5d44c9f8 100644\n--- a/spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/procedures/FastForwardBranchProcedure.java\n+++ b/spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/procedures/FastForwardBranchProcedure.java\n@@ -71,18 +71,18 @@ public StructType outputType() {\n   @Override\n   public InternalRow[] call(InternalRow args) {\n     Identifier tableIdent = toIdentifier(args.getString(0), PARAMETERS[0].name());\n-    String source = args.getString(1);\n-    String target = args.getString(2);\n+    String from = args.getString(1);\n+    String to = args.getString(2);\n \n     return modifyIcebergTable(\n         tableIdent,\n         table -> {\n-          long currentRef = table.currentSnapshot().snapshotId();\n-          table.manageSnapshots().fastForwardBranch(source, target).commit();\n-          long updatedRef = table.currentSnapshot().snapshotId();\n-\n+          Long snapshotBefore =\n+              table.snapshot(from) != null ? table.snapshot(from).snapshotId() : null;\n+          table.manageSnapshots().fastForwardBranch(from, to).commit();\n+          long snapshotAfter = table.snapshot(from).snapshotId();\n           InternalRow outputRow =\n-              newInternalRow(UTF8String.fromString(source), currentRef, updatedRef);\n+              newInternalRow(UTF8String.fromString(from), snapshotBefore, snapshotAfter);\n           return new InternalRow[] {outputRow};\n         });\n   }\n",
    "test_patch": "diff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTablePartitionFields.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTablePartitionFields.java\nindex 2c109f1007d1..931307d2f7b0 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTablePartitionFields.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTablePartitionFields.java\n@@ -20,6 +20,9 @@\n \n import static org.assertj.core.api.Assertions.assertThat;\n \n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n@@ -29,39 +32,45 @@\n import org.apache.spark.sql.connector.catalog.CatalogManager;\n import org.apache.spark.sql.connector.catalog.Identifier;\n import org.apache.spark.sql.connector.catalog.TableCatalog;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestAlterTablePartitionFields extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestAlterTablePartitionFields extends ExtensionsTestBase {\n \n-  @Parameterized.Parameters(name = \"catalogConfig = {0}, formatVersion = {1}\")\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}, formatVersion = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {SparkCatalogConfig.HIVE, 1},\n-      {SparkCatalogConfig.SPARK, 2}\n+      {\n+        SparkCatalogConfig.HIVE.catalogName(),\n+        SparkCatalogConfig.HIVE.implementation(),\n+        SparkCatalogConfig.HIVE.properties(),\n+        1\n+      },\n+      {\n+        SparkCatalogConfig.SPARK.catalogName(),\n+        SparkCatalogConfig.SPARK.implementation(),\n+        SparkCatalogConfig.SPARK.properties(),\n+        2\n+      }\n     };\n   }\n \n-  private final int formatVersion;\n+  @Parameter(index = 3)\n+  private int formatVersion;\n \n-  public TestAlterTablePartitionFields(SparkCatalogConfig catalogConfig, int formatVersion) {\n-    super(catalogConfig.catalogName(), catalogConfig.implementation(), catalogConfig.properties());\n-    this.formatVersion = formatVersion;\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddIdentityPartition() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertTrue(\"Table should start unpartitioned\", table.spec().isUnpartitioned());\n+    assertThat(table.spec().isUnpartitioned()).as(\"Table should start unpartitioned\").isTrue();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD category\", tableName);\n \n@@ -70,15 +79,15 @@ public void testAddIdentityPartition() {\n     PartitionSpec expected =\n         PartitionSpec.builderFor(table.schema()).withSpecId(1).identity(\"category\").build();\n \n-    Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+    assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddBucketPartition() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertTrue(\"Table should start unpartitioned\", table.spec().isUnpartitioned());\n+    assertThat(table.spec().isUnpartitioned()).as(\"Table should start unpartitioned\").isTrue();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD bucket(16, id)\", tableName);\n \n@@ -90,15 +99,15 @@ public void testAddBucketPartition() {\n             .bucket(\"id\", 16, \"id_bucket_16\")\n             .build();\n \n-    Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+    assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddTruncatePartition() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertTrue(\"Table should start unpartitioned\", table.spec().isUnpartitioned());\n+    assertThat(table.spec().isUnpartitioned()).as(\"Table should start unpartitioned\").isTrue();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD truncate(data, 4)\", tableName);\n \n@@ -110,15 +119,15 @@ public void testAddTruncatePartition() {\n             .truncate(\"data\", 4, \"data_trunc_4\")\n             .build();\n \n-    Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+    assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddYearsPartition() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertTrue(\"Table should start unpartitioned\", table.spec().isUnpartitioned());\n+    assertThat(table.spec().isUnpartitioned()).as(\"Table should start unpartitioned\").isTrue();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD years(ts)\", tableName);\n \n@@ -127,15 +136,15 @@ public void testAddYearsPartition() {\n     PartitionSpec expected =\n         PartitionSpec.builderFor(table.schema()).withSpecId(1).year(\"ts\").build();\n \n-    Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+    assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddMonthsPartition() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertTrue(\"Table should start unpartitioned\", table.spec().isUnpartitioned());\n+    assertThat(table.spec().isUnpartitioned()).as(\"Table should start unpartitioned\").isTrue();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD months(ts)\", tableName);\n \n@@ -144,15 +153,15 @@ public void testAddMonthsPartition() {\n     PartitionSpec expected =\n         PartitionSpec.builderFor(table.schema()).withSpecId(1).month(\"ts\").build();\n \n-    Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+    assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddDaysPartition() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertTrue(\"Table should start unpartitioned\", table.spec().isUnpartitioned());\n+    assertThat(table.spec().isUnpartitioned()).as(\"Table should start unpartitioned\").isTrue();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD days(ts)\", tableName);\n \n@@ -161,15 +170,15 @@ public void testAddDaysPartition() {\n     PartitionSpec expected =\n         PartitionSpec.builderFor(table.schema()).withSpecId(1).day(\"ts\").build();\n \n-    Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+    assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddHoursPartition() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertTrue(\"Table should start unpartitioned\", table.spec().isUnpartitioned());\n+    assertThat(table.spec().isUnpartitioned()).as(\"Table should start unpartitioned\").isTrue();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD hours(ts)\", tableName);\n \n@@ -178,10 +187,10 @@ public void testAddHoursPartition() {\n     PartitionSpec expected =\n         PartitionSpec.builderFor(table.schema()).withSpecId(1).hour(\"ts\").build();\n \n-    Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+    assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddYearPartition() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n@@ -198,7 +207,7 @@ public void testAddYearPartition() {\n     assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddMonthPartition() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n@@ -215,7 +224,7 @@ public void testAddMonthPartition() {\n     assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddDayPartition() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n@@ -232,7 +241,7 @@ public void testAddDayPartition() {\n     assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddHourPartition() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n@@ -249,12 +258,12 @@ public void testAddHourPartition() {\n     assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddNamedPartition() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertTrue(\"Table should start unpartitioned\", table.spec().isUnpartitioned());\n+    assertThat(table.spec().isUnpartitioned()).as(\"Table should start unpartitioned\").isTrue();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD bucket(16, id) AS shard\", tableName);\n \n@@ -263,16 +272,15 @@ public void testAddNamedPartition() {\n     PartitionSpec expected =\n         PartitionSpec.builderFor(table.schema()).withSpecId(1).bucket(\"id\", 16, \"shard\").build();\n \n-    Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+    assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropIdentityPartition() {\n     createTable(\"id bigint NOT NULL, category string, data string\", \"category\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\n-        \"Table should start with 1 partition field\", 1, table.spec().fields().size());\n+    assertThat(table.spec().fields()).as(\"Table should start with 1 partition field\").hasSize(1);\n \n     sql(\"ALTER TABLE %s DROP PARTITION FIELD category\", tableName);\n \n@@ -284,19 +292,18 @@ public void testDropIdentityPartition() {\n               .withSpecId(1)\n               .alwaysNull(\"category\", \"category\")\n               .build();\n-      Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+      assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n     } else {\n-      Assert.assertTrue(\"New spec must be unpartitioned\", table.spec().isUnpartitioned());\n+      assertThat(table.spec().isUnpartitioned()).as(\"New spec must be unpartitioned\").isTrue();\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropDaysPartition() {\n     createTable(\"id bigint NOT NULL, ts timestamp, data string\", \"days(ts)\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\n-        \"Table should start with 1 partition field\", 1, table.spec().fields().size());\n+    assertThat(table.spec().fields()).as(\"Table should start with 1 partition field\").hasSize(1);\n \n     sql(\"ALTER TABLE %s DROP PARTITION FIELD days(ts)\", tableName);\n \n@@ -305,19 +312,18 @@ public void testDropDaysPartition() {\n     if (formatVersion == 1) {\n       PartitionSpec expected =\n           PartitionSpec.builderFor(table.schema()).withSpecId(1).alwaysNull(\"ts\", \"ts_day\").build();\n-      Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+      assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n     } else {\n-      Assert.assertTrue(\"New spec must be unpartitioned\", table.spec().isUnpartitioned());\n+      assertThat(table.spec().isUnpartitioned()).as(\"New spec must be unpartitioned\").isTrue();\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropBucketPartition() {\n     createTable(\"id bigint NOT NULL, data string\", \"bucket(16, id)\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\n-        \"Table should start with 1 partition field\", 1, table.spec().fields().size());\n+    assertThat(table.spec().fields()).as(\"Table should start with 1 partition field\").hasSize(1);\n \n     sql(\"ALTER TABLE %s DROP PARTITION FIELD bucket(16, id)\", tableName);\n \n@@ -329,24 +335,24 @@ public void testDropBucketPartition() {\n               .withSpecId(1)\n               .alwaysNull(\"id\", \"id_bucket\")\n               .build();\n-      Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+      assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n     } else {\n-      Assert.assertTrue(\"New spec must be unpartitioned\", table.spec().isUnpartitioned());\n+      assertThat(table.spec().isUnpartitioned()).as(\"New spec must be unpartitioned\").isTrue();\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropPartitionByName() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertTrue(\"Table should start unpartitioned\", table.spec().isUnpartitioned());\n+    assertThat(table.spec().isUnpartitioned()).as(\"Table should start unpartitioned\").isTrue();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD bucket(16, id) AS shard\", tableName);\n \n     table.refresh();\n \n-    Assert.assertEquals(\"Table should have 1 partition field\", 1, table.spec().fields().size());\n+    assertThat(table.spec().fields()).as(\"Table should have 1 partition field\").hasSize(1);\n \n     // Should be recognized as iceberg command even with extra white spaces\n     sql(\"ALTER TABLE %s DROP  PARTITION \\n FIELD shard\", tableName);\n@@ -356,23 +362,23 @@ public void testDropPartitionByName() {\n     if (formatVersion == 1) {\n       PartitionSpec expected =\n           PartitionSpec.builderFor(table.schema()).withSpecId(2).alwaysNull(\"id\", \"shard\").build();\n-      Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+      assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n     } else {\n-      Assert.assertTrue(\"New spec must be unpartitioned\", table.spec().isUnpartitioned());\n+      assertThat(table.spec().isUnpartitioned()).as(\"New spec must be unpartitioned\").isTrue();\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplacePartition() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unpartitioned\", table.spec().isUnpartitioned());\n+    assertThat(table.spec().isUnpartitioned()).as(\"Table should start unpartitioned\").isTrue();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD days(ts)\", tableName);\n     table.refresh();\n     PartitionSpec expected =\n         PartitionSpec.builderFor(table.schema()).withSpecId(1).day(\"ts\").build();\n-    Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+    assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n \n     sql(\"ALTER TABLE %s REPLACE PARTITION FIELD days(ts) WITH hours(ts)\", tableName);\n     table.refresh();\n@@ -391,21 +397,22 @@ public void testReplacePartition() {\n               .addField(\"hour\", 3, 1001, \"ts_hour\")\n               .build();\n     }\n-    Assert.assertEquals(\n-        \"Should changed from daily to hourly partitioned field\", expected, table.spec());\n+    assertThat(table.spec())\n+        .as(\"Should changed from daily to hourly partitioned field\")\n+        .isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplacePartitionAndRename() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unpartitioned\", table.spec().isUnpartitioned());\n+    assertThat(table.spec().isUnpartitioned()).as(\"Table should start unpartitioned\").isTrue();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD days(ts)\", tableName);\n     table.refresh();\n     PartitionSpec expected =\n         PartitionSpec.builderFor(table.schema()).withSpecId(1).day(\"ts\").build();\n-    Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+    assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n \n     sql(\"ALTER TABLE %s REPLACE PARTITION FIELD days(ts) WITH hours(ts) AS hour_col\", tableName);\n     table.refresh();\n@@ -424,21 +431,22 @@ public void testReplacePartitionAndRename() {\n               .addField(\"hour\", 3, 1001, \"hour_col\")\n               .build();\n     }\n-    Assert.assertEquals(\n-        \"Should changed from daily to hourly partitioned field\", expected, table.spec());\n+    assertThat(table.spec())\n+        .as(\"Should changed from daily to hourly partitioned field\")\n+        .isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplaceNamedPartition() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unpartitioned\", table.spec().isUnpartitioned());\n+    assertThat(table.spec().isUnpartitioned()).as(\"Table should start unpartitioned\").isTrue();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD days(ts) AS day_col\", tableName);\n     table.refresh();\n     PartitionSpec expected =\n         PartitionSpec.builderFor(table.schema()).withSpecId(1).day(\"ts\", \"day_col\").build();\n-    Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+    assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n \n     sql(\"ALTER TABLE %s REPLACE PARTITION FIELD day_col WITH hours(ts)\", tableName);\n     table.refresh();\n@@ -457,21 +465,22 @@ public void testReplaceNamedPartition() {\n               .addField(\"hour\", 3, 1001, \"ts_hour\")\n               .build();\n     }\n-    Assert.assertEquals(\n-        \"Should changed from daily to hourly partitioned field\", expected, table.spec());\n+    assertThat(table.spec())\n+        .as(\"Should changed from daily to hourly partitioned field\")\n+        .isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplaceNamedPartitionAndRenameDifferently() {\n     createTable(\"id bigint NOT NULL, category string, ts timestamp, data string\");\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\"Table should start unpartitioned\", table.spec().isUnpartitioned());\n+    assertThat(table.spec().isUnpartitioned()).as(\"Table should start unpartitioned\").isTrue();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD days(ts) AS day_col\", tableName);\n     table.refresh();\n     PartitionSpec expected =\n         PartitionSpec.builderFor(table.schema()).withSpecId(1).day(\"ts\", \"day_col\").build();\n-    Assert.assertEquals(\"Should have new spec field\", expected, table.spec());\n+    assertThat(table.spec()).as(\"Should have new spec field\").isEqualTo(expected);\n \n     sql(\"ALTER TABLE %s REPLACE PARTITION FIELD day_col WITH hours(ts) AS hour_col\", tableName);\n     table.refresh();\n@@ -490,15 +499,15 @@ public void testReplaceNamedPartitionAndRenameDifferently() {\n               .addField(\"hour\", 3, 1001, \"hour_col\")\n               .build();\n     }\n-    Assert.assertEquals(\n-        \"Should changed from daily to hourly partitioned field\", expected, table.spec());\n+    assertThat(table.spec())\n+        .as(\"Should changed from daily to hourly partitioned field\")\n+        .isEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkTableAddDropPartitions() throws Exception {\n     createTable(\"id bigint NOT NULL, ts timestamp, data string\");\n-    Assert.assertEquals(\n-        \"spark table partition should be empty\", 0, sparkTable().partitioning().length);\n+    assertThat(sparkTable().partitioning()).as(\"spark table partition should be empty\").isEmpty();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD bucket(16, id) AS shard\", tableName);\n     assertPartitioningEquals(sparkTable(), 1, \"bucket(16, id)\");\n@@ -517,11 +526,10 @@ public void testSparkTableAddDropPartitions() throws Exception {\n \n     sql(\"ALTER TABLE %s DROP PARTITION FIELD shard\", tableName);\n     sql(\"DESCRIBE %s\", tableName);\n-    Assert.assertEquals(\n-        \"spark table partition should be empty\", 0, sparkTable().partitioning().length);\n+    assertThat(sparkTable().partitioning()).as(\"spark table partition should be empty\").isEmpty();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropColumnOfOldPartitionFieldV1() {\n     // default table created in v1 format\n     sql(\n@@ -533,7 +541,7 @@ public void testDropColumnOfOldPartitionFieldV1() {\n     sql(\"ALTER TABLE %s DROP COLUMN day_of_ts\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropColumnOfOldPartitionFieldV2() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, ts timestamp, day_of_ts date) USING iceberg PARTITIONED BY (day_of_ts) TBLPROPERTIES('format-version' = '2')\",\n@@ -545,11 +553,10 @@ public void testDropColumnOfOldPartitionFieldV2() {\n   }\n \n   private void assertPartitioningEquals(SparkTable table, int len, String transform) {\n-    Assert.assertEquals(\"spark table partition should be \" + len, len, table.partitioning().length);\n-    Assert.assertEquals(\n-        \"latest spark table partition transform should match\",\n-        transform,\n-        table.partitioning()[len - 1].toString());\n+    assertThat(table.partitioning()[len - 1])\n+        .asString()\n+        .as(\"latest spark table partition transform should match\")\n+        .isEqualTo(transform);\n   }\n \n   private SparkTable sparkTable() throws Exception {\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTableSchema.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTableSchema.java\nindex 81bc1a59e144..f36a2e4470e3 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTableSchema.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTableSchema.java\n@@ -18,66 +18,61 @@\n  */\n package org.apache.iceberg.spark.extensions;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n-import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n-\n-public class TestAlterTableSchema extends SparkExtensionsTestBase {\n-  public TestAlterTableSchema(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestAlterTableSchema extends ExtensionsTestBase {\n \n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetIdentifierFields() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, \"\n             + \"location struct<lon:bigint NOT NULL,lat:bigint NOT NULL> NOT NULL) USING iceberg\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\n-        \"Table should start without identifier\", table.schema().identifierFieldIds().isEmpty());\n+    assertThat(table.schema().identifierFieldIds())\n+        .as(\"Table should start without identifier\")\n+        .isEmpty();\n \n     sql(\"ALTER TABLE %s SET IDENTIFIER FIELDS id\", tableName);\n     table.refresh();\n-    Assert.assertEquals(\n-        \"Should have new identifier field\",\n-        Sets.newHashSet(table.schema().findField(\"id\").fieldId()),\n-        table.schema().identifierFieldIds());\n+    assertThat(table.schema().identifierFieldIds())\n+        .containsExactly(table.schema().findField(\"id\").fieldId());\n \n     sql(\"ALTER TABLE %s SET IDENTIFIER FIELDS id, location.lon\", tableName);\n     table.refresh();\n-    Assert.assertEquals(\n-        \"Should have new identifier field\",\n-        Sets.newHashSet(\n+    assertThat(table.schema().identifierFieldIds())\n+        .as(\"Should have new identifier field\")\n+        .containsExactlyInAnyOrder(\n             table.schema().findField(\"id\").fieldId(),\n-            table.schema().findField(\"location.lon\").fieldId()),\n-        table.schema().identifierFieldIds());\n+            table.schema().findField(\"location.lon\").fieldId());\n \n     sql(\"ALTER TABLE %s SET IDENTIFIER FIELDS location.lon\", tableName);\n     table.refresh();\n-    Assert.assertEquals(\n-        \"Should have new identifier field\",\n-        Sets.newHashSet(table.schema().findField(\"location.lon\").fieldId()),\n-        table.schema().identifierFieldIds());\n+    assertThat(table.schema().identifierFieldIds())\n+        .as(\"Should have new identifier field\")\n+        .containsExactly(table.schema().findField(\"location.lon\").fieldId());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetInvalidIdentifierFields() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, id2 bigint) USING iceberg\", tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\n-        \"Table should start without identifier\", table.schema().identifierFieldIds().isEmpty());\n+    assertThat(table.schema().identifierFieldIds())\n+        .as(\"Table should start without identifier\")\n+        .isEmpty();\n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s SET IDENTIFIER FIELDS unknown\", tableName))\n         .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageEndingWith(\"not found in current schema or added columns\");\n@@ -87,56 +82,54 @@ public void testSetInvalidIdentifierFields() {\n         .hasMessageEndingWith(\"not a required field\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropIdentifierFields() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, \"\n             + \"location struct<lon:bigint NOT NULL,lat:bigint NOT NULL> NOT NULL) USING iceberg\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\n-        \"Table should start without identifier\", table.schema().identifierFieldIds().isEmpty());\n+    assertThat(table.schema().identifierFieldIds())\n+        .as(\"Table should start without identifier\")\n+        .isEmpty();\n \n     sql(\"ALTER TABLE %s SET IDENTIFIER FIELDS id, location.lon\", tableName);\n     table.refresh();\n-    Assert.assertEquals(\n-        \"Should have new identifier fields\",\n-        Sets.newHashSet(\n+    assertThat(table.schema().identifierFieldIds())\n+        .as(\"Should have new identifier fields\")\n+        .containsExactlyInAnyOrder(\n             table.schema().findField(\"id\").fieldId(),\n-            table.schema().findField(\"location.lon\").fieldId()),\n-        table.schema().identifierFieldIds());\n+            table.schema().findField(\"location.lon\").fieldId());\n \n     sql(\"ALTER TABLE %s DROP IDENTIFIER FIELDS id\", tableName);\n     table.refresh();\n-    Assert.assertEquals(\n-        \"Should removed identifier field\",\n-        Sets.newHashSet(table.schema().findField(\"location.lon\").fieldId()),\n-        table.schema().identifierFieldIds());\n+    assertThat(table.schema().identifierFieldIds())\n+        .as(\"Should removed identifier field\")\n+        .containsExactly(table.schema().findField(\"location.lon\").fieldId());\n \n     sql(\"ALTER TABLE %s SET IDENTIFIER FIELDS id, location.lon\", tableName);\n     table.refresh();\n-    Assert.assertEquals(\n-        \"Should have new identifier fields\",\n-        Sets.newHashSet(\n+    assertThat(table.schema().identifierFieldIds())\n+        .as(\"Should have new identifier fields\")\n+        .containsExactlyInAnyOrder(\n             table.schema().findField(\"id\").fieldId(),\n-            table.schema().findField(\"location.lon\").fieldId()),\n-        table.schema().identifierFieldIds());\n+            table.schema().findField(\"location.lon\").fieldId());\n \n     sql(\"ALTER TABLE %s DROP IDENTIFIER FIELDS id, location.lon\", tableName);\n     table.refresh();\n-    Assert.assertEquals(\n-        \"Should have no identifier field\", Sets.newHashSet(), table.schema().identifierFieldIds());\n+    assertThat(table.schema().identifierFieldIds()).as(\"Should have no identifier field\").isEmpty();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropInvalidIdentifierFields() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string NOT NULL, \"\n             + \"location struct<lon:bigint NOT NULL,lat:bigint NOT NULL> NOT NULL) USING iceberg\",\n         tableName);\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertTrue(\n-        \"Table should start without identifier\", table.schema().identifierFieldIds().isEmpty());\n+    assertThat(table.schema().identifierFieldIds())\n+        .as(\"Table should start without identifier\")\n+        .isEmpty();\n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s DROP IDENTIFIER FIELDS unknown\", tableName))\n         .isInstanceOf(IllegalArgumentException.class)\n         .hasMessage(\"Cannot complete drop identifier fields operation: field unknown not found\");\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestBranchDDL.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestBranchDDL.java\nindex 69f328fc66a6..f70f2d819013 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestBranchDDL.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestBranchDDL.java\n@@ -22,8 +22,9 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n-import java.util.Map;\n import java.util.concurrent.TimeUnit;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.SnapshotRef;\n import org.apache.iceberg.Table;\n@@ -34,25 +35,25 @@\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.catalyst.parser.extensions.IcebergParseException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestBranchDDL extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestBranchDDL extends ExtensionsTestBase {\n \n-  @Before\n-  public void before() {\n+  @BeforeEach\n+  public void createTable() {\n     sql(\"CREATE TABLE %s (id INT, data STRING) USING iceberg\", tableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Parameterized.Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n       {\n@@ -63,11 +64,7 @@ public static Object[][] parameters() {\n     };\n   }\n \n-  public TestBranchDDL(String catalog, String implementation, Map<String, String> properties) {\n-    super(catalog, implementation, properties);\n-  }\n-\n-  @Test\n+  @TestTemplate\n   public void testCreateBranch() throws NoSuchTableException {\n     Table table = insertRows();\n     long snapshotId = table.currentSnapshot().snapshotId();\n@@ -79,54 +76,64 @@ public void testCreateBranch() throws NoSuchTableException {\n         \"ALTER TABLE %s CREATE BRANCH %s AS OF VERSION %d RETAIN %d DAYS WITH SNAPSHOT RETENTION %d SNAPSHOTS %d days\",\n         tableName, branchName, snapshotId, maxRefAge, minSnapshotsToKeep, maxSnapshotAge);\n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    Assert.assertEquals(table.currentSnapshot().snapshotId(), ref.snapshotId());\n-    Assert.assertEquals(minSnapshotsToKeep, ref.minSnapshotsToKeep());\n-    Assert.assertEquals(TimeUnit.DAYS.toMillis(maxSnapshotAge), ref.maxSnapshotAgeMs().longValue());\n-    Assert.assertEquals(TimeUnit.DAYS.toMillis(maxRefAge), ref.maxRefAgeMs().longValue());\n-\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+              assertThat(ref.minSnapshotsToKeep()).isEqualTo(minSnapshotsToKeep);\n+              assertThat(ref.maxSnapshotAgeMs().longValue())\n+                  .isEqualTo(TimeUnit.DAYS.toMillis(maxSnapshotAge));\n+              assertThat(ref.maxRefAgeMs().longValue())\n+                  .isEqualTo(TimeUnit.DAYS.toMillis(maxRefAge));\n+            });\n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s CREATE BRANCH %s\", tableName, branchName))\n         .isInstanceOf(IllegalArgumentException.class)\n         .hasMessage(\"Ref b1 already exists\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateBranchOnEmptyTable() {\n     String branchName = \"b1\";\n     sql(\"ALTER TABLE %s CREATE BRANCH %s\", tableName, \"b1\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    SnapshotRef mainRef = table.refs().get(SnapshotRef.MAIN_BRANCH);\n-    assertThat(mainRef).isNull();\n-\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref).isNotNull();\n-    assertThat(ref.minSnapshotsToKeep()).isNull();\n-    assertThat(ref.maxSnapshotAgeMs()).isNull();\n-    assertThat(ref.maxRefAgeMs()).isNull();\n-\n-    Snapshot snapshot = table.snapshot(ref.snapshotId());\n-    assertThat(snapshot.parentId()).isNull();\n-    assertThat(snapshot.addedDataFiles(table.io())).isEmpty();\n-    assertThat(snapshot.removedDataFiles(table.io())).isEmpty();\n-    assertThat(snapshot.addedDeleteFiles(table.io())).isEmpty();\n-    assertThat(snapshot.removedDeleteFiles(table.io())).isEmpty();\n+    assertThat(table.refs())\n+        .doesNotContainKey(SnapshotRef.MAIN_BRANCH)\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.minSnapshotsToKeep()).isNull();\n+              assertThat(ref.maxSnapshotAgeMs()).isNull();\n+              assertThat(ref.maxRefAgeMs()).isNull();\n+\n+              Snapshot snapshot = table.snapshot(ref.snapshotId());\n+              assertThat(snapshot.parentId()).isNull();\n+              assertThat(snapshot.addedDataFiles(table.io())).isEmpty();\n+              assertThat(snapshot.removedDataFiles(table.io())).isEmpty();\n+              assertThat(snapshot.addedDeleteFiles(table.io())).isEmpty();\n+              assertThat(snapshot.removedDeleteFiles(table.io())).isEmpty();\n+            });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateBranchUseDefaultConfig() throws NoSuchTableException {\n     Table table = insertRows();\n     String branchName = \"b1\";\n     sql(\"ALTER TABLE %s CREATE BRANCH %s\", tableName, branchName);\n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    Assert.assertEquals(table.currentSnapshot().snapshotId(), ref.snapshotId());\n-    Assert.assertNull(ref.minSnapshotsToKeep());\n-    Assert.assertNull(ref.maxSnapshotAgeMs());\n-    Assert.assertNull(ref.maxRefAgeMs());\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+              assertThat(ref.minSnapshotsToKeep()).isNull();\n+              assertThat(ref.maxSnapshotAgeMs()).isNull();\n+              assertThat(ref.maxRefAgeMs()).isNull();\n+            });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateBranchUseCustomMinSnapshotsToKeep() throws NoSuchTableException {\n     Integer minSnapshotsToKeep = 2;\n     Table table = insertRows();\n@@ -135,14 +142,18 @@ public void testCreateBranchUseCustomMinSnapshotsToKeep() throws NoSuchTableExce\n         \"ALTER TABLE %s CREATE BRANCH %s WITH SNAPSHOT RETENTION %d SNAPSHOTS\",\n         tableName, branchName, minSnapshotsToKeep);\n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    Assert.assertEquals(table.currentSnapshot().snapshotId(), ref.snapshotId());\n-    Assert.assertEquals(minSnapshotsToKeep, ref.minSnapshotsToKeep());\n-    Assert.assertNull(ref.maxSnapshotAgeMs());\n-    Assert.assertNull(ref.maxRefAgeMs());\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+              assertThat(ref.minSnapshotsToKeep()).isEqualTo(minSnapshotsToKeep);\n+              assertThat(ref.maxSnapshotAgeMs()).isNull();\n+              assertThat(ref.maxRefAgeMs()).isNull();\n+            });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateBranchUseCustomMaxSnapshotAge() throws NoSuchTableException {\n     long maxSnapshotAge = 2L;\n     Table table = insertRows();\n@@ -151,14 +162,19 @@ public void testCreateBranchUseCustomMaxSnapshotAge() throws NoSuchTableExceptio\n         \"ALTER TABLE %s CREATE BRANCH %s WITH SNAPSHOT RETENTION %d DAYS\",\n         tableName, branchName, maxSnapshotAge);\n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    Assert.assertNotNull(ref);\n-    Assert.assertNull(ref.minSnapshotsToKeep());\n-    Assert.assertEquals(TimeUnit.DAYS.toMillis(maxSnapshotAge), ref.maxSnapshotAgeMs().longValue());\n-    Assert.assertNull(ref.maxRefAgeMs());\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref).isNotNull();\n+              assertThat(ref.minSnapshotsToKeep()).isNull();\n+              assertThat(ref.maxSnapshotAgeMs().longValue())\n+                  .isEqualTo(TimeUnit.DAYS.toMillis(maxSnapshotAge));\n+              assertThat(ref.maxRefAgeMs()).isNull();\n+            });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateBranchIfNotExists() throws NoSuchTableException {\n     long maxSnapshotAge = 2L;\n     Table table = insertRows();\n@@ -169,14 +185,19 @@ public void testCreateBranchIfNotExists() throws NoSuchTableException {\n     sql(\"ALTER TABLE %s CREATE BRANCH IF NOT EXISTS %s\", tableName, branchName);\n \n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    Assert.assertEquals(table.currentSnapshot().snapshotId(), ref.snapshotId());\n-    Assert.assertNull(ref.minSnapshotsToKeep());\n-    Assert.assertEquals(TimeUnit.DAYS.toMillis(maxSnapshotAge), ref.maxSnapshotAgeMs().longValue());\n-    Assert.assertNull(ref.maxRefAgeMs());\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+              assertThat(ref.minSnapshotsToKeep()).isNull();\n+              assertThat(ref.maxSnapshotAgeMs().longValue())\n+                  .isEqualTo(TimeUnit.DAYS.toMillis(maxSnapshotAge));\n+              assertThat(ref.maxRefAgeMs()).isNull();\n+            });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateBranchUseCustomMinSnapshotsToKeepAndMaxSnapshotAge()\n       throws NoSuchTableException {\n     Integer minSnapshotsToKeep = 2;\n@@ -187,11 +208,16 @@ public void testCreateBranchUseCustomMinSnapshotsToKeepAndMaxSnapshotAge()\n         \"ALTER TABLE %s CREATE BRANCH %s WITH SNAPSHOT RETENTION %d SNAPSHOTS %d DAYS\",\n         tableName, branchName, minSnapshotsToKeep, maxSnapshotAge);\n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    Assert.assertEquals(table.currentSnapshot().snapshotId(), ref.snapshotId());\n-    Assert.assertEquals(minSnapshotsToKeep, ref.minSnapshotsToKeep());\n-    Assert.assertEquals(TimeUnit.DAYS.toMillis(maxSnapshotAge), ref.maxSnapshotAgeMs().longValue());\n-    Assert.assertNull(ref.maxRefAgeMs());\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+              assertThat(ref.minSnapshotsToKeep()).isEqualTo(minSnapshotsToKeep);\n+              assertThat(ref.maxSnapshotAgeMs().longValue())\n+                  .isEqualTo(TimeUnit.DAYS.toMillis(maxSnapshotAge));\n+              assertThat(ref.maxRefAgeMs()).isNull();\n+            });\n \n     assertThatThrownBy(\n             () ->\n@@ -202,18 +228,23 @@ public void testCreateBranchUseCustomMinSnapshotsToKeepAndMaxSnapshotAge()\n         .hasMessageContaining(\"no viable alternative at input 'WITH SNAPSHOT RETENTION'\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateBranchUseCustomMaxRefAge() throws NoSuchTableException {\n     long maxRefAge = 10L;\n     Table table = insertRows();\n     String branchName = \"b1\";\n     sql(\"ALTER TABLE %s CREATE BRANCH %s RETAIN %d DAYS\", tableName, branchName, maxRefAge);\n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    Assert.assertEquals(table.currentSnapshot().snapshotId(), ref.snapshotId());\n-    Assert.assertNull(ref.minSnapshotsToKeep());\n-    Assert.assertNull(ref.maxSnapshotAgeMs());\n-    Assert.assertEquals(TimeUnit.DAYS.toMillis(maxRefAge), ref.maxRefAgeMs().longValue());\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+              assertThat(ref.minSnapshotsToKeep()).isNull();\n+              assertThat(ref.maxSnapshotAgeMs()).isNull();\n+              assertThat(ref.maxRefAgeMs().longValue())\n+                  .isEqualTo(TimeUnit.DAYS.toMillis(maxRefAge));\n+            });\n \n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s CREATE BRANCH %s RETAIN\", tableName, branchName))\n         .isInstanceOf(IcebergParseException.class)\n@@ -234,31 +265,34 @@ public void testCreateBranchUseCustomMaxRefAge() throws NoSuchTableException {\n         .hasMessageContaining(\"mismatched input 'SECONDS' expecting {'DAYS', 'HOURS', 'MINUTES'}\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropBranch() throws NoSuchTableException {\n     insertRows();\n \n     Table table = validationCatalog.loadTable(tableIdent);\n     String branchName = \"b1\";\n     table.manageSnapshots().createBranch(branchName, table.currentSnapshot().snapshotId()).commit();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    Assert.assertEquals(table.currentSnapshot().snapshotId(), ref.snapshotId());\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+            });\n \n     sql(\"ALTER TABLE %s DROP BRANCH %s\", tableName, branchName);\n     table.refresh();\n \n-    ref = table.refs().get(branchName);\n-    Assert.assertNull(ref);\n+    assertThat(table.refs()).doesNotContainKey(branchName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropBranchDoesNotExist() {\n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s DROP BRANCH %s\", tableName, \"nonExistingBranch\"))\n         .isInstanceOf(IllegalArgumentException.class)\n         .hasMessage(\"Branch does not exist: nonExistingBranch\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropBranchFailsForTag() throws NoSuchTableException {\n     String tagName = \"b1\";\n     Table table = insertRows();\n@@ -269,31 +303,29 @@ public void testDropBranchFailsForTag() throws NoSuchTableException {\n         .hasMessage(\"Ref b1 is a tag not a branch\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropBranchNonConformingName() {\n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s DROP BRANCH %s\", tableName, \"123\"))\n         .isInstanceOf(IcebergParseException.class)\n         .hasMessageContaining(\"no viable alternative at input '123'\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropMainBranchFails() {\n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s DROP BRANCH main\", tableName))\n         .isInstanceOf(IllegalArgumentException.class)\n         .hasMessage(\"Cannot remove main branch\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropBranchIfExists() {\n     String branchName = \"nonExistingBranch\";\n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertNull(table.refs().get(branchName));\n+    assertThat(table.refs()).doesNotContainKey(branchName);\n \n     sql(\"ALTER TABLE %s DROP BRANCH IF EXISTS %s\", tableName, branchName);\n     table.refresh();\n-\n-    SnapshotRef ref = table.refs().get(branchName);\n-    Assert.assertNull(ref);\n+    assertThat(table.refs()).doesNotContainKey(branchName);\n   }\n \n   private Table insertRows() throws NoSuchTableException {\n@@ -304,7 +336,7 @@ private Table insertRows() throws NoSuchTableException {\n     return validationCatalog.loadTable(tableIdent);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createOrReplace() throws NoSuchTableException {\n     Table table = insertRows();\n     long first = table.currentSnapshot().snapshotId();\n@@ -320,30 +352,32 @@ public void createOrReplace() throws NoSuchTableException {\n     assertThat(table.refs().get(branchName).snapshotId()).isEqualTo(second);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateOrReplaceBranchOnEmptyTable() {\n     String branchName = \"b1\";\n     sql(\"ALTER TABLE %s CREATE OR REPLACE BRANCH %s\", tableName, \"b1\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    SnapshotRef mainRef = table.refs().get(SnapshotRef.MAIN_BRANCH);\n-    assertThat(mainRef).isNull();\n-\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref).isNotNull();\n-    assertThat(ref.minSnapshotsToKeep()).isNull();\n-    assertThat(ref.maxSnapshotAgeMs()).isNull();\n-    assertThat(ref.maxRefAgeMs()).isNull();\n-\n-    Snapshot snapshot = table.snapshot(ref.snapshotId());\n-    assertThat(snapshot.parentId()).isNull();\n-    assertThat(snapshot.addedDataFiles(table.io())).isEmpty();\n-    assertThat(snapshot.removedDataFiles(table.io())).isEmpty();\n-    assertThat(snapshot.addedDeleteFiles(table.io())).isEmpty();\n-    assertThat(snapshot.removedDeleteFiles(table.io())).isEmpty();\n+    assertThat(table.refs())\n+        .doesNotContainKey(SnapshotRef.MAIN_BRANCH)\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref).isNotNull();\n+              assertThat(ref.minSnapshotsToKeep()).isNull();\n+              assertThat(ref.maxSnapshotAgeMs()).isNull();\n+              assertThat(ref.maxRefAgeMs()).isNull();\n+\n+              Snapshot snapshot = table.snapshot(ref.snapshotId());\n+              assertThat(snapshot.parentId()).isNull();\n+              assertThat(snapshot.addedDataFiles(table.io())).isEmpty();\n+              assertThat(snapshot.removedDataFiles(table.io())).isEmpty();\n+              assertThat(snapshot.addedDeleteFiles(table.io())).isEmpty();\n+              assertThat(snapshot.removedDeleteFiles(table.io())).isEmpty();\n+            });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createOrReplaceWithNonExistingBranch() throws NoSuchTableException {\n     Table table = insertRows();\n     String branchName = \"b1\";\n@@ -357,7 +391,7 @@ public void createOrReplaceWithNonExistingBranch() throws NoSuchTableException {\n     assertThat(table.refs().get(branchName).snapshotId()).isEqualTo(snapshotId);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void replaceBranch() throws NoSuchTableException {\n     Table table = insertRows();\n     long first = table.currentSnapshot().snapshotId();\n@@ -374,12 +408,16 @@ public void replaceBranch() throws NoSuchTableException {\n \n     sql(\"ALTER TABLE %s REPLACE BRANCH %s AS OF VERSION %d\", tableName, branchName, second);\n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref.snapshotId()).isEqualTo(second);\n-    assertThat(ref.maxRefAgeMs()).isEqualTo(expectedMaxRefAgeMs);\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(second);\n+              assertThat(ref.maxRefAgeMs()).isEqualTo(expectedMaxRefAgeMs);\n+            });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void replaceBranchDoesNotExist() throws NoSuchTableException {\n     Table table = insertRows();\n \n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestComputeTableStatsProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestComputeTableStatsProcedure.java\nindex 1597c47bd5d3..c487d4b7c848 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestComputeTableStatsProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestComputeTableStatsProcedure.java\n@@ -22,8 +22,8 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n-import java.util.Map;\n import org.apache.iceberg.BlobMetadata;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.StatisticsFile;\n import org.apache.iceberg.Table;\n@@ -33,22 +33,19 @@\n import org.apache.iceberg.spark.actions.NDVSketchUtil;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n-import org.junit.After;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestComputeTableStatsProcedure extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestComputeTableStatsProcedure extends ExtensionsTestBase {\n \n-  public TestComputeTableStatsProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testProcedureOnEmptyTable() throws NoSuchTableException, ParseException {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     List<Object[]> result =\n@@ -56,7 +53,7 @@ public void testProcedureOnEmptyTable() throws NoSuchTableException, ParseExcept\n     assertThat(result).isEmpty();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testProcedureWithNamedArgs() throws NoSuchTableException, ParseException {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg PARTITIONED BY (data)\",\n@@ -72,7 +69,7 @@ public void testProcedureWithNamedArgs() throws NoSuchTableException, ParseExcep\n     verifyTableStats(tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testProcedureWithPositionalArgs() throws NoSuchTableException, ParseException {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg PARTITIONED BY (data)\",\n@@ -90,7 +87,7 @@ public void testProcedureWithPositionalArgs() throws NoSuchTableException, Parse\n     verifyTableStats(tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testProcedureWithInvalidColumns() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg PARTITIONED BY (data)\",\n@@ -105,7 +102,7 @@ public void testProcedureWithInvalidColumns() {\n         .hasMessageContaining(\"Can't find column id1\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testProcedureWithInvalidSnapshot() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg PARTITIONED BY (data)\",\n@@ -119,7 +116,7 @@ public void testProcedureWithInvalidSnapshot() {\n         .hasMessageContaining(\"Snapshot not found\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testProcedureWithInvalidTable() {\n     assertThatThrownBy(\n             () ->\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java\nindex 6db353b99f40..4d5b569d24cd 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java\n@@ -23,30 +23,27 @@\n \n import java.util.Arrays;\n import java.util.List;\n-import java.util.Map;\n import java.util.stream.Collectors;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.SnapshotRef;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.spark.sql.AnalysisException;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n-\n-public class TestFastForwardBranchProcedure extends SparkExtensionsTestBase {\n-  public TestFastForwardBranchProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestFastForwardBranchProcedure extends ExtensionsTestBase {\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFastForwardBranchUsingPositionalArgs() {\n     sql(\"CREATE TABLE %s (id int NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -97,7 +94,7 @@ public void testFastForwardBranchUsingPositionalArgs() {\n         sql(\"SELECT * FROM %s order by id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFastForwardBranchUsingNamedArgs() {\n     sql(\"CREATE TABLE %s (id int NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -130,7 +127,7 @@ public void testFastForwardBranchUsingNamedArgs() {\n         sql(\"SELECT * FROM %s order by id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFastForwardWhenTargetIsNotAncestorFails() {\n     sql(\"CREATE TABLE %s (id int NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -164,7 +161,7 @@ public void testFastForwardWhenTargetIsNotAncestorFails() {\n         .hasMessage(\"Cannot fast-forward: main is not an ancestor of testBranch\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidFastForwardBranchCases() {\n     assertThatThrownBy(\n             () ->\n@@ -182,8 +179,8 @@ public void testInvalidFastForwardBranchCases() {\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n-              Assert.assertEquals(\"PARSE_SYNTAX_ERROR\", parseException.getErrorClass());\n-              Assert.assertEquals(\"'CALL'\", parseException.getMessageParameters().get(\"error\"));\n+              assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n+              assertThat(parseException.getMessageParameters()).containsEntry(\"error\", \"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.fast_forward('test_table', 'main')\", catalogName))\n@@ -195,4 +192,73 @@ public void testInvalidFastForwardBranchCases() {\n         .isInstanceOf(IllegalArgumentException.class)\n         .hasMessage(\"Cannot handle an empty identifier for argument table\");\n   }\n+\n+  @TestTemplate\n+  public void testFastForwardNonExistingToRefFails() {\n+    sql(\"CREATE TABLE %s (id int NOT NULL, data string) USING iceberg\", tableName);\n+    assertThatThrownBy(\n+            () ->\n+                sql(\n+                    \"CALL %s.system.fast_forward(table => '%s', branch => '%s', to => '%s')\",\n+                    catalogName, tableIdent, SnapshotRef.MAIN_BRANCH, \"non_existing_branch\"))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\"Ref does not exist: non_existing_branch\");\n+  }\n+\n+  @TestTemplate\n+  public void testFastForwardNonMain() {\n+    sql(\"CREATE TABLE %s (id int NOT NULL, data string) USING iceberg\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    table.refresh();\n+\n+    String branch1 = \"branch1\";\n+    sql(\"ALTER TABLE %s CREATE BRANCH %s\", tableName, branch1);\n+    String tableNameWithBranch1 = String.format(\"%s.branch_%s\", tableName, branch1);\n+    sql(\"INSERT INTO TABLE %s VALUES (2, 'b')\", tableNameWithBranch1);\n+    table.refresh();\n+    Snapshot branch1Snapshot = table.snapshot(branch1);\n+\n+    // Create branch2 from branch1\n+    String branch2 = \"branch2\";\n+    sql(\n+        \"ALTER TABLE %s CREATE BRANCH %s AS OF VERSION %d\",\n+        tableName, branch2, branch1Snapshot.snapshotId());\n+    String tableNameWithBranch2 = String.format(\"%s.branch_%s\", tableName, branch2);\n+    sql(\"INSERT INTO TABLE %s VALUES (3, 'c')\", tableNameWithBranch2);\n+    table.refresh();\n+    Snapshot branch2Snapshot = table.snapshot(branch2);\n+    assertThat(\n+            sql(\n+                \"CALL %s.system.fast_forward('%s', '%s', '%s')\",\n+                catalogName, tableIdent, branch1, branch2))\n+        .containsExactly(row(branch1, branch1Snapshot.snapshotId(), branch2Snapshot.snapshotId()));\n+  }\n+\n+  @TestTemplate\n+  public void testFastForwardNonExistingFromMainCreatesBranch() {\n+    sql(\"CREATE TABLE %s (id int NOT NULL, data string) USING iceberg\", tableName);\n+    String branch1 = \"branch1\";\n+    sql(\"ALTER TABLE %s CREATE BRANCH %s\", tableName, branch1);\n+    String branchIdentifier = String.format(\"%s.branch_%s\", tableName, branch1);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", branchIdentifier);\n+    sql(\"INSERT INTO TABLE %s VALUES (2, 'b')\", branchIdentifier);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    table.refresh();\n+    Snapshot branch1Snapshot = table.snapshot(branch1);\n+\n+    assertThat(\n+            sql(\n+                \"CALL %s.system.fast_forward('%s', '%s', '%s')\",\n+                catalogName, tableIdent, SnapshotRef.MAIN_BRANCH, branch1))\n+        .containsExactly(row(SnapshotRef.MAIN_BRANCH, null, branch1Snapshot.snapshotId()));\n+\n+    // Ensure the same behavior for non-main branches\n+    String branch2 = \"branch2\";\n+    assertThat(\n+            sql(\n+                \"CALL %s.system.fast_forward('%s', '%s', '%s')\",\n+                catalogName, tableIdent, branch2, branch1))\n+        .containsExactly(row(branch2, null, branch1Snapshot.snapshotId()));\n+  }\n }\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java\nindex 1a54d8326220..ed20eaa4d54a 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java\n@@ -19,10 +19,11 @@\n package org.apache.iceberg.spark.extensions;\n \n import static org.apache.iceberg.TableProperties.WRITE_AUDIT_PUBLISH_ENABLED;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.exceptions.ValidationException;\n@@ -32,23 +33,19 @@\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestPublishChangesProcedure extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestPublishChangesProcedure extends ExtensionsTestBase {\n \n-  public TestPublishChangesProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testApplyWapChangesUsingPositionalArgs() {\n     String wapId = \"wap_id_1\";\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n@@ -84,7 +81,7 @@ public void testApplyWapChangesUsingPositionalArgs() {\n         sql(\"SELECT * FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testApplyWapChangesUsingNamedArgs() {\n     String wapId = \"wap_id_1\";\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n@@ -122,7 +119,7 @@ public void testApplyWapChangesUsingNamedArgs() {\n         sql(\"SELECT * FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testApplyWapChangesRefreshesRelationCache() {\n     String wapId = \"wap_id_1\";\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n@@ -154,7 +151,7 @@ public void testApplyWapChangesRefreshesRelationCache() {\n     sql(\"UNCACHE TABLE tmp\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testApplyInvalidWapId() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -164,7 +161,7 @@ public void testApplyInvalidWapId() {\n         .hasMessage(\"Cannot apply unknown WAP ID 'not_valid'\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidApplyWapChangesCases() {\n     assertThatThrownBy(\n             () ->\n@@ -179,8 +176,8 @@ public void testInvalidApplyWapChangesCases() {\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n-              Assert.assertEquals(\"PARSE_SYNTAX_ERROR\", parseException.getErrorClass());\n-              Assert.assertEquals(\"'CALL'\", parseException.getMessageParameters().get(\"error\"));\n+              assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n+              assertThat(parseException.getMessageParameters()).containsEntry(\"error\", \"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.publish_changes('t')\", catalogName))\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRegisterTableProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRegisterTableProcedure.java\nindex 3bc971798134..a06a67b7d612 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRegisterTableProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRegisterTableProcedure.java\n@@ -18,8 +18,11 @@\n  */\n package org.apache.iceberg.spark.extensions;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.atIndex;\n+\n import java.util.List;\n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableUtil;\n import org.apache.iceberg.spark.Spark3Util;\n@@ -27,31 +30,28 @@\n import org.apache.spark.sql.catalyst.parser.ParseException;\n import org.apache.spark.sql.functions;\n import org.apache.spark.sql.types.DataTypes;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestRegisterTableProcedure extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRegisterTableProcedure extends ExtensionsTestBase {\n \n-  private final String targetName;\n+  private String targetName;\n \n-  public TestRegisterTableProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n+  @BeforeEach\n+  public void setTargetName() {\n     targetName = tableName(\"register_table\");\n   }\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n-\n-  @After\n+  @AfterEach\n   public void dropTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS %s\", targetName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRegisterTable() throws NoSuchTableException, ParseException {\n     long numRows = 1000;\n \n@@ -69,16 +69,17 @@ public void testRegisterTable() throws NoSuchTableException, ParseException {\n \n     List<Object[]> result =\n         sql(\"CALL %s.system.register_table('%s', '%s')\", catalogName, targetName, metadataJson);\n-    Assert.assertEquals(\"Current Snapshot is not correct\", currentSnapshotId, result.get(0)[0]);\n+    assertThat(result.get(0))\n+        .as(\"Current Snapshot is not correct\")\n+        .contains(currentSnapshotId, atIndex(0));\n \n     List<Object[]> original = sql(\"SELECT * FROM %s\", tableName);\n     List<Object[]> registered = sql(\"SELECT * FROM %s\", targetName);\n     assertEquals(\"Registered table rows should match original table rows\", original, registered);\n-    Assert.assertEquals(\n-        \"Should have the right row count in the procedure result\", numRows, result.get(0)[1]);\n-    Assert.assertEquals(\n-        \"Should have the right datafile count in the procedure result\",\n-        originalFileCount,\n-        result.get(0)[2]);\n+    assertThat(result.get(0))\n+        .as(\"Should have the right row count in the procedure result\")\n+        .contains(numRows, atIndex(1))\n+        .as(\"Should have the right datafile count in the procedure result\")\n+        .contains(originalFileCount, atIndex(2));\n   }\n }\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestReplaceBranch.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestReplaceBranch.java\nindex 443f299a49b3..3b8f99daf0a4 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestReplaceBranch.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestReplaceBranch.java\n@@ -18,11 +18,13 @@\n  */\n package org.apache.iceberg.spark.extensions;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n-import java.util.Map;\n import java.util.concurrent.TimeUnit;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.SnapshotRef;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n@@ -31,16 +33,16 @@\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestReplaceBranch extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestReplaceBranch extends ExtensionsTestBase {\n \n   private static final String[] TIME_UNITS = {\"DAYS\", \"HOURS\", \"MINUTES\"};\n \n-  @Parameterized.Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n       {\n@@ -51,16 +53,12 @@ public static Object[][] parameters() {\n     };\n   }\n \n-  public TestReplaceBranch(String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplaceBranchFailsForTag() throws NoSuchTableException {\n     sql(\"CREATE TABLE %s (id INT, data STRING) USING iceberg\", tableName);\n     String tagName = \"tag1\";\n@@ -84,7 +82,7 @@ public void testReplaceBranchFailsForTag() throws NoSuchTableException {\n         .hasMessage(\"Ref tag1 is a tag not a branch\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplaceBranch() throws NoSuchTableException {\n     sql(\"CREATE TABLE %s (id INT, data STRING) USING iceberg\", tableName);\n     List<SimpleRecord> records =\n@@ -113,14 +111,14 @@ public void testReplaceBranch() throws NoSuchTableException {\n \n     table.refresh();\n     SnapshotRef ref = table.refs().get(branchName);\n-    Assert.assertNotNull(ref);\n-    Assert.assertEquals(second, ref.snapshotId());\n-    Assert.assertEquals(expectedMinSnapshotsToKeep, ref.minSnapshotsToKeep().intValue());\n-    Assert.assertEquals(expectedMaxSnapshotAgeMs, ref.maxSnapshotAgeMs().longValue());\n-    Assert.assertEquals(expectedMaxRefAgeMs, ref.maxRefAgeMs().longValue());\n+    assertThat(ref).isNotNull();\n+    assertThat(ref.snapshotId()).isEqualTo(second);\n+    assertThat(ref.minSnapshotsToKeep().intValue()).isEqualTo(expectedMinSnapshotsToKeep);\n+    assertThat(ref.maxSnapshotAgeMs().longValue()).isEqualTo(expectedMaxSnapshotAgeMs);\n+    assertThat(ref.maxRefAgeMs().longValue()).isEqualTo(expectedMaxRefAgeMs);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplaceBranchDoesNotExist() throws NoSuchTableException {\n     sql(\"CREATE TABLE %s (id INT, data STRING) USING iceberg\", tableName);\n     List<SimpleRecord> records =\n@@ -138,7 +136,7 @@ public void testReplaceBranchDoesNotExist() throws NoSuchTableException {\n         .hasMessage(\"Branch does not exist: someBranch\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplaceBranchWithRetain() throws NoSuchTableException {\n     sql(\"CREATE TABLE %s (id INT, data STRING) USING iceberg\", tableName);\n     List<SimpleRecord> records =\n@@ -164,16 +162,16 @@ public void testReplaceBranchWithRetain() throws NoSuchTableException {\n \n       table.refresh();\n       SnapshotRef ref = table.refs().get(branchName);\n-      Assert.assertNotNull(ref);\n-      Assert.assertEquals(second, ref.snapshotId());\n-      Assert.assertEquals(minSnapshotsToKeep, ref.minSnapshotsToKeep());\n-      Assert.assertEquals(maxSnapshotAgeMs, ref.maxSnapshotAgeMs());\n-      Assert.assertEquals(\n-          TimeUnit.valueOf(timeUnit).toMillis(maxRefAge), ref.maxRefAgeMs().longValue());\n+      assertThat(ref).isNotNull();\n+      assertThat(ref.snapshotId()).isEqualTo(second);\n+      assertThat(ref.minSnapshotsToKeep()).isNull();\n+      assertThat(ref.maxSnapshotAgeMs()).isNull();\n+      assertThat(ref.maxRefAgeMs().longValue())\n+          .isEqualTo(TimeUnit.valueOf(timeUnit).toMillis(maxRefAge));\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplaceBranchWithSnapshotRetention() throws NoSuchTableException {\n     sql(\"CREATE TABLE %s (id INT, data STRING) USING iceberg\", tableName);\n     List<SimpleRecord> records =\n@@ -197,16 +195,16 @@ public void testReplaceBranchWithSnapshotRetention() throws NoSuchTableException\n \n       table.refresh();\n       SnapshotRef ref = table.refs().get(branchName);\n-      Assert.assertNotNull(ref);\n-      Assert.assertEquals(second, ref.snapshotId());\n-      Assert.assertEquals(minSnapshotsToKeep, ref.minSnapshotsToKeep());\n-      Assert.assertEquals(\n-          TimeUnit.valueOf(timeUnit).toMillis(maxSnapshotAge), ref.maxSnapshotAgeMs().longValue());\n-      Assert.assertEquals(maxRefAgeMs, ref.maxRefAgeMs());\n+      assertThat(ref).isNotNull();\n+      assertThat(ref.snapshotId()).isEqualTo(second);\n+      assertThat(ref.minSnapshotsToKeep()).isEqualTo(minSnapshotsToKeep);\n+      assertThat(ref.maxSnapshotAgeMs().longValue())\n+          .isEqualTo(TimeUnit.valueOf(timeUnit).toMillis(maxSnapshotAge));\n+      assertThat(ref.maxRefAgeMs()).isEqualTo(maxRefAgeMs);\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplaceBranchWithRetainAndSnapshotRetention() throws NoSuchTableException {\n     sql(\"CREATE TABLE %s (id INT, data STRING) USING iceberg\", tableName);\n     List<SimpleRecord> records =\n@@ -238,17 +236,17 @@ public void testReplaceBranchWithRetainAndSnapshotRetention() throws NoSuchTable\n \n       table.refresh();\n       SnapshotRef ref = table.refs().get(branchName);\n-      Assert.assertNotNull(ref);\n-      Assert.assertEquals(second, ref.snapshotId());\n-      Assert.assertEquals(minSnapshotsToKeep, ref.minSnapshotsToKeep());\n-      Assert.assertEquals(\n-          TimeUnit.valueOf(timeUnit).toMillis(maxSnapshotAge), ref.maxSnapshotAgeMs().longValue());\n-      Assert.assertEquals(\n-          TimeUnit.valueOf(timeUnit).toMillis(maxRefAge), ref.maxRefAgeMs().longValue());\n+      assertThat(ref).isNotNull();\n+      assertThat(ref.snapshotId()).isEqualTo(second);\n+      assertThat(ref.minSnapshotsToKeep()).isEqualTo(minSnapshotsToKeep);\n+      assertThat(ref.maxSnapshotAgeMs().longValue())\n+          .isEqualTo(TimeUnit.valueOf(timeUnit).toMillis(maxSnapshotAge));\n+      assertThat(ref.maxRefAgeMs().longValue())\n+          .isEqualTo(TimeUnit.valueOf(timeUnit).toMillis(maxRefAge));\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateOrReplace() throws NoSuchTableException {\n     sql(\"CREATE TABLE %s (id INT, data STRING) USING iceberg\", tableName);\n     List<SimpleRecord> records =\n@@ -269,7 +267,7 @@ public void testCreateOrReplace() throws NoSuchTableException {\n \n     table.refresh();\n     SnapshotRef ref = table.refs().get(branchName);\n-    Assert.assertNotNull(ref);\n-    Assert.assertEquals(first, ref.snapshotId());\n+    assertThat(ref).isNotNull();\n+    assertThat(ref.snapshotId()).isEqualTo(first);\n   }\n }\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestTagDDL.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestTagDDL.java\nindex e70326c39eae..f8711cd7eeab 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestTagDDL.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestTagDDL.java\n@@ -23,8 +23,9 @@\n \n import java.util.List;\n import java.util.Locale;\n-import java.util.Map;\n import java.util.concurrent.TimeUnit;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.SnapshotRef;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.exceptions.ValidationException;\n@@ -35,16 +36,16 @@\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.catalyst.parser.extensions.IcebergParseException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestTagDDL extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestTagDDL extends ExtensionsTestBase {\n   private static final String[] TIME_UNITS = {\"DAYS\", \"HOURS\", \"MINUTES\"};\n \n-  @Parameterized.Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n       {\n@@ -55,21 +56,17 @@ public static Object[][] parameters() {\n     };\n   }\n \n-  public TestTagDDL(String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Before\n-  public void before() {\n+  @BeforeEach\n+  public void createTable() {\n     sql(\"CREATE TABLE %s (id INT, data STRING) USING iceberg\", tableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateTagWithRetain() throws NoSuchTableException {\n     Table table = insertRows();\n     long firstSnapshotId = table.currentSnapshot().snapshotId();\n@@ -87,12 +84,12 @@ public void testCreateTagWithRetain() throws NoSuchTableException {\n           tableName, tagName, firstSnapshotId, maxRefAge, timeUnit);\n       table.refresh();\n       SnapshotRef ref = table.refs().get(tagName);\n-      Assert.assertEquals(\n-          \"The tag needs to point to a specific snapshot id.\", firstSnapshotId, ref.snapshotId());\n-      Assert.assertEquals(\n-          \"The tag needs to have the correct max ref age.\",\n-          TimeUnit.valueOf(timeUnit.toUpperCase(Locale.ENGLISH)).toMillis(maxRefAge),\n-          ref.maxRefAgeMs().longValue());\n+      assertThat(ref.snapshotId())\n+          .as(\"The tag needs to point to a specific snapshot id.\")\n+          .isEqualTo(firstSnapshotId);\n+      assertThat(ref.maxRefAgeMs().longValue())\n+          .as(\"The tag needs to have the correct max ref age.\")\n+          .isEqualTo(TimeUnit.valueOf(timeUnit.toUpperCase(Locale.ENGLISH)).toMillis(maxRefAge));\n     }\n \n     String tagName = \"t1\";\n@@ -118,7 +115,7 @@ public void testCreateTagWithRetain() throws NoSuchTableException {\n         .hasMessageContaining(\"mismatched input 'SECONDS' expecting {'DAYS', 'HOURS', 'MINUTES'}\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateTagOnEmptyTable() {\n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s CREATE TAG %s\", tableName, \"abc\"))\n         .isInstanceOf(IllegalArgumentException.class)\n@@ -127,7 +124,7 @@ public void testCreateTagOnEmptyTable() {\n             tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateTagUseDefaultConfig() throws NoSuchTableException {\n     Table table = insertRows();\n     long snapshotId = table.currentSnapshot().snapshotId();\n@@ -141,10 +138,12 @@ public void testCreateTagUseDefaultConfig() throws NoSuchTableException {\n     sql(\"ALTER TABLE %s CREATE TAG %s\", tableName, tagName);\n     table.refresh();\n     SnapshotRef ref = table.refs().get(tagName);\n-    Assert.assertEquals(\n-        \"The tag needs to point to a specific snapshot id.\", snapshotId, ref.snapshotId());\n-    Assert.assertNull(\n-        \"The tag needs to have the default max ref age, which is null.\", ref.maxRefAgeMs());\n+    assertThat(ref.snapshotId())\n+        .as(\"The tag needs to point to a specific snapshot id.\")\n+        .isEqualTo(snapshotId);\n+    assertThat(ref.maxRefAgeMs())\n+        .as(\"The tag needs to have the default max ref age, which is null.\")\n+        .isNull();\n \n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s CREATE TAG %s\", tableName, tagName))\n         .isInstanceOf(IllegalArgumentException.class)\n@@ -163,13 +162,15 @@ public void testCreateTagUseDefaultConfig() throws NoSuchTableException {\n     sql(\"ALTER TABLE %s CREATE TAG %s AS OF VERSION %d\", tableName, tagName, snapshotId);\n     table.refresh();\n     ref = table.refs().get(tagName);\n-    Assert.assertEquals(\n-        \"The tag needs to point to a specific snapshot id.\", snapshotId, ref.snapshotId());\n-    Assert.assertNull(\n-        \"The tag needs to have the default max ref age, which is null.\", ref.maxRefAgeMs());\n+    assertThat(ref.snapshotId())\n+        .as(\"The tag needs to point to a specific snapshot id.\")\n+        .isEqualTo(snapshotId);\n+    assertThat(ref.maxRefAgeMs())\n+        .as(\"The tag needs to have the default max ref age, which is null.\")\n+        .isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateTagIfNotExists() throws NoSuchTableException {\n     long maxSnapshotAge = 2L;\n     Table table = insertRows();\n@@ -179,17 +180,15 @@ public void testCreateTagIfNotExists() throws NoSuchTableException {\n \n     table.refresh();\n     SnapshotRef ref = table.refs().get(tagName);\n-    Assert.assertEquals(\n-        \"The tag needs to point to a specific snapshot id.\",\n-        table.currentSnapshot().snapshotId(),\n-        ref.snapshotId());\n-    Assert.assertEquals(\n-        \"The tag needs to have the correct max ref age.\",\n-        TimeUnit.DAYS.toMillis(maxSnapshotAge),\n-        ref.maxRefAgeMs().longValue());\n+    assertThat(ref.snapshotId())\n+        .as(\"The tag needs to point to a specific snapshot id.\")\n+        .isEqualTo(table.currentSnapshot().snapshotId());\n+    assertThat(ref.maxRefAgeMs().longValue())\n+        .as(\"The tag needs to have the correct max ref age.\")\n+        .isEqualTo(TimeUnit.DAYS.toMillis(maxSnapshotAge));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplaceTagFailsForBranch() throws NoSuchTableException {\n     String branchName = \"branch1\";\n     Table table = insertRows();\n@@ -203,7 +202,7 @@ public void testReplaceTagFailsForBranch() throws NoSuchTableException {\n         .hasMessageContaining(\"Ref branch1 is a branch not a tag\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplaceTag() throws NoSuchTableException {\n     Table table = insertRows();\n     long first = table.currentSnapshot().snapshotId();\n@@ -221,15 +220,15 @@ public void testReplaceTag() throws NoSuchTableException {\n     sql(\"ALTER TABLE %s REPLACE Tag %s AS OF VERSION %d\", tableName, tagName, second);\n     table.refresh();\n     SnapshotRef ref = table.refs().get(tagName);\n-    Assert.assertEquals(\n-        \"The tag needs to point to a specific snapshot id.\", second, ref.snapshotId());\n-    Assert.assertEquals(\n-        \"The tag needs to have the correct max ref age.\",\n-        expectedMaxRefAgeMs,\n-        ref.maxRefAgeMs().longValue());\n+    assertThat(ref.snapshotId())\n+        .as(\"The tag needs to point to a specific snapshot id.\")\n+        .isEqualTo(second);\n+    assertThat(ref.maxRefAgeMs().longValue())\n+        .as(\"The tag needs to have the correct max ref age.\")\n+        .isEqualTo(expectedMaxRefAgeMs);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplaceTagDoesNotExist() throws NoSuchTableException {\n     Table table = insertRows();\n \n@@ -242,7 +241,7 @@ public void testReplaceTagDoesNotExist() throws NoSuchTableException {\n         .hasMessageContaining(\"Tag does not exist\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplaceTagWithRetain() throws NoSuchTableException {\n     Table table = insertRows();\n     long first = table.currentSnapshot().snapshotId();\n@@ -259,16 +258,16 @@ public void testReplaceTagWithRetain() throws NoSuchTableException {\n \n       table.refresh();\n       SnapshotRef ref = table.refs().get(tagName);\n-      Assert.assertEquals(\n-          \"The tag needs to point to a specific snapshot id.\", second, ref.snapshotId());\n-      Assert.assertEquals(\n-          \"The tag needs to have the correct max ref age.\",\n-          TimeUnit.valueOf(timeUnit).toMillis(maxRefAge),\n-          ref.maxRefAgeMs().longValue());\n+      assertThat(ref.snapshotId())\n+          .as(\"The tag needs to point to a specific snapshot id.\")\n+          .isEqualTo(second);\n+      assertThat(ref.maxRefAgeMs().longValue())\n+          .as(\"The tag needs to have the correct max ref age.\")\n+          .isEqualTo(TimeUnit.valueOf(timeUnit).toMillis(maxRefAge));\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCreateOrReplace() throws NoSuchTableException {\n     Table table = insertRows();\n     long first = table.currentSnapshot().snapshotId();\n@@ -280,43 +279,40 @@ public void testCreateOrReplace() throws NoSuchTableException {\n     sql(\"ALTER TABLE %s CREATE OR REPLACE TAG %s AS OF VERSION %d\", tableName, tagName, first);\n     table.refresh();\n     SnapshotRef ref = table.refs().get(tagName);\n-    Assert.assertEquals(\n-        \"The tag needs to point to a specific snapshot id.\", first, ref.snapshotId());\n+    assertThat(ref.snapshotId())\n+        .as(\"The tag needs to point to a specific snapshot id.\")\n+        .isEqualTo(first);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropTag() throws NoSuchTableException {\n     insertRows();\n     Table table = validationCatalog.loadTable(tableIdent);\n     String tagName = \"t1\";\n     table.manageSnapshots().createTag(tagName, table.currentSnapshot().snapshotId()).commit();\n     SnapshotRef ref = table.refs().get(tagName);\n-    Assert.assertEquals(\n-        \"The tag needs to point to a specific snapshot id.\",\n-        table.currentSnapshot().snapshotId(),\n-        ref.snapshotId());\n+    assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n \n     sql(\"ALTER TABLE %s DROP TAG %s\", tableName, tagName);\n     table.refresh();\n-    ref = table.refs().get(tagName);\n-    Assert.assertNull(\"The tag needs to be dropped.\", ref);\n+    assertThat(table.refs()).doesNotContainKey(tagName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropTagNonConformingName() {\n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s DROP TAG %s\", tableName, \"123\"))\n         .isInstanceOf(IcebergParseException.class)\n         .hasMessageContaining(\"no viable alternative at input '123'\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropTagDoesNotExist() {\n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s DROP TAG %s\", tableName, \"nonExistingTag\"))\n         .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"Tag does not exist: nonExistingTag\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropTagFailesForBranch() throws NoSuchTableException {\n     String branchName = \"b1\";\n     Table table = insertRows();\n@@ -327,28 +323,27 @@ public void testDropTagFailesForBranch() throws NoSuchTableException {\n         .hasMessageContaining(\"Ref b1 is a branch not a tag\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropTagIfExists() throws NoSuchTableException {\n     String tagName = \"nonExistingTag\";\n     Table table = insertRows();\n-    Assert.assertNull(\"The tag does not exists.\", table.refs().get(tagName));\n+    assertThat(table.refs()).doesNotContainKey(tagName);\n \n     sql(\"ALTER TABLE %s DROP TAG IF EXISTS %s\", tableName, tagName);\n     table.refresh();\n-    Assert.assertNull(\"The tag still does not exist.\", table.refs().get(tagName));\n+    assertThat(table.refs()).doesNotContainKey(tagName);\n \n     table.manageSnapshots().createTag(tagName, table.currentSnapshot().snapshotId()).commit();\n-    Assert.assertEquals(\n-        \"The tag has been created successfully.\",\n-        table.currentSnapshot().snapshotId(),\n-        table.refs().get(tagName).snapshotId());\n+    assertThat(table.refs().get(tagName).snapshotId())\n+        .as(\"The tag has been created successfully.\")\n+        .isEqualTo(table.currentSnapshot().snapshotId());\n \n     sql(\"ALTER TABLE %s DROP TAG IF EXISTS %s\", tableName, tagName);\n     table.refresh();\n-    Assert.assertNull(\"The tag needs to be dropped.\", table.refs().get(tagName));\n+    assertThat(table.refs()).doesNotContainKey(tagName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void createOrReplaceWithNonExistingTag() throws NoSuchTableException {\n     Table table = insertRows();\n     String tagName = \"t1\";\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTablePartitionFields.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTablePartitionFields.java\nindex 38e5c942c9ff..dd49d8a254c3 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTablePartitionFields.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTablePartitionFields.java\n@@ -507,7 +507,7 @@ public void testReplaceNamedPartitionAndRenameDifferently() {\n   @TestTemplate\n   public void testSparkTableAddDropPartitions() throws Exception {\n     createTable(\"id bigint NOT NULL, ts timestamp, data string\");\n-    assertThat(sparkTable().partitioning()).as(\"spark table partition should be empty\").hasSize(0);\n+    assertThat(sparkTable().partitioning()).as(\"spark table partition should be empty\").isEmpty();\n \n     sql(\"ALTER TABLE %s ADD PARTITION FIELD bucket(16, id) AS shard\", tableName);\n     assertPartitioningEquals(sparkTable(), 1, \"bucket(16, id)\");\n@@ -526,7 +526,7 @@ public void testSparkTableAddDropPartitions() throws Exception {\n \n     sql(\"ALTER TABLE %s DROP PARTITION FIELD shard\", tableName);\n     sql(\"DESCRIBE %s\", tableName);\n-    assertThat(sparkTable().partitioning()).as(\"spark table partition should be empty\").hasSize(0);\n+    assertThat(sparkTable().partitioning()).as(\"spark table partition should be empty\").isEmpty();\n   }\n \n   @TestTemplate\n@@ -554,7 +554,8 @@ public void testDropColumnOfOldPartitionFieldV2() {\n \n   private void assertPartitioningEquals(SparkTable table, int len, String transform) {\n     assertThat(table.partitioning()).as(\"spark table partition should be \" + len).hasSize(len);\n-    assertThat(table.partitioning()[len - 1].toString())\n+    assertThat(table.partitioning()[len - 1])\n+        .asString()\n         .as(\"latest spark table partition transform should match\")\n         .isEqualTo(transform);\n   }\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTableSchema.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTableSchema.java\nindex 71c85b135859..f36a2e4470e3 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTableSchema.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAlterTableSchema.java\n@@ -23,7 +23,6 @@\n \n import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n-import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n@@ -50,23 +49,21 @@ public void testSetIdentifierFields() {\n     sql(\"ALTER TABLE %s SET IDENTIFIER FIELDS id\", tableName);\n     table.refresh();\n     assertThat(table.schema().identifierFieldIds())\n-        .as(\"Should have new identifier field\")\n-        .isEqualTo(Sets.newHashSet(table.schema().findField(\"id\").fieldId()));\n+        .containsExactly(table.schema().findField(\"id\").fieldId());\n \n     sql(\"ALTER TABLE %s SET IDENTIFIER FIELDS id, location.lon\", tableName);\n     table.refresh();\n     assertThat(table.schema().identifierFieldIds())\n         .as(\"Should have new identifier field\")\n-        .isEqualTo(\n-            Sets.newHashSet(\n-                table.schema().findField(\"id\").fieldId(),\n-                table.schema().findField(\"location.lon\").fieldId()));\n+        .containsExactlyInAnyOrder(\n+            table.schema().findField(\"id\").fieldId(),\n+            table.schema().findField(\"location.lon\").fieldId());\n \n     sql(\"ALTER TABLE %s SET IDENTIFIER FIELDS location.lon\", tableName);\n     table.refresh();\n     assertThat(table.schema().identifierFieldIds())\n         .as(\"Should have new identifier field\")\n-        .isEqualTo(Sets.newHashSet(table.schema().findField(\"location.lon\").fieldId()));\n+        .containsExactly(table.schema().findField(\"location.lon\").fieldId());\n   }\n \n   @TestTemplate\n@@ -100,31 +97,27 @@ public void testDropIdentifierFields() {\n     table.refresh();\n     assertThat(table.schema().identifierFieldIds())\n         .as(\"Should have new identifier fields\")\n-        .isEqualTo(\n-            Sets.newHashSet(\n-                table.schema().findField(\"id\").fieldId(),\n-                table.schema().findField(\"location.lon\").fieldId()));\n+        .containsExactlyInAnyOrder(\n+            table.schema().findField(\"id\").fieldId(),\n+            table.schema().findField(\"location.lon\").fieldId());\n \n     sql(\"ALTER TABLE %s DROP IDENTIFIER FIELDS id\", tableName);\n     table.refresh();\n     assertThat(table.schema().identifierFieldIds())\n         .as(\"Should removed identifier field\")\n-        .isEqualTo(Sets.newHashSet(table.schema().findField(\"location.lon\").fieldId()));\n+        .containsExactly(table.schema().findField(\"location.lon\").fieldId());\n \n     sql(\"ALTER TABLE %s SET IDENTIFIER FIELDS id, location.lon\", tableName);\n     table.refresh();\n     assertThat(table.schema().identifierFieldIds())\n         .as(\"Should have new identifier fields\")\n-        .isEqualTo(\n-            Sets.newHashSet(\n-                table.schema().findField(\"id\").fieldId(),\n-                table.schema().findField(\"location.lon\").fieldId()));\n+        .containsExactlyInAnyOrder(\n+            table.schema().findField(\"id\").fieldId(),\n+            table.schema().findField(\"location.lon\").fieldId());\n \n     sql(\"ALTER TABLE %s DROP IDENTIFIER FIELDS id, location.lon\", tableName);\n     table.refresh();\n-    assertThat(table.schema().identifierFieldIds())\n-        .as(\"Should have no identifier field\")\n-        .isEqualTo(Sets.newHashSet());\n+    assertThat(table.schema().identifierFieldIds()).as(\"Should have no identifier field\").isEmpty();\n   }\n \n   @TestTemplate\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestBranchDDL.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestBranchDDL.java\nindex 6bad39dc5818..f70f2d819013 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestBranchDDL.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestBranchDDL.java\n@@ -76,13 +76,17 @@ public void testCreateBranch() throws NoSuchTableException {\n         \"ALTER TABLE %s CREATE BRANCH %s AS OF VERSION %d RETAIN %d DAYS WITH SNAPSHOT RETENTION %d SNAPSHOTS %d days\",\n         tableName, branchName, snapshotId, maxRefAge, minSnapshotsToKeep, maxSnapshotAge);\n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n-    assertThat(ref.minSnapshotsToKeep()).isEqualTo(minSnapshotsToKeep);\n-    assertThat(ref.maxSnapshotAgeMs().longValue())\n-        .isEqualTo(TimeUnit.DAYS.toMillis(maxSnapshotAge));\n-    assertThat(ref.maxRefAgeMs().longValue()).isEqualTo(TimeUnit.DAYS.toMillis(maxRefAge));\n-\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+              assertThat(ref.minSnapshotsToKeep()).isEqualTo(minSnapshotsToKeep);\n+              assertThat(ref.maxSnapshotAgeMs().longValue())\n+                  .isEqualTo(TimeUnit.DAYS.toMillis(maxSnapshotAge));\n+              assertThat(ref.maxRefAgeMs().longValue())\n+                  .isEqualTo(TimeUnit.DAYS.toMillis(maxRefAge));\n+            });\n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s CREATE BRANCH %s\", tableName, branchName))\n         .isInstanceOf(IllegalArgumentException.class)\n         .hasMessage(\"Ref b1 already exists\");\n@@ -94,21 +98,22 @@ public void testCreateBranchOnEmptyTable() {\n     sql(\"ALTER TABLE %s CREATE BRANCH %s\", tableName, \"b1\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    SnapshotRef mainRef = table.refs().get(SnapshotRef.MAIN_BRANCH);\n-    assertThat(mainRef).isNull();\n-\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref).isNotNull();\n-    assertThat(ref.minSnapshotsToKeep()).isNull();\n-    assertThat(ref.maxSnapshotAgeMs()).isNull();\n-    assertThat(ref.maxRefAgeMs()).isNull();\n-\n-    Snapshot snapshot = table.snapshot(ref.snapshotId());\n-    assertThat(snapshot.parentId()).isNull();\n-    assertThat(snapshot.addedDataFiles(table.io())).isEmpty();\n-    assertThat(snapshot.removedDataFiles(table.io())).isEmpty();\n-    assertThat(snapshot.addedDeleteFiles(table.io())).isEmpty();\n-    assertThat(snapshot.removedDeleteFiles(table.io())).isEmpty();\n+    assertThat(table.refs())\n+        .doesNotContainKey(SnapshotRef.MAIN_BRANCH)\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.minSnapshotsToKeep()).isNull();\n+              assertThat(ref.maxSnapshotAgeMs()).isNull();\n+              assertThat(ref.maxRefAgeMs()).isNull();\n+\n+              Snapshot snapshot = table.snapshot(ref.snapshotId());\n+              assertThat(snapshot.parentId()).isNull();\n+              assertThat(snapshot.addedDataFiles(table.io())).isEmpty();\n+              assertThat(snapshot.removedDataFiles(table.io())).isEmpty();\n+              assertThat(snapshot.addedDeleteFiles(table.io())).isEmpty();\n+              assertThat(snapshot.removedDeleteFiles(table.io())).isEmpty();\n+            });\n   }\n \n   @TestTemplate\n@@ -117,11 +122,15 @@ public void testCreateBranchUseDefaultConfig() throws NoSuchTableException {\n     String branchName = \"b1\";\n     sql(\"ALTER TABLE %s CREATE BRANCH %s\", tableName, branchName);\n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n-    assertThat(ref.minSnapshotsToKeep()).isNull();\n-    assertThat(ref.maxSnapshotAgeMs()).isNull();\n-    assertThat(ref.maxRefAgeMs()).isNull();\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+              assertThat(ref.minSnapshotsToKeep()).isNull();\n+              assertThat(ref.maxSnapshotAgeMs()).isNull();\n+              assertThat(ref.maxRefAgeMs()).isNull();\n+            });\n   }\n \n   @TestTemplate\n@@ -133,11 +142,15 @@ public void testCreateBranchUseCustomMinSnapshotsToKeep() throws NoSuchTableExce\n         \"ALTER TABLE %s CREATE BRANCH %s WITH SNAPSHOT RETENTION %d SNAPSHOTS\",\n         tableName, branchName, minSnapshotsToKeep);\n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n-    assertThat(ref.minSnapshotsToKeep()).isEqualTo(minSnapshotsToKeep);\n-    assertThat(ref.maxSnapshotAgeMs()).isNull();\n-    assertThat(ref.maxRefAgeMs()).isNull();\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+              assertThat(ref.minSnapshotsToKeep()).isEqualTo(minSnapshotsToKeep);\n+              assertThat(ref.maxSnapshotAgeMs()).isNull();\n+              assertThat(ref.maxRefAgeMs()).isNull();\n+            });\n   }\n \n   @TestTemplate\n@@ -149,12 +162,16 @@ public void testCreateBranchUseCustomMaxSnapshotAge() throws NoSuchTableExceptio\n         \"ALTER TABLE %s CREATE BRANCH %s WITH SNAPSHOT RETENTION %d DAYS\",\n         tableName, branchName, maxSnapshotAge);\n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref).isNotNull();\n-    assertThat(ref.minSnapshotsToKeep()).isNull();\n-    assertThat(ref.maxSnapshotAgeMs().longValue())\n-        .isEqualTo(TimeUnit.DAYS.toMillis(maxSnapshotAge));\n-    assertThat(ref.maxRefAgeMs()).isNull();\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref).isNotNull();\n+              assertThat(ref.minSnapshotsToKeep()).isNull();\n+              assertThat(ref.maxSnapshotAgeMs().longValue())\n+                  .isEqualTo(TimeUnit.DAYS.toMillis(maxSnapshotAge));\n+              assertThat(ref.maxRefAgeMs()).isNull();\n+            });\n   }\n \n   @TestTemplate\n@@ -168,12 +185,16 @@ public void testCreateBranchIfNotExists() throws NoSuchTableException {\n     sql(\"ALTER TABLE %s CREATE BRANCH IF NOT EXISTS %s\", tableName, branchName);\n \n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n-    assertThat(ref.minSnapshotsToKeep()).isNull();\n-    assertThat(ref.maxSnapshotAgeMs().longValue())\n-        .isEqualTo(TimeUnit.DAYS.toMillis(maxSnapshotAge));\n-    assertThat(ref.maxRefAgeMs()).isNull();\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+              assertThat(ref.minSnapshotsToKeep()).isNull();\n+              assertThat(ref.maxSnapshotAgeMs().longValue())\n+                  .isEqualTo(TimeUnit.DAYS.toMillis(maxSnapshotAge));\n+              assertThat(ref.maxRefAgeMs()).isNull();\n+            });\n   }\n \n   @TestTemplate\n@@ -187,12 +208,16 @@ public void testCreateBranchUseCustomMinSnapshotsToKeepAndMaxSnapshotAge()\n         \"ALTER TABLE %s CREATE BRANCH %s WITH SNAPSHOT RETENTION %d SNAPSHOTS %d DAYS\",\n         tableName, branchName, minSnapshotsToKeep, maxSnapshotAge);\n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n-    assertThat(ref.minSnapshotsToKeep()).isEqualTo(minSnapshotsToKeep);\n-    assertThat(ref.maxSnapshotAgeMs().longValue())\n-        .isEqualTo(TimeUnit.DAYS.toMillis(maxSnapshotAge));\n-    assertThat(ref.maxRefAgeMs()).isNull();\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+              assertThat(ref.minSnapshotsToKeep()).isEqualTo(minSnapshotsToKeep);\n+              assertThat(ref.maxSnapshotAgeMs().longValue())\n+                  .isEqualTo(TimeUnit.DAYS.toMillis(maxSnapshotAge));\n+              assertThat(ref.maxRefAgeMs()).isNull();\n+            });\n \n     assertThatThrownBy(\n             () ->\n@@ -210,11 +235,16 @@ public void testCreateBranchUseCustomMaxRefAge() throws NoSuchTableException {\n     String branchName = \"b1\";\n     sql(\"ALTER TABLE %s CREATE BRANCH %s RETAIN %d DAYS\", tableName, branchName, maxRefAge);\n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n-    assertThat(ref.minSnapshotsToKeep()).isNull();\n-    assertThat(ref.maxSnapshotAgeMs()).isNull();\n-    assertThat(ref.maxRefAgeMs().longValue()).isEqualTo(TimeUnit.DAYS.toMillis(maxRefAge));\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+              assertThat(ref.minSnapshotsToKeep()).isNull();\n+              assertThat(ref.maxSnapshotAgeMs()).isNull();\n+              assertThat(ref.maxRefAgeMs().longValue())\n+                  .isEqualTo(TimeUnit.DAYS.toMillis(maxRefAge));\n+            });\n \n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s CREATE BRANCH %s RETAIN\", tableName, branchName))\n         .isInstanceOf(IcebergParseException.class)\n@@ -242,14 +272,17 @@ public void testDropBranch() throws NoSuchTableException {\n     Table table = validationCatalog.loadTable(tableIdent);\n     String branchName = \"b1\";\n     table.manageSnapshots().createBranch(branchName, table.currentSnapshot().snapshotId()).commit();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n+            });\n \n     sql(\"ALTER TABLE %s DROP BRANCH %s\", tableName, branchName);\n     table.refresh();\n \n-    ref = table.refs().get(branchName);\n-    assertThat(ref).isNull();\n+    assertThat(table.refs()).doesNotContainKey(branchName);\n   }\n \n   @TestTemplate\n@@ -288,13 +321,11 @@ public void testDropMainBranchFails() {\n   public void testDropBranchIfExists() {\n     String branchName = \"nonExistingBranch\";\n     Table table = validationCatalog.loadTable(tableIdent);\n-    assertThat(table.refs().get(branchName)).isNull();\n+    assertThat(table.refs()).doesNotContainKey(branchName);\n \n     sql(\"ALTER TABLE %s DROP BRANCH IF EXISTS %s\", tableName, branchName);\n     table.refresh();\n-\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref).isNull();\n+    assertThat(table.refs()).doesNotContainKey(branchName);\n   }\n \n   private Table insertRows() throws NoSuchTableException {\n@@ -327,21 +358,23 @@ public void testCreateOrReplaceBranchOnEmptyTable() {\n     sql(\"ALTER TABLE %s CREATE OR REPLACE BRANCH %s\", tableName, \"b1\");\n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    SnapshotRef mainRef = table.refs().get(SnapshotRef.MAIN_BRANCH);\n-    assertThat(mainRef).isNull();\n-\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref).isNotNull();\n-    assertThat(ref.minSnapshotsToKeep()).isNull();\n-    assertThat(ref.maxSnapshotAgeMs()).isNull();\n-    assertThat(ref.maxRefAgeMs()).isNull();\n-\n-    Snapshot snapshot = table.snapshot(ref.snapshotId());\n-    assertThat(snapshot.parentId()).isNull();\n-    assertThat(snapshot.addedDataFiles(table.io())).isEmpty();\n-    assertThat(snapshot.removedDataFiles(table.io())).isEmpty();\n-    assertThat(snapshot.addedDeleteFiles(table.io())).isEmpty();\n-    assertThat(snapshot.removedDeleteFiles(table.io())).isEmpty();\n+    assertThat(table.refs())\n+        .doesNotContainKey(SnapshotRef.MAIN_BRANCH)\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref).isNotNull();\n+              assertThat(ref.minSnapshotsToKeep()).isNull();\n+              assertThat(ref.maxSnapshotAgeMs()).isNull();\n+              assertThat(ref.maxRefAgeMs()).isNull();\n+\n+              Snapshot snapshot = table.snapshot(ref.snapshotId());\n+              assertThat(snapshot.parentId()).isNull();\n+              assertThat(snapshot.addedDataFiles(table.io())).isEmpty();\n+              assertThat(snapshot.removedDataFiles(table.io())).isEmpty();\n+              assertThat(snapshot.addedDeleteFiles(table.io())).isEmpty();\n+              assertThat(snapshot.removedDeleteFiles(table.io())).isEmpty();\n+            });\n   }\n \n   @TestTemplate\n@@ -375,9 +408,13 @@ public void replaceBranch() throws NoSuchTableException {\n \n     sql(\"ALTER TABLE %s REPLACE BRANCH %s AS OF VERSION %d\", tableName, branchName, second);\n     table.refresh();\n-    SnapshotRef ref = table.refs().get(branchName);\n-    assertThat(ref.snapshotId()).isEqualTo(second);\n-    assertThat(ref.maxRefAgeMs()).isEqualTo(expectedMaxRefAgeMs);\n+    assertThat(table.refs())\n+        .hasEntrySatisfying(\n+            branchName,\n+            ref -> {\n+              assertThat(ref.snapshotId()).isEqualTo(second);\n+              assertThat(ref.maxRefAgeMs()).isEqualTo(expectedMaxRefAgeMs);\n+            });\n   }\n \n   @TestTemplate\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java\nindex a501bca3cb14..4d5b569d24cd 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java\n@@ -24,6 +24,7 @@\n import java.util.Arrays;\n import java.util.List;\n import java.util.stream.Collectors;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.SnapshotRef;\n import org.apache.iceberg.Table;\n@@ -32,7 +33,9 @@\n import org.apache.spark.sql.catalyst.parser.ParseException;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestFastForwardBranchProcedure extends ExtensionsTestBase {\n \n   @AfterEach\n@@ -177,7 +180,7 @@ public void testInvalidFastForwardBranchCases() {\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n               assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n-              assertThat(parseException.getMessageParameters().get(\"error\")).isEqualTo(\"'CALL'\");\n+              assertThat(parseException.getMessageParameters()).containsEntry(\"error\", \"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.fast_forward('test_table', 'main')\", catalogName))\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java\nindex 043a40ad2cb1..ed20eaa4d54a 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java\n@@ -19,10 +19,11 @@\n package org.apache.iceberg.spark.extensions;\n \n import static org.apache.iceberg.TableProperties.WRITE_AUDIT_PUBLISH_ENABLED;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n-import static org.assertj.core.api.AssertionsForClassTypes.assertThat;\n \n import java.util.List;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.exceptions.ValidationException;\n@@ -34,7 +35,9 @@\n import org.apache.spark.sql.catalyst.parser.ParseException;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestPublishChangesProcedure extends ExtensionsTestBase {\n \n   @AfterEach\n@@ -174,7 +177,7 @@ public void testInvalidApplyWapChangesCases() {\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n               assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n-              assertThat(parseException.getMessageParameters().get(\"error\")).isEqualTo(\"'CALL'\");\n+              assertThat(parseException.getMessageParameters()).containsEntry(\"error\", \"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.publish_changes('t')\", catalogName))\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRegisterTableProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRegisterTableProcedure.java\nindex 2fcf89979de5..a06a67b7d612 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRegisterTableProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRegisterTableProcedure.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg.spark.extensions;\n \n import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.atIndex;\n \n import java.util.List;\n import org.apache.iceberg.ParameterizedTestExtension;\n@@ -68,16 +69,17 @@ public void testRegisterTable() throws NoSuchTableException, ParseException {\n \n     List<Object[]> result =\n         sql(\"CALL %s.system.register_table('%s', '%s')\", catalogName, targetName, metadataJson);\n-    assertThat(result.get(0)[0]).as(\"Current Snapshot is not correct\").isEqualTo(currentSnapshotId);\n+    assertThat(result.get(0))\n+        .as(\"Current Snapshot is not correct\")\n+        .contains(currentSnapshotId, atIndex(0));\n \n     List<Object[]> original = sql(\"SELECT * FROM %s\", tableName);\n     List<Object[]> registered = sql(\"SELECT * FROM %s\", targetName);\n     assertEquals(\"Registered table rows should match original table rows\", original, registered);\n-    assertThat(result.get(0)[1])\n+    assertThat(result.get(0))\n         .as(\"Should have the right row count in the procedure result\")\n-        .isEqualTo(numRows);\n-    assertThat(result.get(0)[2])\n+        .contains(numRows, atIndex(1))\n         .as(\"Should have the right datafile count in the procedure result\")\n-        .isEqualTo(originalFileCount);\n+        .contains(originalFileCount, atIndex(2));\n   }\n }\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java\nindex e746814c978e..7bbfc29e0a83 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteTablePathProcedure.java\n@@ -24,6 +24,7 @@\n import java.nio.file.Path;\n import java.util.List;\n import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RewriteTablePathUtil;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableUtil;\n@@ -31,8 +32,10 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n import org.junit.jupiter.api.io.TempDir;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestRewriteTablePathProcedure extends ExtensionsTestBase {\n   @TempDir private Path staging;\n   @TempDir private Path targetTableDir;\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestTagDDL.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestTagDDL.java\nindex 623af8777475..f8711cd7eeab 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestTagDDL.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestTagDDL.java\n@@ -291,12 +291,11 @@ public void testDropTag() throws NoSuchTableException {\n     String tagName = \"t1\";\n     table.manageSnapshots().createTag(tagName, table.currentSnapshot().snapshotId()).commit();\n     SnapshotRef ref = table.refs().get(tagName);\n-    assertThat(ref.snapshotId()).as(\"\").isEqualTo(table.currentSnapshot().snapshotId());\n+    assertThat(ref.snapshotId()).isEqualTo(table.currentSnapshot().snapshotId());\n \n     sql(\"ALTER TABLE %s DROP TAG %s\", tableName, tagName);\n     table.refresh();\n-    ref = table.refs().get(tagName);\n-    assertThat(ref).as(\"The tag needs to be dropped.\").isNull();\n+    assertThat(table.refs()).doesNotContainKey(tagName);\n   }\n \n   @TestTemplate\n@@ -328,11 +327,11 @@ public void testDropTagFailesForBranch() throws NoSuchTableException {\n   public void testDropTagIfExists() throws NoSuchTableException {\n     String tagName = \"nonExistingTag\";\n     Table table = insertRows();\n-    assertThat(table.refs().get(tagName)).as(\"The tag does not exists.\").isNull();\n+    assertThat(table.refs()).doesNotContainKey(tagName);\n \n     sql(\"ALTER TABLE %s DROP TAG IF EXISTS %s\", tableName, tagName);\n     table.refresh();\n-    assertThat(table.refs().get(tagName)).as(\"The tag still does not exist.\").isNull();\n+    assertThat(table.refs()).doesNotContainKey(tagName);\n \n     table.manageSnapshots().createTag(tagName, table.currentSnapshot().snapshotId()).commit();\n     assertThat(table.refs().get(tagName).snapshotId())\n@@ -341,7 +340,7 @@ public void testDropTagIfExists() throws NoSuchTableException {\n \n     sql(\"ALTER TABLE %s DROP TAG IF EXISTS %s\", tableName, tagName);\n     table.refresh();\n-    assertThat(table.refs().get(tagName)).as(\"The tag needs to be dropped.\").isNull();\n+    assertThat(table.refs()).doesNotContainKey(tagName);\n   }\n \n   @TestTemplate\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12744",
    "pr_id": 12744,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 15,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMigrateTableProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewritePositionDeleteFiles.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewritePositionDeleteFilesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMigrateTableProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMigrateTableProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewritePositionDeleteFiles.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewritePositionDeleteFilesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMigrateTableProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java"
    ],
    "base_commit": "7d7aeb89f26e3325e76afb8e20d6cd5b0ba74711",
    "head_commit": "14584523916173fff3bd56035c69d22f8a3afc6c",
    "repo_url": "https://github.com/apache/iceberg/pull/12744",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12744",
    "dockerfile": "",
    "pr_merged_at": "2025-04-08T11:56:01.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java\nindex a34caabe0152..abdce817de9c 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java\n@@ -20,11 +20,10 @@\n \n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.File;\n-import java.io.IOException;\n import java.util.List;\n-import java.util.Map;\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n import java.util.stream.Collectors;\n@@ -39,6 +38,9 @@\n import org.apache.iceberg.HasTableOperations;\n import org.apache.iceberg.ManifestFiles;\n import org.apache.iceberg.ManifestReader;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n@@ -58,17 +60,14 @@\n import org.apache.spark.sql.types.StructField;\n import org.apache.spark.sql.types.StructType;\n import org.joda.time.DateTime;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Before;\n-import org.junit.Ignore;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runners.Parameterized.Parameters;\n-\n-public class TestAddFilesProcedure extends SparkExtensionsTestBase {\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Disabled;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestAddFilesProcedure extends ExtensionsTestBase {\n \n   @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}, formatVersion = {3}\")\n   public static Object[][] parameters() {\n@@ -94,36 +93,24 @@ public static Object[][] parameters() {\n     };\n   }\n \n-  private final int formatVersion;\n-  private final String implementation;\n+  @Parameter(index = 3)\n+  private int formatVersion;\n+\n   private final String sourceTableName = \"source_table\";\n   private File fileTableDir;\n \n-  public TestAddFilesProcedure(\n-      String catalogName, String implementation, Map<String, String> config, int formatVersion) {\n-    super(catalogName, implementation, config);\n-    this.formatVersion = formatVersion;\n-    this.implementation = implementation;\n-  }\n-\n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n-\n-  @Before\n+  @BeforeEach\n   public void setupTempDirs() {\n-    try {\n-      fileTableDir = temp.newFolder();\n-    } catch (IOException e) {\n-      throw new RuntimeException(e);\n-    }\n+    fileTableDir = temp.toFile();\n   }\n \n-  @After\n+  @AfterEach\n   public void dropTables() {\n     sql(\"DROP TABLE IF EXISTS %s PURGE\", sourceTableName);\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addDataUnpartitioned() {\n     createUnpartitionedFileTable(\"parquet\");\n \n@@ -142,7 +129,7 @@ public void addDataUnpartitioned() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void deleteAndAddBackUnpartitioned() {\n     createUnpartitionedFileTable(\"parquet\");\n \n@@ -168,7 +155,7 @@ public void deleteAndAddBackUnpartitioned() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Ignore // TODO Classpath issues prevent us from actually writing to a Spark ORC table\n+  @Disabled // TODO Classpath issues prevent us from actually writing to a Spark ORC table\n   public void addDataUnpartitionedOrc() {\n     createUnpartitionedFileTable(\"orc\");\n \n@@ -182,7 +169,7 @@ public void addDataUnpartitionedOrc() {\n             \"CALL %s.system.add_files('%s', '`orc`.`%s`')\",\n             catalogName, tableName, fileTableDir.getAbsolutePath());\n \n-    Assert.assertEquals(2L, result);\n+    assertThat(result).isEqualTo(2L);\n \n     assertEquals(\n         \"Iceberg table contains correct data\",\n@@ -190,11 +177,11 @@ public void addDataUnpartitionedOrc() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addAvroFile() throws Exception {\n     // Spark Session Catalog cannot load metadata tables\n     // with \"The namespace in session catalog must have exactly one name part\"\n-    Assume.assumeFalse(catalogName.equals(\"spark_catalog\"));\n+    assumeThat(catalogName).isNotEqualTo(\"spark_catalog\");\n \n     // Create an Avro file\n \n@@ -210,7 +197,7 @@ public void addAvroFile() throws Exception {\n     GenericRecord record2 = new GenericData.Record(schema);\n     record2.put(\"id\", 2L);\n     record2.put(\"data\", \"b\");\n-    File outputFile = temp.newFile(\"test.avro\");\n+    File outputFile = temp.resolve(\"test.avro\").toFile();\n \n     DatumWriter<GenericRecord> datumWriter = new GenericDatumWriter(schema);\n     DataFileWriter<GenericRecord> dataFileWriter = new DataFileWriter(datumWriter);\n@@ -245,7 +232,7 @@ public void addAvroFile() throws Exception {\n   }\n \n   // TODO Adding spark-avro doesn't work in tests\n-  @Ignore\n+  @Disabled\n   public void addDataUnpartitionedAvro() {\n     createUnpartitionedFileTable(\"avro\");\n \n@@ -259,7 +246,7 @@ public void addDataUnpartitionedAvro() {\n             \"CALL %s.system.add_files('%s', '`avro`.`%s`')\",\n             catalogName, tableName, fileTableDir.getAbsolutePath());\n \n-    Assert.assertEquals(2L, result);\n+    assertThat(result).isEqualTo(2L);\n \n     assertEquals(\n         \"Iceberg table contains correct data\",\n@@ -267,7 +254,7 @@ public void addDataUnpartitionedAvro() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addDataUnpartitionedHive() {\n     createUnpartitionedHiveTable();\n \n@@ -284,7 +271,7 @@ public void addDataUnpartitionedHive() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addDataUnpartitionedExtraCol() {\n     createUnpartitionedFileTable(\"parquet\");\n \n@@ -303,7 +290,7 @@ public void addDataUnpartitionedExtraCol() {\n         sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addDataUnpartitionedMissingCol() {\n     createUnpartitionedFileTable(\"parquet\");\n \n@@ -322,7 +309,7 @@ public void addDataUnpartitionedMissingCol() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addDataPartitionedMissingCol() {\n     createPartitionedFileTable(\"parquet\");\n \n@@ -341,7 +328,7 @@ public void addDataPartitionedMissingCol() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addDataPartitioned() {\n     createPartitionedFileTable(\"parquet\");\n \n@@ -361,7 +348,7 @@ public void addDataPartitioned() {\n         sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Ignore // TODO Classpath issues prevent us from actually writing to a Spark ORC table\n+  @Disabled // TODO Classpath issues prevent us from actually writing to a Spark ORC table\n   public void addDataPartitionedOrc() {\n     createPartitionedFileTable(\"orc\");\n \n@@ -375,7 +362,7 @@ public void addDataPartitionedOrc() {\n             \"CALL %s.system.add_files('%s', '`parquet`.`%s`')\",\n             catalogName, tableName, fileTableDir.getAbsolutePath());\n \n-    Assert.assertEquals(8L, result);\n+    assertThat(result).isEqualTo(8L);\n \n     assertEquals(\n         \"Iceberg table contains correct data\",\n@@ -384,7 +371,7 @@ public void addDataPartitionedOrc() {\n   }\n \n   // TODO Adding spark-avro doesn't work in tests\n-  @Ignore\n+  @Disabled\n   public void addDataPartitionedAvro() {\n     createPartitionedFileTable(\"avro\");\n \n@@ -398,7 +385,7 @@ public void addDataPartitionedAvro() {\n             \"CALL %s.system.add_files('%s', '`avro`.`%s`')\",\n             catalogName, tableName, fileTableDir.getAbsolutePath());\n \n-    Assert.assertEquals(8L, result);\n+    assertThat(result).isEqualTo(8L);\n \n     assertEquals(\n         \"Iceberg table contains correct data\",\n@@ -406,7 +393,7 @@ public void addDataPartitionedAvro() {\n         sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addDataPartitionedHive() {\n     createPartitionedHiveTable();\n \n@@ -424,7 +411,7 @@ public void addDataPartitionedHive() {\n         sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addPartitionToPartitioned() {\n     createPartitionedFileTable(\"parquet\");\n \n@@ -444,7 +431,7 @@ public void addPartitionToPartitioned() {\n         sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void deleteAndAddBackPartitioned() {\n     createPartitionedFileTable(\"parquet\");\n \n@@ -471,7 +458,7 @@ public void deleteAndAddBackPartitioned() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addPartitionToPartitionedSnapshotIdInheritanceEnabledInTwoRuns()\n       throws NoSuchTableException, ParseException {\n     createPartitionedFileTable(\"parquet\");\n@@ -500,7 +487,7 @@ public void addPartitionToPartitionedSnapshotIdInheritanceEnabledInTwoRuns()\n     verifyUUIDInPath();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addPartitionsFromHiveSnapshotInheritanceEnabled()\n       throws NoSuchTableException, ParseException {\n     createPartitionedHiveTable();\n@@ -523,7 +510,7 @@ public void addPartitionsFromHiveSnapshotInheritanceEnabled()\n     verifyUUIDInPath();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addDataPartitionedByDateToPartitioned() {\n     createDatePartitionedFileTable(\"parquet\");\n \n@@ -542,7 +529,7 @@ public void addDataPartitionedByDateToPartitioned() {\n         sql(\"SELECT id, name, date FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addDataPartitionedVerifyPartitionTypeInferredCorrectly() {\n     createTableWithTwoPartitions(\"parquet\");\n \n@@ -561,7 +548,7 @@ public void addDataPartitionedVerifyPartitionTypeInferredCorrectly() {\n         sql(sqlFormat, tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addFilteredPartitionsToPartitioned() {\n     createCompositePartitionedTable(\"parquet\");\n \n@@ -581,7 +568,7 @@ public void addFilteredPartitionsToPartitioned() {\n         sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addFilteredPartitionsToPartitioned2() {\n     createCompositePartitionedTable(\"parquet\");\n \n@@ -603,7 +590,7 @@ public void addFilteredPartitionsToPartitioned2() {\n         sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addFilteredPartitionsToPartitionedWithNullValueFilteringOnId() {\n     createCompositePartitionedTableWithNullValueInPartitionColumn(\"parquet\");\n \n@@ -623,7 +610,7 @@ public void addFilteredPartitionsToPartitionedWithNullValueFilteringOnId() {\n         sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addFilteredPartitionsToPartitionedWithNullValueFilteringOnDept() {\n     createCompositePartitionedTableWithNullValueInPartitionColumn(\"parquet\");\n \n@@ -645,7 +632,7 @@ public void addFilteredPartitionsToPartitionedWithNullValueFilteringOnDept() {\n         sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addFileTableOldSpecDataAfterPartitionSpecEvolved()\n       throws NoSuchTableException, ParseException {\n     createPartitionedFileTable(\"parquet\");\n@@ -699,7 +686,7 @@ public void addFileTableOldSpecDataAfterPartitionSpecEvolved()\n     verifyUUIDInPath();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addFileTableNoCompatibleSpec() {\n     createPartitionedFileTable(\"parquet\");\n     createIcebergTable(\n@@ -722,7 +709,7 @@ public void addFileTableNoCompatibleSpec() {\n                 fullTableName, \"[id]\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addWeirdCaseHiveTable() {\n     createWeirdCaseTable();\n \n@@ -747,22 +734,20 @@ public void addWeirdCaseHiveTable() {\n             .collect(Collectors.toList());\n \n     // TODO when this assert breaks Spark fixed the pushdown issue\n-    Assert.assertEquals(\n-        \"If this assert breaks it means that Spark has fixed the pushdown issue\",\n-        0,\n-        sql(\n+    assertThat(\n+            sql(\n                 \"SELECT id, `naMe`, dept, subdept from %s WHERE `naMe` = 'John Doe' ORDER BY id\",\n-                sourceTableName)\n-            .size());\n+                sourceTableName))\n+        .as(\"If this assert breaks it means that Spark has fixed the pushdown issue\")\n+        .isEmpty();\n \n     // Pushdown works for iceberg\n-    Assert.assertEquals(\n-        \"We should be able to pushdown mixed case partition keys\",\n-        2,\n-        sql(\n+    assertThat(\n+            sql(\n                 \"SELECT id, `naMe`, dept, subdept FROM %s WHERE `naMe` = 'John Doe' ORDER BY id\",\n-                tableName)\n-            .size());\n+                tableName))\n+        .as(\"We should be able to pushdown mixed case partition keys\")\n+        .hasSize(2);\n \n     assertEquals(\n         \"Iceberg table contains correct data\",\n@@ -770,7 +755,7 @@ public void addWeirdCaseHiveTable() {\n         sql(\"SELECT id, `naMe`, dept, subdept FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addPartitionToPartitionedHive() {\n     createPartitionedHiveTable();\n \n@@ -790,7 +775,7 @@ public void addPartitionToPartitionedHive() {\n         sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void invalidDataImport() {\n     createPartitionedFileTable(\"parquet\");\n \n@@ -815,7 +800,7 @@ public void invalidDataImport() {\n         .hasMessageContaining(\"that matches the partition columns\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void invalidDataImportPartitioned() {\n     createUnpartitionedFileTable(\"parquet\");\n \n@@ -841,7 +826,7 @@ public void invalidDataImportPartitioned() {\n         .hasMessageContaining(\"that matches the partition columns\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void partitionColumnCountMismatchInFilter() {\n     createPartitionedHiveTable();\n \n@@ -859,7 +844,7 @@ public void partitionColumnCountMismatchInFilter() {\n                 + \" is greater than the number of partitioned columns in table (1)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void invalidPartitionColumnsInFilter() {\n     createPartitionedHiveTable();\n \n@@ -879,7 +864,7 @@ public void invalidPartitionColumnsInFilter() {\n         .hasMessageContaining(\"Valid partition columns: [%s]\", icebergTablePartitionNames);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void addTwice() {\n     createPartitionedHiveTable();\n \n@@ -914,7 +899,7 @@ public void addTwice() {\n         sql(\"SELECT id, name, dept, subdept FROM %s WHERE id = 2 ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void duplicateDataPartitioned() {\n     createPartitionedHiveTable();\n \n@@ -942,7 +927,7 @@ public void duplicateDataPartitioned() {\n                 + \" exist within the target table\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void duplicateDataPartitionedAllowed() {\n     createPartitionedHiveTable();\n \n@@ -979,7 +964,7 @@ public void duplicateDataPartitionedAllowed() {\n         sql(\"SELECT id, name, dept, subdept FROM %s\", tableName, tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void duplicateDataUnpartitioned() {\n     createUnpartitionedHiveTable();\n \n@@ -998,7 +983,7 @@ public void duplicateDataUnpartitioned() {\n                 + \" exist within the target table\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void duplicateDataUnpartitionedAllowed() {\n     createUnpartitionedHiveTable();\n \n@@ -1025,7 +1010,7 @@ public void duplicateDataUnpartitionedAllowed() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testEmptyImportDoesNotThrow() {\n     createIcebergTable(\"id Integer, name String, dept String, subdept String\");\n \n@@ -1054,7 +1039,7 @@ public void testEmptyImportDoesNotThrow() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedImportFromEmptyPartitionDoesNotThrow() {\n     createPartitionedHiveTable();\n \n@@ -1082,7 +1067,7 @@ public void testPartitionedImportFromEmptyPartitionDoesNotThrow() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddFilesWithParallelism() {\n     createUnpartitionedHiveTable();\n \n@@ -1104,7 +1089,7 @@ public void testAddFilesWithParallelism() {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddFilesPartitionedWithParallelism() {\n     createPartitionedHiveTable();\n \n@@ -1145,7 +1130,7 @@ private Dataset<Row> unpartitionedDF() {\n         .repartition(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddFilesToTableWithManySpecs() {\n     createPartitionedHiveTable();\n     createIcebergTable(\"id Integer, name String, dept String, subdept String\"); // Spec 0\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\nindex ca73793a3396..1560abf1123e 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\n@@ -26,16 +26,17 @@\n import java.io.IOException;\n import java.nio.ByteBuffer;\n import java.nio.charset.StandardCharsets;\n+import java.nio.file.Files;\n import java.sql.Timestamp;\n import java.time.Instant;\n import java.util.List;\n-import java.util.Map;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.GenericBlobMetadata;\n import org.apache.iceberg.GenericStatisticsFile;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.PartitionStatisticsFile;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.StatisticsFile;\n@@ -46,7 +47,6 @@\n import org.apache.iceberg.puffin.Puffin;\n import org.apache.iceberg.puffin.PuffinWriter;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.spark.SparkCatalog;\n import org.apache.iceberg.spark.data.TestHelpers;\n@@ -54,23 +54,19 @@\n import org.apache.spark.sql.AnalysisException;\n import org.apache.spark.sql.Encoders;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestExpireSnapshotsProcedure extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestExpireSnapshotsProcedure extends ExtensionsTestBase {\n \n-  public TestExpireSnapshotsProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireSnapshotsInEmptyTable() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -79,7 +75,7 @@ public void testExpireSnapshotsInEmptyTable() {\n         \"Should not delete any files\", ImmutableList.of(row(0L, 0L, 0L, 0L, 0L, 0L)), output);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireSnapshotsUsingPositionalArgs() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -97,7 +93,7 @@ public void testExpireSnapshotsUsingPositionalArgs() {\n     Timestamp secondSnapshotTimestamp =\n         Timestamp.from(Instant.ofEpochMilli(secondSnapshot.timestampMillis()));\n \n-    Assert.assertEquals(\"Should be 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should be 2 snapshots\").hasSize(2);\n \n     // expire without retainLast param\n     List<Object[]> output1 =\n@@ -109,7 +105,7 @@ public void testExpireSnapshotsUsingPositionalArgs() {\n \n     table.refresh();\n \n-    Assert.assertEquals(\"Should expire one snapshot\", 1, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should expire one snapshot\").hasSize(1);\n \n     sql(\"INSERT OVERWRITE %s VALUES (3, 'c')\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (4, 'd')\", tableName);\n@@ -124,7 +120,7 @@ public void testExpireSnapshotsUsingPositionalArgs() {\n \n     Timestamp currentTimestamp = Timestamp.from(Instant.ofEpochMilli(System.currentTimeMillis()));\n \n-    Assert.assertEquals(\"Should be 3 snapshots\", 3, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should be 3 snapshots\").hasSize(3);\n \n     // expire with retainLast param\n     List<Object[]> output =\n@@ -135,7 +131,7 @@ public void testExpireSnapshotsUsingPositionalArgs() {\n         \"Procedure output must match\", ImmutableList.of(row(2L, 0L, 0L, 2L, 1L, 0L)), output);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireSnapshotUsingNamedArgs() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -144,7 +140,7 @@ public void testExpireSnapshotUsingNamedArgs() {\n \n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\"Should be 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should be 2 snapshots\").hasSize(2);\n \n     waitUntilAfter(table.currentSnapshot().timestampMillis());\n \n@@ -158,7 +154,7 @@ public void testExpireSnapshotUsingNamedArgs() {\n         \"Procedure output must match\", ImmutableList.of(row(0L, 0L, 0L, 0L, 1L, 0L)), output);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireSnapshotsGCDisabled() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -169,7 +165,7 @@ public void testExpireSnapshotsGCDisabled() {\n         .hasMessageStartingWith(\"Cannot expire snapshots: GC is disabled\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidExpireSnapshotsCases() {\n     assertThatThrownBy(() -> sql(\"CALL %s.system.expire_snapshots('n', table => 't')\", catalogName))\n         .isInstanceOf(AnalysisException.class)\n@@ -181,8 +177,8 @@ public void testInvalidExpireSnapshotsCases() {\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n-              Assert.assertEquals(\"PARSE_SYNTAX_ERROR\", parseException.getErrorClass());\n-              Assert.assertEquals(\"'CALL'\", parseException.getMessageParameters().get(\"error\"));\n+              assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n+              assertThat(parseException.getMessageParameters().get(\"error\")).isEqualTo(\"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.expire_snapshots()\", catalogName))\n@@ -198,7 +194,7 @@ public void testInvalidExpireSnapshotsCases() {\n         .hasMessage(\"Cannot handle an empty identifier for argument table\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testResolvingTableInAnotherCatalog() throws IOException {\n     String anotherCatalog = \"another_\" + catalogName;\n     spark.conf().set(\"spark.sql.catalog.\" + anotherCatalog, SparkCatalog.class.getName());\n@@ -207,7 +203,7 @@ public void testResolvingTableInAnotherCatalog() throws IOException {\n         .conf()\n         .set(\n             \"spark.sql.catalog.\" + anotherCatalog + \".warehouse\",\n-            \"file:\" + temp.newFolder().toString());\n+            Files.createTempDirectory(temp, \"junit\").toFile().toURI().toString());\n \n     sql(\n         \"CREATE TABLE %s.%s (id bigint NOT NULL, data string) USING iceberg\",\n@@ -222,7 +218,7 @@ public void testResolvingTableInAnotherCatalog() throws IOException {\n         .hasMessageStartingWith(\"Cannot run procedure in catalog\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testConcurrentExpireSnapshots() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -245,7 +241,7 @@ public void testConcurrentExpireSnapshots() {\n         output);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testConcurrentExpireSnapshotsWithInvalidInput() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -266,7 +262,7 @@ public void testConcurrentExpireSnapshotsWithInvalidInput() {\n         .hasMessage(\"max_concurrent_deletes should have value > 0, value: -1\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireDeleteFiles() throws Exception {\n     sql(\n         \"CREATE TABLE %s (id bigint, data string) USING iceberg TBLPROPERTIES\"\n@@ -288,9 +284,8 @@ public void testExpireDeleteFiles() throws Exception {\n \n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\n-        \"Should have 1 delete manifest\", 1, TestHelpers.deleteManifests(table).size());\n-    Assert.assertEquals(\"Should have 1 delete file\", 1, TestHelpers.deleteFiles(table).size());\n+    assertThat(TestHelpers.deleteManifests(table)).as(\"Should have 1 delete manifest\").hasSize(1);\n+    assertThat(TestHelpers.deleteFiles(table)).as(\"Should have 1 delete file\").hasSize(1);\n     Path deleteManifestPath = new Path(TestHelpers.deleteManifests(table).iterator().next().path());\n     DeleteFile deleteFile = TestHelpers.deleteFiles(table).iterator().next();\n     Path deleteFilePath = new Path(deleteFile.location());\n@@ -311,13 +306,14 @@ public void testExpireDeleteFiles() throws Exception {\n     sql(\"INSERT INTO TABLE %s VALUES (6, 'f')\", tableName); // this txn removes the file reference\n     table.refresh();\n \n-    Assert.assertEquals(\n-        \"Should have no delete manifests\", 0, TestHelpers.deleteManifests(table).size());\n-    Assert.assertEquals(\"Should have no delete files\", 0, TestHelpers.deleteFiles(table).size());\n+    assertThat(TestHelpers.deleteManifests(table)).as(\"Should have no delete manifests\").isEmpty();\n+    assertThat(TestHelpers.deleteFiles(table)).as(\"Should have no delete files\").isEmpty();\n \n     FileSystem localFs = FileSystem.getLocal(new Configuration());\n-    Assert.assertTrue(\"Delete manifest should still exist\", localFs.exists(deleteManifestPath));\n-    Assert.assertTrue(\"Delete file should still exist\", localFs.exists(deleteFilePath));\n+    assertThat(localFs.exists(deleteManifestPath))\n+        .as(\"Delete manifest should still exist\")\n+        .isTrue();\n+    assertThat(localFs.exists(deleteFilePath)).as(\"Delete file should still exist\").isTrue();\n \n     Timestamp currentTimestamp = Timestamp.from(Instant.ofEpochMilli(System.currentTimeMillis()));\n     List<Object[]> output =\n@@ -329,11 +325,13 @@ public void testExpireDeleteFiles() throws Exception {\n         \"Should deleted 1 data and pos delete file and 4 manifests and lists (one for each txn)\",\n         ImmutableList.of(row(1L, 1L, 0L, 4L, 4L, 0L)),\n         output);\n-    Assert.assertFalse(\"Delete manifest should be removed\", localFs.exists(deleteManifestPath));\n-    Assert.assertFalse(\"Delete file should be removed\", localFs.exists(deleteFilePath));\n+    assertThat(localFs.exists(deleteManifestPath))\n+        .as(\"Delete manifest should be removed\")\n+        .isFalse();\n+    assertThat(localFs.exists(deleteFilePath)).as(\"Delete file should be removed\").isFalse();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireSnapshotWithStreamResultsEnabled() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -342,7 +340,7 @@ public void testExpireSnapshotWithStreamResultsEnabled() {\n \n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\"Should be 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should be 2 snapshots\").hasSize(2);\n \n     waitUntilAfter(table.currentSnapshot().timestampMillis());\n \n@@ -359,7 +357,7 @@ public void testExpireSnapshotWithStreamResultsEnabled() {\n         \"Procedure output must match\", ImmutableList.of(row(0L, 0L, 0L, 0L, 1L, 0L)), output);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireSnapshotsWithSnapshotId() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -368,7 +366,7 @@ public void testExpireSnapshotsWithSnapshotId() {\n \n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\"Should be 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should be 2 snapshots\").hasSize(2);\n \n     // Expiring the snapshot specified by snapshot_id should keep only a single snapshot.\n     long firstSnapshotId = table.currentSnapshot().parentId();\n@@ -378,16 +376,14 @@ public void testExpireSnapshotsWithSnapshotId() {\n \n     // There should only be one single snapshot left.\n     table.refresh();\n-    Assert.assertEquals(\"Should be 1 snapshots\", 1, Iterables.size(table.snapshots()));\n-    Assert.assertEquals(\n-        \"Snapshot ID should not be present\",\n-        0,\n-        Iterables.size(\n-            Iterables.filter(\n-                table.snapshots(), snapshot -> snapshot.snapshotId() == firstSnapshotId)));\n+    assertThat(table.snapshots())\n+        .hasSize(1)\n+        .as(\"Snapshot ID should not be present\")\n+        .filteredOn(snapshot -> snapshot.snapshotId() == firstSnapshotId)\n+        .isEmpty();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireSnapshotShouldFailForCurrentSnapshot() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -395,7 +391,7 @@ public void testExpireSnapshotShouldFailForCurrentSnapshot() {\n     sql(\"INSERT INTO TABLE %s VALUES (2, 'b')\", tableName);\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should be 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should be 2 snapshots\").hasSize(2);\n \n     assertThatThrownBy(\n             () ->\n@@ -411,7 +407,7 @@ public void testExpireSnapshotShouldFailForCurrentSnapshot() {\n         .hasMessageStartingWith(\"Cannot expire\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireSnapshotsProcedureWorksWithSqlComments() {\n     // Ensure that systems such as dbt, that inject comments into the generated SQL files, will\n     // work with Iceberg-specific DDL\n@@ -422,7 +418,7 @@ public void testExpireSnapshotsProcedureWorksWithSqlComments() {\n \n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\"Should be 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should be 2 snapshots\").hasSize(2);\n \n     waitUntilAfter(table.currentSnapshot().timestampMillis());\n \n@@ -441,10 +437,10 @@ public void testExpireSnapshotsProcedureWorksWithSqlComments() {\n \n     table.refresh();\n \n-    Assert.assertEquals(\"Should be 1 snapshot remaining\", 1, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should be 1 snapshot remaining\").hasSize(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireSnapshotsWithStatisticFiles() throws Exception {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (10, 'abc')\", tableName);\n@@ -495,7 +491,7 @@ public void testExpireSnapshotsWithStatisticFiles() throws Exception {\n         .exists();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireSnapshotsWithPartitionStatisticFiles() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"INSERT INTO TABLE %s VALUES (10, 'abc')\", tableName);\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMigrateTableProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMigrateTableProcedure.java\nindex 95a0290dddd2..a9e309ddeacc 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMigrateTableProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMigrateTableProcedure.java\n@@ -20,51 +20,45 @@\n \n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.List;\n import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.spark.sql.AnalysisException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-\n-public class TestMigrateTableProcedure extends SparkExtensionsTestBase {\n-\n-  public TestMigrateTableProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-  @After\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestMigrateTableProcedure extends ExtensionsTestBase {\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS %s_BACKUP_\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMigrate() throws IOException {\n-    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n-    String location = temp.newFolder().toString();\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         tableName, location);\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n     Object result = scalarSql(\"CALL %s.system.migrate('%s')\", catalogName, tableName);\n \n-    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+    assertThat(result).as(\"Should have added one file\").isEqualTo(1L);\n \n     Table createdTable = validationCatalog.loadTable(tableIdent);\n \n     String tableLocation = createdTable.location().replace(\"file:\", \"\");\n-    Assert.assertEquals(\"Table should have original location\", location, tableLocation);\n+    assertThat(tableLocation).as(\"Table should have original location\").isEqualTo(location);\n \n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n \n@@ -76,10 +70,10 @@ public void testMigrate() throws IOException {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName + \"_BACKUP_\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMigrateWithOptions() throws IOException {\n-    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n-    String location = temp.newFolder().toString();\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         tableName, location);\n@@ -88,15 +82,15 @@ public void testMigrateWithOptions() throws IOException {\n     Object result =\n         scalarSql(\"CALL %s.system.migrate('%s', map('foo', 'bar'))\", catalogName, tableName);\n \n-    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+    assertThat(result).as(\"Should have added one file\").isEqualTo(1L);\n \n     Table createdTable = validationCatalog.loadTable(tableIdent);\n \n     Map<String, String> props = createdTable.properties();\n-    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+    assertThat(props).containsEntry(\"foo\", \"bar\");\n \n     String tableLocation = createdTable.location().replace(\"file:\", \"\");\n-    Assert.assertEquals(\"Table should have original location\", location, tableLocation);\n+    assertThat(tableLocation).as(\"Table should have original location\").isEqualTo(location);\n \n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n \n@@ -108,10 +102,10 @@ public void testMigrateWithOptions() throws IOException {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName + \"_BACKUP_\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMigrateWithDropBackup() throws IOException {\n-    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n-    String location = temp.newFolder().toString();\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         tableName, location);\n@@ -120,14 +114,14 @@ public void testMigrateWithDropBackup() throws IOException {\n     Object result =\n         scalarSql(\n             \"CALL %s.system.migrate(table => '%s', drop_backup => true)\", catalogName, tableName);\n-    Assert.assertEquals(\"Should have added one file\", 1L, result);\n-    Assert.assertFalse(spark.catalog().tableExists(tableName + \"_BACKUP_\"));\n+    assertThat(result).as(\"Should have added one file\").isEqualTo(1L);\n+    assertThat(spark.catalog().tableExists(tableName + \"_BACKUP_\")).isFalse();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMigrateWithBackupTableName() throws IOException {\n-    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n-    String location = temp.newFolder().toString();\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         tableName, location);\n@@ -144,11 +138,10 @@ public void testMigrateWithBackupTableName() throws IOException {\n     assertThat(spark.catalog().tableExists(dbName + \".\" + backupTableName)).isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMigrateWithInvalidMetricsConfig() throws IOException {\n-    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n-\n-    String location = temp.newFolder().toString();\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         tableName, location);\n@@ -162,11 +155,10 @@ public void testMigrateWithInvalidMetricsConfig() throws IOException {\n         .hasMessageStartingWith(\"Invalid metrics config\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMigrateWithConflictingProps() throws IOException {\n-    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n-\n-    String location = temp.newFolder().toString();\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         tableName, location);\n@@ -174,7 +166,7 @@ public void testMigrateWithConflictingProps() throws IOException {\n \n     Object result =\n         scalarSql(\"CALL %s.system.migrate('%s', map('migrated', 'false'))\", catalogName, tableName);\n-    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+    assertThat(result).as(\"Should have added one file\").isEqualTo(1L);\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -182,10 +174,10 @@ public void testMigrateWithConflictingProps() throws IOException {\n         sql(\"SELECT * FROM %s\", tableName));\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should override user value\", \"true\", table.properties().get(\"migrated\"));\n+    assertThat(table.properties()).containsEntry(\"migrated\", \"true\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidMigrateCases() {\n     assertThatThrownBy(() -> sql(\"CALL %s.system.migrate()\", catalogName))\n         .isInstanceOf(AnalysisException.class)\n@@ -200,10 +192,10 @@ public void testInvalidMigrateCases() {\n         .hasMessage(\"Cannot handle an empty identifier for argument table\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMigratePartitionWithSpecialCharacter() throws IOException {\n-    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n-    String location = temp.newFolder().toString();\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string, dt date) USING parquet \"\n             + \"PARTITIONED BY (data, dt) LOCATION '%s'\",\n@@ -217,33 +209,71 @@ public void testMigratePartitionWithSpecialCharacter() throws IOException {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMigrateEmptyPartitionedTable() throws Exception {\n-    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n-    String location = temp.newFolder().toString();\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet PARTITIONED BY (id) LOCATION '%s'\",\n         tableName, location);\n     Object result = scalarSql(\"CALL %s.system.migrate('%s')\", catalogName, tableName);\n-    Assert.assertEquals(0L, result);\n+    assertThat(result).isEqualTo(0L);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMigrateEmptyTable() throws Exception {\n-    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n-    String location = temp.newFolder().toString();\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         tableName, location);\n     Object result = scalarSql(\"CALL %s.system.migrate('%s')\", catalogName, tableName);\n-    Assert.assertEquals(0L, result);\n+    assertThat(result).isEqualTo(0L);\n   }\n \n-  @Test\n-  public void testMigratePartitionedWithParallelism() throws IOException {\n-    Assume.assumeTrue(catalogName.equals(\"spark_catalog\"));\n+  @TestTemplate\n+  public void testMigrateWithParallelism() throws IOException {\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n+    sql(\n+        \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n+        tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (2, 'b')\", tableName);\n+\n+    List<Object[]> result =\n+        sql(\"CALL %s.system.migrate(table => '%s', parallelism => %d)\", catalogName, tableName, 2);\n+    assertEquals(\"Procedure output must match\", ImmutableList.of(row(2L)), result);\n \n-    String location = temp.newFolder().toString();\n+    assertEquals(\n+        \"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(2L, \"b\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @TestTemplate\n+  public void testMigrateWithInvalidParallelism() throws IOException {\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n+    sql(\n+        \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n+        tableName, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (2, 'b')\", tableName);\n+\n+    assertThatThrownBy(\n+            () ->\n+                sql(\n+                    \"CALL %s.system.migrate(table => '%s', parallelism => %d)\",\n+                    catalogName, tableName, -1))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\"Parallelism should be larger than 0\");\n+  }\n+\n+  @TestTemplate\n+  public void testMigratePartitionedWithParallelism() throws IOException {\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet PARTITIONED BY (id) LOCATION '%s'\",\n         tableName, location);\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\nindex 138f084861e2..13351c8bcf26 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\n@@ -31,7 +31,6 @@\n import java.sql.Timestamp;\n import java.time.Instant;\n import java.util.List;\n-import java.util.Map;\n import java.util.UUID;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileSystem;\n@@ -42,6 +41,7 @@\n import org.apache.iceberg.GenericBlobMetadata;\n import org.apache.iceberg.GenericStatisticsFile;\n import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.PartitionStatisticsFile;\n import org.apache.iceberg.ReachableFileUtil;\n@@ -53,7 +53,6 @@\n import org.apache.iceberg.puffin.Puffin;\n import org.apache.iceberg.puffin.PuffinWriter;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.spark.Spark3Util;\n import org.apache.iceberg.spark.data.TestHelpers;\n@@ -65,28 +64,20 @@\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestRemoveOrphanFilesProcedure extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRemoveOrphanFilesProcedure extends ExtensionsTestBase {\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n-\n-  public TestRemoveOrphanFilesProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s PURGE\", tableName);\n     sql(\"DROP TABLE IF EXISTS p PURGE\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesInEmptyTable() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -97,7 +88,7 @@ public void testRemoveOrphanFilesInEmptyTable() {\n     assertEquals(\"Should have no rows\", ImmutableList.of(), sql(\"SELECT * FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesInDataFolder() throws IOException {\n     if (catalogName.equals(\"testhadoop\")) {\n       sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n@@ -106,7 +97,7 @@ public void testRemoveOrphanFilesInDataFolder() throws IOException {\n       // correctly while dropping tables through spark_catalog\n       sql(\n           \"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg LOCATION '%s'\",\n-          tableName, temp.newFolder());\n+          tableName, java.nio.file.Files.createTempDirectory(temp, \"junit\"));\n     }\n \n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -143,7 +134,7 @@ public void testRemoveOrphanFilesInDataFolder() throws IOException {\n                 + \"table => '%s',\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, currentTimestamp);\n-    Assert.assertEquals(\"Should be orphan files in the data folder\", 1, output2.size());\n+    assertThat(output2).as(\"Should be orphan files in the data folder\").hasSize(1);\n \n     // the previous call should have deleted all orphan files\n     List<Object[]> output3 =\n@@ -152,7 +143,7 @@ public void testRemoveOrphanFilesInDataFolder() throws IOException {\n                 + \"table => '%s',\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, currentTimestamp);\n-    Assert.assertEquals(\"Should be no more orphan files in the data folder\", 0, output3.size());\n+    assertThat(output3).as(\"Should be no more orphan files in the data folder\").isEmpty();\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -160,7 +151,7 @@ public void testRemoveOrphanFilesInDataFolder() throws IOException {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesDryRun() throws IOException {\n     if (catalogName.equals(\"testhadoop\")) {\n       sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n@@ -169,7 +160,7 @@ public void testRemoveOrphanFilesDryRun() throws IOException {\n       // correctly while dropping tables through spark_catalog\n       sql(\n           \"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg LOCATION '%s'\",\n-          tableName, temp.newFolder());\n+          tableName, java.nio.file.Files.createTempDirectory(temp, \"junit\"));\n     }\n \n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -194,7 +185,7 @@ public void testRemoveOrphanFilesDryRun() throws IOException {\n                 + \"older_than => TIMESTAMP '%s',\"\n                 + \"dry_run => true)\",\n             catalogName, tableIdent, currentTimestamp);\n-    Assert.assertEquals(\"Should be one orphan files\", 1, output1.size());\n+    assertThat(output1).as(\"Should be one orphan files\").hasSize(1);\n \n     // actually delete orphans\n     List<Object[]> output2 =\n@@ -203,7 +194,7 @@ public void testRemoveOrphanFilesDryRun() throws IOException {\n                 + \"table => '%s',\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, currentTimestamp);\n-    Assert.assertEquals(\"Should be one orphan files\", 1, output2.size());\n+    assertThat(output2).as(\"Should be one orphan files\").hasSize(1);\n \n     // the previous call should have deleted all orphan files\n     List<Object[]> output3 =\n@@ -212,7 +203,7 @@ public void testRemoveOrphanFilesDryRun() throws IOException {\n                 + \"table => '%s',\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, currentTimestamp);\n-    Assert.assertEquals(\"Should be no more orphan files\", 0, output3.size());\n+    assertThat(output3).as(\"Should be no more orphan files\").isEmpty();\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -220,7 +211,7 @@ public void testRemoveOrphanFilesDryRun() throws IOException {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesGCDisabled() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -236,7 +227,7 @@ public void testRemoveOrphanFilesGCDisabled() {\n     sql(\"ALTER TABLE %s SET TBLPROPERTIES ('%s' 'true')\", tableName, GC_ENABLED);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesWap() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     sql(\"ALTER TABLE %s SET TBLPROPERTIES ('%s' 'true')\", tableName, WRITE_AUDIT_PUBLISH_ENABLED);\n@@ -255,7 +246,7 @@ public void testRemoveOrphanFilesWap() {\n     assertEquals(\"Should be no orphan files\", ImmutableList.of(), output);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidRemoveOrphanFilesCases() {\n     assertThatThrownBy(\n             () -> sql(\"CALL %s.system.remove_orphan_files('n', table => 't')\", catalogName))\n@@ -268,8 +259,8 @@ public void testInvalidRemoveOrphanFilesCases() {\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n-              Assert.assertEquals(\"PARSE_SYNTAX_ERROR\", parseException.getErrorClass());\n-              Assert.assertEquals(\"'CALL'\", parseException.getMessageParameters().get(\"error\"));\n+              assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n+              assertThat(parseException.getMessageParameters().get(\"error\")).isEqualTo(\"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.remove_orphan_files()\", catalogName))\n@@ -285,7 +276,7 @@ public void testInvalidRemoveOrphanFilesCases() {\n         .hasMessage(\"Cannot handle an empty identifier for argument table\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testConcurrentRemoveOrphanFiles() throws IOException {\n     if (catalogName.equals(\"testhadoop\")) {\n       sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n@@ -294,7 +285,7 @@ public void testConcurrentRemoveOrphanFiles() throws IOException {\n       // correctly while dropping tables through spark_catalog\n       sql(\n           \"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg LOCATION '%s'\",\n-          tableName, temp.newFolder());\n+          tableName, java.nio.file.Files.createTempDirectory(temp, \"junit\"));\n     }\n \n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n@@ -324,7 +315,7 @@ public void testConcurrentRemoveOrphanFiles() throws IOException {\n                 + \"max_concurrent_deletes => %s,\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, 4, currentTimestamp);\n-    Assert.assertEquals(\"Should be orphan files in the data folder\", 4, output.size());\n+    assertThat(output).as(\"Should be orphan files in the data folder\").hasSize(4);\n \n     // the previous call should have deleted all orphan files\n     List<Object[]> output3 =\n@@ -334,7 +325,7 @@ public void testConcurrentRemoveOrphanFiles() throws IOException {\n                 + \"max_concurrent_deletes => %s,\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, 4, currentTimestamp);\n-    Assert.assertEquals(\"Should be no more orphan files in the data folder\", 0, output3.size());\n+    assertThat(output3).as(\"Should be no more orphan files in the data folder\").isEmpty();\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -342,7 +333,7 @@ public void testConcurrentRemoveOrphanFiles() throws IOException {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testConcurrentRemoveOrphanFilesWithInvalidInput() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n \n@@ -400,7 +391,7 @@ public void testConcurrentRemoveOrphanFilesWithInvalidInput() {\n         .hasMessage(\"Invalid last_modified column: StringType is not a timestamp\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesWithDeleteFiles() throws Exception {\n     sql(\n         \"CREATE TABLE %s (id int, data string) USING iceberg TBLPROPERTIES\"\n@@ -421,9 +412,8 @@ public void testRemoveOrphanFilesWithDeleteFiles() throws Exception {\n     sql(\"DELETE FROM %s WHERE id=1\", tableName);\n \n     Table table = Spark3Util.loadIcebergTable(spark, tableName);\n-    Assert.assertEquals(\n-        \"Should have 1 delete manifest\", 1, TestHelpers.deleteManifests(table).size());\n-    Assert.assertEquals(\"Should have 1 delete file\", 1, TestHelpers.deleteFiles(table).size());\n+    assertThat(TestHelpers.deleteManifests(table)).as(\"Should have 1 delete manifest\").hasSize(1);\n+    assertThat(TestHelpers.deleteFiles(table)).as(\"Should have 1 delete file\").hasSize(1);\n     Path deleteManifestPath = new Path(TestHelpers.deleteManifests(table).iterator().next().path());\n     Path deleteFilePath = new Path(TestHelpers.deleteFiles(table).iterator().next().location());\n \n@@ -438,20 +428,22 @@ public void testRemoveOrphanFilesWithDeleteFiles() throws Exception {\n                 + \"table => '%s',\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, currentTimestamp);\n-    Assert.assertEquals(\"Should be no orphan files\", 0, output.size());\n+    assertThat(output).as(\"Should be no orphan files\").isEmpty();\n \n     FileSystem localFs = FileSystem.getLocal(new Configuration());\n-    Assert.assertTrue(\"Delete manifest should still exist\", localFs.exists(deleteManifestPath));\n-    Assert.assertTrue(\"Delete file should still exist\", localFs.exists(deleteFilePath));\n+    assertThat(localFs.exists(deleteManifestPath))\n+        .as(\"Delete manifest should still exist\")\n+        .isTrue();\n+    assertThat(localFs.exists(deleteFilePath)).as(\"Delete file should still exist\").isTrue();\n \n     records.remove(new SimpleRecord(1, \"a\"));\n     Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableName);\n     List<SimpleRecord> actualRecords =\n         resultDF.as(Encoders.bean(SimpleRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n+    assertThat(actualRecords).as(\"Rows must match\").isEqualTo(records);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesWithStatisticFiles() throws Exception {\n     sql(\n         \"CREATE TABLE %s USING iceberg \"\n@@ -509,10 +501,7 @@ public void testRemoveOrphanFilesWithStatisticFiles() throws Exception {\n             catalogName, tableIdent, currentTimestamp);\n     assertThat(output).as(\"Should be no orphan files\").isEmpty();\n \n-    assertThat(statsLocation.exists()).as(\"stats file should exist\").isTrue();\n-    assertThat(statsLocation.length())\n-        .as(\"stats file length\")\n-        .isEqualTo(statisticsFile.fileSizeInBytes());\n+    assertThat(statsLocation).exists().hasSize(statisticsFile.fileSizeInBytes());\n \n     transaction = table.newTransaction();\n     transaction.updateStatistics().removeStatistics(statisticsFile.snapshotId()).commit();\n@@ -524,14 +513,14 @@ public void testRemoveOrphanFilesWithStatisticFiles() throws Exception {\n                 + \"table => '%s',\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, currentTimestamp);\n-    assertThat(output).as(\"Should be orphan files\").hasSize(1);\n-    assertThat(Iterables.getOnlyElement(output))\n-        .as(\"Deleted files\")\n-        .containsExactly(statsLocation.toURI().toString());\n-    assertThat(statsLocation.exists()).as(\"stats file should be deleted\").isFalse();\n+    assertThat(output)\n+        .hasSize(1)\n+        .first()\n+        .satisfies(files -> assertThat(files).containsExactly(statsLocation.toURI().toString()));\n+    assertThat(statsLocation).doesNotExist();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesWithPartitionStatisticFiles() throws Exception {\n     sql(\n         \"CREATE TABLE %s USING iceberg \"\n@@ -569,10 +558,10 @@ public void testRemoveOrphanFilesWithPartitionStatisticFiles() throws Exception\n                 + \"table => '%s',\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, currentTimestamp);\n-    assertThat(output).as(\"Should be orphan files\").hasSize(1);\n-    assertThat(Iterables.getOnlyElement(output))\n-        .as(\"Deleted files\")\n-        .containsExactly(\"file:\" + partitionStatsLocation);\n+    assertThat(output)\n+        .hasSize(1)\n+        .first()\n+        .satisfies(files -> assertThat(files).containsExactly(\"file:\" + partitionStatsLocation));\n     assertThat(new File(partitionStatsLocation))\n         .as(\"partition stats file should be deleted\")\n         .doesNotExist();\n@@ -598,7 +587,7 @@ private static void commitPartitionStatsTxn(\n     transaction.commitTransaction();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesProcedureWithPrefixMode()\n       throws NoSuchTableException, ParseException, IOException {\n     if (catalogName.equals(\"testhadoop\")) {\n@@ -606,7 +595,7 @@ public void testRemoveOrphanFilesProcedureWithPrefixMode()\n     } else {\n       sql(\n           \"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg LOCATION '%s'\",\n-          tableName, temp.newFolder().toURI().toString());\n+          tableName, java.nio.file.Files.createTempDirectory(temp, \"junit\"));\n     }\n     Table table = Spark3Util.loadIcebergTable(spark, tableName);\n     String location = table.location();\n@@ -663,7 +652,7 @@ public void testRemoveOrphanFilesProcedureWithPrefixMode()\n                 + \"equal_schemes => map('file1', 'file'),\"\n                 + \"file_list_view => '%s')\",\n             catalogName, tableIdent, fileListViewName);\n-    Assert.assertEquals(0, orphanFiles.size());\n+    assertThat(orphanFiles).isEmpty();\n \n     // Test with no equal schemes\n     assertThatThrownBy(\n@@ -681,7 +670,7 @@ public void testRemoveOrphanFilesProcedureWithPrefixMode()\n     sql(\"DROP TABLE %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesProcedureWithEqualAuthorities()\n       throws NoSuchTableException, ParseException, IOException {\n     if (catalogName.equals(\"testhadoop\")) {\n@@ -689,7 +678,7 @@ public void testRemoveOrphanFilesProcedureWithEqualAuthorities()\n     } else {\n       sql(\n           \"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg LOCATION '%s'\",\n-          tableName, temp.newFolder().toURI().toString());\n+          tableName, java.nio.file.Files.createTempDirectory(temp, \"junit\"));\n     }\n     Table table = Spark3Util.loadIcebergTable(spark, tableName);\n     Path originalPath = new Path(table.location());\n@@ -746,7 +735,7 @@ public void testRemoveOrphanFilesProcedureWithEqualAuthorities()\n                 + \"equal_authorities => map('localhost', '%s'),\"\n                 + \"file_list_view => '%s')\",\n             catalogName, tableIdent, originalAuthority, fileListViewName);\n-    Assert.assertEquals(0, orphanFiles.size());\n+    assertThat(orphanFiles).isEmpty();\n \n     // Test with no equal authorities\n     assertThatThrownBy(\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\nindex 1ef122822157..a03751862177 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\n@@ -20,11 +20,15 @@\n \n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.util.Arrays;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.IntStream;\n+import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.EnvironmentContext;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.SnapshotSummary;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.catalog.TableIdentifier;\n@@ -43,34 +47,29 @@\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n import org.apache.spark.sql.internal.SQLConf;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.BeforeClass;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestRewriteDataFilesProcedure extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRewriteDataFilesProcedure extends ExtensionsTestBase {\n \n   private static final String QUOTED_SPECIAL_CHARS_TABLE_NAME = \"`table:with.special:chars`\";\n \n-  public TestRewriteDataFilesProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @BeforeClass\n+  @BeforeAll\n   public static void setupSpark() {\n     // disable AQE as tests assume that writes generate a particular number of files\n     spark.conf().set(SQLConf.ADAPTIVE_EXECUTION_ENABLED().key(), \"false\");\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS %s\", tableName(QUOTED_SPECIAL_CHARS_TABLE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilterCaseSensitivity() {\n     createTable();\n     insertData(10);\n@@ -93,24 +92,25 @@ public void testFilterCaseSensitivity() {\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testZOrderSortExpression() {\n     List<ExtendedParser.RawOrderField> order =\n         ExtendedParser.parseSortOrder(spark, \"c1, zorder(c2, c3)\");\n-    Assert.assertEquals(\"Should parse 2 order fields\", 2, order.size());\n-    Assert.assertEquals(\n-        \"First field should be a ref\", \"c1\", ((NamedReference<?>) order.get(0).term()).name());\n-    Assert.assertTrue(\"Second field should be zorder\", order.get(1).term() instanceof Zorder);\n+    assertThat(order).as(\"Should parse 2 order fields\").hasSize(2);\n+    assertThat(((NamedReference<?>) order.get(0).term()).name())\n+        .as(\"First field should be a ref\")\n+        .isEqualTo(\"c1\");\n+    assertThat(order.get(1).term()).as(\"Second field should be zorder\").isInstanceOf(Zorder.class);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesInEmptyTable() {\n     createTable();\n     List<Object[]> output = sql(\"CALL %s.system.rewrite_data_files('%s')\", catalogName, tableIdent);\n     assertEquals(\"Procedure output must match\", ImmutableList.of(row(0, 0, 0L, 0)), output);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesOnPartitionTable() {\n     createPartitionTable();\n     // create 5 files for each partition (c2 = 'foo' and c2 = 'bar')\n@@ -134,7 +134,7 @@ public void testRewriteDataFilesOnPartitionTable() {\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesOnNonPartitionTable() {\n     createTable();\n     // create 10 files under non-partitioned table\n@@ -158,7 +158,7 @@ public void testRewriteDataFilesOnNonPartitionTable() {\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithOptions() {\n     createTable();\n     // create 10 files under non-partitioned table\n@@ -180,7 +180,7 @@ public void testRewriteDataFilesWithOptions() {\n     assertEquals(\"Data should not change\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithSortStrategy() {\n     createTable();\n     // create 10 files under non-partitioned table\n@@ -208,7 +208,7 @@ public void testRewriteDataFilesWithSortStrategy() {\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithSortStrategyAndMultipleShufflePartitionsPerFile() {\n     createTable();\n     insertData(10 /* file count */);\n@@ -243,7 +243,7 @@ public void testRewriteDataFilesWithSortStrategyAndMultipleShufflePartitionsPerF\n     assertEquals(\"Should have expected rows\", expectedRows, sql(\"SELECT * FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithZOrder() {\n     createTable();\n     // create 10 files under non-partitioned table\n@@ -284,7 +284,7 @@ public void testRewriteDataFilesWithZOrder() {\n     assertEquals(\"Should have expected rows\", expectedRows, sql(\"SELECT * FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithZOrderNullBinaryColumn() {\n     sql(\"CREATE TABLE %s (c1 int, c2 string, c3 binary) USING iceberg\", tableName);\n \n@@ -319,7 +319,7 @@ public void testRewriteDataFilesWithZOrderNullBinaryColumn() {\n             row(1, \"foo\", null));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithZOrderAndMultipleShufflePartitionsPerFile() {\n     createTable();\n     insertData(10 /* file count */);\n@@ -355,7 +355,7 @@ public void testRewriteDataFilesWithZOrderAndMultipleShufflePartitionsPerFile()\n     assertEquals(\"Should have expected rows\", expectedRows, sql(\"SELECT * FROM %s\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithFilter() {\n     createTable();\n     // create 10 files under non-partitioned table\n@@ -383,7 +383,7 @@ public void testRewriteDataFilesWithFilter() {\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithDeterministicTrueFilter() {\n     createTable();\n     // create 10 files under non-partitioned table\n@@ -407,7 +407,7 @@ public void testRewriteDataFilesWithDeterministicTrueFilter() {\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithDeterministicFalseFilter() {\n     createTable();\n     // create 10 files under non-partitioned table\n@@ -426,7 +426,7 @@ public void testRewriteDataFilesWithDeterministicFalseFilter() {\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithFilterOnPartitionTable() {\n     createPartitionTable();\n     // create 5 files for each partition (c2 = 'foo' and c2 = 'bar')\n@@ -454,11 +454,11 @@ public void testRewriteDataFilesWithFilterOnPartitionTable() {\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithFilterOnOnBucketExpression() {\n     // currently spark session catalog only resolve to v1 functions instead of desired v2 functions\n     // https://github.com/apache/spark/blob/branch-3.4/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L2070-L2083\n-    Assume.assumeFalse(catalogName.equals(SparkCatalogConfig.SPARK.catalogName()));\n+    assumeThat(catalogName).isNotEqualTo(SparkCatalogConfig.SPARK.catalogName());\n     createBucketPartitionTable();\n     // create 5 files for each partition (c2 = 'foo' and c2 = 'bar')\n     insertData(10);\n@@ -486,7 +486,7 @@ public void testRewriteDataFilesWithFilterOnOnBucketExpression() {\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithInFilterOnPartitionTable() {\n     createPartitionTable();\n     // create 5 files for each partition (c2 = 'foo' and c2 = 'bar')\n@@ -514,7 +514,7 @@ public void testRewriteDataFilesWithInFilterOnPartitionTable() {\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithAllPossibleFilters() {\n     createPartitionTable();\n     // create 5 files for each partition (c2 = 'foo' and c2 = 'bar')\n@@ -581,11 +581,11 @@ public void testRewriteDataFilesWithAllPossibleFilters() {\n     //     \" where => 'c2 like \\\"%s\\\"')\", catalogName, tableIdent, \"%car%\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithPossibleV2Filters() {\n     // currently spark session catalog only resolve to v1 functions instead of desired v2 functions\n     // https://github.com/apache/spark/blob/branch-3.4/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L2070-L2083\n-    Assume.assumeFalse(catalogName.equals(SparkCatalogConfig.SPARK.catalogName()));\n+    assumeThat(catalogName).isNotEqualTo(SparkCatalogConfig.SPARK.catalogName());\n \n     SystemFunctionPushDownHelper.createPartitionedTable(spark, tableName, \"id\");\n     sql(\n@@ -614,7 +614,7 @@ public void testRewriteDataFilesWithPossibleV2Filters() {\n         catalogName, tableIdent, catalogName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteDataFilesWithInvalidInputs() {\n     createTable();\n     // create 2 files under non-partitioned table\n@@ -712,7 +712,7 @@ public void testRewriteDataFilesWithInvalidInputs() {\n             \"Cannot mix identity sort columns and a Zorder sort expression:\" + \" c1,zorder(c2,c3)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidCasesForRewriteDataFiles() {\n     assertThatThrownBy(\n             () -> sql(\"CALL %s.system.rewrite_data_files('n', table => 't')\", catalogName))\n@@ -725,8 +725,8 @@ public void testInvalidCasesForRewriteDataFiles() {\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n-              Assert.assertEquals(\"PARSE_SYNTAX_ERROR\", parseException.getErrorClass());\n-              Assert.assertEquals(\"'CALL'\", parseException.getMessageParameters().get(\"error\"));\n+              assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n+              assertThat(parseException.getMessageParameters().get(\"error\")).isEqualTo(\"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.rewrite_data_files()\", catalogName))\n@@ -743,9 +743,9 @@ public void testInvalidCasesForRewriteDataFiles() {\n         .hasMessage(\"Cannot handle an empty identifier for parameter 'table'\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackTableWithSpecialChars() {\n-    Assume.assumeTrue(catalogName.equals(SparkCatalogConfig.HADOOP.catalogName()));\n+    assumeThat(catalogName).isEqualTo(SparkCatalogConfig.HADOOP.catalogName());\n \n     TableIdentifier identifier =\n         TableIdentifier.of(\"default\", QUOTED_SPECIAL_CHARS_TABLE_NAME.replaceAll(\"`\", \"\"));\n@@ -775,12 +775,12 @@ public void testBinPackTableWithSpecialChars() {\n     List<Object[]> actualRecords = currentData(tableName(QUOTED_SPECIAL_CHARS_TABLE_NAME));\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n \n-    Assert.assertEquals(\"Table cache must be empty\", 0, SparkTableCache.get().size());\n+    assertThat(SparkTableCache.get().size()).as(\"Table cache must be empty\").isZero();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortTableWithSpecialChars() {\n-    Assume.assumeTrue(catalogName.equals(SparkCatalogConfig.HADOOP.catalogName()));\n+    assumeThat(catalogName).isEqualTo(SparkCatalogConfig.HADOOP.catalogName());\n \n     TableIdentifier identifier =\n         TableIdentifier.of(\"default\", QUOTED_SPECIAL_CHARS_TABLE_NAME.replaceAll(\"`\", \"\"));\n@@ -815,12 +815,12 @@ public void testSortTableWithSpecialChars() {\n     List<Object[]> actualRecords = currentData(tableName(QUOTED_SPECIAL_CHARS_TABLE_NAME));\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n \n-    Assert.assertEquals(\"Table cache must be empty\", 0, SparkTableCache.get().size());\n+    assertThat(SparkTableCache.get().size()).as(\"Table cache must be empty\").isZero();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testZOrderTableWithSpecialChars() {\n-    Assume.assumeTrue(catalogName.equals(SparkCatalogConfig.HADOOP.catalogName()));\n+    assumeThat(catalogName).isEqualTo(SparkCatalogConfig.HADOOP.catalogName());\n \n     TableIdentifier identifier =\n         TableIdentifier.of(\"default\", QUOTED_SPECIAL_CHARS_TABLE_NAME.replaceAll(\"`\", \"\"));\n@@ -855,10 +855,10 @@ public void testZOrderTableWithSpecialChars() {\n     List<Object[]> actualRecords = currentData(tableName(QUOTED_SPECIAL_CHARS_TABLE_NAME));\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n \n-    Assert.assertEquals(\"Table cache must be empty\", 0, SparkTableCache.get().size());\n+    assertThat(SparkTableCache.get().size()).as(\"Table cache must be empty\").isZero();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDefaultSortOrder() {\n     createTable();\n     // add a default sort order for a table\n@@ -891,7 +891,7 @@ public void testDefaultSortOrder() {\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteWithUntranslatedOrUnconvertedFilter() {\n     createTable();\n     assertThatThrownBy(\n@@ -911,6 +911,21 @@ public void testRewriteWithUntranslatedOrUnconvertedFilter() {\n         .hasMessageContaining(\"Cannot convert Spark filter\");\n   }\n \n+  @TestTemplate\n+  public void testRewriteDataFilesSummary() {\n+    createTable();\n+    // create 10 files under non-partitioned table\n+    insertData(10);\n+    sql(\"CALL %s.system.rewrite_data_files(table => '%s')\", catalogName, tableIdent);\n+\n+    Map<String, String> summary = snapshotSummary();\n+    assertThat(summary)\n+        .containsKey(CatalogProperties.APP_ID)\n+        .containsEntry(EnvironmentContext.ENGINE_NAME, \"spark\")\n+        .hasEntrySatisfying(\n+            EnvironmentContext.ENGINE_VERSION, v -> assertThat(v).startsWith(\"3.4\"));\n+  }\n+\n   private void createTable() {\n     sql(\"CREATE TABLE %s (c1 int, c2 string, c3 string) USING iceberg\", tableName);\n   }\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java\nindex 5e137431b20d..ec95adde3885 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java\n@@ -19,12 +19,13 @@\n package org.apache.iceberg.spark.extensions;\n \n import static org.apache.iceberg.TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.sql.Date;\n import java.sql.Timestamp;\n import java.util.List;\n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n@@ -32,30 +33,26 @@\n import org.apache.spark.sql.RowFactory;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestRewriteManifestsProcedure extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRewriteManifestsProcedure extends ExtensionsTestBase {\n \n-  public TestRewriteManifestsProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteManifestsInEmptyTable() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n     List<Object[]> output = sql(\"CALL %s.system.rewrite_manifests('%s')\", catalogName, tableIdent);\n     assertEquals(\"Procedure output must match\", ImmutableList.of(row(0, 0)), output);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteLargeManifests() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg PARTITIONED BY (data)\",\n@@ -64,8 +61,9 @@ public void testRewriteLargeManifests() {\n \n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\n-        \"Must have 1 manifest\", 1, table.currentSnapshot().allManifests(table.io()).size());\n+    assertThat(table.currentSnapshot().allManifests(table.io()))\n+        .as(\"Must have 1 manifest\")\n+        .hasSize(1);\n \n     sql(\"ALTER TABLE %s SET TBLPROPERTIES ('commit.manifest.target-size-bytes' '1')\", tableName);\n \n@@ -74,11 +72,12 @@ public void testRewriteLargeManifests() {\n \n     table.refresh();\n \n-    Assert.assertEquals(\n-        \"Must have 4 manifests\", 4, table.currentSnapshot().allManifests(table.io()).size());\n+    assertThat(table.currentSnapshot().allManifests(table.io()))\n+        .as(\"Must have 4 manifests\")\n+        .hasSize(4);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteManifestsNoOp() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg PARTITIONED BY (data)\",\n@@ -87,8 +86,9 @@ public void testRewriteManifestsNoOp() {\n \n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\n-        \"Must have 1 manifest\", 1, table.currentSnapshot().allManifests(table.io()).size());\n+    assertThat(table.currentSnapshot().allManifests(table.io()))\n+        .as(\"Must have 1 manifest\")\n+        .hasSize(1);\n \n     List<Object[]> output = sql(\"CALL %s.system.rewrite_manifests('%s')\", catalogName, tableIdent);\n     // should not rewrite any manifests for no-op (output of rewrite is same as before and after)\n@@ -96,11 +96,12 @@ public void testRewriteManifestsNoOp() {\n \n     table.refresh();\n \n-    Assert.assertEquals(\n-        \"Must have 1 manifests\", 1, table.currentSnapshot().allManifests(table.io()).size());\n+    assertThat(table.currentSnapshot().allManifests(table.io()))\n+        .as(\"Must have 1 manifest\")\n+        .hasSize(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteLargeManifestsOnDatePartitionedTableWithJava8APIEnabled() {\n     withSQLConf(\n         ImmutableMap.of(\"spark.sql.datetime.java8API.enabled\", \"true\"),\n@@ -126,8 +127,9 @@ public void testRewriteLargeManifestsOnDatePartitionedTableWithJava8APIEnabled()\n \n           Table table = validationCatalog.loadTable(tableIdent);\n \n-          Assert.assertEquals(\n-              \"Must have 1 manifest\", 1, table.currentSnapshot().allManifests(table.io()).size());\n+          assertThat(table.currentSnapshot().allManifests(table.io()))\n+              .as(\"Must have 1 manifest\")\n+              .hasSize(1);\n \n           sql(\n               \"ALTER TABLE %s SET TBLPROPERTIES ('commit.manifest.target-size-bytes' '1')\",\n@@ -139,12 +141,13 @@ public void testRewriteLargeManifestsOnDatePartitionedTableWithJava8APIEnabled()\n \n           table.refresh();\n \n-          Assert.assertEquals(\n-              \"Must have 4 manifests\", 4, table.currentSnapshot().allManifests(table.io()).size());\n+          assertThat(table.currentSnapshot().allManifests(table.io()))\n+              .as(\"Must have 4 manifests\")\n+              .hasSize(4);\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteLargeManifestsOnTimestampPartitionedTableWithJava8APIEnabled() {\n     withSQLConf(\n         ImmutableMap.of(\"spark.sql.datetime.java8API.enabled\", \"true\"),\n@@ -174,8 +177,9 @@ public void testRewriteLargeManifestsOnTimestampPartitionedTableWithJava8APIEnab\n \n           Table table = validationCatalog.loadTable(tableIdent);\n \n-          Assert.assertEquals(\n-              \"Must have 1 manifest\", 1, table.currentSnapshot().allManifests(table.io()).size());\n+          assertThat(table.currentSnapshot().allManifests(table.io()))\n+              .as(\"Must have 1 manifest\")\n+              .hasSize(1);\n \n           sql(\n               \"ALTER TABLE %s SET TBLPROPERTIES ('commit.manifest.target-size-bytes' '1')\",\n@@ -187,12 +191,13 @@ public void testRewriteLargeManifestsOnTimestampPartitionedTableWithJava8APIEnab\n \n           table.refresh();\n \n-          Assert.assertEquals(\n-              \"Must have 4 manifests\", 4, table.currentSnapshot().allManifests(table.io()).size());\n+          assertThat(table.currentSnapshot().allManifests(table.io()))\n+              .as(\"Must have 4 manifests\")\n+              .hasSize(4);\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteSmallManifestsWithSnapshotIdInheritance() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg PARTITIONED BY (data)\",\n@@ -209,8 +214,9 @@ public void testRewriteSmallManifestsWithSnapshotIdInheritance() {\n \n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\n-        \"Must have 4 manifest\", 4, table.currentSnapshot().allManifests(table.io()).size());\n+    assertThat(table.currentSnapshot().allManifests(table.io()))\n+        .as(\"Must have 4 manifests\")\n+        .hasSize(4);\n \n     List<Object[]> output =\n         sql(\"CALL %s.system.rewrite_manifests(table => '%s')\", catalogName, tableIdent);\n@@ -218,11 +224,12 @@ public void testRewriteSmallManifestsWithSnapshotIdInheritance() {\n \n     table.refresh();\n \n-    Assert.assertEquals(\n-        \"Must have 1 manifests\", 1, table.currentSnapshot().allManifests(table.io()).size());\n+    assertThat(table.currentSnapshot().allManifests(table.io()))\n+        .as(\"Must have 1 manifest\")\n+        .hasSize(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteSmallManifestsWithoutCaching() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg PARTITIONED BY (data)\",\n@@ -233,8 +240,9 @@ public void testRewriteSmallManifestsWithoutCaching() {\n \n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\n-        \"Must have 2 manifest\", 2, table.currentSnapshot().allManifests(table.io()).size());\n+    assertThat(table.currentSnapshot().allManifests(table.io()))\n+        .as(\"Must have 2 manifest\")\n+        .hasSize(2);\n \n     List<Object[]> output =\n         sql(\n@@ -244,11 +252,12 @@ public void testRewriteSmallManifestsWithoutCaching() {\n \n     table.refresh();\n \n-    Assert.assertEquals(\n-        \"Must have 1 manifests\", 1, table.currentSnapshot().allManifests(table.io()).size());\n+    assertThat(table.currentSnapshot().allManifests(table.io()))\n+        .as(\"Must have 1 manifest\")\n+        .hasSize(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteManifestsCaseInsensitiveArgs() {\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg PARTITIONED BY (data)\",\n@@ -259,8 +268,9 @@ public void testRewriteManifestsCaseInsensitiveArgs() {\n \n     Table table = validationCatalog.loadTable(tableIdent);\n \n-    Assert.assertEquals(\n-        \"Must have 2 manifest\", 2, table.currentSnapshot().allManifests(table.io()).size());\n+    assertThat(table.currentSnapshot().allManifests(table.io()))\n+        .as(\"Must have 2 manifests\")\n+        .hasSize(2);\n \n     List<Object[]> output =\n         sql(\n@@ -270,11 +280,12 @@ public void testRewriteManifestsCaseInsensitiveArgs() {\n \n     table.refresh();\n \n-    Assert.assertEquals(\n-        \"Must have 1 manifests\", 1, table.currentSnapshot().allManifests(table.io()).size());\n+    assertThat(table.currentSnapshot().allManifests(table.io()))\n+        .as(\"Must have 1 manifest\")\n+        .hasSize(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidRewriteManifestsCases() {\n     assertThatThrownBy(\n             () -> sql(\"CALL %s.system.rewrite_manifests('n', table => 't')\", catalogName))\n@@ -287,8 +298,8 @@ public void testInvalidRewriteManifestsCases() {\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n-              Assert.assertEquals(\"PARSE_SYNTAX_ERROR\", parseException.getErrorClass());\n-              Assert.assertEquals(\"'CALL'\", parseException.getMessageParameters().get(\"error\"));\n+              assertThat(parseException.getErrorClass()).isEqualTo(\"PARSE_SYNTAX_ERROR\");\n+              assertThat(parseException.getMessageParameters().get(\"error\")).isEqualTo(\"'CALL'\");\n             });\n \n     assertThatThrownBy(() -> sql(\"CALL %s.system.rewrite_manifests()\", catalogName))\n@@ -309,7 +320,7 @@ public void testInvalidRewriteManifestsCases() {\n         .hasMessage(\"Cannot handle an empty identifier for argument table\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testReplacePartitionField() {\n     sql(\n         \"CREATE TABLE %s (id int, ts timestamp, day_of_ts date) USING iceberg PARTITIONED BY (day_of_ts)\",\n@@ -336,7 +347,7 @@ public void testReplacePartitionField() {\n         sql(\"SELECT * FROM %s WHERE ts < current_timestamp()\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWriteManifestWithSpecId() {\n     sql(\n         \"CREATE TABLE %s (id int, dt string, hr string) USING iceberg PARTITIONED BY (dt)\",\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewritePositionDeleteFiles.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewritePositionDeleteFiles.java\nindex 2d3d3b851a99..f3be0a870972 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewritePositionDeleteFiles.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewritePositionDeleteFiles.java\n@@ -32,6 +32,7 @@\n import java.time.LocalDateTime;\n import java.util.List;\n import java.util.Map;\n+import java.util.UUID;\n import java.util.function.Function;\n import java.util.stream.Collectors;\n import org.apache.iceberg.ContentFile;\n@@ -42,6 +43,7 @@\n import org.apache.iceberg.Files;\n import org.apache.iceberg.MetadataTableType;\n import org.apache.iceberg.MetadataTableUtils;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PositionDeletesScanTask;\n import org.apache.iceberg.RowDelta;\n import org.apache.iceberg.ScanTask;\n@@ -69,13 +71,10 @@\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.types.StructType;\n-import org.junit.After;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n \n-public class TestRewritePositionDeleteFiles extends SparkExtensionsTestBase {\n+public class TestRewritePositionDeleteFiles extends ExtensionsTestBase {\n \n   private static final Map<String, String> CATALOG_PROPS =\n       ImmutableMap.of(\n@@ -89,8 +88,7 @@ public class TestRewritePositionDeleteFiles extends SparkExtensionsTestBase {\n   private static final int DELETE_FILES_PER_PARTITION = 2;\n   private static final int DELETE_FILE_SIZE = 10;\n \n-  @Parameterized.Parameters(\n-      name = \"formatVersion = {0}, catalogName = {1}, implementation = {2}, config = {3}\")\n+  @Parameters(name = \"formatVersion = {0}, catalogName = {1}, implementation = {2}, config = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n       {\n@@ -101,19 +99,12 @@ public static Object[][] parameters() {\n     };\n   }\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n-\n-  public TestRewritePositionDeleteFiles(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @After\n+  @AfterEach\n   public void cleanup() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDatePartition() throws Exception {\n     createTable(\"date\");\n     Date baseDate = Date.valueOf(\"2023-01-01\");\n@@ -121,14 +112,14 @@ public void testDatePartition() throws Exception {\n     testDanglingDelete();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBooleanPartition() throws Exception {\n     createTable(\"boolean\");\n     insertData(i -> i % 2 == 0, 2);\n     testDanglingDelete(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimestampPartition() throws Exception {\n     createTable(\"timestamp\");\n     Timestamp baseTimestamp = Timestamp.valueOf(\"2023-01-01 15:30:00\");\n@@ -136,7 +127,7 @@ public void testTimestampPartition() throws Exception {\n     testDanglingDelete();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTimestampNtz() throws Exception {\n     createTable(\"timestamp_ntz\");\n     LocalDateTime baseTimestamp = Timestamp.valueOf(\"2023-01-01 15:30:00\").toLocalDateTime();\n@@ -144,14 +135,14 @@ public void testTimestampNtz() throws Exception {\n     testDanglingDelete();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBytePartition() throws Exception {\n     createTable(\"byte\");\n     insertData(i -> i);\n     testDanglingDelete();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDecimalPartition() throws Exception {\n     createTable(\"decimal(18, 10)\");\n     BigDecimal baseDecimal = new BigDecimal(\"1.0\");\n@@ -159,35 +150,35 @@ public void testDecimalPartition() throws Exception {\n     testDanglingDelete();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinaryPartition() throws Exception {\n     createTable(\"binary\");\n     insertData(i -> java.nio.ByteBuffer.allocate(4).putInt(i).array());\n     testDanglingDelete();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCharPartition() throws Exception {\n     createTable(\"char(10)\");\n     insertData(Object::toString);\n     testDanglingDelete();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testVarcharPartition() throws Exception {\n     createTable(\"varchar(10)\");\n     insertData(Object::toString);\n     testDanglingDelete();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testIntPartition() throws Exception {\n     createTable(\"int\");\n     insertData(i -> i);\n     testDanglingDelete();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDaysPartitionTransform() throws Exception {\n     createTable(\"timestamp\", PARTITION_COL, String.format(\"days(%s)\", PARTITION_COL));\n     Timestamp baseTimestamp = Timestamp.valueOf(\"2023-01-01 15:30:00\");\n@@ -195,14 +186,14 @@ public void testDaysPartitionTransform() throws Exception {\n     testDanglingDelete();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNullTransform() throws Exception {\n     createTable(\"int\");\n     insertData(i -> i == 0 ? null : 1, 2);\n     testDanglingDelete(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionColWithDot() throws Exception {\n     String partitionColWithDot = \"`partition.col`\";\n     createTable(\"int\", partitionColWithDot, partitionColWithDot);\n@@ -318,7 +309,8 @@ private void writePosDeletesForFiles(Table table, List<DataFile> files) throws I\n           counter++;\n           if (counter == deleteFileSize) {\n             // Dump to file and reset variables\n-            OutputFile output = Files.localOutput(temp.newFile());\n+            OutputFile output =\n+                Files.localOutput(temp.resolve(UUID.randomUUID().toString()).toFile());\n             deleteFiles.add(writeDeleteFile(table, output, partition, deletes));\n             counter = 0;\n             deletes.clear();\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewritePositionDeleteFilesProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewritePositionDeleteFilesProcedure.java\nindex 261dbcf7b7f3..bec5e06b37a1 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewritePositionDeleteFilesProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewritePositionDeleteFilesProcedure.java\n@@ -20,26 +20,26 @@\n \n import static org.apache.iceberg.SnapshotSummary.ADDED_FILE_SIZE_PROP;\n import static org.apache.iceberg.SnapshotSummary.REMOVED_FILE_SIZE_PROP;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n import java.util.Map;\n+import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.EnvironmentContext;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.spark.data.TestHelpers;\n import org.apache.iceberg.spark.source.SimpleRecord;\n import org.apache.spark.sql.Encoders;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestRewritePositionDeleteFilesProcedure extends SparkExtensionsTestBase {\n-\n-  public TestRewritePositionDeleteFilesProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRewritePositionDeleteFilesProcedure extends ExtensionsTestBase {\n \n   private void createTable() throws Exception {\n     createTable(false);\n@@ -79,12 +79,12 @@ private void createTable(boolean partitioned) throws Exception {\n         .append();\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireDeleteFilesAll() throws Exception {\n     createTable();\n \n@@ -92,7 +92,7 @@ public void testExpireDeleteFilesAll() throws Exception {\n     sql(\"DELETE FROM %s WHERE id=2\", tableName);\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(2, TestHelpers.deleteFiles(table).size());\n+    assertThat(TestHelpers.deleteFiles(table)).hasSize(2);\n \n     List<Object[]> output =\n         sql(\n@@ -114,10 +114,10 @@ public void testExpireDeleteFilesAll() throws Exception {\n                 Long.valueOf(snapshotSummary.get(ADDED_FILE_SIZE_PROP)))),\n         output);\n \n-    Assert.assertEquals(1, TestHelpers.deleteFiles(table).size());\n+    assertThat(TestHelpers.deleteFiles(table)).hasSize(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireDeleteFilesNoOption() throws Exception {\n     createTable();\n \n@@ -128,7 +128,7 @@ public void testExpireDeleteFilesNoOption() throws Exception {\n     sql(\"DELETE FROM %s WHERE id=5\", tableName);\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(5, TestHelpers.deleteFiles(table).size());\n+    assertThat(TestHelpers.deleteFiles(table)).hasSize(5);\n \n     List<Object[]> output =\n         sql(\n@@ -148,7 +148,7 @@ public void testExpireDeleteFilesNoOption() throws Exception {\n         output);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireDeleteFilesFilter() throws Exception {\n     createTable(true);\n \n@@ -160,7 +160,7 @@ public void testExpireDeleteFilesFilter() throws Exception {\n     sql(\"DELETE FROM %s WHERE id = 3 and data='h'\", tableName);\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(6, TestHelpers.deleteFiles(table).size());\n+    assertThat(TestHelpers.deleteFiles(table)).hasSize(6);\n \n     List<Object[]> output =\n         sql(\n@@ -184,26 +184,27 @@ public void testExpireDeleteFilesFilter() throws Exception {\n                 Long.valueOf(snapshotSummary.get(ADDED_FILE_SIZE_PROP)))),\n         output);\n \n-    Assert.assertEquals(4, TestHelpers.deleteFiles(table).size());\n+    assertThat(TestHelpers.deleteFiles(table)).hasSize(4);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidOption() throws Exception {\n     createTable();\n \n-    Assert.assertThrows(\n-        \"Cannot use options [foo], they are not supported by the action or the rewriter BIN-PACK\",\n-        IllegalArgumentException.class,\n-        () ->\n-            sql(\n-                \"CALL %s.system.rewrite_position_delete_files(\"\n-                    + \"table => '%s',\"\n-                    + \"options => map(\"\n-                    + \"'foo', 'bar'))\",\n-                catalogName, tableIdent));\n+    assertThatThrownBy(\n+            () ->\n+                sql(\n+                    \"CALL %s.system.rewrite_position_delete_files(\"\n+                        + \"table => '%s',\"\n+                        + \"options => map(\"\n+                        + \"'foo', 'bar'))\",\n+                    catalogName, tableIdent))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\n+            \"Cannot use options [foo], they are not supported by the action or the rewriter BIN-PACK\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteWithUntranslatedOrUnconvertedFilter() throws Exception {\n     createTable();\n     assertThatThrownBy(\n@@ -223,6 +224,26 @@ public void testRewriteWithUntranslatedOrUnconvertedFilter() throws Exception {\n         .hasMessageContaining(\"Cannot convert Spark filter\");\n   }\n \n+  @TestTemplate\n+  public void testRewriteSummary() throws Exception {\n+    createTable();\n+    sql(\"DELETE FROM %s WHERE id=1\", tableName);\n+\n+    sql(\n+        \"CALL %s.system.rewrite_position_delete_files(\"\n+            + \"table => '%s',\"\n+            + \"options => map(\"\n+            + \"'rewrite-all','true'))\",\n+        catalogName, tableIdent);\n+\n+    Map<String, String> summary = snapshotSummary();\n+    assertThat(summary)\n+        .containsKey(CatalogProperties.APP_ID)\n+        .containsEntry(EnvironmentContext.ENGINE_NAME, \"spark\")\n+        .hasEntrySatisfying(\n+            EnvironmentContext.ENGINE_VERSION, v -> assertThat(v).startsWith(\"3.4\"));\n+  }\n+\n   private Map<String, String> snapshotSummary() {\n     return validationCatalog.loadTable(tableIdent).currentSnapshot().summary();\n   }\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java\nindex 1cd0545207d5..2ca8de50fa14 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java\n@@ -18,42 +18,39 @@\n  */\n package org.apache.iceberg.spark.extensions;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assertions.entry;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.IOException;\n+import java.nio.file.Files;\n+import java.util.List;\n import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.spark.sql.AnalysisException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-\n-public class TestSnapshotTableProcedure extends SparkExtensionsTestBase {\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestSnapshotTableProcedure extends ExtensionsTestBase {\n   private static final String SOURCE_NAME = \"spark_catalog.default.source\";\n \n   // Currently we can only Snapshot only out of the Spark Session Catalog\n \n-  public TestSnapshotTableProcedure(\n-      String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n-\n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS %s PURGE\", SOURCE_NAME);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSnapshot() throws IOException {\n-    String location = temp.newFolder().toString();\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         SOURCE_NAME, location);\n@@ -61,11 +58,13 @@ public void testSnapshot() throws IOException {\n     Object result =\n         scalarSql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, SOURCE_NAME, tableName);\n \n-    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+    assertThat(result).as(\"Should have added one file\").isEqualTo(1L);\n \n     Table createdTable = validationCatalog.loadTable(tableIdent);\n     String tableLocation = createdTable.location();\n-    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+    assertThat(tableLocation)\n+        .as(\"Table should not have the original location\")\n+        .isNotEqualTo(location);\n \n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n \n@@ -75,9 +74,9 @@ public void testSnapshot() throws IOException {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSnapshotWithProperties() throws IOException {\n-    String location = temp.newFolder().toString();\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         SOURCE_NAME, location);\n@@ -87,15 +86,17 @@ public void testSnapshotWithProperties() throws IOException {\n             \"CALL %s.system.snapshot(source_table => '%s', table => '%s', properties => map('foo','bar'))\",\n             catalogName, SOURCE_NAME, tableName);\n \n-    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+    assertThat(result).as(\"Should have added one file\").isEqualTo(1L);\n \n     Table createdTable = validationCatalog.loadTable(tableIdent);\n \n     String tableLocation = createdTable.location();\n-    Assert.assertNotEquals(\"Table should not have the original location\", location, tableLocation);\n+    assertThat(tableLocation)\n+        .as(\"Table should not have the original location\")\n+        .isNotEqualTo(location);\n \n     Map<String, String> props = createdTable.properties();\n-    Assert.assertEquals(\"Should have extra property set\", \"bar\", props.get(\"foo\"));\n+    assertThat(props).as(\"Should have extra property set\").containsEntry(\"foo\", \"bar\");\n \n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n \n@@ -105,13 +106,13 @@ public void testSnapshotWithProperties() throws IOException {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSnapshotWithAlternateLocation() throws IOException {\n-    Assume.assumeTrue(\n-        \"No Snapshoting with Alternate locations with Hadoop Catalogs\",\n-        !catalogName.contains(\"hadoop\"));\n-    String location = temp.newFolder().toString();\n-    String snapshotLocation = temp.newFolder().toString();\n+    assumeThat(catalogName)\n+        .as(\"No Snapshoting with Alternate locations with Hadoop Catalogs\")\n+        .doesNotContain(\"hadoop\");\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n+    String snapshotLocation = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         SOURCE_NAME, location);\n@@ -122,11 +123,12 @@ public void testSnapshotWithAlternateLocation() throws IOException {\n                 catalogName, SOURCE_NAME, tableName, snapshotLocation)\n             .get(0);\n \n-    Assert.assertEquals(\"Should have added one file\", 1L, result[0]);\n+    assertThat(result[0]).as(\"Should have added one file\").isEqualTo(1L);\n \n     String storageLocation = validationCatalog.loadTable(tableIdent).location();\n-    Assert.assertEquals(\n-        \"Snapshot should be made at specified location\", snapshotLocation, storageLocation);\n+    assertThat(storageLocation)\n+        .as(\"Snapshot should be made at specified location\")\n+        .isEqualTo(snapshotLocation);\n \n     sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", tableName);\n \n@@ -136,9 +138,9 @@ public void testSnapshotWithAlternateLocation() throws IOException {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropTable() throws IOException {\n-    String location = temp.newFolder().toString();\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         SOURCE_NAME, location);\n@@ -146,7 +148,7 @@ public void testDropTable() throws IOException {\n \n     Object result =\n         scalarSql(\"CALL %s.system.snapshot('%s', '%s')\", catalogName, SOURCE_NAME, tableName);\n-    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+    assertThat(result).as(\"Should have added one file\").isEqualTo(1L);\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -161,9 +163,9 @@ public void testDropTable() throws IOException {\n         sql(\"SELECT * FROM %s\", SOURCE_NAME));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSnapshotWithConflictingProps() throws IOException {\n-    String location = temp.newFolder().toString();\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         SOURCE_NAME, location);\n@@ -176,7 +178,7 @@ public void testSnapshotWithConflictingProps() throws IOException {\n                 + \"table => '%s',\"\n                 + \"properties => map('%s', 'true', 'snapshot', 'false'))\",\n             catalogName, SOURCE_NAME, tableName, TableProperties.GC_ENABLED);\n-    Assert.assertEquals(\"Should have added one file\", 1L, result);\n+    assertThat(result).as(\"Should have added one file\").isEqualTo(1L);\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -185,14 +187,13 @@ public void testSnapshotWithConflictingProps() throws IOException {\n \n     Table table = validationCatalog.loadTable(tableIdent);\n     Map<String, String> props = table.properties();\n-    Assert.assertEquals(\"Should override user value\", \"true\", props.get(\"snapshot\"));\n-    Assert.assertEquals(\n-        \"Should override user value\", \"false\", props.get(TableProperties.GC_ENABLED));\n+    assertThat(props)\n+        .contains(entry(\"snapshot\", \"true\"), entry(TableProperties.GC_ENABLED, \"false\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidSnapshotsCases() throws IOException {\n-    String location = temp.newFolder().toString();\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n         SOURCE_NAME, location);\n@@ -224,9 +225,47 @@ public void testInvalidSnapshotsCases() throws IOException {\n         .hasMessage(\"Cannot handle an empty identifier for argument table\");\n   }\n \n-  @Test\n+  @TestTemplate\n+  public void testSnapshotWithParallelism() throws IOException {\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n+    sql(\n+        \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n+        SOURCE_NAME, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", SOURCE_NAME);\n+    sql(\"INSERT INTO TABLE %s VALUES (2, 'b')\", SOURCE_NAME);\n+\n+    List<Object[]> result =\n+        sql(\n+            \"CALL %s.system.snapshot(source_table => '%s', table => '%s', parallelism => %d)\",\n+            catalogName, SOURCE_NAME, tableName, 2);\n+    assertEquals(\"Procedure output must match\", ImmutableList.of(row(2L)), result);\n+    assertEquals(\n+        \"Should have expected rows\",\n+        ImmutableList.of(row(1L, \"a\"), row(2L, \"b\")),\n+        sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n+  }\n+\n+  @TestTemplate\n+  public void testSnapshotWithInvalidParallelism() throws IOException {\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n+    sql(\n+        \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n+        SOURCE_NAME, location);\n+    sql(\"INSERT INTO TABLE %s VALUES (1, 'a')\", SOURCE_NAME);\n+    sql(\"INSERT INTO TABLE %s VALUES (2, 'b')\", SOURCE_NAME);\n+\n+    assertThatThrownBy(\n+            () ->\n+                sql(\n+                    \"CALL %s.system.snapshot(source_table => '%s', table => '%s', parallelism => %d)\",\n+                    catalogName, SOURCE_NAME, tableName, -1))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\"Parallelism should be larger than 0\");\n+  }\n+\n+  @TestTemplate\n   public void testSnapshotPartitionedWithParallelism() throws IOException {\n-    String location = temp.newFolder().toString();\n+    String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet PARTITIONED BY (id) LOCATION '%s'\",\n         SOURCE_NAME, location);\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java\nindex 0a18d4b844c8..56aa40aba954 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java\n@@ -23,7 +23,6 @@\n import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.File;\n-import java.nio.file.Path;\n import java.util.List;\n import java.util.regex.Matcher;\n import java.util.regex.Pattern;\n@@ -66,7 +65,6 @@\n import org.junit.jupiter.api.Disabled;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n-import org.junit.jupiter.api.io.TempDir;\n \n @ExtendWith(ParameterizedTestExtension.class)\n public class TestAddFilesProcedure extends ExtensionsTestBase {\n@@ -101,8 +99,6 @@ public static Object[][] parameters() {\n   private final String sourceTableName = \"source_table\";\n   private File fileTableDir;\n \n-  @TempDir private Path temp;\n-\n   @BeforeEach\n   public void setupTempDirs() {\n     fileTableDir = temp.toFile();\n@@ -743,7 +739,7 @@ public void addWeirdCaseHiveTable() {\n                 \"SELECT id, `naMe`, dept, subdept from %s WHERE `naMe` = 'John Doe' ORDER BY id\",\n                 sourceTableName))\n         .as(\"If this assert breaks it means that Spark has fixed the pushdown issue\")\n-        .hasSize(0);\n+        .isEmpty();\n \n     // Pushdown works for iceberg\n     assertThat(\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\nindex 55816d9c0750..1560abf1123e 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\n@@ -36,6 +36,7 @@\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.GenericBlobMetadata;\n import org.apache.iceberg.GenericStatisticsFile;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.PartitionStatisticsFile;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.StatisticsFile;\n@@ -55,7 +56,9 @@\n import org.apache.spark.sql.catalyst.parser.ParseException;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestExpireSnapshotsProcedure extends ExtensionsTestBase {\n \n   @AfterEach\n@@ -303,8 +306,8 @@ public void testExpireDeleteFiles() throws Exception {\n     sql(\"INSERT INTO TABLE %s VALUES (6, 'f')\", tableName); // this txn removes the file reference\n     table.refresh();\n \n-    assertThat(TestHelpers.deleteManifests(table)).as(\"Should have no delete manifests\").hasSize(0);\n-    assertThat(TestHelpers.deleteFiles(table)).as(\"Should have no delete files\").hasSize(0);\n+    assertThat(TestHelpers.deleteManifests(table)).as(\"Should have no delete manifests\").isEmpty();\n+    assertThat(TestHelpers.deleteFiles(table)).as(\"Should have no delete files\").isEmpty();\n \n     FileSystem localFs = FileSystem.getLocal(new Configuration());\n     assertThat(localFs.exists(deleteManifestPath))\n@@ -373,11 +376,11 @@ public void testExpireSnapshotsWithSnapshotId() {\n \n     // There should only be one single snapshot left.\n     table.refresh();\n-    assertThat(table.snapshots()).as(\"Should be 1 snapshots\").hasSize(1);\n     assertThat(table.snapshots())\n+        .hasSize(1)\n         .as(\"Snapshot ID should not be present\")\n         .filteredOn(snapshot -> snapshot.snapshotId() == firstSnapshotId)\n-        .hasSize(0);\n+        .isEmpty();\n   }\n \n   @TestTemplate\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMigrateTableProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMigrateTableProcedure.java\nindex 69e80026e611..c189311a2042 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMigrateTableProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMigrateTableProcedure.java\n@@ -141,7 +141,6 @@ public void testMigrateWithBackupTableName() throws IOException {\n   @TestTemplate\n   public void testMigrateWithInvalidMetricsConfig() throws IOException {\n     assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n-\n     String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n@@ -159,7 +158,6 @@ public void testMigrateWithInvalidMetricsConfig() throws IOException {\n   @TestTemplate\n   public void testMigrateWithConflictingProps() throws IOException {\n     assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n-\n     String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n@@ -236,7 +234,6 @@ public void testMigrateEmptyTable() throws Exception {\n   @TestTemplate\n   public void testMigrateWithParallelism() throws IOException {\n     assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n-\n     String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n@@ -257,7 +254,6 @@ public void testMigrateWithParallelism() throws IOException {\n   @TestTemplate\n   public void testMigrateWithInvalidParallelism() throws IOException {\n     assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n-\n     String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet LOCATION '%s'\",\n@@ -277,7 +273,6 @@ public void testMigrateWithInvalidParallelism() throws IOException {\n   @TestTemplate\n   public void testMigratePartitionedWithParallelism() throws IOException {\n     assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n-\n     String location = Files.createTempDirectory(temp, \"junit\").toFile().toString();\n     sql(\n         \"CREATE TABLE %s (id bigint NOT NULL, data string) USING parquet PARTITIONED BY (id) LOCATION '%s'\",\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\nindex 8508884fc4bf..138ac1e6b056 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\n@@ -41,6 +41,7 @@\n import org.apache.iceberg.GenericBlobMetadata;\n import org.apache.iceberg.GenericStatisticsFile;\n import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.PartitionStatisticsFile;\n import org.apache.iceberg.ReachableFileUtil;\n@@ -52,7 +53,6 @@\n import org.apache.iceberg.puffin.Puffin;\n import org.apache.iceberg.puffin.PuffinWriter;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.spark.Spark3Util;\n import org.apache.iceberg.spark.data.TestHelpers;\n@@ -66,7 +66,9 @@\n import org.apache.spark.sql.catalyst.parser.ParseException;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestRemoveOrphanFilesProcedure extends ExtensionsTestBase {\n \n   @AfterEach\n@@ -140,7 +142,7 @@ public void testRemoveOrphanFilesInDataFolder() throws IOException {\n                 + \"table => '%s',\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, currentTimestamp);\n-    assertThat(output3).as(\"Should be no more orphan files in the data folder\").hasSize(0);\n+    assertThat(output3).as(\"Should be no more orphan files in the data folder\").isEmpty();\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -200,7 +202,7 @@ public void testRemoveOrphanFilesDryRun() throws IOException {\n                 + \"table => '%s',\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, currentTimestamp);\n-    assertThat(output3).as(\"Should be no more orphan files\").hasSize(0);\n+    assertThat(output3).as(\"Should be no more orphan files\").isEmpty();\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -322,7 +324,7 @@ public void testConcurrentRemoveOrphanFiles() throws IOException {\n                 + \"max_concurrent_deletes => %s,\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, 4, currentTimestamp);\n-    assertThat(output3).as(\"Should be no more orphan files in the data folder\").hasSize(0);\n+    assertThat(output3).as(\"Should be no more orphan files in the data folder\").isEmpty();\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -425,7 +427,7 @@ public void testRemoveOrphanFilesWithDeleteFiles() throws Exception {\n                 + \"table => '%s',\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, currentTimestamp);\n-    assertThat(output).as(\"Should be no orphan files\").hasSize(0);\n+    assertThat(output).as(\"Should be no orphan files\").isEmpty();\n \n     FileSystem localFs = FileSystem.getLocal(new Configuration());\n     assertThat(localFs.exists(deleteManifestPath))\n@@ -497,8 +499,7 @@ public void testRemoveOrphanFilesWithStatisticFiles() throws Exception {\n             catalogName, tableIdent, currentTimestamp);\n     assertThat(output).as(\"Should be no orphan files\").isEmpty();\n \n-    assertThat(statsLocation).exists();\n-    assertThat(statsLocation).hasSize(statisticsFile.fileSizeInBytes());\n+    assertThat(statsLocation).exists().hasSize(statisticsFile.fileSizeInBytes());\n \n     transaction = table.newTransaction();\n     transaction.updateStatistics().removeStatistics(statisticsFile.snapshotId()).commit();\n@@ -510,10 +511,10 @@ public void testRemoveOrphanFilesWithStatisticFiles() throws Exception {\n                 + \"table => '%s',\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, currentTimestamp);\n-    assertThat(output).as(\"Should be orphan files\").hasSize(1);\n-    assertThat(Iterables.getOnlyElement(output))\n-        .as(\"Deleted files\")\n-        .containsExactly(statsLocation.toURI().toString());\n+    assertThat(output)\n+        .hasSize(1)\n+        .first()\n+        .satisfies(files -> assertThat(files).containsExactly(statsLocation.toURI().toString()));\n     assertThat(statsLocation).doesNotExist();\n   }\n \n@@ -555,10 +556,10 @@ public void testRemoveOrphanFilesWithPartitionStatisticFiles() throws Exception\n                 + \"table => '%s',\"\n                 + \"older_than => TIMESTAMP '%s')\",\n             catalogName, tableIdent, currentTimestamp);\n-    assertThat(output).as(\"Should be orphan files\").hasSize(1);\n-    assertThat(Iterables.getOnlyElement(output))\n-        .as(\"Deleted files\")\n-        .containsExactly(\"file:\" + partitionStatsLocation);\n+    assertThat(output)\n+        .hasSize(1)\n+        .first()\n+        .satisfies(files -> assertThat(files).containsExactly(\"file:\" + partitionStatsLocation));\n     assertThat(new File(partitionStatsLocation))\n         .as(\"partition stats file should be deleted\")\n         .doesNotExist();\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\nindex efd8d03df52b..72f2a67f75a1 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\n@@ -775,7 +775,7 @@ public void testBinPackTableWithSpecialChars() {\n     List<Object[]> actualRecords = currentData(tableName(QUOTED_SPECIAL_CHARS_TABLE_NAME));\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n \n-    assertThat(SparkTableCache.get().size()).as(\"Table cache must be empty\").isEqualTo(0);\n+    assertThat(SparkTableCache.get().size()).as(\"Table cache must be empty\").isZero();\n   }\n \n   @TestTemplate\n@@ -815,7 +815,7 @@ public void testSortTableWithSpecialChars() {\n     List<Object[]> actualRecords = currentData(tableName(QUOTED_SPECIAL_CHARS_TABLE_NAME));\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n \n-    assertThat(SparkTableCache.get().size()).as(\"Table cache must be empty\").isEqualTo(0);\n+    assertThat(SparkTableCache.get().size()).as(\"Table cache must be empty\").isZero();\n   }\n \n   @TestTemplate\n@@ -855,7 +855,7 @@ public void testZOrderTableWithSpecialChars() {\n     List<Object[]> actualRecords = currentData(tableName(QUOTED_SPECIAL_CHARS_TABLE_NAME));\n     assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n \n-    assertThat(SparkTableCache.get().size()).as(\"Table cache must be empty\").isEqualTo(0);\n+    assertThat(SparkTableCache.get().size()).as(\"Table cache must be empty\").isZero();\n   }\n \n   @TestTemplate\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java\nindex 28ae31ec6aa2..e3a00d56e59a 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSnapshotTableProcedure.java\n@@ -20,6 +20,7 @@\n \n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assertions.entry;\n import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.IOException;\n@@ -186,10 +187,8 @@ public void testSnapshotWithConflictingProps() throws IOException {\n \n     Table table = validationCatalog.loadTable(tableIdent);\n     Map<String, String> props = table.properties();\n-    assertThat(props).as(\"Should override user value\").containsEntry(\"snapshot\", \"true\");\n     assertThat(props)\n-        .as(\"Should override user value\")\n-        .containsEntry(TableProperties.GC_ENABLED, \"false\");\n+        .contains(entry(\"snapshot\", \"true\"), entry(TableProperties.GC_ENABLED, \"false\"));\n   }\n \n   @TestTemplate\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12697",
    "pr_id": 12697,
    "issue_id": 10219,
    "repo": "apache/iceberg",
    "problem_statement": "The \"Emitting watermarks\" feature can't be used in flink sql?\n### Query engine\n\nflink 1.18.0\n\n### Question\n\nHi @stevenzwu In the latest version, use flink sql still cannot define watermarks. This is still not possible when our company wants to use flink sql to implement window aggregation to process ODS data. Are there plans to support this?",
    "issue_word_count": 58,
    "test_files_count": 4,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java",
      "flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java",
      "flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java",
      "flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java",
      "flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java",
      "flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java"
    ],
    "pr_changed_test_files": [
      "flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java",
      "flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java",
      "flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java",
      "flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java"
    ],
    "base_commit": "12737a1282d2e82432c0e5924b3fe33dc4ffb109",
    "head_commit": "a7d203f40cb5bc51ee2eb9865892c8f88d1c281d",
    "repo_url": "https://github.com/apache/iceberg/pull/12697",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12697",
    "dockerfile": "",
    "pr_merged_at": "2025-04-01T22:27:43.000Z",
    "patch": "diff --git a/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java b/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java\nindex 65adce77d9f9..662dc30e27ca 100644\n--- a/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java\n+++ b/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java\n@@ -35,12 +35,14 @@\n import org.apache.flink.table.connector.source.abilities.SupportsFilterPushDown;\n import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n import org.apache.flink.table.connector.source.abilities.SupportsProjectionPushDown;\n+import org.apache.flink.table.connector.source.abilities.SupportsSourceWatermark;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.expressions.ResolvedExpression;\n import org.apache.flink.table.types.DataType;\n import org.apache.iceberg.expressions.Expression;\n import org.apache.iceberg.flink.FlinkConfigOptions;\n import org.apache.iceberg.flink.FlinkFilters;\n+import org.apache.iceberg.flink.FlinkReadOptions;\n import org.apache.iceberg.flink.TableLoader;\n import org.apache.iceberg.flink.source.assigner.SplitAssignerType;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n@@ -53,7 +55,8 @@ public class IcebergTableSource\n     implements ScanTableSource,\n         SupportsProjectionPushDown,\n         SupportsFilterPushDown,\n-        SupportsLimitPushDown {\n+        SupportsLimitPushDown,\n+        SupportsSourceWatermark {\n \n   private int[] projectedFields;\n   private Long limit;\n@@ -175,6 +178,17 @@ public Result applyFilters(List<ResolvedExpression> flinkFilters) {\n     return Result.of(acceptedFilters, flinkFilters);\n   }\n \n+  @Override\n+  public void applySourceWatermark() {\n+    Preconditions.checkArgument(\n+        readableConfig.get(FlinkConfigOptions.TABLE_EXEC_ICEBERG_USE_FLIP27_SOURCE),\n+        \"Source watermarks are supported only in flip-27 iceberg source implementation\");\n+\n+    Preconditions.checkNotNull(\n+        properties.get(FlinkReadOptions.WATERMARK_COLUMN),\n+        \"watermark-column needs to be configured to use source watermark.\");\n+  }\n+\n   @Override\n   public boolean supportsNestedProjection() {\n     // TODO: support nested projection\n\ndiff --git a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java\nindex 65adce77d9f9..662dc30e27ca 100644\n--- a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java\n+++ b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java\n@@ -35,12 +35,14 @@\n import org.apache.flink.table.connector.source.abilities.SupportsFilterPushDown;\n import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n import org.apache.flink.table.connector.source.abilities.SupportsProjectionPushDown;\n+import org.apache.flink.table.connector.source.abilities.SupportsSourceWatermark;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.expressions.ResolvedExpression;\n import org.apache.flink.table.types.DataType;\n import org.apache.iceberg.expressions.Expression;\n import org.apache.iceberg.flink.FlinkConfigOptions;\n import org.apache.iceberg.flink.FlinkFilters;\n+import org.apache.iceberg.flink.FlinkReadOptions;\n import org.apache.iceberg.flink.TableLoader;\n import org.apache.iceberg.flink.source.assigner.SplitAssignerType;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n@@ -53,7 +55,8 @@ public class IcebergTableSource\n     implements ScanTableSource,\n         SupportsProjectionPushDown,\n         SupportsFilterPushDown,\n-        SupportsLimitPushDown {\n+        SupportsLimitPushDown,\n+        SupportsSourceWatermark {\n \n   private int[] projectedFields;\n   private Long limit;\n@@ -175,6 +178,17 @@ public Result applyFilters(List<ResolvedExpression> flinkFilters) {\n     return Result.of(acceptedFilters, flinkFilters);\n   }\n \n+  @Override\n+  public void applySourceWatermark() {\n+    Preconditions.checkArgument(\n+        readableConfig.get(FlinkConfigOptions.TABLE_EXEC_ICEBERG_USE_FLIP27_SOURCE),\n+        \"Source watermarks are supported only in flip-27 iceberg source implementation\");\n+\n+    Preconditions.checkNotNull(\n+        properties.get(FlinkReadOptions.WATERMARK_COLUMN),\n+        \"watermark-column needs to be configured to use source watermark.\");\n+  }\n+\n   @Override\n   public boolean supportsNestedProjection() {\n     // TODO: support nested projection\n",
    "test_patch": "diff --git a/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java b/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java\nindex c8f0b8172d45..0cdaf8371cbd 100644\n--- a/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java\n+++ b/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg.flink.source;\n \n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.io.IOException;\n import java.time.Instant;\n@@ -40,6 +41,7 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.types.Types;\n+import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n \n@@ -53,7 +55,11 @@ public class TestIcebergSourceSql extends TestSqlBase {\n   @BeforeEach\n   @Override\n   public void before() throws IOException {\n-    TableEnvironment tableEnvironment = getTableEnv();\n+    setUpTableEnv(getTableEnv());\n+    setUpTableEnv(getStreamingTableEnv());\n+  }\n+\n+  private static void setUpTableEnv(TableEnvironment tableEnvironment) {\n     Configuration tableConf = tableEnvironment.getConfig().getConfiguration();\n     tableConf.set(FlinkConfigOptions.TABLE_EXEC_ICEBERG_USE_FLIP27_SOURCE, true);\n     // Disable inferring parallelism to avoid interfering watermark tests\n@@ -72,6 +78,11 @@ public void before() throws IOException {\n     tableConf.set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n   }\n \n+  @AfterEach\n+  public void after() throws IOException {\n+    CATALOG_EXTENSION.catalog().dropTable(TestFixtures.TABLE_IDENTIFIER);\n+  }\n+\n   private Record generateRecord(Instant t1, long t2) {\n     Record record = GenericRecord.create(SCHEMA_TS);\n     record.setField(\"t1\", t1.atZone(ZoneId.systemDefault()).toLocalDateTime());\n@@ -178,4 +189,45 @@ public void testReadFlinkDynamicTable() throws Exception {\n         expected,\n         SCHEMA_TS);\n   }\n+\n+  @Test\n+  public void testWatermarkInvalidConfig() {\n+    CATALOG_EXTENSION.catalog().createTable(TestFixtures.TABLE_IDENTIFIER, SCHEMA_TS);\n+\n+    String flinkTable = \"`default_catalog`.`default_database`.flink_table\";\n+    SqlHelpers.sql(\n+        getStreamingTableEnv(),\n+        \"CREATE TABLE %s \"\n+            + \"(eventTS AS CAST(t1 AS TIMESTAMP(3)), \"\n+            + \"WATERMARK FOR eventTS AS SOURCE_WATERMARK()) LIKE iceberg_catalog.`default`.%s\",\n+        flinkTable,\n+        TestFixtures.TABLE);\n+\n+    assertThatThrownBy(() -> SqlHelpers.sql(getStreamingTableEnv(), \"SELECT * FROM %s\", flinkTable))\n+        .isInstanceOf(NullPointerException.class)\n+        .hasMessage(\"watermark-column needs to be configured to use source watermark.\");\n+  }\n+\n+  @Test\n+  public void testWatermarkValidConfig() throws Exception {\n+    List<Record> expected = generateExpectedRecords(true);\n+\n+    String flinkTable = \"`default_catalog`.`default_database`.flink_table\";\n+\n+    SqlHelpers.sql(\n+        getStreamingTableEnv(),\n+        \"CREATE TABLE %s \"\n+            + \"(eventTS AS CAST(t1 AS TIMESTAMP(3)), \"\n+            + \"WATERMARK FOR eventTS AS SOURCE_WATERMARK()) WITH ('watermark-column'='t1') LIKE iceberg_catalog.`default`.%s\",\n+        flinkTable,\n+        TestFixtures.TABLE);\n+\n+    TestHelpers.assertRecordsWithOrder(\n+        SqlHelpers.sql(\n+            getStreamingTableEnv(),\n+            \"SELECT t1, t2 FROM TABLE(TUMBLE(TABLE %s, DESCRIPTOR(eventTS), INTERVAL '1' SECOND))\",\n+            flinkTable),\n+        expected,\n+        SCHEMA_TS);\n+  }\n }\n\ndiff --git a/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java b/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java\nindex f9b776397cfc..dd63154fe03b 100644\n--- a/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java\n+++ b/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java\n@@ -63,6 +63,8 @@ public abstract class TestSqlBase {\n \n   private volatile TableEnvironment tEnv;\n \n+  private volatile TableEnvironment streamingTEnv;\n+\n   protected TableEnvironment getTableEnv() {\n     if (tEnv == null) {\n       synchronized (this) {\n@@ -75,6 +77,19 @@ protected TableEnvironment getTableEnv() {\n     return tEnv;\n   }\n \n+  protected TableEnvironment getStreamingTableEnv() {\n+    if (streamingTEnv == null) {\n+      synchronized (this) {\n+        if (streamingTEnv == null) {\n+          this.streamingTEnv =\n+              TableEnvironment.create(EnvironmentSettings.newInstance().inStreamingMode().build());\n+        }\n+      }\n+    }\n+\n+    return streamingTEnv;\n+  }\n+\n   @BeforeEach\n   public abstract void before() throws IOException;\n \n\ndiff --git a/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java b/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java\nindex c8f0b8172d45..0cdaf8371cbd 100644\n--- a/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java\n+++ b/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg.flink.source;\n \n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.io.IOException;\n import java.time.Instant;\n@@ -40,6 +41,7 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.types.Types;\n+import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n \n@@ -53,7 +55,11 @@ public class TestIcebergSourceSql extends TestSqlBase {\n   @BeforeEach\n   @Override\n   public void before() throws IOException {\n-    TableEnvironment tableEnvironment = getTableEnv();\n+    setUpTableEnv(getTableEnv());\n+    setUpTableEnv(getStreamingTableEnv());\n+  }\n+\n+  private static void setUpTableEnv(TableEnvironment tableEnvironment) {\n     Configuration tableConf = tableEnvironment.getConfig().getConfiguration();\n     tableConf.set(FlinkConfigOptions.TABLE_EXEC_ICEBERG_USE_FLIP27_SOURCE, true);\n     // Disable inferring parallelism to avoid interfering watermark tests\n@@ -72,6 +78,11 @@ public void before() throws IOException {\n     tableConf.set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n   }\n \n+  @AfterEach\n+  public void after() throws IOException {\n+    CATALOG_EXTENSION.catalog().dropTable(TestFixtures.TABLE_IDENTIFIER);\n+  }\n+\n   private Record generateRecord(Instant t1, long t2) {\n     Record record = GenericRecord.create(SCHEMA_TS);\n     record.setField(\"t1\", t1.atZone(ZoneId.systemDefault()).toLocalDateTime());\n@@ -178,4 +189,45 @@ public void testReadFlinkDynamicTable() throws Exception {\n         expected,\n         SCHEMA_TS);\n   }\n+\n+  @Test\n+  public void testWatermarkInvalidConfig() {\n+    CATALOG_EXTENSION.catalog().createTable(TestFixtures.TABLE_IDENTIFIER, SCHEMA_TS);\n+\n+    String flinkTable = \"`default_catalog`.`default_database`.flink_table\";\n+    SqlHelpers.sql(\n+        getStreamingTableEnv(),\n+        \"CREATE TABLE %s \"\n+            + \"(eventTS AS CAST(t1 AS TIMESTAMP(3)), \"\n+            + \"WATERMARK FOR eventTS AS SOURCE_WATERMARK()) LIKE iceberg_catalog.`default`.%s\",\n+        flinkTable,\n+        TestFixtures.TABLE);\n+\n+    assertThatThrownBy(() -> SqlHelpers.sql(getStreamingTableEnv(), \"SELECT * FROM %s\", flinkTable))\n+        .isInstanceOf(NullPointerException.class)\n+        .hasMessage(\"watermark-column needs to be configured to use source watermark.\");\n+  }\n+\n+  @Test\n+  public void testWatermarkValidConfig() throws Exception {\n+    List<Record> expected = generateExpectedRecords(true);\n+\n+    String flinkTable = \"`default_catalog`.`default_database`.flink_table\";\n+\n+    SqlHelpers.sql(\n+        getStreamingTableEnv(),\n+        \"CREATE TABLE %s \"\n+            + \"(eventTS AS CAST(t1 AS TIMESTAMP(3)), \"\n+            + \"WATERMARK FOR eventTS AS SOURCE_WATERMARK()) WITH ('watermark-column'='t1') LIKE iceberg_catalog.`default`.%s\",\n+        flinkTable,\n+        TestFixtures.TABLE);\n+\n+    TestHelpers.assertRecordsWithOrder(\n+        SqlHelpers.sql(\n+            getStreamingTableEnv(),\n+            \"SELECT t1, t2 FROM TABLE(TUMBLE(TABLE %s, DESCRIPTOR(eventTS), INTERVAL '1' SECOND))\",\n+            flinkTable),\n+        expected,\n+        SCHEMA_TS);\n+  }\n }\n\ndiff --git a/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java b/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java\nindex f9b776397cfc..dd63154fe03b 100644\n--- a/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java\n+++ b/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java\n@@ -63,6 +63,8 @@ public abstract class TestSqlBase {\n \n   private volatile TableEnvironment tEnv;\n \n+  private volatile TableEnvironment streamingTEnv;\n+\n   protected TableEnvironment getTableEnv() {\n     if (tEnv == null) {\n       synchronized (this) {\n@@ -75,6 +77,19 @@ protected TableEnvironment getTableEnv() {\n     return tEnv;\n   }\n \n+  protected TableEnvironment getStreamingTableEnv() {\n+    if (streamingTEnv == null) {\n+      synchronized (this) {\n+        if (streamingTEnv == null) {\n+          this.streamingTEnv =\n+              TableEnvironment.create(EnvironmentSettings.newInstance().inStreamingMode().build());\n+        }\n+      }\n+    }\n+\n+    return streamingTEnv;\n+  }\n+\n   @BeforeEach\n   public abstract void before() throws IOException;\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12671",
    "pr_id": 12671,
    "issue_id": 12236,
    "repo": "apache/iceberg",
    "problem_statement": "Move docker-specific tests to integrationTest configuration\n### Feature Request / Improvement\n\nCurrently, docker-specific tests run under the **test** configuration and we'd like to move those to **integrationTest**. Examples of such tests use the `@Testcontainers` annotation\n\n### Query engine\n\nNone\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [ ] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 73,
    "test_files_count": 20,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "aws/src/integration/java/org/apache/iceberg/aws/AwsIntegTestUtil.java",
      "aws/src/integration/java/org/apache/iceberg/aws/TestAssumeRoleAwsClientFactory.java",
      "aws/src/integration/java/org/apache/iceberg/aws/TestDefaultAwsClientFactory.java",
      "aws/src/integration/java/org/apache/iceberg/aws/dynamodb/TestDynamoDbCatalog.java",
      "aws/src/integration/java/org/apache/iceberg/aws/dynamodb/TestDynamoDbLockManager.java",
      "aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogCommitFailure.java",
      "aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogLock.java",
      "aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogNamespace.java",
      "aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogTable.java",
      "aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationAwsClientFactory.java",
      "aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationDataOperations.java",
      "aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationMetadataOperations.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/MinioUtil.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestFlakyS3InputStream.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestMinioUtil.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIOIntegration.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3InputStream.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3MultipartUpload.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3OutputStream.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/signer/S3SignerServlet.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/signer/TestS3RestSigner.java",
      "build.gradle"
    ],
    "pr_changed_test_files": [
      "aws/src/integration/java/org/apache/iceberg/aws/AwsIntegTestUtil.java",
      "aws/src/integration/java/org/apache/iceberg/aws/TestAssumeRoleAwsClientFactory.java",
      "aws/src/integration/java/org/apache/iceberg/aws/TestDefaultAwsClientFactory.java",
      "aws/src/integration/java/org/apache/iceberg/aws/dynamodb/TestDynamoDbCatalog.java",
      "aws/src/integration/java/org/apache/iceberg/aws/dynamodb/TestDynamoDbLockManager.java",
      "aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogCommitFailure.java",
      "aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogLock.java",
      "aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogNamespace.java",
      "aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogTable.java",
      "aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationAwsClientFactory.java",
      "aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationDataOperations.java",
      "aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationMetadataOperations.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestFlakyS3InputStream.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestMinioUtil.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIOIntegration.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3InputStream.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3MultipartUpload.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3OutputStream.java",
      "aws/src/integration/java/org/apache/iceberg/aws/s3/signer/TestS3RestSigner.java"
    ],
    "base_commit": "14122cb736dcc0db52e48caf183d92eb1272da86",
    "head_commit": "31e35e812a2dd3a046a78cc7dee5cde3e24fb43d",
    "repo_url": "https://github.com/apache/iceberg/pull/12671",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12671",
    "dockerfile": "",
    "pr_merged_at": "2025-04-04T07:12:15.000Z",
    "patch": "diff --git a/aws/src/test/java/org/apache/iceberg/aws/s3/MinioUtil.java b/aws/src/integration/java/org/apache/iceberg/aws/s3/MinioUtil.java\nsimilarity index 100%\nrename from aws/src/test/java/org/apache/iceberg/aws/s3/MinioUtil.java\nrename to aws/src/integration/java/org/apache/iceberg/aws/s3/MinioUtil.java\n\ndiff --git a/aws/src/test/java/org/apache/iceberg/aws/s3/signer/S3SignerServlet.java b/aws/src/integration/java/org/apache/iceberg/aws/s3/signer/S3SignerServlet.java\nsimilarity index 100%\nrename from aws/src/test/java/org/apache/iceberg/aws/s3/signer/S3SignerServlet.java\nrename to aws/src/integration/java/org/apache/iceberg/aws/s3/signer/S3SignerServlet.java\n\ndiff --git a/build.gradle b/build.gradle\nindex eb7c82157e40..ab5e9700c6aa 100644\n--- a/build.gradle\n+++ b/build.gradle\n@@ -530,6 +530,7 @@ project(':iceberg-aws') {\n     recommend.set(true)\n   }\n   check.dependsOn('validateS3SignerSpec')\n+  check.dependsOn integrationTest\n }\n \n project(':iceberg-azure') {\n",
    "test_patch": "diff --git a/aws/src/integration/java/org/apache/iceberg/aws/AwsIntegTestUtil.java b/aws/src/integration/java/org/apache/iceberg/aws/AwsIntegTestUtil.java\nindex 6b57cfd68243..4b7017019124 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/AwsIntegTestUtil.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/AwsIntegTestUtil.java\n@@ -46,6 +46,17 @@ public class AwsIntegTestUtil {\n   private static final Logger LOG = LoggerFactory.getLogger(AwsIntegTestUtil.class);\n   private static final int BATCH_DELETION_SIZE = 1000;\n \n+  public static final String AWS_ACCESS_KEY_ID = \"AWS_ACCESS_KEY_ID\";\n+  public static final String AWS_SECRET_ACCESS_KEY = \"AWS_SECRET_ACCESS_KEY\";\n+  public static final String AWS_SESSION_TOKEN = \"AWS_SESSION_TOKEN\";\n+  public static final String AWS_REGION = \"AWS_REGION\";\n+  public static final String AWS_CROSS_REGION = \"AWS_CROSS_REGION\";\n+  public static final String AWS_TEST_BUCKET = \"AWS_TEST_BUCKET\";\n+  public static final String AWS_TEST_CROSS_REGION_BUCKET = \"AWS_TEST_CROSS_REGION_BUCKET\";\n+  public static final String AWS_TEST_ACCOUNT_ID = \"AWS_TEST_ACCOUNT_ID\";\n+  public static final String AWS_TEST_MULTI_REGION_ACCESS_POINT_ALIAS =\n+      \"AWS_TEST_MULTI_REGION_ACCESS_POINT_ALIAS\";\n+\n   private AwsIntegTestUtil() {}\n \n   /**\n@@ -54,7 +65,7 @@ private AwsIntegTestUtil() {}\n    * @return region\n    */\n   public static String testRegion() {\n-    return System.getenv(\"AWS_REGION\");\n+    return System.getenv(AWS_REGION);\n   }\n \n   /**\n@@ -63,10 +74,9 @@ public static String testRegion() {\n    * @return region\n    */\n   public static String testCrossRegion() {\n-    String crossRegion = System.getenv(\"AWS_CROSS_REGION\");\n+    String crossRegion = System.getenv(AWS_CROSS_REGION);\n     Preconditions.checkArgument(\n-        !testRegion().equals(crossRegion),\n-        \"AWS_REGION should not be equal to \" + \"AWS_CROSS_REGION\");\n+        !testRegion().equals(crossRegion), \"AWS_REGION should not be equal to \" + AWS_CROSS_REGION);\n     return crossRegion;\n   }\n \n@@ -76,7 +86,7 @@ public static String testCrossRegion() {\n    * @return bucket name\n    */\n   public static String testBucketName() {\n-    return System.getenv(\"AWS_TEST_BUCKET\");\n+    return System.getenv(AWS_TEST_BUCKET);\n   }\n \n   /**\n@@ -86,7 +96,7 @@ public static String testBucketName() {\n    * @return bucket name\n    */\n   public static String testCrossRegionBucketName() {\n-    return System.getenv(\"AWS_TEST_CROSS_REGION_BUCKET\");\n+    return System.getenv(AWS_TEST_CROSS_REGION_BUCKET);\n   }\n \n   /**\n@@ -95,7 +105,7 @@ public static String testCrossRegionBucketName() {\n    * @return account id\n    */\n   public static String testAccountId() {\n-    return System.getenv(\"AWS_TEST_ACCOUNT_ID\");\n+    return System.getenv(AWS_TEST_ACCOUNT_ID);\n   }\n \n   /**\n@@ -106,7 +116,7 @@ public static String testAccountId() {\n    * @return The alias of S3 multi region access point route to the default S3 bucket\n    */\n   public static String testMultiRegionAccessPointAlias() {\n-    return System.getenv(\"AWS_TEST_MULTI_REGION_ACCESS_POINT_ALIAS\");\n+    return System.getenv(AWS_TEST_MULTI_REGION_ACCESS_POINT_ALIAS);\n   }\n \n   public static void cleanS3GeneralPurposeBucket(S3Client s3, String bucketName, String prefix) {\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/TestAssumeRoleAwsClientFactory.java b/aws/src/integration/java/org/apache/iceberg/aws/TestAssumeRoleAwsClientFactory.java\nindex 3b2fd71021ba..bb4605d63150 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/TestAssumeRoleAwsClientFactory.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/TestAssumeRoleAwsClientFactory.java\n@@ -34,6 +34,8 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariables;\n import software.amazon.awssdk.core.exception.SdkServiceException;\n import software.amazon.awssdk.http.urlconnection.UrlConnectionHttpClient;\n import software.amazon.awssdk.regions.Region;\n@@ -47,6 +49,14 @@\n import software.amazon.awssdk.services.iam.model.PutRolePolicyRequest;\n import software.amazon.awssdk.services.s3.model.S3Exception;\n \n+@EnabledIfEnvironmentVariables({\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_ACCESS_KEY_ID, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SECRET_ACCESS_KEY, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SESSION_TOKEN, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_TEST_ACCOUNT_ID, matches = \"\\\\d{12}\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_REGION, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_TEST_BUCKET, matches = \".*\")\n+})\n public class TestAssumeRoleAwsClientFactory {\n \n   private IamClient iam;\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/TestDefaultAwsClientFactory.java b/aws/src/integration/java/org/apache/iceberg/aws/TestDefaultAwsClientFactory.java\nindex 28fd17234a92..e9803c8bb1da 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/TestDefaultAwsClientFactory.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/TestDefaultAwsClientFactory.java\n@@ -24,6 +24,8 @@\n import org.apache.iceberg.aws.s3.S3FileIOProperties;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariables;\n import software.amazon.awssdk.core.exception.SdkClientException;\n import software.amazon.awssdk.services.dynamodb.DynamoDbClient;\n import software.amazon.awssdk.services.glue.GlueClient;\n@@ -32,6 +34,13 @@\n import software.amazon.awssdk.services.s3.model.GetObjectRequest;\n import software.amazon.awssdk.services.s3.model.S3Exception;\n \n+@EnabledIfEnvironmentVariables({\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_ACCESS_KEY_ID, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SECRET_ACCESS_KEY, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SESSION_TOKEN, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_REGION, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_TEST_BUCKET, matches = \".*\")\n+})\n public class TestDefaultAwsClientFactory {\n \n   @Test\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/dynamodb/TestDynamoDbCatalog.java b/aws/src/integration/java/org/apache/iceberg/aws/dynamodb/TestDynamoDbCatalog.java\nindex 5ee6b3e1cf34..2d83582c1337 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/dynamodb/TestDynamoDbCatalog.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/dynamodb/TestDynamoDbCatalog.java\n@@ -48,6 +48,8 @@\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariables;\n import software.amazon.awssdk.services.dynamodb.DynamoDbClient;\n import software.amazon.awssdk.services.dynamodb.model.DeleteTableRequest;\n import software.amazon.awssdk.services.dynamodb.model.GetItemRequest;\n@@ -56,6 +58,13 @@\n import software.amazon.awssdk.services.s3.model.HeadObjectRequest;\n import software.amazon.awssdk.services.s3.model.NoSuchKeyException;\n \n+@EnabledIfEnvironmentVariables({\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_ACCESS_KEY_ID, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SECRET_ACCESS_KEY, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SESSION_TOKEN, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_REGION, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_TEST_BUCKET, matches = \".*\")\n+})\n public class TestDynamoDbCatalog {\n \n   private static final ForkJoinPool POOL = new ForkJoinPool(16);\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/dynamodb/TestDynamoDbLockManager.java b/aws/src/integration/java/org/apache/iceberg/aws/dynamodb/TestDynamoDbLockManager.java\nindex 120a4d702681..033981df5cc1 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/dynamodb/TestDynamoDbLockManager.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/dynamodb/TestDynamoDbLockManager.java\n@@ -30,12 +30,15 @@\n import java.util.stream.IntStream;\n import org.apache.iceberg.CatalogProperties;\n import org.apache.iceberg.aws.AwsClientFactories;\n+import org.apache.iceberg.aws.AwsIntegTestUtil;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariables;\n import org.mockito.Mockito;\n import software.amazon.awssdk.services.dynamodb.DynamoDbClient;\n import software.amazon.awssdk.services.dynamodb.model.AttributeValue;\n@@ -46,6 +49,12 @@\n import software.amazon.awssdk.services.dynamodb.model.GetItemResponse;\n import software.amazon.awssdk.services.dynamodb.model.ResourceNotFoundException;\n \n+@EnabledIfEnvironmentVariables({\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_ACCESS_KEY_ID, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SECRET_ACCESS_KEY, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SESSION_TOKEN, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_REGION, matches = \".*\")\n+})\n public class TestDynamoDbLockManager {\n \n   private static final ForkJoinPool POOL = new ForkJoinPool(16);\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogCommitFailure.java b/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogCommitFailure.java\nindex a1df3d21da69..177a627efcab 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogCommitFailure.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogCommitFailure.java\n@@ -27,6 +27,7 @@\n import org.apache.iceberg.HasTableOperations;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.aws.AwsIntegTestUtil;\n import org.apache.iceberg.aws.s3.S3TestUtil;\n import org.apache.iceberg.aws.util.RetryDetector;\n import org.apache.iceberg.catalog.TableIdentifier;\n@@ -36,6 +37,8 @@\n import org.apache.iceberg.exceptions.NotFoundException;\n import org.apache.iceberg.types.Types;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariables;\n import org.mockito.Mockito;\n import software.amazon.awssdk.core.metrics.CoreMetric;\n import software.amazon.awssdk.metrics.MetricCollector;\n@@ -49,6 +52,13 @@\n import software.amazon.awssdk.services.s3.model.NoSuchKeyException;\n import software.amazon.awssdk.services.s3.model.S3Exception;\n \n+@EnabledIfEnvironmentVariables({\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_ACCESS_KEY_ID, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SECRET_ACCESS_KEY, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SESSION_TOKEN, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_REGION, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_TEST_BUCKET, matches = \".*\")\n+})\n public class TestGlueCatalogCommitFailure extends GlueTestBase {\n \n   @Test\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogLock.java b/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogLock.java\nindex 3edd9e4acdb4..a63919c54337 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogLock.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogLock.java\n@@ -34,6 +34,7 @@\n import org.apache.iceberg.DataFiles;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.Table;\n+import org.apache.iceberg.aws.AwsIntegTestUtil;\n import org.apache.iceberg.aws.AwsProperties;\n import org.apache.iceberg.aws.dynamodb.DynamoDbLockManager;\n import org.apache.iceberg.aws.s3.S3FileIOProperties;\n@@ -45,9 +46,18 @@\n import org.junit.jupiter.api.AfterAll;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariables;\n import software.amazon.awssdk.services.dynamodb.DynamoDbClient;\n import software.amazon.awssdk.services.dynamodb.model.DeleteTableRequest;\n \n+@EnabledIfEnvironmentVariables({\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_ACCESS_KEY_ID, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SECRET_ACCESS_KEY, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SESSION_TOKEN, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_REGION, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_TEST_BUCKET, matches = \".*\")\n+})\n public class TestGlueCatalogLock extends GlueTestBase {\n \n   private static String lockTableName;\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogNamespace.java b/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogNamespace.java\nindex 7a249c5509f2..c1ddb98fd647 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogNamespace.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogNamespace.java\n@@ -24,6 +24,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.UUID;\n+import org.apache.iceberg.aws.AwsIntegTestUtil;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.exceptions.AlreadyExistsException;\n import org.apache.iceberg.exceptions.NamespaceNotEmptyException;\n@@ -33,12 +34,21 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariables;\n import software.amazon.awssdk.services.glue.model.CreateTableRequest;\n import software.amazon.awssdk.services.glue.model.Database;\n import software.amazon.awssdk.services.glue.model.EntityNotFoundException;\n import software.amazon.awssdk.services.glue.model.GetDatabaseRequest;\n import software.amazon.awssdk.services.glue.model.TableInput;\n \n+@EnabledIfEnvironmentVariables({\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_ACCESS_KEY_ID, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SECRET_ACCESS_KEY, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SESSION_TOKEN, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_REGION, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_TEST_BUCKET, matches = \".*\")\n+})\n public class TestGlueCatalogNamespace extends GlueTestBase {\n \n   @Test\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogTable.java b/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogTable.java\nindex 50883703bae0..aa1e669377b8 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogTable.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogTable.java\n@@ -38,6 +38,7 @@\n import org.apache.iceberg.TableMetadata;\n import org.apache.iceberg.TableOperations;\n import org.apache.iceberg.Transaction;\n+import org.apache.iceberg.aws.AwsIntegTestUtil;\n import org.apache.iceberg.aws.AwsProperties;\n import org.apache.iceberg.aws.s3.S3FileIOProperties;\n import org.apache.iceberg.catalog.Namespace;\n@@ -52,6 +53,8 @@\n import org.apache.iceberg.types.Types.NestedField;\n import org.apache.iceberg.util.LockManagers;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariables;\n import software.amazon.awssdk.services.glue.model.Column;\n import software.amazon.awssdk.services.glue.model.CreateTableRequest;\n import software.amazon.awssdk.services.glue.model.EntityNotFoundException;\n@@ -67,6 +70,13 @@\n import software.amazon.awssdk.services.s3.model.S3Object;\n import software.amazon.awssdk.services.s3.model.Tag;\n \n+@EnabledIfEnvironmentVariables({\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_ACCESS_KEY_ID, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SECRET_ACCESS_KEY, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SESSION_TOKEN, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_REGION, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_TEST_BUCKET, matches = \".*\")\n+})\n public class TestGlueCatalogTable extends GlueTestBase {\n \n   @Test\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationAwsClientFactory.java b/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationAwsClientFactory.java\nindex 55e0b4209abc..c1f8cb9a9293 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationAwsClientFactory.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationAwsClientFactory.java\n@@ -35,6 +35,8 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariables;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n import software.amazon.awssdk.http.urlconnection.UrlConnectionHttpClient;\n@@ -48,6 +50,12 @@\n import software.amazon.awssdk.services.iam.model.GetRolePolicyRequest;\n import software.amazon.awssdk.services.iam.model.PutRolePolicyRequest;\n \n+@EnabledIfEnvironmentVariables({\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_ACCESS_KEY_ID, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SECRET_ACCESS_KEY, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SESSION_TOKEN, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_TEST_ACCOUNT_ID, matches = \"\\\\d{12}\")\n+})\n public class TestLakeFormationAwsClientFactory {\n \n   private static final Logger LOG =\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationDataOperations.java b/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationDataOperations.java\nindex f42db1ef3f3d..132e35684736 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationDataOperations.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationDataOperations.java\n@@ -23,16 +23,26 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataFiles;\n import org.apache.iceberg.Table;\n+import org.apache.iceberg.aws.AwsIntegTestUtil;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.exceptions.ForbiddenException;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariables;\n import software.amazon.awssdk.services.glue.model.AccessDeniedException;\n import software.amazon.awssdk.services.lakeformation.model.Permission;\n import software.amazon.awssdk.services.s3.model.S3Exception;\n \n+@EnabledIfEnvironmentVariables({\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_ACCESS_KEY_ID, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SECRET_ACCESS_KEY, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SESSION_TOKEN, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_REGION, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_TEST_ACCOUNT_ID, matches = \"\\\\d{12}\")\n+})\n public class TestLakeFormationDataOperations extends LakeFormationTestBase {\n \n   private static String testDbName;\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationMetadataOperations.java b/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationMetadataOperations.java\nindex 37465575c0e2..abfd1c596f77 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationMetadataOperations.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationMetadataOperations.java\n@@ -26,14 +26,24 @@\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.UpdateProperties;\n+import org.apache.iceberg.aws.AwsIntegTestUtil;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.exceptions.ForbiddenException;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariables;\n import software.amazon.awssdk.services.glue.model.AccessDeniedException;\n import software.amazon.awssdk.services.lakeformation.model.Permission;\n \n+@EnabledIfEnvironmentVariables({\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_ACCESS_KEY_ID, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SECRET_ACCESS_KEY, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SESSION_TOKEN, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_REGION, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_TEST_ACCOUNT_ID, matches = \"\\\\d{12}\")\n+})\n public class TestLakeFormationMetadataOperations extends LakeFormationTestBase {\n   @Test\n   public void testCreateAndDropDatabaseSuccessful() {\n\ndiff --git a/aws/src/test/java/org/apache/iceberg/aws/s3/TestFlakyS3InputStream.java b/aws/src/integration/java/org/apache/iceberg/aws/s3/TestFlakyS3InputStream.java\nsimilarity index 100%\nrename from aws/src/test/java/org/apache/iceberg/aws/s3/TestFlakyS3InputStream.java\nrename to aws/src/integration/java/org/apache/iceberg/aws/s3/TestFlakyS3InputStream.java\n\ndiff --git a/aws/src/test/java/org/apache/iceberg/aws/s3/TestMinioUtil.java b/aws/src/integration/java/org/apache/iceberg/aws/s3/TestMinioUtil.java\nsimilarity index 100%\nrename from aws/src/test/java/org/apache/iceberg/aws/s3/TestMinioUtil.java\nrename to aws/src/integration/java/org/apache/iceberg/aws/s3/TestMinioUtil.java\n\ndiff --git a/aws/src/test/java/org/apache/iceberg/aws/s3/TestS3FileIO.java b/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java\nsimilarity index 100%\nrename from aws/src/test/java/org/apache/iceberg/aws/s3/TestS3FileIO.java\nrename to aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIO.java\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIOIntegration.java b/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIOIntegration.java\nindex 55e43aa303cc..8017b0974dbb 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIOIntegration.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3FileIOIntegration.java\n@@ -46,6 +46,8 @@\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariables;\n import software.amazon.awssdk.core.sync.RequestBody;\n import software.amazon.awssdk.regions.PartitionMetadata;\n import software.amazon.awssdk.regions.Region;\n@@ -70,6 +72,18 @@\n import software.amazon.awssdk.utils.IoUtils;\n import software.amazon.s3.analyticsaccelerator.util.PrefetchMode;\n \n+@EnabledIfEnvironmentVariables({\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_ACCESS_KEY_ID, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SECRET_ACCESS_KEY, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SESSION_TOKEN, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_TEST_ACCOUNT_ID, matches = \"\\\\d{12}\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_REGION, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_CROSS_REGION, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_TEST_BUCKET, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(\n+      named = AwsIntegTestUtil.AWS_TEST_CROSS_REGION_BUCKET,\n+      matches = \".*\")\n+})\n public class TestS3FileIOIntegration {\n \n   private final Random random = new Random(1);\n\ndiff --git a/aws/src/test/java/org/apache/iceberg/aws/s3/TestS3InputStream.java b/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3InputStream.java\nsimilarity index 100%\nrename from aws/src/test/java/org/apache/iceberg/aws/s3/TestS3InputStream.java\nrename to aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3InputStream.java\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3MultipartUpload.java b/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3MultipartUpload.java\nindex 901e9933b1fd..1916649ccf85 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3MultipartUpload.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3MultipartUpload.java\n@@ -33,9 +33,18 @@\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariable;\n+import org.junit.jupiter.api.condition.EnabledIfEnvironmentVariables;\n import software.amazon.awssdk.services.s3.S3Client;\n \n /** Long-running tests to ensure multipart upload logic is resilient */\n+@EnabledIfEnvironmentVariables({\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_ACCESS_KEY_ID, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SECRET_ACCESS_KEY, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_SESSION_TOKEN, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_REGION, matches = \".*\"),\n+  @EnabledIfEnvironmentVariable(named = AwsIntegTestUtil.AWS_TEST_BUCKET, matches = \".*\")\n+})\n public class TestS3MultipartUpload {\n \n   private final Random random = new Random(1);\n\ndiff --git a/aws/src/test/java/org/apache/iceberg/aws/s3/TestS3OutputStream.java b/aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3OutputStream.java\nsimilarity index 100%\nrename from aws/src/test/java/org/apache/iceberg/aws/s3/TestS3OutputStream.java\nrename to aws/src/integration/java/org/apache/iceberg/aws/s3/TestS3OutputStream.java\n\ndiff --git a/aws/src/test/java/org/apache/iceberg/aws/s3/signer/TestS3RestSigner.java b/aws/src/integration/java/org/apache/iceberg/aws/s3/signer/TestS3RestSigner.java\nsimilarity index 100%\nrename from aws/src/test/java/org/apache/iceberg/aws/s3/signer/TestS3RestSigner.java\nrename to aws/src/integration/java/org/apache/iceberg/aws/s3/signer/TestS3RestSigner.java\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12670",
    "pr_id": 12670,
    "issue_id": 12642,
    "repo": "apache/iceberg",
    "problem_statement": "Rest Catalog: Remove snapshots more efficiently\n### Proposed Change\n\n## Problem\nWe found an issue when expiring old snapshots from a table with a lot of snapshots (+10k). The issue happens when the `expireSnapshots` action triggers a request with a list of `UpdateTableRequest` that will clean up most of the snapshots (~99%). The Rest Catalog server will receive a single request (UpdateTableRequest), with the list of snapshots to be removed from the metadata. However, the metadata updates on each snapshot:\n\nhttps://github.com/apache/iceberg/blob/03ff41c189c7420992be0e4a4ddc63f005e2e0d5/core/src/main/java/org/apache/iceberg/rest/CatalogHandlers.java#L435\n\nThe `applyTo` method will basically just update the Table for 1 snapshot, even though it could receive the entire list of snapshots:\n\nhttps://github.com/apache/iceberg/blob/03ff41c189c7420992be0e4a4ddc63f005e2e0d5/core/src/main/java/org/apache/iceberg/MetadataUpdate.java#L344\n\nThe `removeSnapshots` method delegates the process to the `rewriteSnapshotInternal`:\n\nhttps://github.com/apache/iceberg/blob/03ff41c189c7420992be0e4a4ddc63f005e2e0d5/core/src/main/java/org/apache/iceberg/TableMetadata.java#L1424\n\nThis `rewriteSnapshotInternal` iterates over all snapshots of the table to remove the provided list of snapshots (as I mentioned, **it only passes 1 snapshot**). This is not efficient, when we need to remove a huge amount of snapshots, we need to iterate over the entire list of snapshots (N elements), N times - O(NÀÜ2). \n\nWe notice this issue when we recently enable some tables that are written by streaming jobs (they are often written and generate a lot of snapshots). With +10 tables, having +10k snapshots each, some of them +100k, cause our Rest Catalog server to hit 100% CPU usage constantly:\n\n![Image](https://github.com/user-attachments/assets/50e2e39e-8fc2-44e2-8515-4c815bc4d3ff)\n\n## Proposal\n\nOn the `CatalogHandlers`, we could group by the `MetadataUpdate`, and apply them in bulk:\n\nhttps://github.com/apache/iceberg/blob/03ff41c189c7420992be0e4a4ddc63f005e2e0d5/core/src/main/java/org/apache/iceberg/rest/CatalogHandlers.java#L435\n\n### Proposal document\n\n_No response_\n\n### Specifications\n\n- [x] Table\n- [ ] View\n- [x] REST\n- [ ] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 321,
    "test_files_count": 3,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/MetadataUpdate.java",
      "core/src/main/java/org/apache/iceberg/MetadataUpdateParser.java",
      "core/src/main/java/org/apache/iceberg/TableMetadata.java",
      "core/src/test/java/org/apache/iceberg/TestMetadataUpdateParser.java",
      "core/src/test/java/org/apache/iceberg/TestUpdateRequirements.java",
      "core/src/test/java/org/apache/iceberg/rest/requests/TestCommitTransactionRequestParser.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/TestMetadataUpdateParser.java",
      "core/src/test/java/org/apache/iceberg/TestUpdateRequirements.java",
      "core/src/test/java/org/apache/iceberg/rest/requests/TestCommitTransactionRequestParser.java"
    ],
    "base_commit": "cb3f331e478cbbf3cea852b56f0eace28043124e",
    "head_commit": "38b2626aabc7bc0a4146c1487c15ea692edc9c85",
    "repo_url": "https://github.com/apache/iceberg/pull/12670",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12670",
    "dockerfile": "",
    "pr_merged_at": "2025-04-03T22:05:54.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/MetadataUpdate.java b/core/src/main/java/org/apache/iceberg/MetadataUpdate.java\nindex 8f90b5691a1a..6234d464fbcb 100644\n--- a/core/src/main/java/org/apache/iceberg/MetadataUpdate.java\n+++ b/core/src/main/java/org/apache/iceberg/MetadataUpdate.java\n@@ -328,6 +328,11 @@ public void applyTo(TableMetadata.Builder metadataBuilder) {\n     }\n   }\n \n+  /**\n+   * @deprecated since 1.9.0, will be removed in 2.0.0; Use {@link MetadataUpdate.RemoveSnapshots}\n+   *     instead.\n+   */\n+  @Deprecated\n   class RemoveSnapshot implements MetadataUpdate {\n     private final long snapshotId;\n \n@@ -345,6 +350,23 @@ public void applyTo(TableMetadata.Builder metadataBuilder) {\n     }\n   }\n \n+  class RemoveSnapshots implements MetadataUpdate {\n+    private final Set<Long> snapshotIds;\n+\n+    public RemoveSnapshots(Set<Long> snapshotIds) {\n+      this.snapshotIds = snapshotIds;\n+    }\n+\n+    public Set<Long> snapshotIds() {\n+      return snapshotIds;\n+    }\n+\n+    @Override\n+    public void applyTo(TableMetadata.Builder metadataBuilder) {\n+      metadataBuilder.removeSnapshots(snapshotIds);\n+    }\n+  }\n+\n   class RemoveSnapshotRef implements MetadataUpdate {\n     private final String refName;\n \n\ndiff --git a/core/src/main/java/org/apache/iceberg/MetadataUpdateParser.java b/core/src/main/java/org/apache/iceberg/MetadataUpdateParser.java\nindex 19c48de958bb..26261ab65b62 100644\n--- a/core/src/main/java/org/apache/iceberg/MetadataUpdateParser.java\n+++ b/core/src/main/java/org/apache/iceberg/MetadataUpdateParser.java\n@@ -97,7 +97,7 @@ private MetadataUpdateParser() {}\n   // AddSnapshot\n   private static final String SNAPSHOT = \"snapshot\";\n \n-  // RemoveSnapshot\n+  // RemoveSnapshots\n   private static final String SNAPSHOT_IDS = \"snapshot-ids\";\n \n   // SetSnapshotRef\n@@ -151,6 +151,7 @@ private MetadataUpdateParser() {}\n           .put(MetadataUpdate.RemovePartitionStatistics.class, REMOVE_PARTITION_STATISTICS)\n           .put(MetadataUpdate.AddSnapshot.class, ADD_SNAPSHOT)\n           .put(MetadataUpdate.RemoveSnapshot.class, REMOVE_SNAPSHOTS)\n+          .put(MetadataUpdate.RemoveSnapshots.class, REMOVE_SNAPSHOTS)\n           .put(MetadataUpdate.RemoveSnapshotRef.class, REMOVE_SNAPSHOT_REF)\n           .put(MetadataUpdate.SetSnapshotRef.class, SET_SNAPSHOT_REF)\n           .put(MetadataUpdate.SetProperties.class, SET_PROPERTIES)\n@@ -229,7 +230,14 @@ public static void toJson(MetadataUpdate metadataUpdate, JsonGenerator generator\n         writeAddSnapshot((MetadataUpdate.AddSnapshot) metadataUpdate, generator);\n         break;\n       case REMOVE_SNAPSHOTS:\n-        writeRemoveSnapshots((MetadataUpdate.RemoveSnapshot) metadataUpdate, generator);\n+        MetadataUpdate.RemoveSnapshots removeSnapshots;\n+        if (metadataUpdate instanceof MetadataUpdate.RemoveSnapshot) {\n+          Long snapshotId = ((MetadataUpdate.RemoveSnapshot) metadataUpdate).snapshotId();\n+          removeSnapshots = new MetadataUpdate.RemoveSnapshots(ImmutableSet.of(snapshotId));\n+        } else {\n+          removeSnapshots = (MetadataUpdate.RemoveSnapshots) metadataUpdate;\n+        }\n+        writeRemoveSnapshots(removeSnapshots, generator);\n         break;\n       case REMOVE_SNAPSHOT_REF:\n         writeRemoveSnapshotRef((MetadataUpdate.RemoveSnapshotRef) metadataUpdate, generator);\n@@ -417,11 +425,9 @@ private static void writeAddSnapshot(MetadataUpdate.AddSnapshot update, JsonGene\n     SnapshotParser.toJson(update.snapshot(), gen);\n   }\n \n-  // TODO - Reconcile the spec's set-based removal with the current class implementation that only\n-  // handles one value.\n-  private static void writeRemoveSnapshots(MetadataUpdate.RemoveSnapshot update, JsonGenerator gen)\n+  private static void writeRemoveSnapshots(MetadataUpdate.RemoveSnapshots update, JsonGenerator gen)\n       throws IOException {\n-    JsonUtil.writeLongArray(SNAPSHOT_IDS, ImmutableSet.of(update.snapshotId()), gen);\n+    JsonUtil.writeLongArray(SNAPSHOT_IDS, update.snapshotIds(), gen);\n   }\n \n   private static void writeSetSnapshotRef(MetadataUpdate.SetSnapshotRef update, JsonGenerator gen)\n@@ -557,11 +563,17 @@ private static MetadataUpdate readAddSnapshot(JsonNode node) {\n   private static MetadataUpdate readRemoveSnapshots(JsonNode node) {\n     Set<Long> snapshotIds = JsonUtil.getLongSetOrNull(SNAPSHOT_IDS, node);\n     Preconditions.checkArgument(\n-        snapshotIds != null && snapshotIds.size() == 1,\n-        \"Invalid set of snapshot ids to remove. Expected one value but received: %s\",\n+        snapshotIds != null,\n+        \"Invalid set of snapshot ids to remove: must be non-null\",\n         snapshotIds);\n-    Long snapshotId = Iterables.getOnlyElement(snapshotIds);\n-    return new MetadataUpdate.RemoveSnapshot(snapshotId);\n+    MetadataUpdate metadataUpdate;\n+    if (snapshotIds.size() == 1) {\n+      Long snapshotId = Iterables.getOnlyElement(snapshotIds);\n+      metadataUpdate = new MetadataUpdate.RemoveSnapshot(snapshotId);\n+    } else {\n+      metadataUpdate = new MetadataUpdate.RemoveSnapshots(snapshotIds);\n+    }\n+    return metadataUpdate;\n   }\n \n   private static MetadataUpdate readSetSnapshotRef(JsonNode node) {\n\ndiff --git a/core/src/main/java/org/apache/iceberg/TableMetadata.java b/core/src/main/java/org/apache/iceberg/TableMetadata.java\nindex 61a127fed697..8925f04fb42c 100644\n--- a/core/src/main/java/org/apache/iceberg/TableMetadata.java\n+++ b/core/src/main/java/org/apache/iceberg/TableMetadata.java\n@@ -1436,12 +1436,14 @@ public Builder removeSnapshots(Collection<Long> idsToRemove) {\n     private Builder rewriteSnapshotsInternal(Collection<Long> idsToRemove, boolean suppress) {\n       List<Snapshot> retainedSnapshots =\n           Lists.newArrayListWithExpectedSize(snapshots.size() - idsToRemove.size());\n+      Set<Long> snapshotIdsToRemove = Sets.newHashSet();\n+\n       for (Snapshot snapshot : snapshots) {\n         long snapshotId = snapshot.snapshotId();\n         if (idsToRemove.contains(snapshotId)) {\n           snapshotsById.remove(snapshotId);\n           if (!suppress) {\n-            changes.add(new MetadataUpdate.RemoveSnapshot(snapshotId));\n+            snapshotIdsToRemove.add(snapshotId);\n           }\n           removeStatistics(snapshotId);\n           removePartitionStatistics(snapshotId);\n@@ -1450,6 +1452,10 @@ private Builder rewriteSnapshotsInternal(Collection<Long> idsToRemove, boolean s\n         }\n       }\n \n+      if (!snapshotIdsToRemove.isEmpty()) {\n+        changes.add(new MetadataUpdate.RemoveSnapshots(snapshotIdsToRemove));\n+      }\n+\n       this.snapshots = retainedSnapshots;\n \n       // remove any refs that are no longer valid\n@@ -1865,7 +1871,11 @@ private static List<HistoryEntry> updateSnapshotLog(\n       Set<Long> intermediateSnapshotIds = intermediateSnapshotIdSet(changes, currentSnapshotId);\n       boolean hasIntermediateSnapshots = !intermediateSnapshotIds.isEmpty();\n       boolean hasRemovedSnapshots =\n-          changes.stream().anyMatch(MetadataUpdate.RemoveSnapshot.class::isInstance);\n+          changes.stream()\n+              .anyMatch(\n+                  change ->\n+                      change instanceof MetadataUpdate.RemoveSnapshots\n+                          || change instanceof MetadataUpdate.RemoveSnapshot);\n \n       if (!hasIntermediateSnapshots && !hasRemovedSnapshots) {\n         return snapshotLog;\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/TestMetadataUpdateParser.java b/core/src/test/java/org/apache/iceberg/TestMetadataUpdateParser.java\nindex 4d66163d6c79..aa01146e9f28 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetadataUpdateParser.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetadataUpdateParser.java\n@@ -35,6 +35,7 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.Pair;\n@@ -415,9 +416,9 @@ public void testAddSnapshotFromJson() throws IOException {\n     assertEquals(action, expected, MetadataUpdateParser.fromJson(json));\n   }\n \n-  /** RemoveSnapshots * */\n+  /** RemoveSnapshot * */\n   @Test\n-  public void testRemoveSnapshotsFromJson() {\n+  public void testRemoveSnapshotFromJson() {\n     String action = MetadataUpdateParser.REMOVE_SNAPSHOTS;\n     long snapshotId = 2L;\n     String json = String.format(\"{\\\"action\\\":\\\"%s\\\",\\\"snapshot-ids\\\":[2]}\", action);\n@@ -426,7 +427,7 @@ public void testRemoveSnapshotsFromJson() {\n   }\n \n   @Test\n-  public void testRemoveSnapshotsToJson() {\n+  public void testRemoveSnapshotToJson() {\n     String action = MetadataUpdateParser.REMOVE_SNAPSHOTS;\n     long snapshotId = 2L;\n     String expected = String.format(\"{\\\"action\\\":\\\"%s\\\",\\\"snapshot-ids\\\":[2]}\", action);\n@@ -437,6 +438,28 @@ public void testRemoveSnapshotsToJson() {\n         .isEqualTo(expected);\n   }\n \n+  /** RemoveSnapshots * */\n+  @Test\n+  public void testRemoveSnapshotsFromJson() {\n+    String action = MetadataUpdateParser.REMOVE_SNAPSHOTS;\n+    Set<Long> snapshotIds = Sets.newHashSet(2L, 3L);\n+    String json = String.format(\"{\\\"action\\\":\\\"%s\\\",\\\"snapshot-ids\\\":[2,3]}\", action);\n+    MetadataUpdate expected = new MetadataUpdate.RemoveSnapshots(snapshotIds);\n+    assertEquals(action, expected, MetadataUpdateParser.fromJson(json));\n+  }\n+\n+  @Test\n+  public void testRemoveSnapshotsToJson() {\n+    String action = MetadataUpdateParser.REMOVE_SNAPSHOTS;\n+    Set<Long> snapshotIds = Sets.newHashSet(2L, 3L);\n+    String expected = String.format(\"{\\\"action\\\":\\\"%s\\\",\\\"snapshot-ids\\\":[2,3]}\", action);\n+    MetadataUpdate update = new MetadataUpdate.RemoveSnapshots(snapshotIds);\n+    String actual = MetadataUpdateParser.toJson(update);\n+    assertThat(actual)\n+        .as(\"Remove snapshots should serialize to the correct JSON value\")\n+        .isEqualTo(expected);\n+  }\n+\n   /** RemoveSnapshotRef * */\n   @Test\n   public void testRemoveSnapshotRefFromJson() {\n@@ -1018,9 +1041,15 @@ public void assertEquals(\n             (MetadataUpdate.AddSnapshot) expectedUpdate, (MetadataUpdate.AddSnapshot) actualUpdate);\n         break;\n       case MetadataUpdateParser.REMOVE_SNAPSHOTS:\n-        assertEqualsRemoveSnapshots(\n-            (MetadataUpdate.RemoveSnapshot) expectedUpdate,\n-            (MetadataUpdate.RemoveSnapshot) actualUpdate);\n+        if (actualUpdate instanceof MetadataUpdate.RemoveSnapshot) {\n+          assertEqualsRemoveSnapshot(\n+              (MetadataUpdate.RemoveSnapshot) expectedUpdate,\n+              (MetadataUpdate.RemoveSnapshot) actualUpdate);\n+        } else {\n+          assertEqualsRemoveSnapshots(\n+              (MetadataUpdate.RemoveSnapshots) expectedUpdate,\n+              (MetadataUpdate.RemoveSnapshots) actualUpdate);\n+        }\n         break;\n       case MetadataUpdateParser.REMOVE_SNAPSHOT_REF:\n         assertEqualsRemoveSnapshotRef(\n@@ -1224,13 +1253,20 @@ private static void assertEqualsAddSnapshot(\n     assertThat(actual.snapshot().schemaId()).isEqualTo(expected.snapshot().schemaId());\n   }\n \n-  private static void assertEqualsRemoveSnapshots(\n+  private static void assertEqualsRemoveSnapshot(\n       MetadataUpdate.RemoveSnapshot expected, MetadataUpdate.RemoveSnapshot actual) {\n     assertThat(actual.snapshotId())\n-        .as(\"Snapshots to remove should be the same\")\n+        .as(\"Snapshot to remove should be the same\")\n         .isEqualTo(expected.snapshotId());\n   }\n \n+  private static void assertEqualsRemoveSnapshots(\n+      MetadataUpdate.RemoveSnapshots expected, MetadataUpdate.RemoveSnapshots actual) {\n+    assertThat(actual.snapshotIds())\n+        .as(\"Snapshots to remove should be the same\")\n+        .isEqualTo(expected.snapshotIds());\n+  }\n+\n   private static void assertEqualsSetSnapshotRef(\n       MetadataUpdate.SetSnapshotRef expected, MetadataUpdate.SetSnapshotRef actual) {\n     // Non-null fields\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestUpdateRequirements.java b/core/src/test/java/org/apache/iceberg/TestUpdateRequirements.java\nindex 25075fc5251d..449547477537 100644\n--- a/core/src/test/java/org/apache/iceberg/TestUpdateRequirements.java\n+++ b/core/src/test/java/org/apache/iceberg/TestUpdateRequirements.java\n@@ -25,6 +25,7 @@\n import static org.mockito.Mockito.when;\n \n import java.util.List;\n+import java.util.Set;\n import java.util.UUID;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.exceptions.CommitFailedException;\n@@ -713,6 +714,30 @@ public void addAndRemoveSnapshot() {\n     assertTableUUID(requirements);\n   }\n \n+  @Test\n+  public void addAndRemoveSnapshots() {\n+    List<UpdateRequirement> requirements =\n+        UpdateRequirements.forUpdateTable(\n+            metadata, ImmutableList.of(new MetadataUpdate.AddSnapshot(mock(Snapshot.class))));\n+    requirements.forEach(req -> req.validate(metadata));\n+\n+    assertThat(requirements)\n+        .hasSize(1)\n+        .hasOnlyElementsOfTypes(UpdateRequirement.AssertTableUUID.class);\n+\n+    assertTableUUID(requirements);\n+\n+    requirements =\n+        UpdateRequirements.forUpdateTable(\n+            metadata, ImmutableList.of(new MetadataUpdate.RemoveSnapshots(Set.of(0L))));\n+\n+    assertThat(requirements)\n+        .hasSize(1)\n+        .hasOnlyElementsOfTypes(UpdateRequirement.AssertTableUUID.class);\n+\n+    assertTableUUID(requirements);\n+  }\n+\n   @Test\n   public void setAndRemoveSnapshotRef() {\n     long snapshotId = 14L;\n@@ -747,7 +772,7 @@ public void setAndRemoveSnapshotRef() {\n \n     requirements =\n         UpdateRequirements.forUpdateTable(\n-            metadata, ImmutableList.of(new MetadataUpdate.RemoveSnapshot(0L)));\n+            metadata, ImmutableList.of(new MetadataUpdate.RemoveSnapshots(Set.of(0L))));\n     requirements.forEach(req -> req.validate(metadata));\n \n     assertThat(requirements)\n\ndiff --git a/core/src/test/java/org/apache/iceberg/rest/requests/TestCommitTransactionRequestParser.java b/core/src/test/java/org/apache/iceberg/rest/requests/TestCommitTransactionRequestParser.java\nindex c10e25d6e631..97bc5bba5cf0 100644\n--- a/core/src/test/java/org/apache/iceberg/rest/requests/TestCommitTransactionRequestParser.java\n+++ b/core/src/test/java/org/apache/iceberg/rest/requests/TestCommitTransactionRequestParser.java\n@@ -26,6 +26,7 @@\n import org.apache.iceberg.UpdateRequirement;\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.junit.jupiter.api.Test;\n \n public class TestCommitTransactionRequestParser {\n@@ -155,6 +156,85 @@ public void roundTripSerde() {\n         .isEqualTo(expectedJson);\n   }\n \n+  @Test\n+  public void roundTripSerdeWithMultipleSnapshots() {\n+    String uuid = \"2cc52516-5e73-41f2-b139-545d41a4e151\";\n+    UpdateTableRequest commitTableRequestOne =\n+        UpdateTableRequest.create(\n+            TableIdentifier.of(\"ns1\", \"table1\"),\n+            ImmutableList.of(\n+                new UpdateRequirement.AssertTableUUID(uuid),\n+                new UpdateRequirement.AssertTableDoesNotExist()),\n+            ImmutableList.of(\n+                new MetadataUpdate.AssignUUID(uuid), new MetadataUpdate.SetCurrentSchema(23)));\n+\n+    UpdateTableRequest commitTableRequestTwo =\n+        UpdateTableRequest.create(\n+            TableIdentifier.of(\"ns1\", \"table2\"),\n+            ImmutableList.of(\n+                new UpdateRequirement.AssertDefaultSpecID(4),\n+                new UpdateRequirement.AssertCurrentSchemaID(24)),\n+            ImmutableList.of(\n+                new MetadataUpdate.RemoveSnapshots(Sets.newHashSet(101L, 102L)),\n+                new MetadataUpdate.SetCurrentSchema(25)));\n+\n+    CommitTransactionRequest request =\n+        new CommitTransactionRequest(\n+            ImmutableList.of(commitTableRequestOne, commitTableRequestTwo));\n+\n+    String expectedJson =\n+        \"{\\n\"\n+            + \"  \\\"table-changes\\\" : [ {\\n\"\n+            + \"    \\\"identifier\\\" : {\\n\"\n+            + \"      \\\"namespace\\\" : [ \\\"ns1\\\" ],\\n\"\n+            + \"      \\\"name\\\" : \\\"table1\\\"\\n\"\n+            + \"    },\\n\"\n+            + \"    \\\"requirements\\\" : [ {\\n\"\n+            + \"      \\\"type\\\" : \\\"assert-table-uuid\\\",\\n\"\n+            + \"      \\\"uuid\\\" : \\\"2cc52516-5e73-41f2-b139-545d41a4e151\\\"\\n\"\n+            + \"    }, {\\n\"\n+            + \"      \\\"type\\\" : \\\"assert-create\\\"\\n\"\n+            + \"    } ],\\n\"\n+            + \"    \\\"updates\\\" : [ {\\n\"\n+            + \"      \\\"action\\\" : \\\"assign-uuid\\\",\\n\"\n+            + \"      \\\"uuid\\\" : \\\"2cc52516-5e73-41f2-b139-545d41a4e151\\\"\\n\"\n+            + \"    }, {\\n\"\n+            + \"      \\\"action\\\" : \\\"set-current-schema\\\",\\n\"\n+            + \"      \\\"schema-id\\\" : 23\\n\"\n+            + \"    } ]\\n\"\n+            + \"  }, {\\n\"\n+            + \"    \\\"identifier\\\" : {\\n\"\n+            + \"      \\\"namespace\\\" : [ \\\"ns1\\\" ],\\n\"\n+            + \"      \\\"name\\\" : \\\"table2\\\"\\n\"\n+            + \"    },\\n\"\n+            + \"    \\\"requirements\\\" : [ {\\n\"\n+            + \"      \\\"type\\\" : \\\"assert-default-spec-id\\\",\\n\"\n+            + \"      \\\"default-spec-id\\\" : 4\\n\"\n+            + \"    }, {\\n\"\n+            + \"      \\\"type\\\" : \\\"assert-current-schema-id\\\",\\n\"\n+            + \"      \\\"current-schema-id\\\" : 24\\n\"\n+            + \"    } ],\\n\"\n+            + \"    \\\"updates\\\" : [ {\\n\"\n+            + \"      \\\"action\\\" : \\\"remove-snapshots\\\",\\n\"\n+            + \"      \\\"snapshot-ids\\\" : [ 101, 102 ]\\n\"\n+            + \"    }, {\\n\"\n+            + \"      \\\"action\\\" : \\\"set-current-schema\\\",\\n\"\n+            + \"      \\\"schema-id\\\" : 25\\n\"\n+            + \"    } ]\\n\"\n+            + \"  } ]\\n\"\n+            + \"}\";\n+\n+    String json = CommitTransactionRequestParser.toJson(request, true);\n+    assertThat(json).isEqualTo(expectedJson);\n+\n+    // can't do an equality comparison on CommitTransactionRequest because updates/requirements\n+    // don't implement equals/hashcode\n+    assertThat(\n+            CommitTransactionRequestParser.toJson(\n+                CommitTransactionRequestParser.fromJson(json), true))\n+        .isEqualTo(expectedJson);\n+  }\n+\n   @Test\n   public void emptyRequirementsAndUpdates() {\n     CommitTransactionRequest commitTxRequest =\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12656",
    "pr_id": 12656,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 15,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "spark/v3.4/build.gradle",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteDelete.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteMerge.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteUpdate.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestDelete.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadDelete.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadUpdate.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestUpdate.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteDelete.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteMerge.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteUpdate.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteDelete.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteMerge.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteUpdate.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestDelete.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadDelete.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadUpdate.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestUpdate.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteDelete.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteMerge.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteUpdate.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java"
    ],
    "base_commit": "695374d8bfbd22f4347cfe43b54a93c41b88f166",
    "head_commit": "f65bd22e6980ebe083d61973412ab8c6cece974f",
    "repo_url": "https://github.com/apache/iceberg/pull/12656",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12656",
    "dockerfile": "",
    "pr_merged_at": "2025-03-27T09:31:38.000Z",
    "patch": "diff --git a/spark/v3.4/build.gradle b/spark/v3.4/build.gradle\nindex bf28635f3c57..6b841c995826 100644\n--- a/spark/v3.4/build.gradle\n+++ b/spark/v3.4/build.gradle\n@@ -184,6 +184,7 @@ project(\":iceberg-spark:iceberg-spark-extensions-${sparkMajorVersion}_${scalaVer\n \n     testImplementation libs.avro.avro\n     testImplementation libs.parquet.hadoop\n+    testImplementation libs.awaitility\n     testImplementation libs.junit.vintage.engine\n     testImplementation \"org.apache.datafusion:comet-spark-spark${sparkMajorVersion}_${scalaVersion}:0.5.0\"\n \n",
    "test_patch": "diff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java\nindex da5a4f577cfb..b5d641576314 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/SparkRowLevelOperationsTestBase.java\n@@ -48,11 +48,15 @@\n import java.util.Map;\n import java.util.Random;\n import java.util.Set;\n+import java.util.UUID;\n import java.util.concurrent.ThreadLocalRandom;\n import java.util.stream.Collectors;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.Files;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PlanningMode;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n@@ -74,44 +78,33 @@\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.execution.SparkPlan;\n-import org.junit.Assert;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-import org.junit.runners.Parameterized.Parameters;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-@RunWith(Parameterized.class)\n-public abstract class SparkRowLevelOperationsTestBase extends SparkExtensionsTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public abstract class SparkRowLevelOperationsTestBase extends ExtensionsTestBase {\n \n   private static final Random RANDOM = ThreadLocalRandom.current();\n \n-  protected final String fileFormat;\n-  protected final boolean vectorized;\n-  protected final String distributionMode;\n-  protected final boolean fanoutEnabled;\n-  protected final String branch;\n-  protected final PlanningMode planningMode;\n-  protected final int formatVersion;\n-\n-  public SparkRowLevelOperationsTestBase(\n-      String catalogName,\n-      String implementation,\n-      Map<String, String> config,\n-      String fileFormat,\n-      boolean vectorized,\n-      String distributionMode,\n-      boolean fanoutEnabled,\n-      String branch,\n-      PlanningMode planningMode,\n-      int formatVersion) {\n-    super(catalogName, implementation, config);\n-    this.fileFormat = fileFormat;\n-    this.vectorized = vectorized;\n-    this.distributionMode = distributionMode;\n-    this.fanoutEnabled = fanoutEnabled;\n-    this.branch = branch;\n-    this.planningMode = planningMode;\n-    this.formatVersion = formatVersion;\n-  }\n+  @Parameter(index = 3)\n+  protected FileFormat fileFormat;\n+\n+  @Parameter(index = 4)\n+  protected boolean vectorized;\n+\n+  @Parameter(index = 5)\n+  protected String distributionMode;\n+\n+  @Parameter(index = 6)\n+  protected boolean fanoutEnabled;\n+\n+  @Parameter(index = 7)\n+  protected String branch;\n+\n+  @Parameter(index = 8)\n+  protected PlanningMode planningMode;\n+\n+  @Parameter(index = 9)\n+  protected int formatVersion;\n \n   @Parameters(\n       name =\n@@ -126,7 +119,7 @@ public static Object[][] parameters() {\n         ImmutableMap.of(\n             \"type\", \"hive\",\n             \"default-namespace\", \"default\"),\n-        \"orc\",\n+        FileFormat.ORC,\n         true,\n         WRITE_DISTRIBUTION_MODE_NONE,\n         true,\n@@ -140,7 +133,7 @@ public static Object[][] parameters() {\n         ImmutableMap.of(\n             \"type\", \"hive\",\n             \"default-namespace\", \"default\"),\n-        \"parquet\",\n+        FileFormat.PARQUET,\n         true,\n         WRITE_DISTRIBUTION_MODE_NONE,\n         false,\n@@ -152,7 +145,7 @@ public static Object[][] parameters() {\n         \"testhadoop\",\n         SparkCatalog.class.getName(),\n         ImmutableMap.of(\"type\", \"hadoop\"),\n-        \"parquet\",\n+        FileFormat.PARQUET,\n         RANDOM.nextBoolean(),\n         WRITE_DISTRIBUTION_MODE_HASH,\n         true,\n@@ -171,7 +164,7 @@ public static Object[][] parameters() {\n             \"cache-enabled\",\n                 \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n             ),\n-        \"avro\",\n+        FileFormat.AVRO,\n         false,\n         WRITE_DISTRIBUTION_MODE_RANGE,\n         false,\n@@ -183,7 +176,7 @@ public static Object[][] parameters() {\n         \"testhadoop\",\n         SparkCatalog.class.getName(),\n         ImmutableMap.of(\"type\", \"hadoop\"),\n-        \"parquet\",\n+        FileFormat.PARQUET,\n         RANDOM.nextBoolean(),\n         WRITE_DISTRIBUTION_MODE_HASH,\n         true,\n@@ -206,7 +199,7 @@ public static Object[][] parameters() {\n             \"cache-enabled\",\n             \"false\" // Spark will delete tables using v1, leaving the cache out of sync\n             ),\n-        \"avro\",\n+        FileFormat.AVRO,\n         false,\n         WRITE_DISTRIBUTION_MODE_RANGE,\n         false,\n@@ -237,18 +230,18 @@ protected void initTable() {\n         formatVersion);\n \n     switch (fileFormat) {\n-      case \"parquet\":\n+      case PARQUET:\n         sql(\n             \"ALTER TABLE %s SET TBLPROPERTIES('%s' '%b')\",\n             tableName, PARQUET_VECTORIZATION_ENABLED, vectorized);\n         break;\n-      case \"orc\":\n+      case ORC:\n         sql(\n             \"ALTER TABLE %s SET TBLPROPERTIES('%s' '%b')\",\n             tableName, ORC_VECTORIZATION_ENABLED, vectorized);\n         break;\n-      case \"avro\":\n-        Assert.assertFalse(vectorized);\n+      case AVRO:\n+        assertThat(vectorized).isFalse();\n         break;\n     }\n \n@@ -354,7 +347,7 @@ protected void validateSnapshot(\n       String deletedDataFiles,\n       String addedDeleteFiles,\n       String addedDataFiles) {\n-    Assert.assertEquals(\"Operation must match\", operation, snapshot.operation());\n+    assertThat(snapshot.operation()).as(\"Operation must match\").isEqualTo(operation);\n     validateProperty(snapshot, CHANGED_PARTITION_COUNT_PROP, changedPartitionCount);\n     validateProperty(snapshot, DELETED_FILES_PROP, deletedDataFiles);\n     validateProperty(snapshot, ADDED_DELETE_FILES_PROP, addedDeleteFiles);\n@@ -367,14 +360,15 @@ protected void validateSnapshot(\n \n   protected void validateProperty(Snapshot snapshot, String property, Set<String> expectedValues) {\n     String actual = snapshot.summary().get(property);\n-    Assert.assertTrue(\n-        \"Snapshot property \"\n-            + property\n-            + \" has unexpected value, actual = \"\n-            + actual\n-            + \", expected one of : \"\n-            + String.join(\",\", expectedValues),\n-        expectedValues.contains(actual));\n+    assertThat(actual)\n+        .as(\n+            \"Snapshot property \"\n+                + property\n+                + \" has unexpected value, actual = \"\n+                + actual\n+                + \", expected one of : \"\n+                + String.join(\",\", expectedValues))\n+        .isIn(expectedValues);\n   }\n \n   protected void validateProperty(Snapshot snapshot, String property, String expectedValue) {\n@@ -397,7 +391,9 @@ protected void sleep(long millis) {\n \n   protected DataFile writeDataFile(Table table, List<GenericRecord> records) {\n     try {\n-      OutputFile file = Files.localOutput(temp.newFile());\n+      OutputFile file =\n+          Files.localOutput(\n+              temp.resolve(fileFormat.addExtension(UUID.randomUUID().toString())).toFile());\n \n       DataWriter<GenericRecord> dataWriter =\n           Parquet.writeData(file)\n@@ -443,7 +439,7 @@ protected boolean supportsVectorization() {\n   }\n \n   private boolean isParquet() {\n-    return fileFormat.equalsIgnoreCase(FileFormat.PARQUET.name());\n+    return fileFormat.equals(FileFormat.PARQUET);\n   }\n \n   private boolean isCopyOnWrite() {\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteDelete.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteDelete.java\nindex 9818386dc4e6..f7ded0c4d7d2 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteDelete.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteDelete.java\n@@ -19,9 +19,14 @@\n package org.apache.iceberg.spark.extensions;\n \n import static org.apache.iceberg.TableProperties.DELETE_ISOLATION_LEVEL;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n+import java.io.File;\n+import java.io.IOException;\n import java.util.Collections;\n+import java.util.List;\n import java.util.Map;\n import java.util.concurrent.ExecutionException;\n import java.util.concurrent.ExecutorService;\n@@ -33,15 +38,20 @@\n import java.util.concurrent.atomic.AtomicInteger;\n import org.apache.iceberg.AppendFiles;\n import org.apache.iceberg.DataFile;\n-import org.apache.iceberg.PlanningMode;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowLevelOperationMode;\n+import org.apache.iceberg.Schema;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TestHelpers;\n+import org.apache.iceberg.data.FileHelpers;\n import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.io.OutputFile;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n import org.apache.iceberg.spark.Spark3Util;\n import org.apache.iceberg.spark.SparkSQLProperties;\n@@ -49,47 +59,24 @@\n import org.apache.spark.sql.Encoders;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.internal.SQLConf;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Test;\n+import org.awaitility.Awaitility;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestCopyOnWriteDelete extends TestDelete {\n \n-  public TestCopyOnWriteDelete(\n-      String catalogName,\n-      String implementation,\n-      Map<String, String> config,\n-      String fileFormat,\n-      Boolean vectorized,\n-      String distributionMode,\n-      boolean fanoutEnabled,\n-      String branch,\n-      PlanningMode planningMode,\n-      int formatVersion) {\n-    super(\n-        catalogName,\n-        implementation,\n-        config,\n-        fileFormat,\n-        vectorized,\n-        distributionMode,\n-        fanoutEnabled,\n-        branch,\n-        planningMode,\n-        formatVersion);\n-  }\n-\n   @Override\n   protected Map<String, String> extraTableProperties() {\n     return ImmutableMap.of(\n         TableProperties.DELETE_MODE, RowLevelOperationMode.COPY_ON_WRITE.modeName());\n   }\n \n-  @Test\n+  @TestTemplate\n   public synchronized void testDeleteWithConcurrentTableRefresh() throws Exception {\n     // this test can only be run with Hive tables as it requires a reliable lock\n     // also, the table cache must be enabled so that the same table instance can be reused\n-    Assume.assumeTrue(catalogName.equalsIgnoreCase(\"testhive\"));\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"testhive\");\n \n     createAndInitUnpartitionedTable();\n     createOrReplaceView(\"deleted_id\", Collections.singletonList(1), Encoders.INT());\n@@ -115,9 +102,11 @@ public synchronized void testDeleteWithConcurrentTableRefresh() throws Exception\n         executorService.submit(\n             () -> {\n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> barrier.get() >= currentNumOperations * 2);\n \n                 sql(\"DELETE FROM %s WHERE id IN (SELECT * FROM deleted_id)\", commitTarget());\n \n@@ -134,9 +123,11 @@ public synchronized void testDeleteWithConcurrentTableRefresh() throws Exception\n               record.set(1, \"hr\"); // dep\n \n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (shouldAppend.get() && barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> !shouldAppend.get() || barrier.get() >= currentNumOperations * 2);\n \n                 if (!shouldAppend.get()) {\n                   return;\n@@ -150,7 +141,6 @@ public synchronized void testDeleteWithConcurrentTableRefresh() throws Exception\n                   }\n \n                   appendFiles.commit();\n-                  sleep(10);\n                 }\n \n                 barrier.incrementAndGet();\n@@ -169,10 +159,10 @@ public synchronized void testDeleteWithConcurrentTableRefresh() throws Exception\n     }\n \n     executorService.shutdown();\n-    Assert.assertTrue(\"Timeout\", executorService.awaitTermination(2, TimeUnit.MINUTES));\n+    assertThat(executorService.awaitTermination(2, TimeUnit.MINUTES)).as(\"Timeout\").isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRuntimeFilteringWithPreservedDataGrouping() throws NoSuchTableException {\n     createAndInitPartitionedTable();\n \n@@ -190,7 +180,7 @@ public void testRuntimeFilteringWithPreservedDataGrouping() throws NoSuchTableEx\n     withSQLConf(sqlConf, () -> sql(\"DELETE FROM %s WHERE id = 2\", commitTarget()));\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 3 snapshots\", 3, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 3 snapshots\").hasSize(3);\n \n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n     validateCopyOnWrite(currentSnapshot, \"1\", \"1\", \"1\");\n@@ -200,4 +190,38 @@ public void testRuntimeFilteringWithPreservedDataGrouping() throws NoSuchTableEx\n         ImmutableList.of(row(1, \"hardware\"), row(1, \"hr\"), row(3, \"hr\")),\n         sql(\"SELECT * FROM %s ORDER BY id, dep\", selectTarget()));\n   }\n+\n+  @TestTemplate\n+  public void testEqualityDeletePreservation() throws NoSuchTableException, IOException {\n+    createAndInitPartitionedTable();\n+    append(tableName, new Employee(1, \"hr\"), new Employee(2, \"hr\"), new Employee(3, \"hr\"));\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    OutputFile out = Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile()));\n+    Schema deleteSchema = table.schema().select(\"id\");\n+    GenericRecord deleteRecord = GenericRecord.create(deleteSchema);\n+    DeleteFile eqDelete =\n+        FileHelpers.writeDeleteFile(\n+            table,\n+            out,\n+            TestHelpers.Row.of(\"hr\"),\n+            List.of(deleteRecord.copy(\"id\", 2)),\n+            deleteSchema);\n+\n+    table.newRowDelta().addDeletes(eqDelete).commit();\n+\n+    sql(\"REFRESH TABLE %s\", tableName);\n+\n+    assertEquals(\n+        \"Equality delete should remove row with id 2\",\n+        ImmutableList.of(row(1, \"hr\"), row(3, \"hr\")),\n+        sql(\"SELECT * FROM %s ORDER BY id, dep\", tableName));\n+\n+    sql(\"DELETE FROM %s WHERE id = 3\", tableName);\n+\n+    assertEquals(\n+        \"COW Delete should remove row with id 3\",\n+        ImmutableList.of(row(1, \"hr\")),\n+        sql(\"SELECT * FROM %s ORDER BY id, dep\", tableName));\n+  }\n }\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteMerge.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteMerge.java\nindex 5a51b7be0b2a..8d509c2952a8 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteMerge.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteMerge.java\n@@ -19,7 +19,9 @@\n package org.apache.iceberg.spark.extensions;\n \n import static org.apache.iceberg.TableProperties.MERGE_ISOLATION_LEVEL;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.util.Collections;\n import java.util.Map;\n@@ -32,7 +34,7 @@\n import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicInteger;\n import org.apache.iceberg.DataFile;\n-import org.apache.iceberg.PlanningMode;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n@@ -40,54 +42,29 @@\n import org.apache.iceberg.data.GenericRecord;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n import org.apache.iceberg.spark.Spark3Util;\n import org.apache.iceberg.spark.SparkSQLProperties;\n import org.apache.iceberg.util.SnapshotUtil;\n import org.apache.spark.sql.Encoders;\n import org.apache.spark.sql.internal.SQLConf;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Test;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestCopyOnWriteMerge extends TestMerge {\n \n-  public TestCopyOnWriteMerge(\n-      String catalogName,\n-      String implementation,\n-      Map<String, String> config,\n-      String fileFormat,\n-      boolean vectorized,\n-      String distributionMode,\n-      boolean fanoutEnabled,\n-      String branch,\n-      PlanningMode planningMode,\n-      int formatVersion) {\n-    super(\n-        catalogName,\n-        implementation,\n-        config,\n-        fileFormat,\n-        vectorized,\n-        distributionMode,\n-        fanoutEnabled,\n-        branch,\n-        planningMode,\n-        formatVersion);\n-  }\n-\n   @Override\n   protected Map<String, String> extraTableProperties() {\n     return ImmutableMap.of(\n         TableProperties.MERGE_MODE, RowLevelOperationMode.COPY_ON_WRITE.modeName());\n   }\n \n-  @Test\n+  @TestTemplate\n   public synchronized void testMergeWithConcurrentTableRefresh() throws Exception {\n     // this test can only be run with Hive tables as it requires a reliable lock\n     // also, the table cache must be enabled so that the same table instance can be reused\n-    Assume.assumeTrue(catalogName.equalsIgnoreCase(\"testhive\"));\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"testhive\");\n \n     createAndInitTable(\"id INT, dep STRING\");\n     createOrReplaceView(\"source\", Collections.singletonList(1), Encoders.INT());\n@@ -167,10 +144,10 @@ public synchronized void testMergeWithConcurrentTableRefresh() throws Exception\n     }\n \n     executorService.shutdown();\n-    Assert.assertTrue(\"Timeout\", executorService.awaitTermination(2, TimeUnit.MINUTES));\n+    assertThat(executorService.awaitTermination(2, TimeUnit.MINUTES)).as(\"Timeout\").isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRuntimeFilteringWithReportedPartitioning() {\n     createAndInitTable(\"id INT, dep STRING\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD dep\", tableName);\n@@ -201,7 +178,7 @@ public void testRuntimeFilteringWithReportedPartitioning() {\n                 commitTarget()));\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 3 snapshots\", 3, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 3 snapshots\").hasSize(3);\n \n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n     validateCopyOnWrite(currentSnapshot, \"1\", \"1\", \"1\");\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteUpdate.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteUpdate.java\nindex 92d75e3ebc48..21d1377b2b98 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteUpdate.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteUpdate.java\n@@ -19,7 +19,9 @@\n package org.apache.iceberg.spark.extensions;\n \n import static org.apache.iceberg.TableProperties.UPDATE_ISOLATION_LEVEL;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.util.Map;\n import java.util.concurrent.ExecutionException;\n@@ -32,7 +34,7 @@\n import java.util.concurrent.atomic.AtomicInteger;\n import org.apache.iceberg.AppendFiles;\n import org.apache.iceberg.DataFile;\n-import org.apache.iceberg.PlanningMode;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n@@ -40,53 +42,29 @@\n import org.apache.iceberg.data.GenericRecord;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n import org.apache.iceberg.spark.Spark3Util;\n import org.apache.iceberg.spark.SparkSQLProperties;\n import org.apache.iceberg.util.SnapshotUtil;\n import org.apache.spark.sql.internal.SQLConf;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.Test;\n+import org.awaitility.Awaitility;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestCopyOnWriteUpdate extends TestUpdate {\n \n-  public TestCopyOnWriteUpdate(\n-      String catalogName,\n-      String implementation,\n-      Map<String, String> config,\n-      String fileFormat,\n-      boolean vectorized,\n-      String distributionMode,\n-      boolean fanoutEnabled,\n-      String branch,\n-      PlanningMode planningMode,\n-      int formatVersion) {\n-    super(\n-        catalogName,\n-        implementation,\n-        config,\n-        fileFormat,\n-        vectorized,\n-        distributionMode,\n-        fanoutEnabled,\n-        branch,\n-        planningMode,\n-        formatVersion);\n-  }\n-\n   @Override\n   protected Map<String, String> extraTableProperties() {\n     return ImmutableMap.of(\n         TableProperties.UPDATE_MODE, RowLevelOperationMode.COPY_ON_WRITE.modeName());\n   }\n \n-  @Test\n+  @TestTemplate\n   public synchronized void testUpdateWithConcurrentTableRefresh() throws Exception {\n     // this test can only be run with Hive tables as it requires a reliable lock\n     // also, the table cache must be enabled so that the same table instance can be reused\n-    Assume.assumeTrue(catalogName.equalsIgnoreCase(\"testhive\"));\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"testhive\");\n \n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -111,9 +89,11 @@ public synchronized void testUpdateWithConcurrentTableRefresh() throws Exception\n         executorService.submit(\n             () -> {\n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> barrier.get() >= currentNumOperations * 2);\n \n                 sql(\"UPDATE %s SET id = -1 WHERE id = 1\", commitTarget());\n \n@@ -130,9 +110,11 @@ public synchronized void testUpdateWithConcurrentTableRefresh() throws Exception\n               record.set(1, \"hr\"); // dep\n \n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (shouldAppend.get() && barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> !shouldAppend.get() || barrier.get() >= currentNumOperations * 2);\n \n                 if (!shouldAppend.get()) {\n                   return;\n@@ -146,7 +128,6 @@ public synchronized void testUpdateWithConcurrentTableRefresh() throws Exception\n                   }\n \n                   appendFiles.commit();\n-                  sleep(10);\n                 }\n \n                 barrier.incrementAndGet();\n@@ -165,10 +146,10 @@ public synchronized void testUpdateWithConcurrentTableRefresh() throws Exception\n     }\n \n     executorService.shutdown();\n-    Assert.assertTrue(\"Timeout\", executorService.awaitTermination(2, TimeUnit.MINUTES));\n+    assertThat(executorService.awaitTermination(2, TimeUnit.MINUTES)).as(\"Timeout\").isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRuntimeFilteringWithReportedPartitioning() {\n     createAndInitTable(\"id INT, dep STRING\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD dep\", tableName);\n@@ -189,7 +170,7 @@ public void testRuntimeFilteringWithReportedPartitioning() {\n     withSQLConf(sqlConf, () -> sql(\"UPDATE %s SET id = -1 WHERE id = 2\", commitTarget()));\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 3 snapshots\", 3, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 3 snapshots\").hasSize(3);\n \n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n     validateCopyOnWrite(currentSnapshot, \"1\", \"1\", \"1\");\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestDelete.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestDelete.java\nindex e4e71c88c661..1dd6db48f7d8 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestDelete.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestDelete.java\n@@ -51,8 +51,9 @@\n import org.apache.iceberg.AppendFiles;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DistributionMode;\n+import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.ManifestFile;\n-import org.apache.iceberg.PlanningMode;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.SnapshotRef;\n@@ -63,7 +64,6 @@\n import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n import org.apache.iceberg.spark.Spark3Util;\n@@ -82,44 +82,21 @@\n import org.apache.spark.sql.execution.SparkPlan;\n import org.apache.spark.sql.execution.datasources.v2.OptimizeMetadataOnlyDeleteFromTable;\n import org.apache.spark.sql.internal.SQLConf;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.BeforeClass;\n-import org.junit.Test;\n+import org.awaitility.Awaitility;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public abstract class TestDelete extends SparkRowLevelOperationsTestBase {\n \n-  public TestDelete(\n-      String catalogName,\n-      String implementation,\n-      Map<String, String> config,\n-      String fileFormat,\n-      Boolean vectorized,\n-      String distributionMode,\n-      boolean fanoutEnabled,\n-      String branch,\n-      PlanningMode planningMode,\n-      int formatVersion) {\n-    super(\n-        catalogName,\n-        implementation,\n-        config,\n-        fileFormat,\n-        vectorized,\n-        distributionMode,\n-        fanoutEnabled,\n-        branch,\n-        planningMode,\n-        formatVersion);\n-  }\n-\n-  @BeforeClass\n+  @BeforeAll\n   public static void setupSparkConf() {\n     spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS deleted_id\");\n@@ -127,7 +104,7 @@ public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS parquet_table\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithVectorizedReads() throws NoSuchTableException {\n     assumeThat(supportsVectorization()).isTrue();\n \n@@ -142,7 +119,7 @@ public void testDeleteWithVectorizedReads() throws NoSuchTableException {\n     assertAllBatchScansVectorized(plan);\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 3 snapshots\", 3, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 3 snapshots\").hasSize(3);\n \n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n     if (mode(table) == COPY_ON_WRITE) {\n@@ -157,7 +134,7 @@ public void testDeleteWithVectorizedReads() throws NoSuchTableException {\n         sql(\"SELECT * FROM %s ORDER BY id ASC\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCoalesceDelete() throws Exception {\n     createAndInitUnpartitionedTable();\n \n@@ -217,11 +194,12 @@ public void testCoalesceDelete() throws Exception {\n       validateProperty(snapshot, SnapshotSummary.ADDED_DELETE_FILES_PROP, \"1\");\n     }\n \n-    Assert.assertEquals(\n-        \"Row count must match\", 200L, scalarSql(\"SELECT COUNT(*) FROM %s\", commitTarget()));\n+    assertThat(scalarSql(\"SELECT COUNT(*) FROM %s\", commitTarget()))\n+        .as(\"Row count must match\")\n+        .isEqualTo(200L);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSkewDelete() throws Exception {\n     createAndInitPartitionedTable();\n \n@@ -279,11 +257,12 @@ public void testSkewDelete() throws Exception {\n       validateProperty(snapshot, SnapshotSummary.ADDED_DELETE_FILES_PROP, \"4\");\n     }\n \n-    Assert.assertEquals(\n-        \"Row count must match\", 200L, scalarSql(\"SELECT COUNT(*) FROM %s\", commitTarget()));\n+    assertThat(scalarSql(\"SELECT COUNT(*) FROM %s\", commitTarget()))\n+        .as(\"Row count must match\")\n+        .isEqualTo(200L);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithoutScanningTable() throws Exception {\n     createAndInitPartitionedTable();\n \n@@ -318,9 +297,11 @@ public void testDeleteWithoutScanningTable() throws Exception {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteFileThenMetadataDelete() throws Exception {\n-    Assume.assumeFalse(\"Avro does not support metadata delete\", fileFormat.equals(\"avro\"));\n+    assumeThat(fileFormat)\n+        .as(\"Avro does not support metadata delete\")\n+        .isNotEqualTo(FileFormat.AVRO);\n     createAndInitUnpartitionedTable();\n     createBranchIfNeeded();\n     sql(\"INSERT INTO TABLE %s VALUES (1, 'hr'), (2, 'hardware'), (null, 'hr')\", commitTarget());\n@@ -335,8 +316,9 @@ public void testDeleteFileThenMetadataDelete() throws Exception {\n     sql(\"DELETE FROM %s AS t WHERE t.id = 1\", commitTarget());\n \n     List<DataFile> dataFilesAfter = TestHelpers.dataFiles(table, branch);\n-    Assert.assertTrue(\n-        \"Data file should have been removed\", dataFilesBefore.size() > dataFilesAfter.size());\n+    assertThat(dataFilesAfter)\n+        .as(\"Data file should have been removed\")\n+        .hasSizeLessThan(dataFilesBefore.size());\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -344,7 +326,7 @@ public void testDeleteFileThenMetadataDelete() throws Exception {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithPartitionedTable() throws Exception {\n     createAndInitPartitionedTable();\n \n@@ -360,8 +342,9 @@ public void testDeleteWithPartitionedTable() throws Exception {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n     List<Row> rowLevelDeletePartitions =\n         spark.sql(\"SELECT * FROM \" + tableName + \".partitions \").collectAsList();\n-    Assert.assertEquals(\n-        \"row level delete does not reduce number of partition\", 2, rowLevelDeletePartitions.size());\n+    assertThat(rowLevelDeletePartitions)\n+        .as(\"row level delete does not reduce number of partition\")\n+        .hasSize(2);\n \n     // partition aligned delete\n     sql(\"DELETE FROM %s WHERE dep = 'hr'\", tableName);\n@@ -372,11 +355,10 @@ public void testDeleteWithPartitionedTable() throws Exception {\n         sql(\"SELECT * FROM %s ORDER BY id\", tableName));\n     List<Row> actualPartitions =\n         spark.sql(\"SELECT * FROM \" + tableName + \".partitions \").collectAsList();\n-    Assert.assertEquals(\n-        \"partition aligned delete results in 1 partition\", 1, actualPartitions.size());\n+    assertThat(actualPartitions).as(\"partition aligned delete results in 1 partition\").hasSize(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithFalseCondition() {\n     createAndInitUnpartitionedTable();\n \n@@ -386,7 +368,7 @@ public void testDeleteWithFalseCondition() {\n     sql(\"DELETE FROM %s WHERE id = 1 AND id > 20\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 2 snapshots\").hasSize(2);\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -394,16 +376,16 @@ public void testDeleteWithFalseCondition() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteFromEmptyTable() {\n-    Assume.assumeFalse(\"Custom branch does not exist for empty table\", \"test\".equals(branch));\n+    assumeThat(branch).as(\"Custom branch does not exist for empty table\").isNotEqualTo(\"test\");\n     createAndInitUnpartitionedTable();\n \n     sql(\"DELETE FROM %s WHERE id IN (1)\", commitTarget());\n     sql(\"DELETE FROM %s WHERE dep = 'hr'\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 2 snapshots\").hasSize(2);\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -411,9 +393,9 @@ public void testDeleteFromEmptyTable() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteFromNonExistingCustomBranch() {\n-    Assume.assumeTrue(\"Test only applicable to custom branch\", \"test\".equals(branch));\n+    assumeThat(branch).as(\"Test only applicable to custom branch\").isEqualTo(\"test\");\n     createAndInitUnpartitionedTable();\n \n     assertThatThrownBy(() -> sql(\"DELETE FROM %s WHERE id IN (1)\", commitTarget()))\n@@ -421,7 +403,7 @@ public void testDeleteFromNonExistingCustomBranch() {\n         .hasMessage(\"Cannot use branch (does not exist): test\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExplain() {\n     createAndInitUnpartitionedTable();\n \n@@ -433,7 +415,7 @@ public void testExplain() {\n     sql(\"EXPLAIN DELETE FROM %s WHERE true\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 1 snapshot\", 1, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 1 snapshot\").hasSize(1);\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -441,7 +423,7 @@ public void testExplain() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", commitTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithAlias() {\n     createAndInitUnpartitionedTable();\n \n@@ -456,7 +438,7 @@ public void testDeleteWithAlias() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithDynamicFileFiltering() throws NoSuchTableException {\n     createAndInitPartitionedTable();\n \n@@ -467,7 +449,7 @@ public void testDeleteWithDynamicFileFiltering() throws NoSuchTableException {\n     sql(\"DELETE FROM %s WHERE id = 2\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 3 snapshots\", 3, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 3 snapshots\").hasSize(3);\n \n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n     if (mode(table) == COPY_ON_WRITE) {\n@@ -482,7 +464,7 @@ public void testDeleteWithDynamicFileFiltering() throws NoSuchTableException {\n         sql(\"SELECT * FROM %s ORDER BY id, dep\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteNonExistingRecords() {\n     createAndInitPartitionedTable();\n \n@@ -492,11 +474,11 @@ public void testDeleteNonExistingRecords() {\n     sql(\"DELETE FROM %s AS t WHERE t.id > 10\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 2 snapshots\").hasSize(2);\n \n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n \n-    if (fileFormat.equals(\"orc\") || fileFormat.equals(\"parquet\")) {\n+    if (fileFormat.equals(FileFormat.ORC) || fileFormat.equals(FileFormat.PARQUET)) {\n       validateDelete(currentSnapshot, \"0\", null);\n     } else {\n       if (mode(table) == COPY_ON_WRITE) {\n@@ -512,7 +494,7 @@ public void testDeleteNonExistingRecords() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void deleteSingleRecordProducesDeleteOperation() throws NoSuchTableException {\n     createAndInitPartitionedTable();\n     append(tableName, new Employee(1, \"eng\"), new Employee(2, \"eng\"), new Employee(3, \"eng\"));\n@@ -538,7 +520,7 @@ public void deleteSingleRecordProducesDeleteOperation() throws NoSuchTableExcept\n         .containsExactlyInAnyOrder(row(1, \"eng\"), row(3, \"eng\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithoutCondition() {\n     createAndInitPartitionedTable();\n \n@@ -550,7 +532,7 @@ public void testDeleteWithoutCondition() {\n     sql(\"DELETE FROM %s\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 4 snapshots\", 4, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 4 snapshots\").hasSize(4);\n \n     // should be a delete instead of an overwrite as it is done through a metadata operation\n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n@@ -560,7 +542,7 @@ public void testDeleteWithoutCondition() {\n         \"Should have expected rows\", ImmutableList.of(), sql(\"SELECT * FROM %s\", commitTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteUsingMetadataWithComplexCondition() {\n     createAndInitPartitionedTable();\n \n@@ -574,7 +556,7 @@ public void testDeleteUsingMetadataWithComplexCondition() {\n         commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 4 snapshots\", 4, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 4 snapshots\").hasSize(4);\n \n     // should be a delete instead of an overwrite as it is done through a metadata operation\n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n@@ -586,7 +568,7 @@ public void testDeleteUsingMetadataWithComplexCondition() {\n         sql(\"SELECT * FROM %s\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithArbitraryPartitionPredicates() {\n     createAndInitPartitionedTable();\n \n@@ -599,7 +581,7 @@ public void testDeleteWithArbitraryPartitionPredicates() {\n     sql(\"DELETE FROM %s WHERE id = 10 OR dep LIKE '%%ware'\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 4 snapshots\", 4, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 4 snapshots\").hasSize(4);\n \n     // should be a \"delete\" instead of an \"overwrite\" as only data files have been removed (COW) /\n     // delete files have been added (MOR)\n@@ -617,7 +599,7 @@ public void testDeleteWithArbitraryPartitionPredicates() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithNonDeterministicCondition() {\n     createAndInitPartitionedTable();\n \n@@ -629,7 +611,7 @@ public void testDeleteWithNonDeterministicCondition() {\n         .hasMessageStartingWith(\"nondeterministic expressions are only allowed\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithFoldableConditions() {\n     createAndInitPartitionedTable();\n \n@@ -665,10 +647,10 @@ public void testDeleteWithFoldableConditions() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 2 snapshots\").hasSize(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithNullConditions() {\n     createAndInitPartitionedTable();\n \n@@ -700,13 +682,13 @@ public void testDeleteWithNullConditions() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 3 snapshots\", 3, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 3 snapshots\").hasSize(3);\n \n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n     validateDelete(currentSnapshot, \"1\", \"1\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithInAndNotInConditions() {\n     createAndInitUnpartitionedTable();\n \n@@ -732,9 +714,9 @@ public void testDeleteWithInAndNotInConditions() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithMultipleRowGroupsParquet() throws NoSuchTableException {\n-    Assume.assumeTrue(fileFormat.equalsIgnoreCase(\"parquet\"));\n+    assumeThat(fileFormat).isEqualTo(FileFormat.PARQUET);\n \n     createAndInitPartitionedTable();\n \n@@ -755,15 +737,15 @@ public void testDeleteWithMultipleRowGroupsParquet() throws NoSuchTableException\n     df.coalesce(1).writeTo(tableName).append();\n     createBranchIfNeeded();\n \n-    Assert.assertEquals(200, spark.table(commitTarget()).count());\n+    assertThat(spark.table(commitTarget()).count()).isEqualTo(200);\n \n     // delete a record from one of two row groups and copy over the second one\n     sql(\"DELETE FROM %s WHERE id IN (200, 201)\", commitTarget());\n \n-    Assert.assertEquals(199, spark.table(commitTarget()).count());\n+    assertThat(spark.table(commitTarget()).count()).isEqualTo(199);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithConditionOnNestedColumn() {\n     createAndInitNestedColumnsTable();\n \n@@ -782,7 +764,7 @@ public void testDeleteWithConditionOnNestedColumn() {\n         \"Should have expected rows\", ImmutableList.of(), sql(\"SELECT id FROM %s\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithInSubquery() throws NoSuchTableException {\n     createAndInitUnpartitionedTable();\n \n@@ -829,7 +811,7 @@ public void testDeleteWithInSubquery() throws NoSuchTableException {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithMultiColumnInSubquery() throws NoSuchTableException {\n     createAndInitUnpartitionedTable();\n \n@@ -847,7 +829,7 @@ public void testDeleteWithMultiColumnInSubquery() throws NoSuchTableException {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithNotInSubquery() throws NoSuchTableException {\n     createAndInitUnpartitionedTable();\n \n@@ -907,9 +889,9 @@ public void testDeleteWithNotInSubquery() throws NoSuchTableException {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteOnNonIcebergTableNotSupported() {\n-    Assume.assumeTrue(catalogName.equalsIgnoreCase(\"spark_catalog\"));\n+    assumeThat(catalogName).isEqualToIgnoringCase(\"spark_catalog\");\n \n     sql(\"CREATE TABLE parquet_table (c1 INT, c2 INT) USING parquet\");\n \n@@ -918,7 +900,7 @@ public void testDeleteOnNonIcebergTableNotSupported() {\n         .hasMessageContaining(\"does not support DELETE\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithExistSubquery() throws NoSuchTableException {\n     createAndInitUnpartitionedTable();\n \n@@ -963,7 +945,7 @@ public void testDeleteWithExistSubquery() throws NoSuchTableException {\n         sql(\"SELECT * FROM %s\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithNotExistsSubquery() throws NoSuchTableException {\n     createAndInitUnpartitionedTable();\n \n@@ -999,7 +981,7 @@ public void testDeleteWithNotExistsSubquery() throws NoSuchTableException {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithScalarSubquery() throws NoSuchTableException {\n     createAndInitUnpartitionedTable();\n \n@@ -1020,7 +1002,7 @@ public void testDeleteWithScalarSubquery() throws NoSuchTableException {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteThatRequiresGroupingBeforeWrite() throws NoSuchTableException {\n     createAndInitPartitionedTable();\n \n@@ -1038,20 +1020,21 @@ public void testDeleteThatRequiresGroupingBeforeWrite() throws NoSuchTableExcept\n       spark.conf().set(\"spark.sql.shuffle.partitions\", \"1\");\n \n       sql(\"DELETE FROM %s t WHERE id IN (SELECT * FROM deleted_id)\", commitTarget());\n-      Assert.assertEquals(\n-          \"Should have expected num of rows\", 8L, spark.table(commitTarget()).count());\n+      assertThat(spark.table(commitTarget()).count())\n+          .as(\"Should have expected num of rows\")\n+          .isEqualTo(8L);\n     } finally {\n       spark.conf().set(\"spark.sql.shuffle.partitions\", originalNumOfShufflePartitions);\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public synchronized void testDeleteWithSerializableIsolation() throws InterruptedException {\n     // cannot run tests with concurrency for Hadoop tables without atomic renames\n-    Assume.assumeFalse(catalogName.equalsIgnoreCase(\"testhadoop\"));\n+    assumeThat(catalogName).isNotEqualToIgnoringCase(\"testhadoop\");\n     // if caching is off, the table is eagerly refreshed during runtime filtering\n     // this can cause a validation exception as concurrent changes would be visible\n-    Assume.assumeTrue(cachingCatalogEnabled());\n+    assumeThat(cachingCatalogEnabled()).isTrue();\n \n     createAndInitUnpartitionedTable();\n     createOrReplaceView(\"deleted_id\", Collections.singletonList(1), Encoders.INT());\n@@ -1075,9 +1058,11 @@ public synchronized void testDeleteWithSerializableIsolation() throws Interrupte\n         executorService.submit(\n             () -> {\n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> barrier.get() >= currentNumOperations * 2);\n \n                 sql(\"DELETE FROM %s WHERE id IN (SELECT * FROM deleted_id)\", commitTarget());\n \n@@ -1097,9 +1082,11 @@ public synchronized void testDeleteWithSerializableIsolation() throws Interrupte\n               record.set(1, \"hr\"); // dep\n \n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (shouldAppend.get() && barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> !shouldAppend.get() || barrier.get() >= currentNumOperations * 2);\n \n                 if (!shouldAppend.get()) {\n                   return;\n@@ -1113,7 +1100,6 @@ public synchronized void testDeleteWithSerializableIsolation() throws Interrupte\n                   }\n \n                   appendFiles.commit();\n-                  sleep(10);\n                 }\n \n                 barrier.incrementAndGet();\n@@ -1132,17 +1118,17 @@ public synchronized void testDeleteWithSerializableIsolation() throws Interrupte\n     }\n \n     executorService.shutdown();\n-    Assert.assertTrue(\"Timeout\", executorService.awaitTermination(2, TimeUnit.MINUTES));\n+    assertThat(executorService.awaitTermination(2, TimeUnit.MINUTES)).as(\"Timeout\").isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public synchronized void testDeleteWithSnapshotIsolation()\n       throws InterruptedException, ExecutionException {\n     // cannot run tests with concurrency for Hadoop tables without atomic renames\n-    Assume.assumeFalse(catalogName.equalsIgnoreCase(\"testhadoop\"));\n+    assumeThat(catalogName).isNotEqualToIgnoringCase(\"testhadoop\");\n     // if caching is off, the table is eagerly refreshed during runtime filtering\n     // this can cause a validation exception as concurrent changes would be visible\n-    Assume.assumeTrue(cachingCatalogEnabled());\n+    assumeThat(cachingCatalogEnabled()).isTrue();\n \n     createAndInitUnpartitionedTable();\n     createOrReplaceView(\"deleted_id\", Collections.singletonList(1), Encoders.INT());\n@@ -1166,9 +1152,11 @@ public synchronized void testDeleteWithSnapshotIsolation()\n         executorService.submit(\n             () -> {\n               for (int numOperations = 0; numOperations < 20; numOperations++) {\n-                while (barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> barrier.get() >= currentNumOperations * 2);\n \n                 sql(\"DELETE FROM %s WHERE id IN (SELECT * FROM deleted_id)\", commitTarget());\n \n@@ -1188,9 +1176,11 @@ public synchronized void testDeleteWithSnapshotIsolation()\n               record.set(1, \"hr\"); // dep\n \n               for (int numOperations = 0; numOperations < 20; numOperations++) {\n-                while (shouldAppend.get() && barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> !shouldAppend.get() || barrier.get() >= currentNumOperations * 2);\n \n                 if (!shouldAppend.get()) {\n                   return;\n@@ -1204,7 +1194,6 @@ public synchronized void testDeleteWithSnapshotIsolation()\n                   }\n \n                   appendFiles.commit();\n-                  sleep(10);\n                 }\n \n                 barrier.incrementAndGet();\n@@ -1219,10 +1208,10 @@ public synchronized void testDeleteWithSnapshotIsolation()\n     }\n \n     executorService.shutdown();\n-    Assert.assertTrue(\"Timeout\", executorService.awaitTermination(2, TimeUnit.MINUTES));\n+    assertThat(executorService.awaitTermination(2, TimeUnit.MINUTES)).as(\"Timeout\").isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteRefreshesRelationCache() throws NoSuchTableException {\n     createAndInitPartitionedTable();\n \n@@ -1243,7 +1232,7 @@ public void testDeleteRefreshesRelationCache() throws NoSuchTableException {\n     sql(\"DELETE FROM %s WHERE id = 1\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 3 snapshots\", 3, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 3 snapshots\").hasSize(3);\n \n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n     if (mode(table) == COPY_ON_WRITE) {\n@@ -1264,7 +1253,7 @@ public void testDeleteRefreshesRelationCache() throws NoSuchTableException {\n     spark.sql(\"UNCACHE TABLE tmp\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithMultipleSpecs() {\n     createAndInitTable(\"id INT, dep STRING, category STRING\");\n \n@@ -1290,7 +1279,7 @@ public void testDeleteWithMultipleSpecs() {\n     sql(\"DELETE FROM %s WHERE id IN (1, 3, 5, 7)\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 5 snapshots\", 5, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 5 snapshots\").hasSize(5);\n \n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n     if (mode(table) == COPY_ON_WRITE) {\n@@ -1307,9 +1296,9 @@ public void testDeleteWithMultipleSpecs() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteToWapBranch() throws NoSuchTableException {\n-    Assume.assumeTrue(\"WAP branch only works for table identifier without branch\", branch == null);\n+    assumeThat(branch).as(\"WAP branch only works for table identifier without branch\").isNull();\n \n     createAndInitPartitionedTable();\n     sql(\n@@ -1321,40 +1310,36 @@ public void testDeleteToWapBranch() throws NoSuchTableException {\n         ImmutableMap.of(SparkSQLProperties.WAP_BRANCH, \"wap\"),\n         () -> {\n           sql(\"DELETE FROM %s t WHERE id=0\", tableName);\n-          Assert.assertEquals(\n-              \"Should have expected num of rows when reading table\",\n-              2L,\n-              spark.table(tableName).count());\n-          Assert.assertEquals(\n-              \"Should have expected num of rows when reading WAP branch\",\n-              2L,\n-              spark.table(tableName + \".branch_wap\").count());\n-          Assert.assertEquals(\n-              \"Should not modify main branch\", 3L, spark.table(tableName + \".branch_main\").count());\n+          assertThat(spark.table(tableName).count())\n+              .as(\"Should have expected num of rows when reading table\")\n+              .isEqualTo(2L);\n+          assertThat(spark.table(tableName + \".branch_wap\").count())\n+              .as(\"Should have expected num of rows when reading WAP branch\")\n+              .isEqualTo(2L);\n+          assertThat(spark.table(tableName + \".branch_main\").count())\n+              .as(\"Should not modify main branch\")\n+              .isEqualTo(3L);\n         });\n \n     withSQLConf(\n         ImmutableMap.of(SparkSQLProperties.WAP_BRANCH, \"wap\"),\n         () -> {\n           sql(\"DELETE FROM %s t WHERE id=1\", tableName);\n-          Assert.assertEquals(\n-              \"Should have expected num of rows when reading table with multiple writes\",\n-              1L,\n-              spark.table(tableName).count());\n-          Assert.assertEquals(\n-              \"Should have expected num of rows when reading WAP branch with multiple writes\",\n-              1L,\n-              spark.table(tableName + \".branch_wap\").count());\n-          Assert.assertEquals(\n-              \"Should not modify main branch with multiple writes\",\n-              3L,\n-              spark.table(tableName + \".branch_main\").count());\n+          assertThat(spark.table(tableName).count())\n+              .as(\"Should have expected num of rows when reading table with multiple writes\")\n+              .isEqualTo(1L);\n+          assertThat(spark.table(tableName + \".branch_wap\").count())\n+              .as(\"Should have expected num of rows when reading WAP branch with multiple writes\")\n+              .isEqualTo(1L);\n+          assertThat(spark.table(tableName + \".branch_main\").count())\n+              .as(\"Should not modify main branch with multiple writes\")\n+              .isEqualTo(3L);\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteToWapBranchWithTableBranchIdentifier() throws NoSuchTableException {\n-    Assume.assumeTrue(\"Test must have branch name part in table identifier\", branch != null);\n+    assumeThat(branch).as(\"Test must have branch name part in table identifier\").isNotNull();\n \n     createAndInitPartitionedTable();\n     sql(\n@@ -1374,7 +1359,7 @@ public void testDeleteToWapBranchWithTableBranchIdentifier() throws NoSuchTableE\n                         branch)));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteToCustomWapBranchWithoutWhereClause() throws NoSuchTableException {\n     assumeThat(branch)\n         .as(\"Run only if custom WAP branch is not main\")\n@@ -1410,6 +1395,28 @@ public void testDeleteToCustomWapBranchWithoutWhereClause() throws NoSuchTableEx\n         });\n   }\n \n+  @TestTemplate\n+  public void testDeleteWithFilterOnNestedColumn() {\n+    createAndInitNestedColumnsTable();\n+\n+    sql(\"INSERT INTO TABLE %s VALUES (1, named_struct(\\\"c1\\\", 3, \\\"c2\\\", \\\"v1\\\"))\", tableName);\n+    sql(\"INSERT INTO TABLE %s VALUES (2, named_struct(\\\"c1\\\", 2, \\\"c2\\\", \\\"v2\\\"))\", tableName);\n+\n+    sql(\"DELETE FROM %s WHERE complex.c1 > 3\", tableName);\n+    assertEquals(\n+        \"Should have expected rows\",\n+        ImmutableList.of(row(1), row(2)),\n+        sql(\"SELECT id FROM %s order by id\", tableName));\n+\n+    sql(\"DELETE FROM %s WHERE complex.c1 = 3\", tableName);\n+    assertEquals(\n+        \"Should have expected rows\", ImmutableList.of(row(2)), sql(\"SELECT id FROM %s\", tableName));\n+\n+    sql(\"DELETE FROM %s t WHERE t.complex.c1 = 2\", tableName);\n+    assertEquals(\n+        \"Should have expected rows\", ImmutableList.of(), sql(\"SELECT id FROM %s\", tableName));\n+  }\n+\n   // TODO: multiple stripes for ORC\n \n   protected void createAndInitPartitionedTable() {\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java\nindex 4e5515f352ef..80eafb15e651 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java\n@@ -48,7 +48,8 @@\n import org.apache.iceberg.AppendFiles;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DistributionMode;\n-import org.apache.iceberg.PlanningMode;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.SnapshotSummary;\n@@ -70,50 +71,27 @@\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.execution.SparkPlan;\n import org.apache.spark.sql.internal.SQLConf;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.BeforeClass;\n-import org.junit.Test;\n+import org.awaitility.Awaitility;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public abstract class TestMerge extends SparkRowLevelOperationsTestBase {\n \n-  public TestMerge(\n-      String catalogName,\n-      String implementation,\n-      Map<String, String> config,\n-      String fileFormat,\n-      boolean vectorized,\n-      String distributionMode,\n-      boolean fanoutEnabled,\n-      String branch,\n-      PlanningMode planningMode,\n-      int formatVersion) {\n-    super(\n-        catalogName,\n-        implementation,\n-        config,\n-        fileFormat,\n-        vectorized,\n-        distributionMode,\n-        fanoutEnabled,\n-        branch,\n-        planningMode,\n-        formatVersion);\n-  }\n-\n-  @BeforeClass\n+  @BeforeAll\n   public static void setupSparkConf() {\n     spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS source\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithVectorizedReads() {\n     assumeThat(supportsVectorization()).isTrue();\n \n@@ -155,7 +133,7 @@ public void testMergeWithVectorizedReads() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCoalesceMerge() {\n     createAndInitTable(\"id INT, salary INT, dep STRING\");\n \n@@ -221,13 +199,12 @@ public void testCoalesceMerge() {\n       validateProperty(currentSnapshot, SnapshotSummary.ADDED_DELETE_FILES_PROP, \"1\");\n     }\n \n-    Assert.assertEquals(\n-        \"Row count must match\",\n-        400L,\n-        scalarSql(\"SELECT COUNT(*) FROM %s WHERE salary = -1\", commitTarget()));\n+    assertThat(scalarSql(\"SELECT COUNT(*) FROM %s WHERE salary = -1\", commitTarget()))\n+        .as(\"Row count must match\")\n+        .isEqualTo(400L);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSkewMerge() {\n     createAndInitTable(\"id INT, salary INT, dep STRING\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD dep\", tableName);\n@@ -297,13 +274,12 @@ public void testSkewMerge() {\n       validateProperty(currentSnapshot, SnapshotSummary.ADDED_DELETE_FILES_PROP, \"4\");\n     }\n \n-    Assert.assertEquals(\n-        \"Row count must match\",\n-        400L,\n-        scalarSql(\"SELECT COUNT(*) FROM %s WHERE salary = -1\", commitTarget()));\n+    assertThat(scalarSql(\"SELECT COUNT(*) FROM %s WHERE salary = -1\", commitTarget()))\n+        .as(\"Row count must match\")\n+        .isEqualTo(400L);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeConditionSplitIntoTargetPredicateAndJoinCondition() {\n     createAndInitTable(\n         \"id INT, salary INT, dep STRING, sub_dep STRING\",\n@@ -352,7 +328,7 @@ public void testMergeConditionSplitIntoTargetPredicateAndJoinCondition() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithStaticPredicatePushDown() {\n     createAndInitTable(\"id BIGINT, dep STRING\");\n \n@@ -369,7 +345,7 @@ public void testMergeWithStaticPredicatePushDown() {\n \n     Snapshot snapshot = SnapshotUtil.latestSnapshot(table, branch);\n     String dataFilesCount = snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP);\n-    Assert.assertEquals(\"Must have 2 files before MERGE\", \"2\", dataFilesCount);\n+    assertThat(dataFilesCount).as(\"Must have 2 files before MERGE\").isEqualTo(\"2\");\n \n     createOrReplaceView(\n         \"source\", \"{ \\\"id\\\": 1, \\\"dep\\\": \\\"finance\\\" }\\n\" + \"{ \\\"id\\\": 2, \\\"dep\\\": \\\"hardware\\\" }\");\n@@ -405,9 +381,9 @@ public void testMergeWithStaticPredicatePushDown() {\n         sql(\"SELECT * FROM %s ORDER BY id, dep\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeIntoEmptyTargetInsertAllNonMatchingRows() {\n-    Assume.assumeFalse(\"Custom branch does not exist for empty table\", \"test\".equals(branch));\n+    assumeThat(branch).as(\"Custom branch does not exist for empty table\").isNotEqualTo(\"test\");\n     createAndInitTable(\"id INT, dep STRING\");\n \n     createOrReplaceView(\n@@ -436,9 +412,9 @@ public void testMergeIntoEmptyTargetInsertAllNonMatchingRows() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeIntoEmptyTargetInsertOnlyMatchingRows() {\n-    Assume.assumeFalse(\"Custom branch does not exist for empty table\", \"test\".equals(branch));\n+    assumeThat(branch).as(\"Custom branch does not exist for empty table\").isNotEqualTo(\"test\");\n     createAndInitTable(\"id INT, dep STRING\");\n \n     createOrReplaceView(\n@@ -466,7 +442,7 @@ public void testMergeIntoEmptyTargetInsertOnlyMatchingRows() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithOnlyUpdateClause() {\n     createAndInitTable(\n         \"id INT, dep STRING\",\n@@ -497,7 +473,7 @@ public void testMergeWithOnlyUpdateClause() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithOnlyUpdateClauseAndNullValues() {\n     createAndInitTable(\n         \"id INT, dep STRING\",\n@@ -530,7 +506,7 @@ public void testMergeWithOnlyUpdateClauseAndNullValues() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithOnlyUpdateNullUnmatchedValues() {\n     createAndInitTable(\n         \"id INT, value INT\", \"{ \\\"id\\\": 1, \\\"value\\\": 2 }\\n\" + \"{ \\\"id\\\": 6, \\\"value\\\": null }\");\n@@ -556,7 +532,7 @@ public void testMergeWithOnlyUpdateNullUnmatchedValues() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithOnlyUpdateSingleFieldNullUnmatchedValues() {\n     createAndInitTable(\n         \"id INT, value INT\", \"{ \\\"id\\\": 1, \\\"value\\\": 2 }\\n\" + \"{ \\\"id\\\": 6, \\\"value\\\": null }\");\n@@ -582,7 +558,7 @@ public void testMergeWithOnlyUpdateSingleFieldNullUnmatchedValues() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithOnlyDeleteNullUnmatchedValues() {\n     createAndInitTable(\n         \"id INT, value INT\", \"{ \\\"id\\\": 1, \\\"value\\\": 2 }\\n\" + \"{ \\\"id\\\": 6, \\\"value\\\": null }\");\n@@ -602,7 +578,7 @@ public void testMergeWithOnlyDeleteNullUnmatchedValues() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithOnlyDeleteClause() {\n     createAndInitTable(\n         \"id INT, dep STRING\",\n@@ -632,8 +608,8 @@ public void testMergeWithOnlyDeleteClause() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n-  public void testMergeWithAllCauses() {\n+  @TestTemplate\n+  public void testMergeWithAllClauses() {\n     createAndInitTable(\n         \"id INT, dep STRING\",\n         \"{ \\\"id\\\": 1, \\\"dep\\\": \\\"emp-id-one\\\" }\\n\" + \"{ \\\"id\\\": 6, \\\"dep\\\": \\\"emp-id-6\\\" }\");\n@@ -667,7 +643,7 @@ public void testMergeWithAllCauses() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithAllCausesWithExplicitColumnSpecification() {\n     createAndInitTable(\n         \"id INT, dep STRING\",\n@@ -702,7 +678,7 @@ public void testMergeWithAllCausesWithExplicitColumnSpecification() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithSourceCTE() {\n     createAndInitTable(\n         \"id INT, dep STRING\",\n@@ -738,7 +714,7 @@ public void testMergeWithSourceCTE() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithSourceFromSetOps() {\n     createAndInitTable(\n         \"id INT, dep STRING\",\n@@ -778,7 +754,7 @@ public void testMergeWithSourceFromSetOps() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithOneMatchingBranchButMultipleSourceRowsForTargetRow() {\n     createAndInitTable(\n         \"id INT, dep STRING\",\n@@ -813,7 +789,7 @@ public void testMergeWithOneMatchingBranchButMultipleSourceRowsForTargetRow() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithMultipleUpdatesForTargetRowSmallTargetLargeSource() {\n     createAndInitTable(\n         \"id INT, dep STRING\",\n@@ -850,7 +826,7 @@ public void testMergeWithMultipleUpdatesForTargetRowSmallTargetLargeSource() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void\n       testMergeWithMultipleUpdatesForTargetRowSmallTargetLargeSourceEnabledHashShuffleJoin() {\n     createAndInitTable(\n@@ -892,7 +868,7 @@ public void testMergeWithMultipleUpdatesForTargetRowSmallTargetLargeSource() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithMultipleUpdatesForTargetRowSmallTargetLargeSourceNoEqualityCondition() {\n     createAndInitTable(\"id INT, dep STRING\", \"{ \\\"id\\\": 1, \\\"dep\\\": \\\"emp-id-one\\\" }\");\n \n@@ -931,7 +907,7 @@ public void testMergeWithMultipleUpdatesForTargetRowSmallTargetLargeSourceNoEqua\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithMultipleUpdatesForTargetRowSmallTargetLargeSourceNoNotMatchedActions() {\n     createAndInitTable(\n         \"id INT, dep STRING\",\n@@ -966,7 +942,7 @@ public void testMergeWithMultipleUpdatesForTargetRowSmallTargetLargeSourceNoNotM\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void\n       testMergeWithMultipleUpdatesForTargetRowSmallTargetLargeSourceNoNotMatchedActionsNoEqualityCondition() {\n     createAndInitTable(\"id INT, dep STRING\", \"{ \\\"id\\\": 1, \\\"dep\\\": \\\"emp-id-one\\\" }\");\n@@ -1000,7 +976,7 @@ public void testMergeWithMultipleUpdatesForTargetRowSmallTargetLargeSourceNoNotM\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithMultipleUpdatesForTargetRow() {\n     createAndInitTable(\n         \"id INT, dep STRING\",\n@@ -1038,7 +1014,7 @@ public void testMergeWithMultipleUpdatesForTargetRow() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithUnconditionalDelete() {\n     createAndInitTable(\n         \"id INT, dep STRING\",\n@@ -1071,7 +1047,7 @@ public void testMergeWithUnconditionalDelete() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithSingleConditionalDelete() {\n     createAndInitTable(\n         \"id INT, dep STRING\",\n@@ -1107,7 +1083,7 @@ public void testMergeWithSingleConditionalDelete() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithIdentityTransform() {\n     for (DistributionMode mode : DistributionMode.values()) {\n       createAndInitTable(\"id INT, dep STRING\");\n@@ -1153,7 +1129,7 @@ public void testMergeWithIdentityTransform() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithDaysTransform() {\n     for (DistributionMode mode : DistributionMode.values()) {\n       createAndInitTable(\"id INT, ts TIMESTAMP\");\n@@ -1201,7 +1177,7 @@ public void testMergeWithDaysTransform() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithBucketTransform() {\n     for (DistributionMode mode : DistributionMode.values()) {\n       createAndInitTable(\"id INT, dep STRING\");\n@@ -1247,7 +1223,7 @@ public void testMergeWithBucketTransform() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithTruncateTransform() {\n     for (DistributionMode mode : DistributionMode.values()) {\n       createAndInitTable(\"id INT, dep STRING\");\n@@ -1293,7 +1269,7 @@ public void testMergeWithTruncateTransform() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeIntoPartitionedAndOrderedTable() {\n     for (DistributionMode mode : DistributionMode.values()) {\n       createAndInitTable(\"id INT, dep STRING\");\n@@ -1340,7 +1316,7 @@ public void testMergeIntoPartitionedAndOrderedTable() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSelfMerge() {\n     createAndInitTable(\n         \"id INT, v STRING\", \"{ \\\"id\\\": 1, \\\"v\\\": \\\"v1\\\" }\\n\" + \"{ \\\"id\\\": 2, \\\"v\\\": \\\"v2\\\" }\");\n@@ -1363,7 +1339,7 @@ public void testSelfMerge() {\n         \"Output should match\", expectedRows, sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSelfMergeWithCaching() {\n     createAndInitTable(\n         \"id INT, v STRING\", \"{ \\\"id\\\": 1, \\\"v\\\": \\\"v1\\\" }\\n\" + \"{ \\\"id\\\": 2, \\\"v\\\": \\\"v2\\\" }\");\n@@ -1388,7 +1364,7 @@ public void testSelfMergeWithCaching() {\n         \"Output should match\", expectedRows, sql(\"SELECT * FROM %s ORDER BY id\", commitTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithSourceAsSelfSubquery() {\n     createAndInitTable(\n         \"id INT, v STRING\", \"{ \\\"id\\\": 1, \\\"v\\\": \\\"v1\\\" }\\n\" + \"{ \\\"id\\\": 2, \\\"v\\\": \\\"v2\\\" }\");\n@@ -1413,13 +1389,13 @@ public void testMergeWithSourceAsSelfSubquery() {\n         \"Output should match\", expectedRows, sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public synchronized void testMergeWithSerializableIsolation() throws InterruptedException {\n     // cannot run tests with concurrency for Hadoop tables without atomic renames\n-    Assume.assumeFalse(catalogName.equalsIgnoreCase(\"testhadoop\"));\n+    assumeThat(catalogName).isNotEqualToIgnoringCase(\"testhadoop\");\n     // if caching is off, the table is eagerly refreshed during runtime filtering\n     // this can cause a validation exception as concurrent changes would be visible\n-    Assume.assumeTrue(cachingCatalogEnabled());\n+    assumeThat(cachingCatalogEnabled()).isTrue();\n \n     createAndInitTable(\"id INT, dep STRING\");\n     createOrReplaceView(\"source\", Collections.singletonList(1), Encoders.INT());\n@@ -1443,9 +1419,11 @@ public synchronized void testMergeWithSerializableIsolation() throws Interrupted\n         executorService.submit(\n             () -> {\n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> barrier.get() >= currentNumOperations * 2);\n \n                 sql(\n                     \"MERGE INTO %s t USING source s \"\n@@ -1470,9 +1448,11 @@ public synchronized void testMergeWithSerializableIsolation() throws Interrupted\n               record.set(1, \"hr\"); // dep\n \n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (shouldAppend.get() && barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> !shouldAppend.get() || barrier.get() >= currentNumOperations * 2);\n \n                 if (!shouldAppend.get()) {\n                   return;\n@@ -1485,7 +1465,6 @@ public synchronized void testMergeWithSerializableIsolation() throws Interrupted\n                     appendFiles.toBranch(branch);\n                   }\n                   appendFiles.commit();\n-                  sleep(10);\n                 }\n \n                 barrier.incrementAndGet();\n@@ -1504,17 +1483,17 @@ public synchronized void testMergeWithSerializableIsolation() throws Interrupted\n     }\n \n     executorService.shutdown();\n-    Assert.assertTrue(\"Timeout\", executorService.awaitTermination(2, TimeUnit.MINUTES));\n+    assertThat(executorService.awaitTermination(2, TimeUnit.MINUTES)).as(\"Timeout\").isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public synchronized void testMergeWithSnapshotIsolation()\n       throws InterruptedException, ExecutionException {\n     // cannot run tests with concurrency for Hadoop tables without atomic renames\n-    Assume.assumeFalse(catalogName.equalsIgnoreCase(\"testhadoop\"));\n+    assumeThat(catalogName).isNotEqualToIgnoringCase(\"testhadoop\");\n     // if caching is off, the table is eagerly refreshed during runtime filtering\n     // this can cause a validation exception as concurrent changes would be visible\n-    Assume.assumeTrue(cachingCatalogEnabled());\n+    assumeThat(cachingCatalogEnabled()).isTrue();\n \n     createAndInitTable(\"id INT, dep STRING\");\n     createOrReplaceView(\"source\", Collections.singletonList(1), Encoders.INT());\n@@ -1538,9 +1517,11 @@ public synchronized void testMergeWithSnapshotIsolation()\n         executorService.submit(\n             () -> {\n               for (int numOperations = 0; numOperations < 20; numOperations++) {\n-                while (barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> barrier.get() >= currentNumOperations * 2);\n \n                 sql(\n                     \"MERGE INTO %s t USING source s \"\n@@ -1565,9 +1546,11 @@ public synchronized void testMergeWithSnapshotIsolation()\n               record.set(1, \"hr\"); // dep\n \n               for (int numOperations = 0; numOperations < 20; numOperations++) {\n-                while (shouldAppend.get() && barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> !shouldAppend.get() || barrier.get() >= currentNumOperations * 2);\n \n                 if (!shouldAppend.get()) {\n                   return;\n@@ -1581,7 +1564,6 @@ public synchronized void testMergeWithSnapshotIsolation()\n                   }\n \n                   appendFiles.commit();\n-                  sleep(10);\n                 }\n \n                 barrier.incrementAndGet();\n@@ -1596,10 +1578,10 @@ public synchronized void testMergeWithSnapshotIsolation()\n     }\n \n     executorService.shutdown();\n-    Assert.assertTrue(\"Timeout\", executorService.awaitTermination(2, TimeUnit.MINUTES));\n+    assertThat(executorService.awaitTermination(2, TimeUnit.MINUTES)).as(\"Timeout\").isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithExtraColumnsInSource() {\n     createAndInitTable(\n         \"id INT, v STRING\", \"{ \\\"id\\\": 1, \\\"v\\\": \\\"v1\\\" }\\n\" + \"{ \\\"id\\\": 2, \\\"v\\\": \\\"v2\\\" }\");\n@@ -1629,7 +1611,7 @@ public void testMergeWithExtraColumnsInSource() {\n         \"Output should match\", expectedRows, sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithNullsInTargetAndSource() {\n     createAndInitTable(\n         \"id INT, v STRING\", \"{ \\\"id\\\": null, \\\"v\\\": \\\"v1\\\" }\\n\" + \"{ \\\"id\\\": 2, \\\"v\\\": \\\"v2\\\" }\");\n@@ -1657,7 +1639,7 @@ public void testMergeWithNullsInTargetAndSource() {\n         \"Output should match\", expectedRows, sql(\"SELECT * FROM %s ORDER BY v\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithNullSafeEquals() {\n     createAndInitTable(\n         \"id INT, v STRING\", \"{ \\\"id\\\": null, \\\"v\\\": \\\"v1\\\" }\\n\" + \"{ \\\"id\\\": 2, \\\"v\\\": \\\"v2\\\" }\");\n@@ -1684,7 +1666,7 @@ public void testMergeWithNullSafeEquals() {\n         \"Output should match\", expectedRows, sql(\"SELECT * FROM %s ORDER BY v\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithNullCondition() {\n     createAndInitTable(\n         \"id INT, v STRING\", \"{ \\\"id\\\": null, \\\"v\\\": \\\"v1\\\" }\\n\" + \"{ \\\"id\\\": 2, \\\"v\\\": \\\"v2\\\" }\");\n@@ -1712,7 +1694,7 @@ public void testMergeWithNullCondition() {\n         \"Output should match\", expectedRows, sql(\"SELECT * FROM %s ORDER BY v\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithNullActionConditions() {\n     createAndInitTable(\n         \"id INT, v STRING\", \"{ \\\"id\\\": 1, \\\"v\\\": \\\"v1\\\" }\\n\" + \"{ \\\"id\\\": 2, \\\"v\\\": \\\"v2\\\" }\");\n@@ -1763,7 +1745,7 @@ public void testMergeWithNullActionConditions() {\n         \"Output should match\", expectedRows2, sql(\"SELECT * FROM %s ORDER BY v\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithMultipleMatchingActions() {\n     createAndInitTable(\n         \"id INT, v STRING\", \"{ \\\"id\\\": 1, \\\"v\\\": \\\"v1\\\" }\\n\" + \"{ \\\"id\\\": 2, \\\"v\\\": \\\"v2\\\" }\");\n@@ -1792,9 +1774,9 @@ public void testMergeWithMultipleMatchingActions() {\n         \"Output should match\", expectedRows, sql(\"SELECT * FROM %s ORDER BY v\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithMultipleRowGroupsParquet() throws NoSuchTableException {\n-    Assume.assumeTrue(fileFormat.equalsIgnoreCase(\"parquet\"));\n+    assumeThat(fileFormat).isEqualTo(FileFormat.PARQUET);\n \n     createAndInitTable(\"id INT, dep STRING\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD dep\", tableName);\n@@ -1818,7 +1800,7 @@ public void testMergeWithMultipleRowGroupsParquet() throws NoSuchTableException\n     df.coalesce(1).writeTo(tableName).append();\n     createBranchIfNeeded();\n \n-    Assert.assertEquals(200, spark.table(commitTarget()).count());\n+    assertThat(spark.table(commitTarget()).count()).isEqualTo(200);\n \n     // update a record from one of two row groups and copy over the second one\n     sql(\n@@ -1828,10 +1810,10 @@ public void testMergeWithMultipleRowGroupsParquet() throws NoSuchTableException\n             + \"  UPDATE SET dep = 'x'\",\n         commitTarget());\n \n-    Assert.assertEquals(200, spark.table(commitTarget()).count());\n+    assertThat(spark.table(commitTarget()).count()).isEqualTo(200);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeInsertOnly() {\n     createAndInitTable(\n         \"id STRING, v STRING\",\n@@ -1863,7 +1845,7 @@ public void testMergeInsertOnly() {\n         \"Output should match\", expectedRows, sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeInsertOnlyWithCondition() {\n     createAndInitTable(\"id INTEGER, v INTEGER\", \"{ \\\"id\\\": 1, \\\"v\\\": 1 }\");\n     createOrReplaceView(\n@@ -1889,7 +1871,7 @@ public void testMergeInsertOnlyWithCondition() {\n         \"Output should match\", expectedRows, sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeAlignsUpdateAndInsertActions() {\n     createAndInitTable(\"id INT, a INT, b STRING\", \"{ \\\"id\\\": 1, \\\"a\\\": 2, \\\"b\\\": \\\"str\\\" }\");\n     createOrReplaceView(\n@@ -1912,7 +1894,7 @@ public void testMergeAlignsUpdateAndInsertActions() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeMixedCaseAlignsUpdateAndInsertActions() {\n     createAndInitTable(\"id INT, a INT, b STRING\", \"{ \\\"id\\\": 1, \\\"a\\\": 2, \\\"b\\\": \\\"str\\\" }\");\n     createOrReplaceView(\n@@ -1944,7 +1926,7 @@ public void testMergeMixedCaseAlignsUpdateAndInsertActions() {\n         sql(\"SELECT * FROM %s WHERE b = 'new_str_2'ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeUpdatesNestedStructFields() {\n     createAndInitTable(\n         \"id INT, s STRUCT<c1:INT,c2:STRUCT<a:ARRAY<INT>,m:MAP<STRING, STRING>>>\",\n@@ -1991,7 +1973,7 @@ public void testMergeUpdatesNestedStructFields() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithInferredCasts() {\n     createAndInitTable(\"id INT, s STRING\", \"{ \\\"id\\\": 1, \\\"s\\\": \\\"value\\\" }\");\n     createOrReplaceView(\"source\", \"{ \\\"id\\\": 1, \\\"c1\\\": -2}\");\n@@ -2010,7 +1992,7 @@ public void testMergeWithInferredCasts() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeModifiesNullStruct() {\n     createAndInitTable(\"id INT, s STRUCT<n1:INT,n2:INT>\", \"{ \\\"id\\\": 1, \\\"s\\\": null }\");\n     createOrReplaceView(\"source\", \"{ \\\"id\\\": 1, \\\"n1\\\": -10 }\");\n@@ -2028,7 +2010,7 @@ public void testMergeModifiesNullStruct() {\n         sql(\"SELECT * FROM %s\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeRefreshesRelationCache() {\n     createAndInitTable(\"id INT, name STRING\", \"{ \\\"id\\\": 1, \\\"name\\\": \\\"n1\\\" }\");\n     createOrReplaceView(\"source\", \"{ \\\"id\\\": 1, \\\"name\\\": \\\"n2\\\" }\");\n@@ -2054,7 +2036,7 @@ public void testMergeRefreshesRelationCache() {\n     spark.sql(\"UNCACHE TABLE tmp\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithMultipleNotMatchedActions() {\n     createAndInitTable(\"id INT, dep STRING\", \"{ \\\"id\\\": 0, \\\"dep\\\": \\\"emp-id-0\\\" }\");\n \n@@ -2087,7 +2069,7 @@ public void testMergeWithMultipleNotMatchedActions() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithMultipleConditionalNotMatchedActions() {\n     createAndInitTable(\"id INT, dep STRING\", \"{ \\\"id\\\": 0, \\\"dep\\\": \\\"emp-id-0\\\" }\");\n \n@@ -2119,7 +2101,7 @@ public void testMergeWithMultipleConditionalNotMatchedActions() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeResolvesColumnsByName() {\n     createAndInitTable(\n         \"id INT, badge INT, dep STRING\",\n@@ -2154,7 +2136,7 @@ public void testMergeResolvesColumnsByName() {\n         sql(\"SELECT id, badge, dep FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeShouldResolveWhenThereAreNoUnresolvedExpressionsOrColumns() {\n     // ensures that MERGE INTO will resolve into the correct action even if no columns\n     // or otherwise unresolved expressions exist in the query (testing SPARK-34962)\n@@ -2189,7 +2171,7 @@ public void testMergeShouldResolveWhenThereAreNoUnresolvedExpressionsOrColumns()\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithTableWithNonNullableColumn() {\n     createAndInitTable(\n         \"id INT NOT NULL, dep STRING\",\n@@ -2223,7 +2205,7 @@ public void testMergeWithTableWithNonNullableColumn() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithNonExistingColumns() {\n     createAndInitTable(\n         \"id INT, c STRUCT<n1:INT,n2:STRUCT<dn1:INT,dn2:INT>>\",\n@@ -2266,7 +2248,7 @@ public void testMergeWithNonExistingColumns() {\n         .hasMessageContaining(\"cannot resolve invalid_col in MERGE command\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithInvalidColumnsInInsert() {\n     createAndInitTable(\n         \"id INT, c STRUCT<n1:INT,n2:STRUCT<dn1:INT,dn2:INT>>\",\n@@ -2311,7 +2293,7 @@ public void testMergeWithInvalidColumnsInInsert() {\n         .hasMessageContaining(\"must provide values for all columns of the target table\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithInvalidUpdates() {\n     createAndInitTable(\n         \"id INT, a ARRAY<STRUCT<c1:INT,c2:INT>>, m MAP<STRING,STRING>\",\n@@ -2341,7 +2323,7 @@ public void testMergeWithInvalidUpdates() {\n         .hasMessageContaining(\"Updating nested fields is only supported for structs\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithConflictingUpdates() {\n     createAndInitTable(\n         \"id INT, c STRUCT<n1:INT,n2:STRUCT<dn1:INT,dn2:INT>>\",\n@@ -2382,7 +2364,7 @@ public void testMergeWithConflictingUpdates() {\n         .hasMessageContaining(\"Updates are in conflict\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithInvalidAssignments() {\n     createAndInitTable(\n         \"id INT NOT NULL, s STRUCT<n1:INT NOT NULL,n2:STRUCT<dn1:INT,dn2:INT>> NOT NULL\",\n@@ -2454,7 +2436,7 @@ public void testMergeWithInvalidAssignments() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithNonDeterministicConditions() {\n     createAndInitTable(\n         \"id INT, c STRUCT<n1:INT,n2:STRUCT<dn1:INT,dn2:INT>>\",\n@@ -2510,7 +2492,7 @@ public void testMergeWithNonDeterministicConditions() {\n             \"Non-deterministic functions are not supported in INSERT conditions of MERGE operations\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithAggregateExpressions() {\n     createAndInitTable(\n         \"id INT, c STRUCT<n1:INT,n2:STRUCT<dn1:INT,dn2:INT>>\",\n@@ -2566,7 +2548,7 @@ public void testMergeWithAggregateExpressions() {\n             \"Agg functions are not supported in INSERT conditions of MERGE operations\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithSubqueriesInConditions() {\n     createAndInitTable(\n         \"id INT, c STRUCT<n1:INT,n2:STRUCT<dn1:INT,dn2:INT>>\",\n@@ -2622,7 +2604,7 @@ public void testMergeWithSubqueriesInConditions() {\n             \"Subqueries are not supported in conditions of MERGE operations. Found a subquery in the INSERT condition\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithTargetColumnsInInsertConditions() {\n     createAndInitTable(\"id INT, c2 INT\", \"{ \\\"id\\\": 1, \\\"c2\\\": 2 }\");\n     createOrReplaceView(\"source\", \"{ \\\"id\\\": 1, \\\"value\\\": 11 }\");\n@@ -2639,7 +2621,7 @@ public void testMergeWithTargetColumnsInInsertConditions() {\n         .hasMessageContaining(\"Cannot resolve [c2] in INSERT condition of MERGE operation\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithNonIcebergTargetTableNotSupported() {\n     createOrReplaceView(\"target\", \"{ \\\"c1\\\": -100, \\\"c2\\\": -200 }\");\n     createOrReplaceView(\"source\", \"{ \\\"c1\\\": -100, \\\"c2\\\": -200 }\");\n@@ -2659,7 +2641,7 @@ public void testMergeWithNonIcebergTargetTableNotSupported() {\n    * Tests a merge where both the source and target are evaluated to be partitioned by\n    * SingePartition at planning time but DynamicFileFilterExec will return an empty target.\n    */\n-  @Test\n+  @TestTemplate\n   public void testMergeSinglePartitionPartitioning() {\n     // This table will only have a single file and a single partition\n     createAndInitTable(\"id INT\", \"{\\\"id\\\": -1}\");\n@@ -2680,9 +2662,9 @@ public void testMergeSinglePartitionPartitioning() {\n     assertEquals(\"Should correctly add the non-matching rows\", expectedRows, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeEmptyTable() {\n-    Assume.assumeFalse(\"Custom branch does not exist for empty table\", \"test\".equals(branch));\n+    assumeThat(branch).as(\"Custom branch does not exist for empty table\").isNotEqualTo(\"test\");\n     // This table will only have a single file and a single partition\n     createAndInitTable(\"id INT\", null);\n \n@@ -2701,9 +2683,9 @@ public void testMergeEmptyTable() {\n     assertEquals(\"Should correctly add the non-matching rows\", expectedRows, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeNonExistingBranch() {\n-    Assume.assumeTrue(\"Test only applicable to custom branch\", \"test\".equals(branch));\n+    assumeThat(branch).as(\"Test only applicable to custom branch\").isEqualTo(\"test\");\n     createAndInitTable(\"id INT\", null);\n \n     // Coalesce forces our source into a SinglePartition distribution\n@@ -2719,9 +2701,9 @@ public void testMergeNonExistingBranch() {\n         .hasMessage(\"Cannot use branch (does not exist): test\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeToWapBranch() {\n-    Assume.assumeTrue(\"WAP branch only works for table identifier without branch\", branch == null);\n+    assumeThat(branch).as(\"WAP branch only works for table identifier without branch\").isNull();\n \n     createAndInitTable(\"id INT\", \"{\\\"id\\\": -1}\");\n     ImmutableList<Object[]> originalRows = ImmutableList.of(row(-1));\n@@ -2780,9 +2762,9 @@ public void testMergeToWapBranch() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeToWapBranchWithTableBranchIdentifier() {\n-    Assume.assumeTrue(\"Test must have branch name part in table identifier\", branch != null);\n+    assumeThat(branch).as(\"Test must have branch name part in table identifier\").isNotNull();\n \n     createAndInitTable(\"id INT\", \"{\\\"id\\\": -1}\");\n     sql(\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadDelete.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadDelete.java\nindex 496864ed622c..0d9be093c96c 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadDelete.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadDelete.java\n@@ -32,7 +32,7 @@\n import java.util.stream.Collectors;\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileFormat;\n-import org.apache.iceberg.PlanningMode;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowDelta;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n@@ -53,48 +53,25 @@\n import org.apache.spark.sql.Encoders;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.connector.catalog.Identifier;\n-import org.junit.Assert;\n-import org.junit.Test;\n-import org.junit.runners.Parameterized;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestMergeOnReadDelete extends TestDelete {\n \n-  public TestMergeOnReadDelete(\n-      String catalogName,\n-      String implementation,\n-      Map<String, String> config,\n-      String fileFormat,\n-      Boolean vectorized,\n-      String distributionMode,\n-      boolean fanoutEnabled,\n-      String branch,\n-      PlanningMode planningMode,\n-      int formatVersion) {\n-    super(\n-        catalogName,\n-        implementation,\n-        config,\n-        fileFormat,\n-        vectorized,\n-        distributionMode,\n-        fanoutEnabled,\n-        branch,\n-        planningMode,\n-        formatVersion);\n-  }\n-\n   @Override\n   protected Map<String, String> extraTableProperties() {\n     return ImmutableMap.of(\n         TableProperties.DELETE_MODE, RowLevelOperationMode.MERGE_ON_READ.modeName());\n   }\n \n-  @Parameterized.AfterParam\n-  public static void clearTestSparkCatalogCache() {\n+  @BeforeEach\n+  public void clearTestSparkCatalogCache() {\n     TestSparkCatalog.clearTables();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithExecutorCacheLocality() throws NoSuchTableException {\n     createAndInitPartitionedTable();\n \n@@ -118,19 +95,19 @@ public void testDeleteWithExecutorCacheLocality() throws NoSuchTableException {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteFileGranularity() throws NoSuchTableException {\n     assumeThat(formatVersion).isEqualTo(2);\n     checkDeleteFileGranularity(DeleteGranularity.FILE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeletePartitionGranularity() throws NoSuchTableException {\n     assumeThat(formatVersion).isEqualTo(2);\n     checkDeleteFileGranularity(DeleteGranularity.PARTITION);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPositionDeletesAreMaintainedDuringDelete() throws NoSuchTableException {\n     sql(\n         \"CREATE TABLE %s (id int, data string) USING iceberg PARTITIONED BY (id) TBLPROPERTIES\"\n@@ -170,7 +147,7 @@ public void testPositionDeletesAreMaintainedDuringDelete() throws NoSuchTableExc\n         sql(\"SELECT * FROM %s ORDER BY id ASC\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedPositionDeletesAreMaintainedDuringDelete()\n       throws NoSuchTableException {\n     sql(\n@@ -211,7 +188,7 @@ public void testUnpartitionedPositionDeletesAreMaintainedDuringDelete()\n         sql(\"SELECT * FROM %s ORDER BY id ASC\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDeleteWithDVAndHistoricalPositionDeletes() {\n     assumeThat(formatVersion).isEqualTo(2);\n     createTableWithDeleteGranularity(\n@@ -290,7 +267,7 @@ private void checkDeleteFileGranularity(DeleteGranularity deleteGranularity)\n         sql(\"SELECT * FROM %s ORDER BY id ASC, dep ASC\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCommitUnknownException() {\n     createAndInitTable(\"id INT, dep STRING, category STRING\");\n \n@@ -347,7 +324,7 @@ public void testCommitUnknownException() {\n         sql(\"SELECT * FROM %s ORDER BY id\", \"dummy_catalog.default.table\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAggregatePushDownInMergeOnReadDelete() {\n     createAndInitTable(\"id LONG, data INT\");\n     sql(\n@@ -367,8 +344,9 @@ public void testAggregatePushDownInMergeOnReadDelete() {\n       explainContainsPushDownAggregates = true;\n     }\n \n-    Assert.assertFalse(\n-        \"min/max/count not pushed down for deleted\", explainContainsPushDownAggregates);\n+    assertThat(explainContainsPushDownAggregates)\n+        .as(\"min/max/count not pushed down for deleted\")\n+        .isFalse();\n \n     List<Object[]> actual = sql(select, selectTarget());\n     List<Object[]> expected = Lists.newArrayList();\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java\nindex cd1c57962c36..361faade7e37 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java\n@@ -28,7 +28,7 @@\n import java.util.stream.IntStream;\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileFormat;\n-import org.apache.iceberg.PlanningMode;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n@@ -40,53 +40,31 @@\n import org.apache.iceberg.util.ContentFileUtil;\n import org.apache.iceberg.util.SnapshotUtil;\n import org.apache.spark.sql.Encoders;\n-import org.junit.Test;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestMergeOnReadMerge extends TestMerge {\n \n-  public TestMergeOnReadMerge(\n-      String catalogName,\n-      String implementation,\n-      Map<String, String> config,\n-      String fileFormat,\n-      boolean vectorized,\n-      String distributionMode,\n-      boolean fanoutEnabled,\n-      String branch,\n-      PlanningMode planningMode,\n-      int formatVersion) {\n-    super(\n-        catalogName,\n-        implementation,\n-        config,\n-        fileFormat,\n-        vectorized,\n-        distributionMode,\n-        fanoutEnabled,\n-        branch,\n-        planningMode,\n-        formatVersion);\n-  }\n-\n   @Override\n   protected Map<String, String> extraTableProperties() {\n     return ImmutableMap.of(\n         TableProperties.MERGE_MODE, RowLevelOperationMode.MERGE_ON_READ.modeName());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeDeleteFileGranularity() {\n     assumeThat(formatVersion).isEqualTo(2);\n     checkMergeDeleteGranularity(DeleteGranularity.FILE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeDeletePartitionGranularity() {\n     assumeThat(formatVersion).isEqualTo(2);\n     checkMergeDeleteGranularity(DeleteGranularity.PARTITION);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMergeWithDVAndHistoricalPositionDeletes() {\n     assumeThat(formatVersion).isEqualTo(2);\n     createTableWithDeleteGranularity(\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadUpdate.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadUpdate.java\nindex abfaa0e5876a..7fa9ffcd5f89 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadUpdate.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadUpdate.java\n@@ -27,7 +27,7 @@\n import java.util.stream.Collectors;\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileFormat;\n-import org.apache.iceberg.PlanningMode;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n@@ -38,53 +38,31 @@\n import org.apache.iceberg.spark.data.TestHelpers;\n import org.apache.iceberg.util.ContentFileUtil;\n import org.apache.iceberg.util.SnapshotUtil;\n-import org.junit.Test;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestMergeOnReadUpdate extends TestUpdate {\n \n-  public TestMergeOnReadUpdate(\n-      String catalogName,\n-      String implementation,\n-      Map<String, String> config,\n-      String fileFormat,\n-      boolean vectorized,\n-      String distributionMode,\n-      boolean fanoutEnabled,\n-      String branch,\n-      PlanningMode planningMode,\n-      int formatVersion) {\n-    super(\n-        catalogName,\n-        implementation,\n-        config,\n-        fileFormat,\n-        vectorized,\n-        distributionMode,\n-        fanoutEnabled,\n-        branch,\n-        planningMode,\n-        formatVersion);\n-  }\n-\n   @Override\n   protected Map<String, String> extraTableProperties() {\n     return ImmutableMap.of(\n         TableProperties.UPDATE_MODE, RowLevelOperationMode.MERGE_ON_READ.modeName());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateFileGranularity() {\n     assumeThat(formatVersion).isEqualTo(2);\n     checkUpdateFileGranularity(DeleteGranularity.FILE);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdatePartitionGranularity() {\n     assumeThat(formatVersion).isEqualTo(2);\n     checkUpdateFileGranularity(DeleteGranularity.PARTITION);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPositionDeletesAreMaintainedDuringUpdate() {\n     // Range distribution will produce partition scoped deletes which will not be cleaned up\n     assumeThat(distributionMode).isNotEqualToIgnoringCase(\"range\");\n@@ -111,7 +89,7 @@ public void testPositionDeletesAreMaintainedDuringUpdate() {\n         sql(\"SELECT * FROM %s ORDER BY dep ASC, id ASC\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedPositionDeletesAreMaintainedDuringUpdate() {\n     assumeThat(formatVersion).isEqualTo(2);\n     // Range distribution will produce partition scoped deletes which will not be cleaned up\n@@ -159,7 +137,7 @@ public void testUnpartitionedPositionDeletesAreMaintainedDuringUpdate() {\n         sql(\"SELECT * FROM %s ORDER BY dep ASC, id ASC\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithDVAndHistoricalPositionDeletes() {\n     assumeThat(formatVersion).isEqualTo(2);\n     createTableWithDeleteGranularity(\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestUpdate.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestUpdate.java\nindex fc9efd92cfc8..d955364378a9 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestUpdate.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestUpdate.java\n@@ -50,7 +50,8 @@\n import org.apache.iceberg.AppendFiles;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DistributionMode;\n-import org.apache.iceberg.PlanningMode;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.SnapshotSummary;\n@@ -73,44 +74,21 @@\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.execution.SparkPlan;\n import org.apache.spark.sql.internal.SQLConf;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Assume;\n-import org.junit.BeforeClass;\n-import org.junit.Test;\n+import org.awaitility.Awaitility;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public abstract class TestUpdate extends SparkRowLevelOperationsTestBase {\n \n-  public TestUpdate(\n-      String catalogName,\n-      String implementation,\n-      Map<String, String> config,\n-      String fileFormat,\n-      boolean vectorized,\n-      String distributionMode,\n-      boolean fanoutEnabled,\n-      String branch,\n-      PlanningMode planningMode,\n-      int formatVersion) {\n-    super(\n-        catalogName,\n-        implementation,\n-        config,\n-        fileFormat,\n-        vectorized,\n-        distributionMode,\n-        fanoutEnabled,\n-        branch,\n-        planningMode,\n-        formatVersion);\n-  }\n-\n-  @BeforeClass\n+  @BeforeAll\n   public static void setupSparkConf() {\n     spark.conf().set(\"spark.sql.shuffle.partitions\", \"4\");\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS updated_id\");\n@@ -118,7 +96,7 @@ public void removeTables() {\n     sql(\"DROP TABLE IF EXISTS deleted_employee\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithVectorizedReads() {\n     assumeThat(supportsVectorization()).isTrue();\n \n@@ -137,7 +115,7 @@ public void testUpdateWithVectorizedReads() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCoalesceUpdate() {\n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -197,13 +175,12 @@ public void testCoalesceUpdate() {\n       validateProperty(snapshot, SnapshotSummary.ADDED_DELETE_FILES_PROP, \"1\");\n     }\n \n-    Assert.assertEquals(\n-        \"Row count must match\",\n-        200L,\n-        scalarSql(\"SELECT COUNT(*) FROM %s WHERE id = -1\", commitTarget()));\n+    assertThat(scalarSql(\"SELECT COUNT(*) FROM %s WHERE id = -1\", commitTarget()))\n+        .as(\"Row count must match\")\n+        .isEqualTo(200L);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSkewUpdate() {\n     createAndInitTable(\"id INT, dep STRING\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD dep\", tableName);\n@@ -262,13 +239,12 @@ public void testSkewUpdate() {\n       validateProperty(snapshot, SnapshotSummary.ADDED_DELETE_FILES_PROP, \"4\");\n     }\n \n-    Assert.assertEquals(\n-        \"Row count must match\",\n-        200L,\n-        scalarSql(\"SELECT COUNT(*) FROM %s WHERE id = -1\", commitTarget()));\n+    assertThat(scalarSql(\"SELECT COUNT(*) FROM %s WHERE id = -1\", commitTarget()))\n+        .as(\"Row count must match\")\n+        .isEqualTo(200L);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExplain() {\n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -280,7 +256,7 @@ public void testExplain() {\n     sql(\"EXPLAIN UPDATE %s SET dep = 'invalid' WHERE true\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 1 snapshot\", 1, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 1 snapshot\").hasSize(1);\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -288,16 +264,16 @@ public void testExplain() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateEmptyTable() {\n-    Assume.assumeFalse(\"Custom branch does not exist for empty table\", \"test\".equals(branch));\n+    assumeThat(branch).as(\"Custom branch does not exist for empty table\").isNotEqualTo(\"test\");\n     createAndInitTable(\"id INT, dep STRING\");\n \n     sql(\"UPDATE %s SET dep = 'invalid' WHERE id IN (1)\", commitTarget());\n     sql(\"UPDATE %s SET id = -1 WHERE dep = 'hr'\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 2 snapshots\").hasSize(2);\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -305,9 +281,9 @@ public void testUpdateEmptyTable() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateNonExistingCustomBranch() {\n-    Assume.assumeTrue(\"Test only applicable to custom branch\", \"test\".equals(branch));\n+    assumeThat(branch).as(\"Test only applicable to custom branch\").isEqualTo(\"test\");\n     createAndInitTable(\"id INT, dep STRING\");\n \n     assertThatThrownBy(() -> sql(\"UPDATE %s SET dep = 'invalid' WHERE id IN (1)\", commitTarget()))\n@@ -315,7 +291,7 @@ public void testUpdateNonExistingCustomBranch() {\n         .hasMessage(\"Cannot use branch (does not exist): test\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithAlias() {\n     createAndInitTable(\"id INT, dep STRING\", \"{ \\\"id\\\": 1, \\\"dep\\\": \\\"a\\\" }\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD dep\", tableName);\n@@ -323,7 +299,7 @@ public void testUpdateWithAlias() {\n     sql(\"UPDATE %s AS t SET t.dep = 'invalid'\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 2 snapshots\").hasSize(2);\n \n     assertEquals(\n         \"Should have expected rows\",\n@@ -331,7 +307,7 @@ public void testUpdateWithAlias() {\n         sql(\"SELECT * FROM %s\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateAlignsAssignments() {\n     createAndInitTable(\"id INT, c1 INT, c2 INT\");\n \n@@ -346,7 +322,7 @@ public void testUpdateAlignsAssignments() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithUnsupportedPartitionPredicate() {\n     createAndInitTable(\"id INT, dep STRING\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD dep\", tableName);\n@@ -362,7 +338,7 @@ public void testUpdateWithUnsupportedPartitionPredicate() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithDynamicFileFiltering() {\n     createAndInitTable(\"id INT, dep STRING\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD dep\", tableName);\n@@ -376,7 +352,7 @@ public void testUpdateWithDynamicFileFiltering() {\n     sql(\"UPDATE %s SET id = cast('-1' AS INT) WHERE id = 2\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 3 snapshots\", 3, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 3 snapshots\").hasSize(3);\n \n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n     if (mode(table) == COPY_ON_WRITE) {\n@@ -391,7 +367,7 @@ public void testUpdateWithDynamicFileFiltering() {\n         sql(\"SELECT * FROM %s ORDER BY id, dep\", commitTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateNonExistingRecords() {\n     createAndInitTable(\"id INT, dep STRING\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD dep\", tableName);\n@@ -402,7 +378,7 @@ public void testUpdateNonExistingRecords() {\n     sql(\"UPDATE %s SET id = -1 WHERE id > 10\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 2 snapshots\", 2, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 2 snapshots\").hasSize(2);\n \n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n     if (mode(table) == COPY_ON_WRITE) {\n@@ -417,7 +393,7 @@ public void testUpdateNonExistingRecords() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithoutCondition() {\n     createAndInitTable(\"id INT, dep STRING\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD dep\", tableName);\n@@ -439,13 +415,13 @@ public void testUpdateWithoutCondition() {\n         });\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 4 snapshots\", 4, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 4 snapshots\").hasSize(4);\n \n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n \n-    Assert.assertEquals(\"Operation must match\", OVERWRITE, currentSnapshot.operation());\n+    assertThat(currentSnapshot.operation()).as(\"Operation must match\").isEqualTo(OVERWRITE);\n     if (mode(table) == COPY_ON_WRITE) {\n-      Assert.assertEquals(\"Operation must match\", OVERWRITE, currentSnapshot.operation());\n+      assertThat(currentSnapshot.operation()).as(\"Operation must match\").isEqualTo(OVERWRITE);\n       validateProperty(currentSnapshot, CHANGED_PARTITION_COUNT_PROP, \"2\");\n       validateProperty(currentSnapshot, DELETED_FILES_PROP, \"3\");\n       validateProperty(currentSnapshot, ADDED_FILES_PROP, ImmutableSet.of(\"2\", \"3\"));\n@@ -461,7 +437,7 @@ public void testUpdateWithoutCondition() {\n         sql(\"SELECT * FROM %s ORDER BY dep ASC\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithNullConditions() {\n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -494,7 +470,7 @@ public void testUpdateWithNullConditions() {\n         sql(\"SELECT * FROM %s ORDER BY id\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithInAndNotInConditions() {\n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -524,9 +500,9 @@ public void testUpdateWithInAndNotInConditions() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST, dep\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithMultipleRowGroupsParquet() throws NoSuchTableException {\n-    Assume.assumeTrue(fileFormat.equalsIgnoreCase(\"parquet\"));\n+    assumeThat(fileFormat).isEqualTo(FileFormat.PARQUET);\n \n     createAndInitTable(\"id INT, dep STRING\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD dep\", tableName);\n@@ -548,15 +524,15 @@ public void testUpdateWithMultipleRowGroupsParquet() throws NoSuchTableException\n     df.coalesce(1).writeTo(tableName).append();\n     createBranchIfNeeded();\n \n-    Assert.assertEquals(200, spark.table(commitTarget()).count());\n+    assertThat(spark.table(commitTarget()).count()).isEqualTo(200);\n \n     // update a record from one of two row groups and copy over the second one\n     sql(\"UPDATE %s SET id = -1 WHERE id IN (200, 201)\", commitTarget());\n \n-    Assert.assertEquals(200, spark.table(commitTarget()).count());\n+    assertThat(spark.table(commitTarget()).count()).isEqualTo(200);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateNestedStructFields() {\n     createAndInitTable(\n         \"id INT, s STRUCT<c1:INT,c2:STRUCT<a:ARRAY<INT>,m:MAP<STRING, STRING>>>\",\n@@ -589,7 +565,7 @@ public void testUpdateNestedStructFields() {\n         sql(\"SELECT * FROM %s\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithUserDefinedDistribution() {\n     createAndInitTable(\"id INT, c2 INT, c3 INT\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD bucket(8, c3)\", tableName);\n@@ -626,13 +602,13 @@ public void testUpdateWithUserDefinedDistribution() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public synchronized void testUpdateWithSerializableIsolation() throws InterruptedException {\n     // cannot run tests with concurrency for Hadoop tables without atomic renames\n-    Assume.assumeFalse(catalogName.equalsIgnoreCase(\"testhadoop\"));\n+    assumeThat(catalogName).isNotEqualToIgnoringCase(\"testhadoop\");\n     // if caching is off, the table is eagerly refreshed during runtime filtering\n     // this can cause a validation exception as concurrent changes would be visible\n-    Assume.assumeTrue(cachingCatalogEnabled());\n+    assumeThat(cachingCatalogEnabled()).isTrue();\n \n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -655,9 +631,11 @@ public synchronized void testUpdateWithSerializableIsolation() throws Interrupte\n         executorService.submit(\n             () -> {\n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> barrier.get() >= currentNumOperations * 2);\n \n                 sql(\"UPDATE %s SET id = -1 WHERE id = 1\", commitTarget());\n \n@@ -677,9 +655,11 @@ public synchronized void testUpdateWithSerializableIsolation() throws Interrupte\n               record.set(1, \"hr\"); // dep\n \n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (shouldAppend.get() && barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> !shouldAppend.get() || barrier.get() >= currentNumOperations * 2);\n \n                 if (!shouldAppend.get()) {\n                   return;\n@@ -693,7 +673,6 @@ public synchronized void testUpdateWithSerializableIsolation() throws Interrupte\n                   }\n \n                   appendFiles.commit();\n-                  sleep(10);\n                 }\n \n                 barrier.incrementAndGet();\n@@ -712,17 +691,17 @@ public synchronized void testUpdateWithSerializableIsolation() throws Interrupte\n     }\n \n     executorService.shutdown();\n-    Assert.assertTrue(\"Timeout\", executorService.awaitTermination(2, TimeUnit.MINUTES));\n+    assertThat(executorService.awaitTermination(2, TimeUnit.MINUTES)).as(\"Timeout\").isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public synchronized void testUpdateWithSnapshotIsolation()\n       throws InterruptedException, ExecutionException {\n     // cannot run tests with concurrency for Hadoop tables without atomic renames\n-    Assume.assumeFalse(catalogName.equalsIgnoreCase(\"testhadoop\"));\n+    assumeThat(catalogName).isNotEqualToIgnoringCase(\"testhadoop\");\n     // if caching is off, the table is eagerly refreshed during runtime filtering\n     // this can cause a validation exception as concurrent changes would be visible\n-    Assume.assumeTrue(cachingCatalogEnabled());\n+    assumeThat(cachingCatalogEnabled()).isTrue();\n \n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -745,9 +724,11 @@ public synchronized void testUpdateWithSnapshotIsolation()\n         executorService.submit(\n             () -> {\n               for (int numOperations = 0; numOperations < 20; numOperations++) {\n-                while (barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> barrier.get() >= currentNumOperations * 2);\n \n                 sql(\"UPDATE %s SET id = -1 WHERE id = 1\", tableName);\n \n@@ -767,9 +748,11 @@ public synchronized void testUpdateWithSnapshotIsolation()\n               record.set(1, \"hr\"); // dep\n \n               for (int numOperations = 0; numOperations < 20; numOperations++) {\n-                while (shouldAppend.get() && barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> !shouldAppend.get() || barrier.get() >= currentNumOperations * 2);\n \n                 if (!shouldAppend.get()) {\n                   return;\n@@ -783,7 +766,6 @@ public synchronized void testUpdateWithSnapshotIsolation()\n                   }\n \n                   appendFiles.commit();\n-                  sleep(10);\n                 }\n \n                 barrier.incrementAndGet();\n@@ -798,10 +780,10 @@ public synchronized void testUpdateWithSnapshotIsolation()\n     }\n \n     executorService.shutdown();\n-    Assert.assertTrue(\"Timeout\", executorService.awaitTermination(2, TimeUnit.MINUTES));\n+    assertThat(executorService.awaitTermination(2, TimeUnit.MINUTES)).as(\"Timeout\").isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithInferredCasts() {\n     createAndInitTable(\"id INT, s STRING\", \"{ \\\"id\\\": 1, \\\"s\\\": \\\"value\\\" }\");\n \n@@ -813,7 +795,7 @@ public void testUpdateWithInferredCasts() {\n         sql(\"SELECT * FROM %s\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateModifiesNullStruct() {\n     createAndInitTable(\"id INT, s STRUCT<n1:INT,n2:INT>\", \"{ \\\"id\\\": 1, \\\"s\\\": null }\");\n \n@@ -825,7 +807,7 @@ public void testUpdateModifiesNullStruct() {\n         sql(\"SELECT * FROM %s\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateRefreshesRelationCache() {\n     createAndInitTable(\"id INT, dep STRING\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD dep\", tableName);\n@@ -850,7 +832,7 @@ public void testUpdateRefreshesRelationCache() {\n     sql(\"UPDATE %s SET id = -1 WHERE id = 1\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 3 snapshots\", 3, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 3 snapshots\").hasSize(3);\n \n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n     if (mode(table) == COPY_ON_WRITE) {\n@@ -872,7 +854,7 @@ public void testUpdateRefreshesRelationCache() {\n     spark.sql(\"UNCACHE TABLE tmp\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithInSubquery() {\n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -922,7 +904,7 @@ public void testUpdateWithInSubquery() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST, dep\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithInSubqueryAndDynamicFileFiltering() {\n     createAndInitTable(\"id INT, dep STRING\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD dep\", tableName);\n@@ -940,7 +922,7 @@ public void testUpdateWithInSubqueryAndDynamicFileFiltering() {\n     sql(\"UPDATE %s SET id = -1 WHERE id IN (SELECT * FROM updated_id)\", commitTarget());\n \n     Table table = validationCatalog.loadTable(tableIdent);\n-    Assert.assertEquals(\"Should have 3 snapshots\", 3, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Should have 3 snapshots\").hasSize(3);\n \n     Snapshot currentSnapshot = SnapshotUtil.latestSnapshot(table, branch);\n     if (mode(table) == COPY_ON_WRITE) {\n@@ -955,7 +937,7 @@ public void testUpdateWithInSubqueryAndDynamicFileFiltering() {\n         sql(\"SELECT * FROM %s ORDER BY id, dep\", commitTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithSelfSubquery() {\n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -991,7 +973,7 @@ public void testUpdateWithSelfSubquery() {\n         sql(\"SELECT * FROM %s ORDER BY id, dep\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithMultiColumnInSubquery() {\n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -1015,7 +997,7 @@ public void testUpdateWithMultiColumnInSubquery() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithNotInSubquery() {\n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -1053,7 +1035,7 @@ public void testUpdateWithNotInSubquery() {\n         sql(\"SELECT * FROM %s ORDER BY id ASC NULLS LAST, dep\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithExistSubquery() {\n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -1105,7 +1087,7 @@ public void testUpdateWithExistSubquery() {\n         sql(\"SELECT * FROM %s ORDER BY id, dep\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithNotExistsSubquery() {\n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -1148,7 +1130,7 @@ public void testUpdateWithNotExistsSubquery() {\n         sql(\"SELECT * FROM %s ORDER BY id, dep\", selectTarget()));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithScalarSubquery() {\n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -1175,7 +1157,7 @@ public void testUpdateWithScalarSubquery() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateThatRequiresGroupingBeforeWrite() {\n     createAndInitTable(\"id INT, dep STRING\");\n     sql(\"ALTER TABLE %s ADD PARTITION FIELD dep\", tableName);\n@@ -1213,14 +1195,15 @@ public void testUpdateThatRequiresGroupingBeforeWrite() {\n       spark.conf().set(\"spark.sql.shuffle.partitions\", \"1\");\n \n       sql(\"UPDATE %s t SET id = -1 WHERE id IN (SELECT * FROM updated_id)\", commitTarget());\n-      Assert.assertEquals(\n-          \"Should have expected num of rows\", 12L, spark.table(commitTarget()).count());\n+      assertThat(spark.table(commitTarget()).count())\n+          .as(\"Should have expected num of rows\")\n+          .isEqualTo(12L);\n     } finally {\n       spark.conf().set(\"spark.sql.shuffle.partitions\", originalNumOfShufflePartitions);\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithVectorization() {\n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -1243,7 +1226,7 @@ public void testUpdateWithVectorization() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateModifyPartitionSourceField() throws NoSuchTableException {\n     createAndInitTable(\"id INT, dep STRING, country STRING\");\n \n@@ -1283,10 +1266,10 @@ public void testUpdateModifyPartitionSourceField() throws NoSuchTableException {\n     sql(\n         \"UPDATE %s SET id = -1 WHERE id IN (10, 11, 12, 13, 14, 15, 16, 17, 18, 19)\",\n         commitTarget());\n-    Assert.assertEquals(30L, scalarSql(\"SELECT count(*) FROM %s WHERE id = -1\", selectTarget()));\n+    assertThat(scalarSql(\"SELECT count(*) FROM %s WHERE id = -1\", selectTarget())).isEqualTo(30L);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithStaticPredicatePushdown() {\n     createAndInitTable(\"id INT, dep STRING\");\n \n@@ -1303,7 +1286,7 @@ public void testUpdateWithStaticPredicatePushdown() {\n \n     Snapshot snapshot = SnapshotUtil.latestSnapshot(table, branch);\n     String dataFilesCount = snapshot.summary().get(SnapshotSummary.TOTAL_DATA_FILES_PROP);\n-    Assert.assertEquals(\"Must have 2 files before UPDATE\", \"2\", dataFilesCount);\n+    assertThat(dataFilesCount).as(\"Must have 2 files before UPDATE\").isEqualTo(\"2\");\n \n     // remove the data file from the 'hr' partition to ensure it is not scanned\n     DataFile dataFile = Iterables.getOnlyElement(snapshot.addedDataFiles(table.io()));\n@@ -1317,7 +1300,7 @@ public void testUpdateWithStaticPredicatePushdown() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithInvalidUpdates() {\n     createAndInitTable(\n         \"id INT, a ARRAY<STRUCT<c1:INT,c2:INT>>, m MAP<STRING,STRING>\",\n@@ -1332,7 +1315,7 @@ public void testUpdateWithInvalidUpdates() {\n         .hasMessageContaining(\"Updating nested fields is only supported for structs\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithConflictingAssignments() {\n     createAndInitTable(\n         \"id INT, c STRUCT<n1:INT,n2:STRUCT<dn1:INT,dn2:INT>>\", \"{ \\\"id\\\": 0, \\\"s\\\": null }\");\n@@ -1355,7 +1338,7 @@ public void testUpdateWithConflictingAssignments() {\n         .hasMessageStartingWith(\"Updates are in conflict for these columns\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithInvalidAssignments() {\n     createAndInitTable(\n         \"id INT NOT NULL, s STRUCT<n1:INT NOT NULL,n2:STRUCT<dn1:INT,dn2:INT>> NOT NULL\",\n@@ -1393,7 +1376,7 @@ public void testUpdateWithInvalidAssignments() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateWithNonDeterministicCondition() {\n     createAndInitTable(\"id INT, dep STRING\", \"{ \\\"id\\\": 1, \\\"dep\\\": \\\"hr\\\" }\");\n \n@@ -1404,7 +1387,7 @@ public void testUpdateWithNonDeterministicCondition() {\n             \"nondeterministic expressions are only allowed in Project, Filter, Aggregate or Window\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateOnNonIcebergTableNotSupported() {\n     createOrReplaceView(\"testtable\", \"{ \\\"c1\\\": -100, \\\"c2\\\": -200 }\");\n \n@@ -1413,9 +1396,9 @@ public void testUpdateOnNonIcebergTableNotSupported() {\n         .hasMessage(\"UPDATE TABLE is not supported temporarily.\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateToWAPBranch() {\n-    Assume.assumeTrue(\"WAP branch only works for table identifier without branch\", branch == null);\n+    assumeThat(branch).as(\"WAP branch only works for table identifier without branch\").isNull();\n \n     createAndInitTable(\n         \"id INT, dep STRING\", \"{ \\\"id\\\": 1, \\\"dep\\\": \\\"hr\\\" }\\n\" + \"{ \\\"id\\\": 2, \\\"dep\\\": \\\"a\\\" }\");\n@@ -1427,42 +1410,36 @@ public void testUpdateToWAPBranch() {\n         ImmutableMap.of(SparkSQLProperties.WAP_BRANCH, \"wap\"),\n         () -> {\n           sql(\"UPDATE %s SET dep='hr' WHERE dep='a'\", tableName);\n-          Assert.assertEquals(\n-              \"Should have expected num of rows when reading table\",\n-              2L,\n-              sql(\"SELECT * FROM %s WHERE dep='hr'\", tableName).size());\n-          Assert.assertEquals(\n-              \"Should have expected num of rows when reading WAP branch\",\n-              2L,\n-              sql(\"SELECT * FROM %s.branch_wap WHERE dep='hr'\", tableName).size());\n-          Assert.assertEquals(\n-              \"Should not modify main branch\",\n-              1L,\n-              sql(\"SELECT * FROM %s.branch_main WHERE dep='hr'\", tableName).size());\n+          assertThat(sql(\"SELECT * FROM %s WHERE dep='hr'\", tableName))\n+              .as(\"Should have expected num of rows when reading table\")\n+              .hasSize(2);\n+          assertThat(sql(\"SELECT * FROM %s.branch_wap WHERE dep='hr'\", tableName))\n+              .as(\"Should have expected num of rows when reading WAP branch\")\n+              .hasSize(2);\n+          assertThat(sql(\"SELECT * FROM %s.branch_main WHERE dep='hr'\", tableName))\n+              .as(\"Should not modify main branch\")\n+              .hasSize(1);\n         });\n \n     withSQLConf(\n         ImmutableMap.of(SparkSQLProperties.WAP_BRANCH, \"wap\"),\n         () -> {\n           sql(\"UPDATE %s SET dep='b' WHERE dep='hr'\", tableName);\n-          Assert.assertEquals(\n-              \"Should have expected num of rows when reading table with multiple writes\",\n-              2L,\n-              sql(\"SELECT * FROM %s WHERE dep='b'\", tableName).size());\n-          Assert.assertEquals(\n-              \"Should have expected num of rows when reading WAP branch with multiple writes\",\n-              2L,\n-              sql(\"SELECT * FROM %s.branch_wap WHERE dep='b'\", tableName).size());\n-          Assert.assertEquals(\n-              \"Should not modify main branch with multiple writes\",\n-              0L,\n-              sql(\"SELECT * FROM %s.branch_main WHERE dep='b'\", tableName).size());\n+          assertThat(sql(\"SELECT * FROM %s WHERE dep='b'\", tableName))\n+              .as(\"Should have expected num of rows when reading table with multiple writes\")\n+              .hasSize(2);\n+          assertThat(sql(\"SELECT * FROM %s.branch_wap WHERE dep='b'\", tableName))\n+              .as(\"Should have expected num of rows when reading WAP branch with multiple writes\")\n+              .hasSize(2);\n+          assertThat(sql(\"SELECT * FROM %s.branch_main WHERE dep='b'\", tableName))\n+              .as(\"Should not modify main branch with multiple writes\")\n+              .hasSize(0);\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUpdateToWapBranchWithTableBranchIdentifier() {\n-    Assume.assumeTrue(\"Test must have branch name part in table identifier\", branch != null);\n+    assumeThat(branch).as(\"Test must have branch name part in table identifier\").isNotNull();\n \n     createAndInitTable(\"id INT, dep STRING\", \"{ \\\"id\\\": 1, \\\"dep\\\": \\\"hr\\\" }\");\n     sql(\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteDelete.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteDelete.java\nindex b0f298156441..f7ded0c4d7d2 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteDelete.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteDelete.java\n@@ -59,6 +59,7 @@\n import org.apache.spark.sql.Encoders;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.internal.SQLConf;\n+import org.awaitility.Awaitility;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n \n@@ -101,9 +102,11 @@ public synchronized void testDeleteWithConcurrentTableRefresh() throws Exception\n         executorService.submit(\n             () -> {\n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> barrier.get() >= currentNumOperations * 2);\n \n                 sql(\"DELETE FROM %s WHERE id IN (SELECT * FROM deleted_id)\", commitTarget());\n \n@@ -120,9 +123,11 @@ public synchronized void testDeleteWithConcurrentTableRefresh() throws Exception\n               record.set(1, \"hr\"); // dep\n \n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (shouldAppend.get() && barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> !shouldAppend.get() || barrier.get() >= currentNumOperations * 2);\n \n                 if (!shouldAppend.get()) {\n                   return;\n@@ -136,7 +141,6 @@ public synchronized void testDeleteWithConcurrentTableRefresh() throws Exception\n                   }\n \n                   appendFiles.commit();\n-                  sleep(10);\n                 }\n \n                 barrier.incrementAndGet();\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteMerge.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteMerge.java\nindex 1fb1238de635..fef8b28c689a 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteMerge.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteMerge.java\n@@ -34,6 +34,7 @@\n import java.util.concurrent.atomic.AtomicBoolean;\n import java.util.concurrent.atomic.AtomicInteger;\n import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n@@ -47,8 +48,11 @@\n import org.apache.iceberg.util.SnapshotUtil;\n import org.apache.spark.sql.Encoders;\n import org.apache.spark.sql.internal.SQLConf;\n+import org.awaitility.Awaitility;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestCopyOnWriteMerge extends TestMerge {\n \n   @Override\n@@ -87,9 +91,11 @@ public synchronized void testMergeWithConcurrentTableRefresh() throws Exception\n         executorService.submit(\n             () -> {\n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> barrier.get() >= currentNumOperations * 2);\n \n                 sql(\n                     \"MERGE INTO %s t USING source s \"\n@@ -111,9 +117,11 @@ public synchronized void testMergeWithConcurrentTableRefresh() throws Exception\n               record.set(1, \"hr\"); // dep\n \n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (shouldAppend.get() && barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> !shouldAppend.get() || barrier.get() >= currentNumOperations * 2);\n \n                 if (!shouldAppend.get()) {\n                   return;\n@@ -122,7 +130,6 @@ public synchronized void testMergeWithConcurrentTableRefresh() throws Exception\n                 for (int numAppends = 0; numAppends < 5; numAppends++) {\n                   DataFile dataFile = writeDataFile(table, ImmutableList.of(record));\n                   table.newFastAppend().appendFile(dataFile).commit();\n-                  sleep(10);\n                 }\n \n                 barrier.incrementAndGet();\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteUpdate.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteUpdate.java\nindex 5bc7b22f9a09..21d1377b2b98 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteUpdate.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCopyOnWriteUpdate.java\n@@ -34,6 +34,7 @@\n import java.util.concurrent.atomic.AtomicInteger;\n import org.apache.iceberg.AppendFiles;\n import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n@@ -46,8 +47,11 @@\n import org.apache.iceberg.spark.SparkSQLProperties;\n import org.apache.iceberg.util.SnapshotUtil;\n import org.apache.spark.sql.internal.SQLConf;\n+import org.awaitility.Awaitility;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestCopyOnWriteUpdate extends TestUpdate {\n \n   @Override\n@@ -85,9 +89,11 @@ public synchronized void testUpdateWithConcurrentTableRefresh() throws Exception\n         executorService.submit(\n             () -> {\n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> barrier.get() >= currentNumOperations * 2);\n \n                 sql(\"UPDATE %s SET id = -1 WHERE id = 1\", commitTarget());\n \n@@ -104,9 +110,11 @@ public synchronized void testUpdateWithConcurrentTableRefresh() throws Exception\n               record.set(1, \"hr\"); // dep\n \n               for (int numOperations = 0; numOperations < Integer.MAX_VALUE; numOperations++) {\n-                while (shouldAppend.get() && barrier.get() < numOperations * 2) {\n-                  sleep(10);\n-                }\n+                int currentNumOperations = numOperations;\n+                Awaitility.await()\n+                    .pollInterval(10, TimeUnit.MILLISECONDS)\n+                    .atMost(5, TimeUnit.SECONDS)\n+                    .until(() -> !shouldAppend.get() || barrier.get() >= currentNumOperations * 2);\n \n                 if (!shouldAppend.get()) {\n                   return;\n@@ -120,7 +128,6 @@ public synchronized void testUpdateWithConcurrentTableRefresh() throws Exception\n                   }\n \n                   appendFiles.commit();\n-                  sleep(10);\n                 }\n \n                 barrier.incrementAndGet();\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java\nindex 0060b2ffcc3d..cfa77250bb9a 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMerge.java\n@@ -52,6 +52,7 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DistributionMode;\n import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.SnapshotSummary;\n@@ -79,7 +80,9 @@\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public abstract class TestMerge extends SparkRowLevelOperationsTestBase {\n \n   @BeforeAll\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java\nindex cb2cf801e0c3..737f19e86a95 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java\n@@ -28,6 +28,7 @@\n import java.util.stream.IntStream;\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n@@ -40,7 +41,9 @@\n import org.apache.iceberg.util.SnapshotUtil;\n import org.apache.spark.sql.Encoders;\n import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestMergeOnReadMerge extends TestMerge {\n \n   @Override\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12637",
    "pr_id": 12637,
    "issue_id": 11814,
    "repo": "apache/iceberg",
    "problem_statement": "Table corruption using lock-free Hive commits\n### Apache Iceberg version\n\n1.6.1\n\n### Query engine\n\nSpark\n\n### Please describe the bug üêû\n\nWe observed the following situation happen a few times now when using lock-free Hive catalog commits introduced in https://github.com/apache/iceberg/pull/6570:\r\n\r\n We run an `ALTER TABLE table SET TBLPROPERTIES ('key' = 'value')` or any other operation that results in an Iceberg commit, either Spark or any other engine. For whatever reason the connection to the Hive metastore is broken and the HMS operation fails during the first attempt:\r\n```\r\nWARN org.apache.hadoop.hive.metastore.RetryingMetaStoreClient: MetaStoreClient lost connection. Attempting to reconnect (1 of 1) after 1s. alter_table_with_environmentContext\r\norg.apache.thrift.transport.TTransportException: java.net.SocketException: Connection reset\r\n<...>\r\nat org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client.recv_alter_table_with_environment_context(ThriftHiveMetastore.java:1693)\r\n<...>\r\nat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:169)\r\n<...>\r\nat org.apache.iceberg.hive.MetastoreUtil.alterTable(MetastoreUtil.java:78)\r\nat org.apache.iceberg.hive.HiveOperationsBase.lambda$persistTable$0(HiveOperationsBase.java:112)\r\n<...>\r\nat org.apache.iceberg.hive.HiveTableOperations.doCommit(HiveTableOperations.java:239)\r\nat org.apache.iceberg.BaseMetastoreTableOperations.commit(BaseMetastoreTableOperations.java:135)\r\n<...>\r\nat org.apache.iceberg.spark.SparkCatalog.alterTable(SparkCatalog.java:345)\r\n<...>\r\n```\r\nbut the operation actually succeeds and updates the metadata location, which means that when the `RetryingMetaStoreClient` attempts resubmitting the operation, it fails with:\r\n```\r\nMetaException(message:The table has been modified. The parameter value for key 'metadata_location' is '<new>'. The expected was value was '<previous>')\r\n```\r\nThe Iceberg commit is then considered failed and the new metadata file is cleaned up in the `finally` block [here](https://github.com/apache/iceberg/blob/b428fbc59bd1579f4dc918a5cd48fce667d81ce1/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java#L320) before retrying the commit. But the problem is that the Hive table has the new metadata location set, so when Iceberg tries refreshing the table it fails, because the new metadata file no longer exists, leaving the table in a corrupted state.\r\n\r\nI suppose a fix could be checking the exception and ignoring the case when the already set location is equal to the new metadata location, but parsing the error message sounds very hacky.\n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [X] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 395,
    "test_files_count": 4,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/BaseMetastoreOperations.java",
      "core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java",
      "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/HiveMetastoreExtension.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommitLocks.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java",
      "hive-metastore/src/test/resources/hive-schema-3.1.0.derby.sql"
    ],
    "pr_changed_test_files": [
      "hive-metastore/src/test/java/org/apache/iceberg/hive/HiveMetastoreExtension.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommitLocks.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java",
      "hive-metastore/src/test/resources/hive-schema-3.1.0.derby.sql"
    ],
    "base_commit": "03ff41c189c7420992be0e4a4ddc63f005e2e0d5",
    "head_commit": "5c5f6724ac4c8434925623a74e7ec61cc12c44d3",
    "repo_url": "https://github.com/apache/iceberg/pull/12637",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12637",
    "dockerfile": "",
    "pr_merged_at": "2025-04-02T12:34:32.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/BaseMetastoreOperations.java b/core/src/main/java/org/apache/iceberg/BaseMetastoreOperations.java\nindex 09c2249046f4..0635b56a7fba 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseMetastoreOperations.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseMetastoreOperations.java\n@@ -49,20 +49,58 @@ public enum CommitStatus {\n    * were attempting to set. This is used as a last resort when we are dealing with exceptions that\n    * may indicate the commit has failed but don't have proof that this is the case. Note that all\n    * the previous locations must also be searched on the chance that a second committer was able to\n-   * successfully commit on top of our commit.\n+   * successfully commit on top of our commit. When the {@code newMetadataLocation} is not in the\n+   * history or the {@code commitStatusSupplier} fails repeatedly the method returns {@link\n+   * CommitStatus#UNKNOWN}, because possible pending retries might still commit the change.\n    *\n    * @param tableOrViewName full name of the Table/View\n    * @param newMetadataLocation the path of the new commit file\n    * @param properties properties for retry\n    * @param commitStatusSupplier check if the latest metadata presents or not using metadata\n    *     location for table.\n-   * @return Commit Status of Success, Failure or Unknown\n+   * @return Commit Status of Success or Unknown\n    */\n   protected CommitStatus checkCommitStatus(\n       String tableOrViewName,\n       String newMetadataLocation,\n       Map<String, String> properties,\n       Supplier<Boolean> commitStatusSupplier) {\n+    CommitStatus strictStatus =\n+        checkCommitStatusStrict(\n+            tableOrViewName, newMetadataLocation, properties, commitStatusSupplier);\n+    if (strictStatus == CommitStatus.FAILURE) {\n+      LOG.warn(\n+          \"Commit status check: Commit to {} of {} unknown, new metadata location is not current \"\n+              + \"or in history\",\n+          tableOrViewName,\n+          newMetadataLocation);\n+      return CommitStatus.UNKNOWN;\n+    }\n+    return strictStatus;\n+  }\n+\n+  /**\n+   * Attempt to load the content and see if any current or past metadata location matches the one we\n+   * were attempting to set. This is used as a last resort when we are dealing with exceptions that\n+   * may indicate the commit has failed and don't have proof that this is the case, but we can be\n+   * sure that no retry attempts for the commit will be successful later. Note that all the previous\n+   * locations must also be searched on the chance that a second committer was able to successfully\n+   * commit on top of our commit. When the {@code newMetadataLocation} is not in the history the\n+   * method returns {@link CommitStatus#FAILURE}, when the {@code commitStatusSupplier} fails\n+   * repeatedly the method returns {@link CommitStatus#UNKNOWN}.\n+   *\n+   * @param tableOrViewName full name of the Table/View\n+   * @param newMetadataLocation the path of the new commit file\n+   * @param properties properties for retry\n+   * @param commitStatusSupplier check if the latest metadata presents or not using metadata\n+   *     location for table.\n+   * @return Commit Status of Success, Failure or Unknown\n+   */\n+  protected CommitStatus checkCommitStatusStrict(\n+      String tableOrViewName,\n+      String newMetadataLocation,\n+      Map<String, String> properties,\n+      Supplier<Boolean> commitStatusSupplier) {\n     int maxAttempts =\n         PropertyUtil.propertyAsInt(\n             properties, COMMIT_NUM_STATUS_CHECKS, COMMIT_NUM_STATUS_CHECKS_DEFAULT);\n@@ -98,11 +136,7 @@ protected CommitStatus checkCommitStatus(\n                     newMetadataLocation);\n                 status.set(CommitStatus.SUCCESS);\n               } else {\n-                LOG.warn(\n-                    \"Commit status check: Commit to {} of {} unknown, new metadata location is not current \"\n-                        + \"or in history\",\n-                    tableOrViewName,\n-                    newMetadataLocation);\n+                status.set(CommitStatus.FAILURE);\n               }\n             });\n \n\ndiff --git a/core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java b/core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java\nindex dbab9e813966..9fa52d52ea5d 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseMetastoreTableOperations.java\n@@ -23,6 +23,7 @@\n import java.util.concurrent.atomic.AtomicReference;\n import java.util.function.Function;\n import java.util.function.Predicate;\n+import org.apache.iceberg.BaseMetastoreOperations.CommitStatus;\n import org.apache.iceberg.encryption.EncryptionManager;\n import org.apache.iceberg.exceptions.AlreadyExistsException;\n import org.apache.iceberg.exceptions.CommitFailedException;\n@@ -286,20 +287,39 @@ public long newSnapshotId() {\n    * were attempting to set. This is used as a last resort when we are dealing with exceptions that\n    * may indicate the commit has failed but are not proof that this is the case. Past locations must\n    * also be searched on the chance that a second committer was able to successfully commit on top\n-   * of our commit.\n+   * of our commit. When the {@code newMetadataLocation} is not found, the method returns {@link\n+   * CommitStatus#UNKNOWN}.\n    *\n    * @param newMetadataLocation the path of the new commit file\n    * @param config metadata to use for configuration\n-   * @return Commit Status of Success, Failure or Unknown\n+   * @return Commit Status of Success, Unknown\n    */\n   protected CommitStatus checkCommitStatus(String newMetadataLocation, TableMetadata config) {\n-    return CommitStatus.valueOf(\n-        checkCommitStatus(\n-                tableName(),\n-                newMetadataLocation,\n-                config.properties(),\n-                () -> checkCurrentMetadataLocation(newMetadataLocation))\n-            .name());\n+    return checkCommitStatus(\n+        tableName(),\n+        newMetadataLocation,\n+        config.properties(),\n+        () -> checkCurrentMetadataLocation(newMetadataLocation));\n+  }\n+\n+  /**\n+   * Attempt to load the table and see if any current or past metadata location matches the one we\n+   * were attempting to set. This is used as a last resort when we are dealing with exceptions that\n+   * may indicate the commit has failed but are not proof that this is the case. Past locations must\n+   * also be searched on the chance that a second committer was able to successfully commit on top\n+   * of our commit. When the {@code newMetadataLocation} is not found, the method returns {@link\n+   * CommitStatus#FAILURE}.\n+   *\n+   * @param newMetadataLocation the path of the new commit file\n+   * @param config metadata to use for configuration\n+   * @return Commit Status of Success, Failure or Unknown\n+   */\n+  protected CommitStatus checkCommitStatusStrict(String newMetadataLocation, TableMetadata config) {\n+    return checkCommitStatusStrict(\n+        tableName(),\n+        newMetadataLocation,\n+        config.properties(),\n+        () -> checkCurrentMetadataLocation(newMetadataLocation));\n   }\n \n   /**\n\ndiff --git a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java\nindex 619f20ab87a3..0e801b57e5eb 100644\n--- a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java\n+++ b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java\n@@ -268,16 +268,6 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n         throw e;\n \n       } catch (Throwable e) {\n-        if (e.getMessage() != null\n-            && e.getMessage()\n-                .contains(\n-                    \"The table has been modified. The parameter value for key '\"\n-                        + HiveTableOperations.METADATA_LOCATION_PROP\n-                        + \"' is\")) {\n-          throw new CommitFailedException(\n-              e, \"The table %s.%s has been modified concurrently\", database, tableName);\n-        }\n-\n         if (e.getMessage() != null\n             && e.getMessage().contains(\"Table/View 'HIVE_LOCKS' does not exist\")) {\n           throw new RuntimeException(\n@@ -287,15 +277,31 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n               e);\n         }\n \n-        LOG.error(\n-            \"Cannot tell if commit to {}.{} succeeded, attempting to reconnect and check.\",\n-            database,\n-            tableName,\n-            e);\n         commitStatus = BaseMetastoreOperations.CommitStatus.UNKNOWN;\n-        commitStatus =\n-            BaseMetastoreOperations.CommitStatus.valueOf(\n-                checkCommitStatus(newMetadataLocation, metadata).name());\n+        if (e.getMessage() != null\n+            && e.getMessage()\n+                .contains(\n+                    \"The table has been modified. The parameter value for key '\"\n+                        + HiveTableOperations.METADATA_LOCATION_PROP\n+                        + \"' is\")) {\n+          // It's possible the HMS client incorrectly retries a successful operation, due to network\n+          // issue for example, and triggers this exception. So we need double-check to make sure\n+          // this is really a concurrent modification. Hitting this exception means no pending\n+          // requests, if any, can succeed later, so it's safe to check status in strict mode\n+          commitStatus = checkCommitStatusStrict(newMetadataLocation, metadata);\n+          if (commitStatus == BaseMetastoreOperations.CommitStatus.FAILURE) {\n+            throw new CommitFailedException(\n+                e, \"The table %s.%s has been modified concurrently\", database, tableName);\n+          }\n+        } else {\n+          LOG.error(\n+              \"Cannot tell if commit to {}.{} succeeded, attempting to reconnect and check.\",\n+              database,\n+              tableName,\n+              e);\n+          commitStatus = checkCommitStatus(newMetadataLocation, metadata);\n+        }\n+\n         switch (commitStatus) {\n           case SUCCESS:\n             break;\n",
    "test_patch": "diff --git a/hive-metastore/src/test/java/org/apache/iceberg/hive/HiveMetastoreExtension.java b/hive-metastore/src/test/java/org/apache/iceberg/hive/HiveMetastoreExtension.java\nindex c750ff4de62e..fe37223423fa 100644\n--- a/hive-metastore/src/test/java/org/apache/iceberg/hive/HiveMetastoreExtension.java\n+++ b/hive-metastore/src/test/java/org/apache/iceberg/hive/HiveMetastoreExtension.java\n@@ -48,7 +48,7 @@ public void beforeAll(ExtensionContext extensionContext) throws Exception {\n       }\n     }\n \n-    metastore.start(hiveConfWithOverrides);\n+    metastore.start(hiveConfWithOverrides, 5, true);\n     metastoreClient = new HiveMetaStoreClient(hiveConfWithOverrides);\n     if (null != databaseName) {\n       String dbPath = metastore.getDatabasePath(databaseName);\n\ndiff --git a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommitLocks.java b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommitLocks.java\nindex d12a8503313b..0ffcb057095f 100644\n--- a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommitLocks.java\n+++ b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommitLocks.java\n@@ -22,6 +22,7 @@\n import static org.apache.iceberg.types.Types.NestedField.required;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.mockito.ArgumentMatchers.anyString;\n import static org.mockito.Mockito.any;\n import static org.mockito.Mockito.atLeastOnce;\n import static org.mockito.Mockito.doAnswer;\n@@ -63,6 +64,7 @@\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableMetadata;\n+import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.exceptions.CommitFailedException;\n import org.apache.iceberg.hadoop.ConfigProperties;\n@@ -205,6 +207,37 @@ public static void cleanup() {\n     }\n   }\n \n+  @Test\n+  public void testMultipleAlterTableForNoLock() throws Exception {\n+    Table table = catalog.loadTable(TABLE_IDENTIFIER);\n+    table.updateProperties().set(TableProperties.HIVE_LOCK_ENABLED, \"false\").commit();\n+    spyOps.refresh();\n+    TableMetadata metadataV3 = spyOps.current();\n+    AtomicReference<Throwable> alterTableException = new AtomicReference<>(null);\n+    doAnswer(\n+            i -> {\n+              try {\n+                // mock a situation where alter table is unexpectedly invoked more than once\n+                i.callRealMethod();\n+                return i.callRealMethod();\n+              } catch (Throwable e) {\n+                alterTableException.compareAndSet(null, e);\n+                throw e;\n+              }\n+            })\n+        .when(spyClient)\n+        .alter_table_with_environmentContext(anyString(), anyString(), any(), any());\n+    spyOps.commit(metadataV3, metadataV1);\n+    verify(spyClient, times(1))\n+        .alter_table_with_environmentContext(anyString(), anyString(), any(), any());\n+    assertThat(alterTableException)\n+        .as(\"Expecting to trigger an exception indicating table has been modified\")\n+        .hasValueMatching(\n+            t ->\n+                t.getMessage()\n+                    .contains(\"The table has been modified. The parameter value for key '\"));\n+  }\n+\n   @Test\n   public void testLockAcquisitionAtFirstTime() throws TException, InterruptedException {\n     doReturn(acquiredLockResponse).when(spyClient).lock(any());\n\ndiff --git a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java\nindex c141f0cced02..9736b32e8727 100644\n--- a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java\n+++ b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java\n@@ -153,10 +153,21 @@ public void start(HiveConf conf) {\n    * @param poolSize The number of threads in the executor pool\n    */\n   public void start(HiveConf conf, int poolSize) {\n+    start(conf, poolSize, false);\n+  }\n+\n+  /**\n+   * Starts a TestHiveMetastore with a provided connection pool size and HiveConf.\n+   *\n+   * @param conf The hive configuration to use\n+   * @param poolSize The number of threads in the executor pool\n+   * @param directSql Used to turn on directSql\n+   */\n+  public void start(HiveConf conf, int poolSize, boolean directSql) {\n     try {\n       TServerSocket socket = new TServerSocket(0);\n       int port = socket.getServerSocket().getLocalPort();\n-      initConf(conf, port);\n+      initConf(conf, port, directSql);\n \n       this.hiveConf = conf;\n       this.server = newThriftServer(socket, poolSize, hiveConf);\n@@ -261,11 +272,11 @@ private TServer newThriftServer(TServerSocket socket, int poolSize, HiveConf con\n     return new TThreadPoolServer(args);\n   }\n \n-  private void initConf(HiveConf conf, int port) {\n+  private void initConf(HiveConf conf, int port, boolean directSql) {\n     conf.set(HiveConf.ConfVars.METASTOREURIS.varname, \"thrift://localhost:\" + port);\n     conf.set(\n         HiveConf.ConfVars.METASTOREWAREHOUSE.varname, \"file:\" + HIVE_LOCAL_DIR.getAbsolutePath());\n-    conf.set(HiveConf.ConfVars.METASTORE_TRY_DIRECT_SQL.varname, \"false\");\n+    conf.set(HiveConf.ConfVars.METASTORE_TRY_DIRECT_SQL.varname, String.valueOf(directSql));\n     conf.set(HiveConf.ConfVars.METASTORE_DISALLOW_INCOMPATIBLE_COL_TYPE_CHANGES.varname, \"false\");\n     conf.set(\"iceberg.hive.client-pool-size\", \"2\");\n     // Setting this to avoid thrift exception during running Iceberg tests outside Iceberg.\n\ndiff --git a/hive-metastore/src/test/resources/hive-schema-3.1.0.derby.sql b/hive-metastore/src/test/resources/hive-schema-3.1.0.derby.sql\nindex 55097d6639f2..b7b095c81ac1 100644\n--- a/hive-metastore/src/test/resources/hive-schema-3.1.0.derby.sql\n+++ b/hive-metastore/src/test/resources/hive-schema-3.1.0.derby.sql\n@@ -52,9 +52,9 @@ CREATE TABLE \"APP\".\"DATABASE_PARAMS\" (\"DB_ID\" BIGINT NOT NULL, \"PARAM_KEY\" VARCH\n \n CREATE TABLE \"APP\".\"TBL_COL_PRIVS\" (\"TBL_COLUMN_GRANT_ID\" BIGINT NOT NULL, \"COLUMN_NAME\" VARCHAR(767), \"CREATE_TIME\" INTEGER NOT NULL, \"GRANT_OPTION\" SMALLINT NOT NULL, \"GRANTOR\" VARCHAR(128), \"GRANTOR_TYPE\" VARCHAR(128), \"PRINCIPAL_NAME\" VARCHAR(128), \"PRINCIPAL_TYPE\" VARCHAR(128), \"TBL_COL_PRIV\" VARCHAR(128), \"TBL_ID\" BIGINT, \"AUTHORIZER\" VARCHAR(128));\n \n-CREATE TABLE \"APP\".\"SERDE_PARAMS\" (\"SERDE_ID\" BIGINT NOT NULL, \"PARAM_KEY\" VARCHAR(256) NOT NULL, \"PARAM_VALUE\" CLOB);\n+CREATE TABLE \"APP\".\"SERDE_PARAMS\" (\"SERDE_ID\" BIGINT NOT NULL, \"PARAM_KEY\" VARCHAR(256) NOT NULL, \"PARAM_VALUE\" VARCHAR(32672));\n \n-CREATE TABLE \"APP\".\"COLUMNS_V2\" (\"CD_ID\" BIGINT NOT NULL, \"COMMENT\" VARCHAR(4000), \"COLUMN_NAME\" VARCHAR(767) NOT NULL, \"TYPE_NAME\" CLOB, \"INTEGER_IDX\" INTEGER NOT NULL);\n+CREATE TABLE \"APP\".\"COLUMNS_V2\" (\"CD_ID\" BIGINT NOT NULL, \"COMMENT\" VARCHAR(4000), \"COLUMN_NAME\" VARCHAR(767) NOT NULL, \"TYPE_NAME\" VARCHAR(32672), \"INTEGER_IDX\" INTEGER NOT NULL);\n \n CREATE TABLE \"APP\".\"SORT_COLS\" (\"SD_ID\" BIGINT NOT NULL, \"COLUMN_NAME\" VARCHAR(767), \"ORDER\" INTEGER NOT NULL, \"INTEGER_IDX\" INTEGER NOT NULL);\n \n@@ -130,7 +130,7 @@ CREATE TABLE \"APP\".\"TAB_COL_STATS\"(\n     \"BIT_VECTOR\" BLOB\n );\n \n-CREATE TABLE \"APP\".\"TABLE_PARAMS\" (\"TBL_ID\" BIGINT NOT NULL, \"PARAM_KEY\" VARCHAR(256) NOT NULL, \"PARAM_VALUE\" CLOB);\n+CREATE TABLE \"APP\".\"TABLE_PARAMS\" (\"TBL_ID\" BIGINT NOT NULL, \"PARAM_KEY\" VARCHAR(256) NOT NULL, \"PARAM_VALUE\" VARCHAR(32672));\n \n CREATE TABLE \"APP\".\"BUCKETING_COLS\" (\"SD_ID\" BIGINT NOT NULL, \"BUCKET_COL_NAME\" VARCHAR(256), \"INTEGER_IDX\" INTEGER NOT NULL);\n \n@@ -138,7 +138,7 @@ CREATE TABLE \"APP\".\"TYPE_FIELDS\" (\"TYPE_NAME\" BIGINT NOT NULL, \"COMMENT\" VARCHAR\n \n CREATE TABLE \"APP\".\"NUCLEUS_TABLES\" (\"CLASS_NAME\" VARCHAR(128) NOT NULL, \"TABLE_NAME\" VARCHAR(128) NOT NULL, \"TYPE\" VARCHAR(4) NOT NULL, \"OWNER\" VARCHAR(2) NOT NULL, \"VERSION\" VARCHAR(20) NOT NULL, \"INTERFACE_NAME\" VARCHAR(256) DEFAULT NULL);\n \n-CREATE TABLE \"APP\".\"SD_PARAMS\" (\"SD_ID\" BIGINT NOT NULL, \"PARAM_KEY\" VARCHAR(256) NOT NULL, \"PARAM_VALUE\" CLOB);\n+CREATE TABLE \"APP\".\"SD_PARAMS\" (\"SD_ID\" BIGINT NOT NULL, \"PARAM_KEY\" VARCHAR(256) NOT NULL, \"PARAM_VALUE\" VARCHAR(32672));\n \n CREATE TABLE \"APP\".\"SKEWED_STRING_LIST\" (\"STRING_LIST_ID\" BIGINT NOT NULL);\n \n@@ -218,7 +218,7 @@ CREATE TABLE \"APP\".\"MV_CREATION_METADATA\" (\n   \"CAT_NAME\" VARCHAR(256) NOT NULL,\n   \"DB_NAME\" VARCHAR(128) NOT NULL,\n   \"TBL_NAME\" VARCHAR(256) NOT NULL,\n-  \"TXN_LIST\" CLOB,\n+  \"TXN_LIST\" VARCHAR(32672),\n   \"MATERIALIZATION_TIME\" BIGINT NOT NULL\n );\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12624",
    "pr_id": 12624,
    "issue_id": 7040,
    "repo": "apache/iceberg",
    "problem_statement": "Add checkstyle rule to ensure AssertJ assertions always check for underlying exception message\n### Feature Request / Improvement\n\nWe should ensure that test code asserting on exceptions should always check for the underlying exception message as shown below:\r\n\r\n```\r\n// should fail because it doesn't check for the underlying message\r\nAssertions.assertThatThrownBy(() -> ...)\r\n        .isInstanceOf(ValidationException.class);\r\n\r\n// should not fail as it checks for the exception message\r\nAssertions.assertThatThrownBy(() -> ...)\r\n        .isInstanceOf(ValidationException.class)\r\n        .hasMessageContaining(...);\r\n\r\n// should fail because it doesn't check for the underlying message\r\nAssertions.assertThatExceptionOfType(NotFoundException.class)\r\n        .isThrownBy(() -> ...);\r\n\r\n// should not fail as it checks for the exception message\r\nAssertions.assertThatExceptionOfType(NotFoundException.class)\r\n        .isThrownBy(() -> ...)\r\n        .withMessageContaining(...);\r\n```\n\n### Query engine\n\nNone",
    "issue_word_count": 103,
    "test_files_count": 62,
    "non_test_files_count": 1,
    "pr_changed_files": [
      ".baseline/checkstyle/checkstyle.xml",
      "aliyun/src/test/java/org/apache/iceberg/aliyun/oss/mock/TestLocalAliyunOSS.java",
      "api/src/test/java/org/apache/iceberg/expressions/TestExpressionBinding.java",
      "api/src/test/java/org/apache/iceberg/expressions/TestPathParsing.java",
      "api/src/test/java/org/apache/iceberg/expressions/TestTimestampLiteralConversions.java",
      "api/src/test/java/org/apache/iceberg/io/TestCloseableGroup.java",
      "api/src/test/java/org/apache/iceberg/io/TestCloseableIterable.java",
      "api/src/test/java/org/apache/iceberg/util/TestExceptionUtil.java",
      "api/src/test/java/org/apache/iceberg/variants/TestSerializedMetadata.java",
      "aws/src/integration/java/org/apache/iceberg/aws/TestAssumeRoleAwsClientFactory.java",
      "aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogTable.java",
      "aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationAwsClientFactory.java",
      "core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java",
      "core/src/test/java/org/apache/iceberg/TestClientPoolImpl.java",
      "core/src/test/java/org/apache/iceberg/TestManifestReaderStats.java",
      "core/src/test/java/org/apache/iceberg/avro/TestReadProjection.java",
      "core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCommits.java",
      "core/src/test/java/org/apache/iceberg/inmemory/TestInMemoryFileIO.java",
      "core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java",
      "core/src/test/java/org/apache/iceberg/util/TestPartitionMap.java",
      "data/src/test/java/org/apache/iceberg/data/TestReadProjection.java",
      "data/src/test/java/org/apache/iceberg/io/TestGenericSortedPosDeleteWriter.java",
      "dell/src/test/java/org/apache/iceberg/dell/mock/ecs/TestExceptionCode.java",
      "flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java",
      "flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java",
      "flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java",
      "flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java",
      "gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSInputStreamTest.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveViewCommits.java",
      "kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/JsonToMapTransformTest.java",
      "kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/JsonToMapUtilsTest.java",
      "kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/KafkaMetadataTransformTest.java",
      "kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/MongoDebeziumTransformTest.java",
      "kafka-connect/kafka-connect-transforms/src/test/java/org/debezium/connector/mongodb/transforms/MongoArrayConverterTest.java",
      "nessie/src/test/java/org/apache/iceberg/nessie/TestNessieTable.java",
      "orc/src/test/java/org/apache/iceberg/orc/TestORCFileIOProxies.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java"
    ],
    "pr_changed_test_files": [
      "aliyun/src/test/java/org/apache/iceberg/aliyun/oss/mock/TestLocalAliyunOSS.java",
      "api/src/test/java/org/apache/iceberg/expressions/TestExpressionBinding.java",
      "api/src/test/java/org/apache/iceberg/expressions/TestPathParsing.java",
      "api/src/test/java/org/apache/iceberg/expressions/TestTimestampLiteralConversions.java",
      "api/src/test/java/org/apache/iceberg/io/TestCloseableGroup.java",
      "api/src/test/java/org/apache/iceberg/io/TestCloseableIterable.java",
      "api/src/test/java/org/apache/iceberg/util/TestExceptionUtil.java",
      "api/src/test/java/org/apache/iceberg/variants/TestSerializedMetadata.java",
      "aws/src/integration/java/org/apache/iceberg/aws/TestAssumeRoleAwsClientFactory.java",
      "aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogTable.java",
      "aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationAwsClientFactory.java",
      "core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java",
      "core/src/test/java/org/apache/iceberg/TestClientPoolImpl.java",
      "core/src/test/java/org/apache/iceberg/TestManifestReaderStats.java",
      "core/src/test/java/org/apache/iceberg/avro/TestReadProjection.java",
      "core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCommits.java",
      "core/src/test/java/org/apache/iceberg/inmemory/TestInMemoryFileIO.java",
      "core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java",
      "core/src/test/java/org/apache/iceberg/util/TestPartitionMap.java",
      "data/src/test/java/org/apache/iceberg/data/TestReadProjection.java",
      "data/src/test/java/org/apache/iceberg/io/TestGenericSortedPosDeleteWriter.java",
      "dell/src/test/java/org/apache/iceberg/dell/mock/ecs/TestExceptionCode.java",
      "flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java",
      "flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java",
      "flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java",
      "flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java",
      "gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSInputStreamTest.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveViewCommits.java",
      "kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/JsonToMapTransformTest.java",
      "kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/JsonToMapUtilsTest.java",
      "kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/KafkaMetadataTransformTest.java",
      "kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/MongoDebeziumTransformTest.java",
      "kafka-connect/kafka-connect-transforms/src/test/java/org/debezium/connector/mongodb/transforms/MongoArrayConverterTest.java",
      "nessie/src/test/java/org/apache/iceberg/nessie/TestNessieTable.java",
      "orc/src/test/java/org/apache/iceberg/orc/TestORCFileIOProxies.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java"
    ],
    "base_commit": "c94fae4fac71f72d56de316595b26612bb04a515",
    "head_commit": "9c1ef82595d72e9752974b013fff8b7f75f72f7a",
    "repo_url": "https://github.com/apache/iceberg/pull/12624",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12624",
    "dockerfile": "",
    "pr_merged_at": "2025-03-24T10:09:28.000Z",
    "patch": "diff --git a/.baseline/checkstyle/checkstyle.xml b/.baseline/checkstyle/checkstyle.xml\nindex 43d3d0c41c32..0cb18e4e09ea 100644\n--- a/.baseline/checkstyle/checkstyle.xml\n+++ b/.baseline/checkstyle/checkstyle.xml\n@@ -92,6 +92,20 @@\n         <property name=\"format\" value=\"^\\s*import\\s+static\\s+(?!\\Qorg.assertj.core.api.Assertions.\\E).*\\.assertThatThrownBy;\"/>\n         <property name=\"message\" value=\"assertThatThrownBy() should be statically imported from org.assertj.core.api.Assertions\"/>\n     </module>\n+    <module name=\"RegexpMultiline\">\n+        <property name=\"id\" value=\"AssertThatThrownByWithMessageCheck\"/>\n+        <property name=\"fileExtensions\" value=\"java\"/>\n+        <property name=\"matchAcrossLines\" value=\"true\"/>\n+        <property name=\"format\" value=\"assertThatThrownBy\\((?:(?!\\.hasMessage\\w*\\().)*?isInstanceOf\\((?:(?!\\.hasMessage\\w*\\().)*?;\"/>\n+        <property name=\"message\" value=\"assertThatThrownBy must include a message check like .hasMessage(...)\"/>\n+    </module>\n+    <module name=\"RegexpMultiline\">\n+        <property name=\"id\" value=\"AssertThatExceptionOfTypeWithMessageCheck\"/>\n+        <property name=\"fileExtensions\" value=\"java\"/>\n+        <property name=\"matchAcrossLines\" value=\"true\"/>\n+        <property name=\"format\" value=\"assertThatExceptionOfType\\((?:(?!\\.withMessage\\w*\\().)*?isThrownBy\\((?:(?!\\.withMessage\\w*\\().)*?;\"/>\n+        <property name=\"message\" value=\"assertThatExceptionOfType must include a message check like .withMessage(...)\"/>\n+    </module>\n     <module name=\"RegexpMultiline\">\n         <property name=\"format\" value=\"^\\s*import\\s+\\Qorg.assertj.core.api.Assertions;\\E\" />\n         <property name=\"message\" value=\"org.assertj.core.api.Assertions should only be used with static imports\" />\n",
    "test_patch": "diff --git a/aliyun/src/test/java/org/apache/iceberg/aliyun/oss/mock/TestLocalAliyunOSS.java b/aliyun/src/test/java/org/apache/iceberg/aliyun/oss/mock/TestLocalAliyunOSS.java\nindex a661c172ed12..f51c4d0b8c98 100644\n--- a/aliyun/src/test/java/org/apache/iceberg/aliyun/oss/mock/TestLocalAliyunOSS.java\n+++ b/aliyun/src/test/java/org/apache/iceberg/aliyun/oss/mock/TestLocalAliyunOSS.java\n@@ -52,6 +52,7 @@ public class TestLocalAliyunOSS {\n   private final String bucketName = OSS_TEST_EXTENSION.testBucketName();\n   private final Random random = new Random(1);\n \n+  @SuppressWarnings(\"checkstyle:AssertThatThrownByWithMessageCheck\")\n   private static void assertThrows(Runnable runnable, String expectedErrorCode) {\n     assertThatThrownBy(runnable::run)\n         .isInstanceOf(OSSException.class)\n\ndiff --git a/api/src/test/java/org/apache/iceberg/expressions/TestExpressionBinding.java b/api/src/test/java/org/apache/iceberg/expressions/TestExpressionBinding.java\nindex 40919cb4cbb0..8af5406a73c9 100644\n--- a/api/src/test/java/org/apache/iceberg/expressions/TestExpressionBinding.java\n+++ b/api/src/test/java/org/apache/iceberg/expressions/TestExpressionBinding.java\n@@ -381,7 +381,8 @@ public void testExtractExpressionBindingPaths(String path) {\n   @FieldSource(\"UNSUPPORTED_PATHS\")\n   public void testExtractBindingWithInvalidPath(String path) {\n     assertThatThrownBy(() -> Binder.bind(STRUCT, lessThan(extract(\"var\", path, \"long\"), 100)))\n-        .isInstanceOf(IllegalArgumentException.class);\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageMatching(\"(Unsupported|Invalid) path.*\");\n   }\n \n   @Test\n\ndiff --git a/api/src/test/java/org/apache/iceberg/expressions/TestPathParsing.java b/api/src/test/java/org/apache/iceberg/expressions/TestPathParsing.java\nindex 13fa2ef7184f..71ad60d02c09 100644\n--- a/api/src/test/java/org/apache/iceberg/expressions/TestPathParsing.java\n+++ b/api/src/test/java/org/apache/iceberg/expressions/TestPathParsing.java\n@@ -66,6 +66,8 @@ public void testExtractExpressionBindingPaths(String path) {\n   @ParameterizedTest\n   @FieldSource(\"INVALID_PATHS\")\n   public void testExtractBindingWithInvalidPath(String path) {\n-    assertThatThrownBy(() -> PathUtil.parse(path)).isInstanceOf(IllegalArgumentException.class);\n+    assertThatThrownBy(() -> PathUtil.parse(path))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageMatching(\"(Unsupported|Invalid) path.*\");\n   }\n }\n\ndiff --git a/api/src/test/java/org/apache/iceberg/expressions/TestTimestampLiteralConversions.java b/api/src/test/java/org/apache/iceberg/expressions/TestTimestampLiteralConversions.java\nindex 379ad4db5e97..3aae3b6da0c5 100644\n--- a/api/src/test/java/org/apache/iceberg/expressions/TestTimestampLiteralConversions.java\n+++ b/api/src/test/java/org/apache/iceberg/expressions/TestTimestampLiteralConversions.java\n@@ -165,12 +165,14 @@ public void testTimestampNanosWithZoneConversion() {\n \n     assertThatThrownBy(() -> isoTimestampNanosWithZoneOffset.to(Types.TimestampType.withoutZone()))\n         .as(\"Should not convert timestamp with offset to a timestamp without zone\")\n-        .isInstanceOf(DateTimeParseException.class);\n+        .isInstanceOf(DateTimeParseException.class)\n+        .hasMessageContaining(\"could not be parsed\");\n \n     assertThatThrownBy(\n             () -> isoTimestampNanosWithZoneOffset.to(Types.TimestampNanoType.withoutZone()))\n         .as(\"Should not convert timestamp with offset to a timestamp without zone\")\n-        .isInstanceOf(DateTimeParseException.class);\n+        .isInstanceOf(DateTimeParseException.class)\n+        .hasMessageContaining(\"could not be parsed\");\n \n     assertThat(isoTimestampNanosWithZoneOffset.to(Types.TimestampType.withZone()).value())\n         .isEqualTo(1510842668000000L);\n@@ -186,12 +188,14 @@ public void testTimestampMicrosWithZoneConversion() {\n \n     assertThatThrownBy(() -> isoTimestampMicrosWithZoneOffset.to(Types.TimestampType.withoutZone()))\n         .as(\"Should not convert timestamp with offset to a timestamp without zone\")\n-        .isInstanceOf(DateTimeParseException.class);\n+        .isInstanceOf(DateTimeParseException.class)\n+        .hasMessageContaining(\"could not be parsed\");\n \n     assertThatThrownBy(\n             () -> isoTimestampMicrosWithZoneOffset.to(Types.TimestampNanoType.withoutZone()))\n         .as(\"Should not convert timestamp with offset to a timestamp without zone\")\n-        .isInstanceOf(DateTimeParseException.class);\n+        .isInstanceOf(DateTimeParseException.class)\n+        .hasMessageContaining(\"could not be parsed\");\n \n     assertThat(isoTimestampMicrosWithZoneOffset.to(Types.TimestampType.withZone()).value())\n         .isEqualTo(1510842668000001L);\n@@ -207,12 +211,14 @@ public void testTimestampNanosWithoutZoneConversion() {\n \n     assertThatThrownBy(() -> isoTimestampNanosWithoutZoneOffset.to(Types.TimestampType.withZone()))\n         .as(\"Should not convert timestamp without offset to a timestamp with zone\")\n-        .isInstanceOf(DateTimeParseException.class);\n+        .isInstanceOf(DateTimeParseException.class)\n+        .hasMessageContaining(\"could not be parsed\");\n \n     assertThatThrownBy(\n             () -> isoTimestampNanosWithoutZoneOffset.to(Types.TimestampNanoType.withZone()))\n         .as(\"Should not convert timestamp without offset to a timestamp with zone\")\n-        .isInstanceOf(DateTimeParseException.class);\n+        .isInstanceOf(DateTimeParseException.class)\n+        .hasMessageContaining(\"could not be parsed\");\n \n     assertThat(isoTimestampNanosWithoutZoneOffset.to(Types.TimestampType.withoutZone()).value())\n         .isEqualTo(1510842668000000L);\n@@ -228,12 +234,14 @@ public void testTimestampMicrosWithoutZoneConversion() {\n \n     assertThatThrownBy(() -> isoTimestampMicrosWithoutZoneOffset.to(Types.TimestampType.withZone()))\n         .as(\"Should not convert timestamp without offset to a timestamp with zone\")\n-        .isInstanceOf(DateTimeParseException.class);\n+        .isInstanceOf(DateTimeParseException.class)\n+        .hasMessageContaining(\"could not be parsed\");\n \n     assertThatThrownBy(\n             () -> isoTimestampMicrosWithoutZoneOffset.to(Types.TimestampNanoType.withZone()))\n         .as(\"Should not convert timestamp without offset to a timestamp with zone\")\n-        .isInstanceOf(DateTimeParseException.class);\n+        .isInstanceOf(DateTimeParseException.class)\n+        .hasMessageContaining(\"could not be parsed\");\n \n     assertThat(isoTimestampMicrosWithoutZoneOffset.to(Types.TimestampType.withoutZone()).value())\n         .isEqualTo(1510842668000001L);\n\ndiff --git a/api/src/test/java/org/apache/iceberg/io/TestCloseableGroup.java b/api/src/test/java/org/apache/iceberg/io/TestCloseableGroup.java\nindex 8003c34e6be3..4ae2548b46df 100644\n--- a/api/src/test/java/org/apache/iceberg/io/TestCloseableGroup.java\n+++ b/api/src/test/java/org/apache/iceberg/io/TestCloseableGroup.java\n@@ -92,7 +92,9 @@ public void notSuppressExceptionIfSetSuppressIsFalse() throws Exception {\n     closeableGroup.addCloseable(closeable2);\n     closeableGroup.addCloseable(closeable3);\n \n-    assertThatThrownBy(closeableGroup::close).isEqualTo(ioException);\n+    assertThatThrownBy(closeableGroup::close)\n+        .hasMessage(ioException.getMessage())\n+        .isEqualTo(ioException);\n     Mockito.verify(closeable1).close();\n     Mockito.verify(closeable2).close();\n     Mockito.verifyNoInteractions(closeable3);\n@@ -112,7 +114,9 @@ public void notSuppressExceptionIfSetSuppressIsFalseForAutoCloseable() throws Ex\n     closeableGroup.addCloseable(closeable2);\n     closeableGroup.addCloseable(closeable3);\n \n-    assertThatThrownBy(closeableGroup::close).isEqualTo(ioException);\n+    assertThatThrownBy(closeableGroup::close)\n+        .hasMessage(ioException.getMessage())\n+        .isEqualTo(ioException);\n     Mockito.verify(closeable1).close();\n     Mockito.verify(closeable2).close();\n     Mockito.verifyNoInteractions(closeable3);\n@@ -129,6 +133,7 @@ public void wrapAutoCloseableFailuresWithRuntimeException() throws Exception {\n \n     assertThatThrownBy(closeableGroup::close)\n         .isInstanceOf(RuntimeException.class)\n+        .hasMessageContaining(generalException.getMessage())\n         .hasRootCause(generalException);\n   }\n \n\ndiff --git a/api/src/test/java/org/apache/iceberg/io/TestCloseableIterable.java b/api/src/test/java/org/apache/iceberg/io/TestCloseableIterable.java\nindex ac60c6ff82c4..9b6ff64afd84 100644\n--- a/api/src/test/java/org/apache/iceberg/io/TestCloseableIterable.java\n+++ b/api/src/test/java/org/apache/iceberg/io/TestCloseableIterable.java\n@@ -92,7 +92,9 @@ public void testConcatWithEmptyIterables() {\n     CloseableIterable<Integer> concat5 =\n         CloseableIterable.concat(Lists.newArrayList(empty, empty, empty));\n \n-    assertThatThrownBy(() -> Iterables.getLast(concat5)).isInstanceOf(NoSuchElementException.class);\n+    assertThatThrownBy(() -> Iterables.getLast(concat5))\n+        .isInstanceOf(NoSuchElementException.class)\n+        .hasMessage(null);\n   }\n \n   @Test\n\ndiff --git a/api/src/test/java/org/apache/iceberg/util/TestExceptionUtil.java b/api/src/test/java/org/apache/iceberg/util/TestExceptionUtil.java\nindex 20315176b11a..0a0c599032bd 100644\n--- a/api/src/test/java/org/apache/iceberg/util/TestExceptionUtil.java\n+++ b/api/src/test/java/org/apache/iceberg/util/TestExceptionUtil.java\n@@ -52,6 +52,7 @@ public void testRunSafely() {\n                     },\n                     CustomCheckedException.class))\n         .isInstanceOf(CustomCheckedException.class)\n+        .hasMessage(exc.getMessage())\n         .isEqualTo(exc)\n         .extracting(e -> Arrays.asList(e.getSuppressed()))\n         .asInstanceOf(InstanceOfAssertFactories.LIST)\n@@ -81,6 +82,7 @@ public void testRunSafelyTwoExceptions() {\n                     CustomCheckedException.class,\n                     IOException.class))\n         .isInstanceOf(CustomCheckedException.class)\n+        .hasMessage(exc.getMessage())\n         .isEqualTo(exc)\n         .extracting(e -> Arrays.asList(e.getSuppressed()))\n         .asInstanceOf(InstanceOfAssertFactories.LIST)\n@@ -111,6 +113,7 @@ public void testRunSafelyThreeExceptions() {\n                     IOException.class,\n                     ClassNotFoundException.class))\n         .isInstanceOf(CustomCheckedException.class)\n+        .hasMessage(exc.getMessage())\n         .isEqualTo(exc)\n         .extracting(e -> Arrays.asList(e.getSuppressed()))\n         .asInstanceOf(InstanceOfAssertFactories.LIST)\n@@ -136,6 +139,7 @@ public void testRunSafelyRuntimeExceptions() {\n                       throw suppressedTwo;\n                     }))\n         .isInstanceOf(RuntimeException.class)\n+        .hasMessage(exc.getMessage())\n         .isEqualTo(exc)\n         .extracting(e -> Arrays.asList(e.getSuppressed()))\n         .asInstanceOf(InstanceOfAssertFactories.LIST)\n\ndiff --git a/api/src/test/java/org/apache/iceberg/variants/TestSerializedMetadata.java b/api/src/test/java/org/apache/iceberg/variants/TestSerializedMetadata.java\nindex 034480954a77..078d2745b663 100644\n--- a/api/src/test/java/org/apache/iceberg/variants/TestSerializedMetadata.java\n+++ b/api/src/test/java/org/apache/iceberg/variants/TestSerializedMetadata.java\n@@ -39,7 +39,9 @@ public void testEmptyVariantMetadata() {\n \n     assertThat(metadata.isSorted()).isFalse();\n     assertThat(metadata.dictionarySize()).isEqualTo(0);\n-    assertThatThrownBy(() -> metadata.get(0)).isInstanceOf(ArrayIndexOutOfBoundsException.class);\n+    assertThatThrownBy(() -> metadata.get(0))\n+        .isInstanceOf(ArrayIndexOutOfBoundsException.class)\n+        .hasMessageContaining(\"out of bounds\");\n   }\n \n   @Test\n@@ -109,7 +111,9 @@ public void testReadString() {\n     assertThat(metadata.get(2)).isEqualTo(\"c\");\n     assertThat(metadata.get(3)).isEqualTo(\"d\");\n     assertThat(metadata.get(4)).isEqualTo(\"e\");\n-    assertThatThrownBy(() -> metadata.get(5)).isInstanceOf(ArrayIndexOutOfBoundsException.class);\n+    assertThatThrownBy(() -> metadata.get(5))\n+        .isInstanceOf(ArrayIndexOutOfBoundsException.class)\n+        .hasMessageContaining(\"out of bounds\");\n   }\n \n   @Test\n@@ -125,7 +129,9 @@ public void testMultibyteString() {\n     assertThat(metadata.get(2)).isEqualTo(\"xyz\");\n     assertThat(metadata.get(3)).isEqualTo(\"d\");\n     assertThat(metadata.get(4)).isEqualTo(\"e\");\n-    assertThatThrownBy(() -> metadata.get(5)).isInstanceOf(ArrayIndexOutOfBoundsException.class);\n+    assertThatThrownBy(() -> metadata.get(5))\n+        .isInstanceOf(ArrayIndexOutOfBoundsException.class)\n+        .hasMessageContaining(\"out of bounds\");\n   }\n \n   @Test\n@@ -142,7 +148,9 @@ public void testTwoByteOffsets() {\n     assertThat(metadata.get(2)).isEqualTo(\"xyz\");\n     assertThat(metadata.get(3)).isEqualTo(\"d\");\n     assertThat(metadata.get(4)).isEqualTo(\"e\");\n-    assertThatThrownBy(() -> metadata.get(5)).isInstanceOf(ArrayIndexOutOfBoundsException.class);\n+    assertThatThrownBy(() -> metadata.get(5))\n+        .isInstanceOf(ArrayIndexOutOfBoundsException.class)\n+        .hasMessageContaining(\"out of bounds\");\n   }\n \n   @Test\n@@ -227,14 +235,18 @@ public void testInvalidMetadataVersion() {\n   }\n \n   @Test\n+  @SuppressWarnings(\"checkstyle:AssertThatThrownByWithMessageCheck\")\n   public void testMissingLength() {\n+    // no check on the underlying error msg as it might be missing based on the JDK version\n     assertThatThrownBy(() -> SerializedMetadata.from(new byte[] {0x01}))\n         .isInstanceOf(IndexOutOfBoundsException.class);\n   }\n \n   @Test\n+  @SuppressWarnings(\"checkstyle:AssertThatThrownByWithMessageCheck\")\n   public void testLengthTooShort() {\n     // missing the 4th length byte\n+    // no check on the underlying error msg as it might be missing based on the JDK version\n     assertThatThrownBy(\n             () -> SerializedMetadata.from(new byte[] {(byte) 0b11010001, 0x00, 0x00, 0x00}))\n         .isInstanceOf(IndexOutOfBoundsException.class);\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/TestAssumeRoleAwsClientFactory.java b/aws/src/integration/java/org/apache/iceberg/aws/TestAssumeRoleAwsClientFactory.java\nindex fc6f2da9c68f..3b2fd71021ba 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/TestAssumeRoleAwsClientFactory.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/TestAssumeRoleAwsClientFactory.java\n@@ -135,7 +135,8 @@ public void testAssumeRoleGlueCatalog() {\n             () ->\n                 glueCatalog.createNamespace(\n                     Namespace.of(\"denied_\" + UUID.randomUUID().toString().replace(\"-\", \"\"))))\n-        .isInstanceOf(AccessDeniedException.class);\n+        .isInstanceOf(AccessDeniedException.class)\n+        .hasMessageContaining(\"not authorized to perform: glue:CreateDatabase\");\n \n     Namespace namespace = Namespace.of(\"allowed_\" + UUID.randomUUID().toString().replace(\"-\", \"\"));\n     try {\n@@ -180,6 +181,7 @@ public void testAssumeRoleS3FileIO() throws Exception {\n                     .newInputFile(\"s3://\" + AwsIntegTestUtil.testBucketName() + \"/denied/file\")\n                     .exists())\n         .isInstanceOf(S3Exception.class)\n+        .hasMessageContaining(\"Forbidden\")\n         .asInstanceOf(InstanceOfAssertFactories.type(S3Exception.class))\n         .extracting(SdkServiceException::statusCode)\n         .isEqualTo(403);\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogTable.java b/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogTable.java\nindex 6ca2fe021612..50883703bae0 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogTable.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/glue/TestGlueCatalogTable.java\n@@ -713,7 +713,8 @@ public void testRegisterTableAlreadyExists() {\n     Table table = glueCatalog.loadTable(identifier);\n     String metadataLocation = ((BaseTable) table).operations().current().metadataFileLocation();\n     assertThatThrownBy(() -> glueCatalog.registerTable(identifier, metadataLocation))\n-        .isInstanceOf(AlreadyExistsException.class);\n+        .isInstanceOf(AlreadyExistsException.class)\n+        .hasMessageContaining(\"Table already exists\");\n     assertThat(glueCatalog.dropTable(identifier, true)).isTrue();\n     assertThat(glueCatalog.dropNamespace(Namespace.of(namespace))).isTrue();\n   }\n\ndiff --git a/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationAwsClientFactory.java b/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationAwsClientFactory.java\nindex 8a3bb71ce362..55e0b4209abc 100644\n--- a/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationAwsClientFactory.java\n+++ b/aws/src/integration/java/org/apache/iceberg/aws/lakeformation/TestLakeFormationAwsClientFactory.java\n@@ -151,7 +151,8 @@ public void testLakeFormationEnabledGlueCatalog() throws Exception {\n         Namespace.of(\"denied_\" + UUID.randomUUID().toString().replace(\"-\", \"\"));\n     try {\n       assertThatThrownBy(() -> glueCatalog.createNamespace(deniedNamespace))\n-          .isInstanceOf(AccessDeniedException.class);\n+          .isInstanceOf(AccessDeniedException.class)\n+          .hasMessageContaining(\"not authorized to perform: glue:CreateDatabase\");\n     } catch (AssertionError e) {\n       glueCatalog.dropNamespace(deniedNamespace);\n       throw e;\n\ndiff --git a/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java b/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java\nindex ab3cb563c175..70f4436ff172 100644\n--- a/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java\n@@ -591,7 +591,9 @@ public void testPositionDeletesGroup() {\n     assertThat(group.filter(5)).isEqualTo(new DeleteFile[] {});\n \n     // it should not be possible to add more elements upon indexing\n-    assertThatThrownBy(() -> group.add(file1)).isInstanceOf(IllegalStateException.class);\n+    assertThatThrownBy(() -> group.add(file1))\n+        .isInstanceOf(IllegalStateException.class)\n+        .hasMessage(\"Can't add files upon indexing\");\n   }\n \n   @TestTemplate\n@@ -625,7 +627,9 @@ public void testEqualityDeletesGroup() {\n     assertThat(group.filter(4, FILE_A)).isEqualTo(new DeleteFile[] {});\n \n     // it should not be possible to add more elements upon indexing\n-    assertThatThrownBy(() -> group.add(SPEC, file1)).isInstanceOf(IllegalStateException.class);\n+    assertThatThrownBy(() -> group.add(SPEC, file1))\n+        .isInstanceOf(IllegalStateException.class)\n+        .hasMessage(\"Can't add files upon indexing\");\n   }\n \n   @TestTemplate\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestClientPoolImpl.java b/core/src/test/java/org/apache/iceberg/TestClientPoolImpl.java\nindex ba5d5af62204..8204c8640a79 100644\n--- a/core/src/test/java/org/apache/iceberg/TestClientPoolImpl.java\n+++ b/core/src/test/java/org/apache/iceberg/TestClientPoolImpl.java\n@@ -52,7 +52,8 @@ public void testRetriesExhaustedAndSurfacesFailure() {\n         new MockClientPoolImpl(2, RetryableException.class, true, maxRetries)) {\n       assertThatThrownBy(\n               () -> mockClientPool.run(client -> client.succeedAfter(succeedAfterAttempts)))\n-          .isInstanceOf(RetryableException.class);\n+          .isInstanceOf(RetryableException.class)\n+          .hasMessage(null);\n       assertThat(mockClientPool.reconnectionAttempts()).isEqualTo(maxRetries);\n     }\n   }\n@@ -62,7 +63,8 @@ public void testNoRetryingNonRetryableException() {\n     try (MockClientPoolImpl mockClientPool =\n         new MockClientPoolImpl(2, RetryableException.class, true, 3)) {\n       assertThatThrownBy(() -> mockClientPool.run(MockClient::failWithNonRetryable, true))\n-          .isInstanceOf(NonRetryableException.class);\n+          .isInstanceOf(NonRetryableException.class)\n+          .hasMessage(null);\n       assertThat(mockClientPool.reconnectionAttempts()).isEqualTo(0);\n     }\n   }\n@@ -72,7 +74,8 @@ public void testNoRetryingWhenDisabled() {\n     try (MockClientPoolImpl mockClientPool =\n         new MockClientPoolImpl(2, RetryableException.class, false, 3)) {\n       assertThatThrownBy(() -> mockClientPool.run(client -> client.succeedAfter(3)))\n-          .isInstanceOf(RetryableException.class);\n+          .isInstanceOf(RetryableException.class)\n+          .hasMessage(null);\n       assertThat(mockClientPool.reconnectionAttempts()).isEqualTo(0);\n     }\n   }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestManifestReaderStats.java b/core/src/test/java/org/apache/iceberg/TestManifestReaderStats.java\nindex 4a5554e72d76..9e1aa255d1a4 100644\n--- a/core/src/test/java/org/apache/iceberg/TestManifestReaderStats.java\n+++ b/core/src/test/java/org/apache/iceberg/TestManifestReaderStats.java\n@@ -221,32 +221,38 @@ private void assertFullStats(DataFile dataFile) {\n \n     if (dataFile.valueCounts() != null) {\n       assertThatThrownBy(() -> dataFile.valueCounts().clear(), \"Should not be modifiable\")\n-          .isInstanceOf(UnsupportedOperationException.class);\n+          .isInstanceOf(UnsupportedOperationException.class)\n+          .hasMessage(null);\n     }\n \n     if (dataFile.nullValueCounts() != null) {\n       assertThatThrownBy(() -> dataFile.nullValueCounts().clear(), \"Should not be modifiable\")\n-          .isInstanceOf(UnsupportedOperationException.class);\n+          .isInstanceOf(UnsupportedOperationException.class)\n+          .hasMessage(null);\n     }\n \n     if (dataFile.nanValueCounts() != null) {\n       assertThatThrownBy(() -> dataFile.nanValueCounts().clear(), \"Should not be modifiable\")\n-          .isInstanceOf(UnsupportedOperationException.class);\n+          .isInstanceOf(UnsupportedOperationException.class)\n+          .hasMessage(null);\n     }\n \n     if (dataFile.upperBounds() != null) {\n       assertThatThrownBy(() -> dataFile.upperBounds().clear(), \"Should not be modifiable\")\n-          .isInstanceOf(UnsupportedOperationException.class);\n+          .isInstanceOf(UnsupportedOperationException.class)\n+          .hasMessage(null);\n     }\n \n     if (dataFile.lowerBounds() != null) {\n       assertThatThrownBy(() -> dataFile.lowerBounds().clear(), \"Should not be modifiable\")\n-          .isInstanceOf(UnsupportedOperationException.class);\n+          .isInstanceOf(UnsupportedOperationException.class)\n+          .hasMessage(null);\n     }\n \n     if (dataFile.columnSizes() != null) {\n       assertThatThrownBy(() -> dataFile.columnSizes().clear(), \"Should not be modifiable\")\n-          .isInstanceOf(UnsupportedOperationException.class);\n+          .isInstanceOf(UnsupportedOperationException.class)\n+          .hasMessage(null);\n     }\n \n     assertThat(dataFile.location())\n@@ -267,8 +273,10 @@ private void assertStatsDropped(DataFile dataFile) {\n         .isEqualTo(FILE_PATH); // always select file path in all test cases\n   }\n \n+  @SuppressWarnings(\"checkstyle:AssertThatThrownByWithMessageCheck\")\n   private void assertNullRecordCount(DataFile dataFile) {\n     // record count is a primitive type, accessing null record count will throw NPE\n+    // no check on the underlying error msg as it might be missing based on the JDK version\n     assertThatThrownBy(dataFile::recordCount).isInstanceOf(NullPointerException.class);\n   }\n }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/avro/TestReadProjection.java b/core/src/test/java/org/apache/iceberg/avro/TestReadProjection.java\nindex 30de81266efc..4d4e090f42a9 100644\n--- a/core/src/test/java/org/apache/iceberg/avro/TestReadProjection.java\n+++ b/core/src/test/java/org/apache/iceberg/avro/TestReadProjection.java\n@@ -133,6 +133,7 @@ public void testReorderedProjection() throws Exception {\n   }\n \n   @Test\n+  @SuppressWarnings(\"checkstyle:AssertThatThrownByWithMessageCheck\")\n   public void testEmptyProjection() throws Exception {\n     Schema schema =\n         new Schema(\n@@ -147,6 +148,7 @@ public void testEmptyProjection() throws Exception {\n \n     assertThat(projected).as(\"Should read a non-null record\").isNotNull();\n     // this is expected because there are no values\n+    // no check on the underlying error msg as it might be missing based on the JDK version\n     assertThatThrownBy(() -> projected.get(0)).isInstanceOf(ArrayIndexOutOfBoundsException.class);\n   }\n \n\ndiff --git a/core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCommits.java b/core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCommits.java\nindex 87ae72431726..c9611043d67c 100644\n--- a/core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCommits.java\n+++ b/core/src/test/java/org/apache/iceberg/hadoop/TestHadoopCommits.java\n@@ -381,7 +381,8 @@ private void testRenameWithFileSystem(FileSystem mockFs) throws Exception {\n     // inject the mockFS into the TableOperations\n     doReturn(mockFs).when(spyOps).getFileSystem(any(), any());\n     assertThatThrownBy(() -> spyOps.commit(tops.current(), meta1))\n-        .isInstanceOf(CommitFailedException.class);\n+        .isInstanceOf(CommitFailedException.class)\n+        .hasMessage(\"Cannot commit changes based on stale table metadata\");\n \n     // Verifies that there is no temporary metadata.json files left on rename failures.\n     Set<String> actual =\n\ndiff --git a/core/src/test/java/org/apache/iceberg/inmemory/TestInMemoryFileIO.java b/core/src/test/java/org/apache/iceberg/inmemory/TestInMemoryFileIO.java\nindex 174d054e9c6e..262d34536e6b 100644\n--- a/core/src/test/java/org/apache/iceberg/inmemory/TestInMemoryFileIO.java\n+++ b/core/src/test/java/org/apache/iceberg/inmemory/TestInMemoryFileIO.java\n@@ -57,14 +57,16 @@ public void testBasicEndToEnd() throws IOException {\n   public void testNewInputFileNotFound() {\n     InMemoryFileIO fileIO = new InMemoryFileIO();\n     assertThatExceptionOfType(NotFoundException.class)\n-        .isThrownBy(() -> fileIO.newInputFile(\"s3://nonexistent/file\"));\n+        .isThrownBy(() -> fileIO.newInputFile(\"s3://nonexistent/file\"))\n+        .withMessageContaining(\"No in-memory file found\");\n   }\n \n   @Test\n   public void testDeleteFileNotFound() {\n     InMemoryFileIO fileIO = new InMemoryFileIO();\n     assertThatExceptionOfType(NotFoundException.class)\n-        .isThrownBy(() -> fileIO.deleteFile(\"s3://nonexistent/file\"));\n+        .isThrownBy(() -> fileIO.deleteFile(\"s3://nonexistent/file\"))\n+        .withMessageContaining(\"No in-memory file found\");\n   }\n \n   @Test\n@@ -73,7 +75,8 @@ public void testCreateNoOverwrite() {\n     InMemoryFileIO fileIO = new InMemoryFileIO();\n     fileIO.addFile(location, \"hello world\".getBytes());\n     assertThatExceptionOfType(AlreadyExistsException.class)\n-        .isThrownBy(() -> fileIO.newOutputFile(location).create());\n+        .isThrownBy(() -> fileIO.newOutputFile(location).create())\n+        .withMessage(\"Already exists\");\n   }\n \n   @Test\n\ndiff --git a/core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java b/core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java\nindex a1d69ff4741a..d1f2183bbbfa 100644\n--- a/core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java\n+++ b/core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java\n@@ -2273,7 +2273,8 @@ public void testCleanupUncommitedFilesForCleanableFailures() {\n         .when(adapter)\n         .execute(reqMatcher(HTTPMethod.POST), any(), any(), any());\n     assertThatThrownBy(() -> catalog.loadTable(TABLE).newFastAppend().appendFile(file).commit())\n-        .isInstanceOf(NotAuthorizedException.class);\n+        .isInstanceOf(NotAuthorizedException.class)\n+        .hasMessage(\"not authorized\");\n \n     // Extract the UpdateTableRequest to determine the path of the manifest list that should be\n     // cleaned up\n@@ -2288,7 +2289,8 @@ public void testCleanupUncommitedFilesForCleanableFailures() {\n                   (MetadataUpdate.AddSnapshot) body.updates().get(0);\n               assertThatThrownBy(\n                       () -> table.io().newInputFile(addSnapshot.snapshot().manifestListLocation()))\n-                  .isInstanceOf(NotFoundException.class);\n+                  .isInstanceOf(NotFoundException.class)\n+                  .hasMessageContaining(\"No in-memory file found\");\n             });\n   }\n \n@@ -2308,7 +2310,8 @@ public void testNoCleanupForNonCleanableExceptions() {\n         .when(adapter)\n         .execute(reqMatcher(HTTPMethod.POST), any(), any(), any());\n     assertThatThrownBy(() -> catalog.loadTable(TABLE).newFastAppend().appendFile(FILE_A).commit())\n-        .isInstanceOf(ServiceFailureException.class);\n+        .isInstanceOf(ServiceFailureException.class)\n+        .hasMessage(\"some service failure\");\n \n     // Extract the UpdateTableRequest to determine the path of the manifest list that should still\n     // exist even though the commit failed\n@@ -2344,7 +2347,8 @@ public void testCleanupCleanableExceptionsCreate() {\n     Transaction createTableTransaction = catalog.newCreateTableTransaction(newTable, SCHEMA);\n     createTableTransaction.newAppend().appendFile(FILE_A).commit();\n     assertThatThrownBy(createTableTransaction::commitTransaction)\n-        .isInstanceOf(NotAuthorizedException.class);\n+        .isInstanceOf(NotAuthorizedException.class)\n+        .hasMessage(\"not authorized\");\n \n     assertThat(allRequests(adapter))\n         .anySatisfy(\n@@ -2367,7 +2371,8 @@ public void testCleanupCleanableExceptionsCreate() {\n                               .loadTable(TABLE)\n                               .io()\n                               .newInputFile(addSnapshot.snapshot().manifestListLocation()))\n-                  .isInstanceOf(NotFoundException.class);\n+                  .isInstanceOf(NotFoundException.class)\n+                  .hasMessageContaining(\"No in-memory file found\");\n             });\n   }\n \n@@ -2389,7 +2394,8 @@ public void testNoCleanupForNonCleanableCreateTransaction() {\n     Transaction createTableTransaction = catalog.newCreateTableTransaction(newTable, SCHEMA);\n     createTableTransaction.newAppend().appendFile(FILE_A).commit();\n     assertThatThrownBy(createTableTransaction::commitTransaction)\n-        .isInstanceOf(ServiceFailureException.class);\n+        .isInstanceOf(ServiceFailureException.class)\n+        .hasMessage(\"some service failure\");\n \n     assertThat(allRequests(adapter))\n         .anySatisfy(\n@@ -2429,7 +2435,8 @@ public void testCleanupCleanableExceptionsReplace() {\n     Transaction replaceTableTransaction = catalog.newReplaceTableTransaction(TABLE, SCHEMA, false);\n     replaceTableTransaction.newAppend().appendFile(FILE_A).commit();\n     assertThatThrownBy(replaceTableTransaction::commitTransaction)\n-        .isInstanceOf(NotAuthorizedException.class);\n+        .isInstanceOf(NotAuthorizedException.class)\n+        .hasMessage(\"not authorized\");\n \n     assertThat(allRequests(adapter))\n         .anySatisfy(\n@@ -2449,7 +2456,8 @@ public void testCleanupCleanableExceptionsReplace() {\n               String manifestListLocation = addSnapshot.snapshot().manifestListLocation();\n               assertThatThrownBy(\n                       () -> catalog.loadTable(TABLE).io().newInputFile(manifestListLocation))\n-                  .isInstanceOf(NotFoundException.class);\n+                  .isInstanceOf(NotFoundException.class)\n+                  .hasMessageContaining(\"No in-memory file found\");\n             });\n   }\n \n@@ -2470,7 +2478,8 @@ public void testNoCleanupForNonCleanableReplaceTransaction() {\n     Transaction replaceTableTransaction = catalog.newReplaceTableTransaction(TABLE, SCHEMA, false);\n     replaceTableTransaction.newAppend().appendFile(FILE_A).commit();\n     assertThatThrownBy(replaceTableTransaction::commitTransaction)\n-        .isInstanceOf(ServiceFailureException.class);\n+        .isInstanceOf(ServiceFailureException.class)\n+        .hasMessage(\"some service failure\");\n \n     assertThat(allRequests(adapter))\n         .anySatisfy(\n\ndiff --git a/core/src/test/java/org/apache/iceberg/util/TestPartitionMap.java b/core/src/test/java/org/apache/iceberg/util/TestPartitionMap.java\nindex 63b6f49e0bcb..e3d4f199a011 100644\n--- a/core/src/test/java/org/apache/iceberg/util/TestPartitionMap.java\n+++ b/core/src/test/java/org/apache/iceberg/util/TestPartitionMap.java\n@@ -231,7 +231,9 @@ public void testConcurrentReadAccess() throws InterruptedException {\n   }\n \n   @Test\n+  @SuppressWarnings(\"checkstyle:AssertThatThrownByWithMessageCheck\")\n   public void testNullKey() {\n+    // no check on the underlying error msg as it might be missing based on the JDK version\n     PartitionMap<String> map = PartitionMap.create(SPECS);\n     assertThatThrownBy(() -> map.put(null, \"value\")).isInstanceOf(NullPointerException.class);\n     assertThatThrownBy(() -> map.get(null)).isInstanceOf(NullPointerException.class);\n@@ -253,15 +255,20 @@ public void testUnmodifiableViews() {\n     map.put(BY_DATA_SPEC.specId(), Row.of(\"bbb\"), \"v2\");\n \n     assertThatThrownBy(() -> map.keySet().add(Pair.of(1, null)))\n-        .isInstanceOf(UnsupportedOperationException.class);\n+        .isInstanceOf(UnsupportedOperationException.class)\n+        .hasMessage(null);\n     assertThatThrownBy(() -> map.values().add(\"other\"))\n-        .isInstanceOf(UnsupportedOperationException.class);\n+        .isInstanceOf(UnsupportedOperationException.class)\n+        .hasMessage(null);\n     assertThatThrownBy(() -> map.entrySet().add(null))\n-        .isInstanceOf(UnsupportedOperationException.class);\n+        .isInstanceOf(UnsupportedOperationException.class)\n+        .hasMessage(null);\n     assertThatThrownBy(() -> map.entrySet().iterator().next().setValue(\"other\"))\n-        .isInstanceOf(UnsupportedOperationException.class);\n+        .isInstanceOf(UnsupportedOperationException.class)\n+        .hasMessage(\"Cannot set value\");\n     assertThatThrownBy(() -> map.entrySet().iterator().remove())\n-        .isInstanceOf(UnsupportedOperationException.class);\n+        .isInstanceOf(UnsupportedOperationException.class)\n+        .hasMessage(null);\n   }\n \n   @Test\n\ndiff --git a/data/src/test/java/org/apache/iceberg/data/TestReadProjection.java b/data/src/test/java/org/apache/iceberg/data/TestReadProjection.java\nindex 2f0e17c7907d..9eaa707e51d8 100644\n--- a/data/src/test/java/org/apache/iceberg/data/TestReadProjection.java\n+++ b/data/src/test/java/org/apache/iceberg/data/TestReadProjection.java\n@@ -181,6 +181,7 @@ public void testRenamedAddedField() throws Exception {\n   }\n \n   @Test\n+  @SuppressWarnings(\"checkstyle:AssertThatThrownByWithMessageCheck\")\n   public void testEmptyProjection() throws Exception {\n     Schema schema =\n         new Schema(\n@@ -195,6 +196,7 @@ public void testEmptyProjection() throws Exception {\n \n     assertThat(projected).as(\"Should read a non-null record\").isNotNull();\n     // this is expected because there are no values\n+    // no check on the underlying error msg as it might be missing based on the JDK version\n     assertThatThrownBy(() -> projected.get(0)).isInstanceOf(ArrayIndexOutOfBoundsException.class);\n   }\n \n\ndiff --git a/data/src/test/java/org/apache/iceberg/io/TestGenericSortedPosDeleteWriter.java b/data/src/test/java/org/apache/iceberg/io/TestGenericSortedPosDeleteWriter.java\nindex e7f9d90f0bb2..8ef5215e496c 100644\n--- a/data/src/test/java/org/apache/iceberg/io/TestGenericSortedPosDeleteWriter.java\n+++ b/data/src/test/java/org/apache/iceberg/io/TestGenericSortedPosDeleteWriter.java\n@@ -168,6 +168,7 @@ public void testSortedPosDelete() throws IOException {\n   }\n \n   @TestTemplate\n+  @SuppressWarnings(\"checkstyle:AssertThatThrownByWithMessageCheck\")\n   public void testSortedPosDeleteWithSchemaAndNullRow() throws IOException {\n     List<Record> rowSet =\n         Lists.newArrayList(createRow(0, \"aaa\"), createRow(1, \"bbb\"), createRow(2, \"ccc\"));\n\ndiff --git a/dell/src/test/java/org/apache/iceberg/dell/mock/ecs/TestExceptionCode.java b/dell/src/test/java/org/apache/iceberg/dell/mock/ecs/TestExceptionCode.java\nindex 4e7455619237..8253a4933c52 100644\n--- a/dell/src/test/java/org/apache/iceberg/dell/mock/ecs/TestExceptionCode.java\n+++ b/dell/src/test/java/org/apache/iceberg/dell/mock/ecs/TestExceptionCode.java\n@@ -68,6 +68,7 @@ public void testExceptionCode() {\n         });\n   }\n \n+  @SuppressWarnings(\"checkstyle:AssertThatThrownByWithMessageCheck\")\n   public void assertS3Exception(String message, int httpCode, String errorCode, Runnable task) {\n     assertThatThrownBy(task::run)\n         .isInstanceOf(S3Exception.class)\n\ndiff --git a/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java b/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java\nindex 87e2c5065e11..c07ebed8cef9 100644\n--- a/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java\n+++ b/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java\n@@ -427,6 +427,7 @@ public void testAlterTableModifyColumnType() {\n     // validation.\n     assertThatThrownBy(() -> sql(\"ALTER TABLE tl MODIFY (dt INTEGER)\"))\n         .isInstanceOf(TableException.class)\n+        .hasMessageContaining(\"Could not execute AlterTable\")\n         .hasRootCauseInstanceOf(IllegalArgumentException.class)\n         .hasRootCauseMessage(\"Cannot change column type: dt: string -> int\");\n   }\n@@ -557,6 +558,7 @@ public void testAlterTableConstraint() {\n     // because Iceberg's SchemaUpdate does not allow incompatible changes.\n     assertThatThrownBy(() -> sql(\"ALTER TABLE tl MODIFY (PRIMARY KEY (col1) NOT ENFORCED)\"))\n         .isInstanceOf(TableException.class)\n+        .hasMessageContaining(\"Could not execute AlterTable\")\n         .hasRootCauseInstanceOf(IllegalArgumentException.class)\n         .hasRootCauseMessage(\"Cannot add field col1 as an identifier field: not a required field\");\n \n@@ -564,12 +566,14 @@ public void testAlterTableConstraint() {\n     // because Iceberg's SchemaUpdate does not allow incompatible changes.\n     assertThatThrownBy(() -> sql(\"ALTER TABLE tl MODIFY (PRIMARY KEY (id, col1) NOT ENFORCED)\"))\n         .isInstanceOf(TableException.class)\n+        .hasMessageContaining(\"Could not execute AlterTable\")\n         .hasRootCauseInstanceOf(IllegalArgumentException.class)\n         .hasRootCauseMessage(\"Cannot add field col1 as an identifier field: not a required field\");\n \n     // Dropping constraints is not supported yet\n     assertThatThrownBy(() -> sql(\"ALTER TABLE tl DROP PRIMARY KEY\"))\n         .isInstanceOf(TableException.class)\n+        .hasMessageContaining(\"Could not execute AlterTable\")\n         .hasRootCauseInstanceOf(UnsupportedOperationException.class)\n         .hasRootCauseMessage(\"Unsupported table change: DropConstraint.\");\n   }\n\ndiff --git a/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java b/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java\nindex 800cce96edac..f0c48c4bb253 100644\n--- a/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java\n+++ b/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java\n@@ -434,7 +434,8 @@ public void testRewriteNoConflictWithEqualityDeletes() throws IOException {\n                     .useStartingSequenceNumber(false)\n                     .execute(),\n             \"Rewrite using new sequence number should fail\")\n-        .isInstanceOf(ValidationException.class);\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessageContaining(\"Cannot commit, found new delete for replaced data file\");\n \n     // Rewrite using the starting sequence number should succeed\n     RewriteDataFilesActionResult result =\n\ndiff --git a/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java b/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java\nindex de086bc9e451..c4f50658b7a4 100644\n--- a/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java\n+++ b/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java\n@@ -427,6 +427,7 @@ public void testAlterTableModifyColumnType() {\n     // validation.\n     assertThatThrownBy(() -> sql(\"ALTER TABLE tl MODIFY (dt INTEGER)\"))\n         .isInstanceOf(TableException.class)\n+        .hasMessageContaining(\"Could not execute AlterTable\")\n         .hasRootCauseInstanceOf(IllegalArgumentException.class)\n         .hasRootCauseMessage(\"Cannot change column type: dt: string -> int\");\n   }\n@@ -557,6 +558,7 @@ public void testAlterTableConstraint() {\n     // because Iceberg's SchemaUpdate does not allow incompatible changes.\n     assertThatThrownBy(() -> sql(\"ALTER TABLE tl MODIFY (PRIMARY KEY (col1) NOT ENFORCED)\"))\n         .isInstanceOf(TableException.class)\n+        .hasMessageContaining(\"Could not execute AlterTable\")\n         .hasRootCauseInstanceOf(IllegalArgumentException.class)\n         .hasRootCauseMessage(\"Cannot add field col1 as an identifier field: not a required field\");\n \n@@ -564,12 +566,14 @@ public void testAlterTableConstraint() {\n     // because Iceberg's SchemaUpdate does not allow incompatible changes.\n     assertThatThrownBy(() -> sql(\"ALTER TABLE tl MODIFY (PRIMARY KEY (id, col1) NOT ENFORCED)\"))\n         .isInstanceOf(TableException.class)\n+        .hasMessageContaining(\"Could not execute AlterTable\")\n         .hasRootCauseInstanceOf(IllegalArgumentException.class)\n         .hasRootCauseMessage(\"Cannot add field col1 as an identifier field: not a required field\");\n \n     // Dropping constraints is not supported yet\n     assertThatThrownBy(() -> sql(\"ALTER TABLE tl DROP PRIMARY KEY\"))\n         .isInstanceOf(TableException.class)\n+        .hasMessageContaining(\"Could not execute AlterTable\")\n         .hasRootCauseInstanceOf(UnsupportedOperationException.class)\n         .hasRootCauseMessage(\"Unsupported table change: DropConstraint.\");\n   }\n\ndiff --git a/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java b/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java\nindex aedc6edd6991..5dcc298dbece 100644\n--- a/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java\n+++ b/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java\n@@ -434,7 +434,8 @@ public void testRewriteNoConflictWithEqualityDeletes() throws IOException {\n                     .useStartingSequenceNumber(false)\n                     .execute(),\n             \"Rewrite using new sequence number should fail\")\n-        .isInstanceOf(ValidationException.class);\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessageContaining(\"Cannot commit, found new delete for replaced data file\");\n \n     // Rewrite using the starting sequence number should succeed\n     RewriteDataFilesActionResult result =\n\ndiff --git a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java\nindex 86c3f4fdbca0..b401d2d1cf9c 100644\n--- a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java\n+++ b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/TestFlinkCatalogTable.java\n@@ -461,6 +461,7 @@ public void testAlterTableModifyColumnType() {\n     // validation.\n     assertThatThrownBy(() -> sql(\"ALTER TABLE tl MODIFY (dt INTEGER)\"))\n         .isInstanceOf(TableException.class)\n+        .hasMessageContaining(\"Could not execute AlterTable\")\n         .hasRootCauseInstanceOf(IllegalArgumentException.class)\n         .hasRootCauseMessage(\"Cannot change column type: dt: string -> int\");\n   }\n@@ -591,6 +592,7 @@ public void testAlterTableConstraint() {\n     // because Iceberg's SchemaUpdate does not allow incompatible changes.\n     assertThatThrownBy(() -> sql(\"ALTER TABLE tl MODIFY (PRIMARY KEY (col1) NOT ENFORCED)\"))\n         .isInstanceOf(TableException.class)\n+        .hasMessageContaining(\"Could not execute AlterTable\")\n         .hasRootCauseInstanceOf(IllegalArgumentException.class)\n         .hasRootCauseMessage(\"Cannot add field col1 as an identifier field: not a required field\");\n \n@@ -598,12 +600,14 @@ public void testAlterTableConstraint() {\n     // because Iceberg's SchemaUpdate does not allow incompatible changes.\n     assertThatThrownBy(() -> sql(\"ALTER TABLE tl MODIFY (PRIMARY KEY (id, col1) NOT ENFORCED)\"))\n         .isInstanceOf(TableException.class)\n+        .hasMessageContaining(\"Could not execute AlterTable\")\n         .hasRootCauseInstanceOf(IllegalArgumentException.class)\n         .hasRootCauseMessage(\"Cannot add field col1 as an identifier field: not a required field\");\n \n     // Dropping constraints is not supported yet\n     assertThatThrownBy(() -> sql(\"ALTER TABLE tl DROP PRIMARY KEY\"))\n         .isInstanceOf(TableException.class)\n+        .hasMessageContaining(\"Could not execute AlterTable\")\n         .hasRootCauseInstanceOf(UnsupportedOperationException.class)\n         .hasRootCauseMessage(\"Unsupported table change: DropConstraint.\");\n   }\n\ndiff --git a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java\nindex a4dde8af3c12..bb33983b3277 100644\n--- a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java\n+++ b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java\n@@ -441,7 +441,8 @@ public void testRewriteNoConflictWithEqualityDeletes() throws IOException {\n                     .useStartingSequenceNumber(false)\n                     .execute(),\n             \"Rewrite using new sequence number should fail\")\n-        .isInstanceOf(ValidationException.class);\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessageContaining(\"Cannot commit, found new delete for replaced data file\");\n \n     // Rewrite using the starting sequence number should succeed\n     RewriteDataFilesActionResult result =\n\ndiff --git a/gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSInputStreamTest.java b/gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSInputStreamTest.java\nindex db6b5d93893b..f73086c6002c 100644\n--- a/gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSInputStreamTest.java\n+++ b/gcp/src/test/java/org/apache/iceberg/gcp/gcs/GCSInputStreamTest.java\n@@ -166,7 +166,9 @@ public void testClose() throws Exception {\n     SeekableInputStream closed =\n         new GCSInputStream(storage, blobId, null, gcpProperties, MetricsContext.nullMetrics());\n     closed.close();\n-    assertThatThrownBy(() -> closed.seek(0)).isInstanceOf(IllegalStateException.class);\n+    assertThatThrownBy(() -> closed.seek(0))\n+        .isInstanceOf(IllegalStateException.class)\n+        .hasMessage(\"already closed\");\n   }\n \n   @Test\n\ndiff --git a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java\nindex 754ed55e81e8..0b6f31b73b71 100644\n--- a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java\n+++ b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java\n@@ -442,7 +442,8 @@ public void testSuccessCommitWhenCheckCommitStatusOOM() throws TException, Inter\n     }\n \n     assertThatThrownBy(() -> spyOps.commit(metadataV2, metadataV1))\n-        .isInstanceOf(OutOfMemoryError.class);\n+        .isInstanceOf(OutOfMemoryError.class)\n+        .hasMessage(null);\n \n     ops.refresh();\n \n\ndiff --git a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveViewCommits.java b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveViewCommits.java\nindex ae251aacebca..ee0a88f46b95 100644\n--- a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveViewCommits.java\n+++ b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveViewCommits.java\n@@ -479,7 +479,8 @@ public void testSuccessCommitWhenCheckCommitStatusOOM() throws TException, Inter\n     }\n \n     assertThatThrownBy(() -> spyOps.commit(metadataV2, metadataV1))\n-        .isInstanceOf(OutOfMemoryError.class);\n+        .isInstanceOf(OutOfMemoryError.class)\n+        .hasMessage(null);\n \n     ops.refresh();\n \n\ndiff --git a/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/JsonToMapTransformTest.java b/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/JsonToMapTransformTest.java\nindex 87df4b1c904f..a04eee2c0be6 100644\n--- a/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/JsonToMapTransformTest.java\n+++ b/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/JsonToMapTransformTest.java\n@@ -84,7 +84,9 @@ public void shouldThrowExceptionNonJsonObjects() {\n               offset,\n               timestamp,\n               TimestampType.CREATE_TIME);\n-      assertThatThrownBy(() -> smt.apply(record)).isInstanceOf(JsonToMapException.class);\n+      assertThatThrownBy(() -> smt.apply(record))\n+          .isInstanceOf(JsonToMapException.class)\n+          .hasMessageContaining(\"not valid json\");\n     }\n   }\n \n@@ -103,7 +105,9 @@ public void shouldThrowExceptionInvalidJson() {\n               offset,\n               timestamp,\n               TimestampType.CREATE_TIME);\n-      assertThatThrownBy(() -> smt.apply(record)).isInstanceOf(JsonToMapException.class);\n+      assertThatThrownBy(() -> smt.apply(record))\n+          .isInstanceOf(JsonToMapException.class)\n+          .hasMessageContaining(\"not valid json\");\n     }\n   }\n \n\ndiff --git a/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/JsonToMapUtilsTest.java b/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/JsonToMapUtilsTest.java\nindex eb34b04962bc..4096ec3ea77b 100644\n--- a/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/JsonToMapUtilsTest.java\n+++ b/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/JsonToMapUtilsTest.java\n@@ -124,7 +124,8 @@ public void exactStringsFromComplexNodes() {\n   public void primitiveBasedOnSchemaThrows() {\n     assertThatThrownBy(\n             () -> JsonToMapUtils.extractValue(objNode.get(\"string\"), Schema.Type.STRUCT, \"\"))\n-        .isInstanceOf(RuntimeException.class);\n+        .isInstanceOf(RuntimeException.class)\n+        .hasMessageContaining(\"Unexpected type STRUCT for field\");\n   }\n \n   @Test\n@@ -327,9 +328,17 @@ public void addToStruct() {\n     assertThat(result.get(\"empty_string\")).isEqualTo(\"\");\n \n     // assert empty fields don't show up on the struct\n-    assertThatThrownBy(() -> result.get(\"null\")).isInstanceOf(RuntimeException.class);\n-    assertThatThrownBy(() -> result.get(\"empty_obj\")).isInstanceOf(RuntimeException.class);\n-    assertThatThrownBy(() -> result.get(\"empty_arr\")).isInstanceOf(RuntimeException.class);\n-    assertThatThrownBy(() -> result.get(\"empty_arr_arr\")).isInstanceOf(RuntimeException.class);\n+    assertThatThrownBy(() -> result.get(\"null\"))\n+        .isInstanceOf(RuntimeException.class)\n+        .hasMessageContaining(\"not a valid field name\");\n+    assertThatThrownBy(() -> result.get(\"empty_obj\"))\n+        .isInstanceOf(RuntimeException.class)\n+        .hasMessageContaining(\"not a valid field name\");\n+    assertThatThrownBy(() -> result.get(\"empty_arr\"))\n+        .isInstanceOf(RuntimeException.class)\n+        .hasMessageContaining(\"not a valid field name\");\n+    assertThatThrownBy(() -> result.get(\"empty_arr_arr\"))\n+        .isInstanceOf(RuntimeException.class)\n+        .hasMessageContaining(\"not a valid field name\");\n   }\n }\n\ndiff --git a/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/KafkaMetadataTransformTest.java b/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/KafkaMetadataTransformTest.java\nindex 395cfcdf609c..310f361cd277 100644\n--- a/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/KafkaMetadataTransformTest.java\n+++ b/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/KafkaMetadataTransformTest.java\n@@ -83,8 +83,12 @@ public void testThrowIfNotExpectedValue() {\n             TimestampType.CREATE_TIME);\n     try (KafkaMetadataTransform smt = new KafkaMetadataTransform()) {\n       smt.configure(ImmutableMap.of());\n-      assertThatThrownBy(() -> smt.apply(recordNotMap)).isInstanceOf(RuntimeException.class);\n-      assertThatThrownBy(() -> smt.apply(recordNotStruct)).isInstanceOf(RuntimeException.class);\n+      assertThatThrownBy(() -> smt.apply(recordNotMap))\n+          .isInstanceOf(RuntimeException.class)\n+          .hasMessageContaining(\"Only Map objects supported in absence of schema\");\n+      assertThatThrownBy(() -> smt.apply(recordNotStruct))\n+          .isInstanceOf(RuntimeException.class)\n+          .hasMessageContaining(\"Only Struct objects supported\");\n     }\n   }\n \n@@ -186,7 +190,8 @@ public void testAppendsToStuctsExternalShouldThrowIfInvalid() {\n               () -> {\n                 smt.configure(ImmutableMap.of(\"external_field\", \"external,*,,,value\"));\n               })\n-          .isInstanceOf(RuntimeException.class);\n+          .isInstanceOf(RuntimeException.class)\n+          .hasMessageContaining(\"Could not parse external,*,,,value\");\n     }\n   }\n \n\ndiff --git a/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/MongoDebeziumTransformTest.java b/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/MongoDebeziumTransformTest.java\nindex 38bfa233d681..7e6855341738 100644\n--- a/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/MongoDebeziumTransformTest.java\n+++ b/kafka-connect/kafka-connect-transforms/src/test/java/org/apache/iceberg/connect/transforms/MongoDebeziumTransformTest.java\n@@ -563,6 +563,8 @@ public void shouldThrowExceptionWhenMissingAllRequiredFields() {\n             TimestampType.CREATE_TIME);\n \n     MongoDebeziumTransform smt = getTransformer(\"array\");\n-    assertThatThrownBy(() -> smt.apply(record)).isInstanceOf(IllegalArgumentException.class);\n+    assertThatThrownBy(() -> smt.apply(record))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"malformed record topic\");\n   }\n }\n\ndiff --git a/kafka-connect/kafka-connect-transforms/src/test/java/org/debezium/connector/mongodb/transforms/MongoArrayConverterTest.java b/kafka-connect/kafka-connect-transforms/src/test/java/org/debezium/connector/mongodb/transforms/MongoArrayConverterTest.java\nindex 65a7857ac2fb..5df9d85596b0 100644\n--- a/kafka-connect/kafka-connect-transforms/src/test/java/org/debezium/connector/mongodb/transforms/MongoArrayConverterTest.java\n+++ b/kafka-connect/kafka-connect-transforms/src/test/java/org/debezium/connector/mongodb/transforms/MongoArrayConverterTest.java\n@@ -113,7 +113,8 @@ public void shouldDetectHeterogenousArray() {\n                 converter.addFieldSchema(entry, builder);\n               }\n             })\n-        .isInstanceOf(RuntimeException.class);\n+        .isInstanceOf(RuntimeException.class)\n+        .hasMessageContaining(\"not a homogenous array\");\n   }\n \n   @Test\n@@ -127,7 +128,8 @@ public void shouldDetectHeterogenousDocumentInArray() {\n                 converter.addFieldSchema(entry, builder);\n               }\n             })\n-        .isInstanceOf(RuntimeException.class);\n+        .isInstanceOf(RuntimeException.class)\n+        .hasMessageContaining(\"not the same type for all documents in the array\");\n   }\n \n   @Test\n\ndiff --git a/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieTable.java b/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieTable.java\nindex ca507eae575a..34de63fbef07 100644\n--- a/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieTable.java\n+++ b/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieTable.java\n@@ -494,7 +494,8 @@ public void testRegisterTableFailureScenarios()\n             () ->\n                 catalog.registerTable(\n                     TABLE_IDENTIFIER, \"file:\" + metadataVersionFiles.get(0) + \"invalidName\"))\n-        .isInstanceOf(NotFoundException.class);\n+        .isInstanceOf(NotFoundException.class)\n+        .hasMessageContaining(\"Failed to open input stream for file\");\n     // Case 5: null identifier\n     assertThatThrownBy(\n             () ->\n\ndiff --git a/orc/src/test/java/org/apache/iceberg/orc/TestORCFileIOProxies.java b/orc/src/test/java/org/apache/iceberg/orc/TestORCFileIOProxies.java\nindex 9338c27b9733..0b5d6eafc64c 100644\n--- a/orc/src/test/java/org/apache/iceberg/orc/TestORCFileIOProxies.java\n+++ b/orc/src/test/java/org/apache/iceberg/orc/TestORCFileIOProxies.java\n@@ -45,7 +45,8 @@ public void testInputFileSystem() throws IOException {\n \n     // Cannot use the filesystem for any other operation\n     assertThatThrownBy(() -> ifs.getFileStatus(new Path(localFile.location())))\n-        .isInstanceOf(UnsupportedOperationException.class);\n+        .isInstanceOf(UnsupportedOperationException.class)\n+        .hasMessage(null);\n \n     // Cannot use the filesystem for any other path\n     assertThatThrownBy(() -> ifs.open(new Path(\"/tmp/dummy\")))\n@@ -67,7 +68,8 @@ public void testOutputFileSystem() throws IOException {\n     }\n     // No other operation is supported\n     assertThatThrownBy(() -> ofs.open(new Path(outputFile.location())))\n-        .isInstanceOf(UnsupportedOperationException.class);\n+        .isInstanceOf(UnsupportedOperationException.class)\n+        .hasMessage(null);\n     // No other path is supported\n     assertThatThrownBy(() -> ofs.create(new Path(\"/tmp/dummy\")))\n         .isInstanceOf(IllegalArgumentException.class)\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java\nindex 55a413063eec..a26f58074f14 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java\n@@ -76,6 +76,7 @@ public static void stopSpark() {\n   @Test\n   public void testDelegateUnsupportedProcedure() {\n     assertThatThrownBy(() -> parser.parsePlan(\"CALL cat.d.t()\"))\n+        .hasMessageContaining(\"Syntax error\")\n         .isInstanceOf(ParseException.class)\n         .satisfies(\n             exception -> {\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\nindex b956db3d5512..550a763a33e0 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\n@@ -177,6 +177,7 @@ public void testInvalidCherrypickSnapshotCases() {\n \n     assertThatThrownBy(() -> sql(\"CALL %s.custom.cherrypick_snapshot('n', 't', 1L)\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\nindex cb02f94dd0f1..ca73793a3396 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\n@@ -177,6 +177,7 @@ public void testInvalidExpireSnapshotsCases() {\n \n     assertThatThrownBy(() -> sql(\"CALL %s.custom.expire_snapshots('n', 't')\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java\nindex fea147b3e61f..6db353b99f40 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java\n@@ -178,6 +178,7 @@ public void testInvalidFastForwardBranchCases() {\n             () ->\n                 sql(\"CALL %s.custom.fast_forward('test_table', 'main', 'newBranch')\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java\nindex 6bf1beec1eac..1a54d8326220 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java\n@@ -175,6 +175,7 @@ public void testInvalidApplyWapChangesCases() {\n     assertThatThrownBy(\n             () -> sql(\"CALL %s.custom.publish_changes('n', 't', 'not_valid')\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\nindex fe549f20dc86..138f084861e2 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\n@@ -264,6 +264,7 @@ public void testInvalidRemoveOrphanFilesCases() {\n \n     assertThatThrownBy(() -> sql(\"CALL %s.custom.remove_orphan_files('n', 't')\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\nindex 7c739fc8f61d..1ef122822157 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\n@@ -720,6 +720,7 @@ public void testInvalidCasesForRewriteDataFiles() {\n         .hasMessage(\"Named and positional arguments cannot be mixed\");\n \n     assertThatThrownBy(() -> sql(\"CALL %s.custom.rewrite_data_files('n', 't')\", catalogName))\n+        .hasMessageContaining(\"Syntax error\")\n         .isInstanceOf(ParseException.class)\n         .satisfies(\n             exception -> {\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java\nindex 4feb3ae8e49c..5e137431b20d 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java\n@@ -283,6 +283,7 @@ public void testInvalidRewriteManifestsCases() {\n \n     assertThatThrownBy(() -> sql(\"CALL %s.custom.rewrite_manifests('n', 't')\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\nindex ba57f6a6823f..6bae280a134a 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n@@ -260,6 +260,7 @@ public void testInvalidRollbackToSnapshotCases() {\n \n     assertThatThrownBy(() -> sql(\"CALL %s.custom.rollback_to_snapshot('n', 't', 1L)\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\nindex f38e58199843..b624eb2534f7 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\n@@ -303,6 +303,7 @@ public void testInvalidRollbackToTimestampCases() {\n     assertThatThrownBy(\n             () -> sql(\"CALL %s.custom.rollback_to_timestamp('n', 't', %s)\", catalogName, timestamp))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\nindex 0232e80da1bc..e593d1262f63 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\n@@ -208,6 +208,7 @@ public void testInvalidRollbackToSnapshotCases() {\n \n     assertThatThrownBy(() -> sql(\"CALL %s.custom.set_current_snapshot('n', 't', 1L)\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\nindex 38c8425fd923..b3f9423c4703 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\n@@ -127,6 +127,7 @@ public void testReorderedProjection() throws Exception {\n   }\n \n   @Test\n+  @SuppressWarnings(\"checkstyle:AssertThatThrownByWithMessageCheck\")\n   public void testEmptyProjection() throws Exception {\n     Schema schema =\n         new Schema(\n@@ -141,6 +142,7 @@ public void testEmptyProjection() throws Exception {\n \n     Assert.assertNotNull(\"Should read a non-null record\", projected);\n     // this is expected because there are no values\n+    // no check on the underlying error msg as it might be missing based on the JDK version\n     assertThatThrownBy(() -> projected.get(0)).isInstanceOf(ArrayIndexOutOfBoundsException.class);\n   }\n \n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java\nindex ade19de36fe9..54e100727380 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCallStatementParser.java\n@@ -72,6 +72,7 @@ public static void stopSpark() {\n   public void testDelegateUnsupportedProcedure() {\n     assertThatThrownBy(() -> parser.parsePlan(\"CALL cat.d.t()\"))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\nindex 08b0754df43d..4c89f9eb0bcc 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCherrypickSnapshotProcedure.java\n@@ -171,6 +171,7 @@ public void testInvalidCherrypickSnapshotCases() {\n \n     assertThatThrownBy(() -> sql(\"CALL %s.custom.cherrypick_snapshot('n', 't', 1L)\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\nindex 191216f64d75..55816d9c0750 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestExpireSnapshotsProcedure.java\n@@ -170,6 +170,7 @@ public void testInvalidExpireSnapshotsCases() {\n \n     assertThatThrownBy(() -> sql(\"CALL %s.custom.expire_snapshots('n', 't')\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java\nindex 7eb334f70aa2..a501bca3cb14 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestFastForwardBranchProcedure.java\n@@ -172,6 +172,7 @@ public void testInvalidFastForwardBranchCases() {\n             () ->\n                 sql(\"CALL %s.custom.fast_forward('test_table', 'main', 'newBranch')\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java\nindex 6284d88a1550..043a40ad2cb1 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestPublishChangesProcedure.java\n@@ -169,6 +169,7 @@ public void testInvalidApplyWapChangesCases() {\n     assertThatThrownBy(\n             () -> sql(\"CALL %s.custom.publish_changes('n', 't', 'not_valid')\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\nindex a4aa4d8f7385..8508884fc4bf 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRemoveOrphanFilesProcedure.java\n@@ -252,6 +252,7 @@ public void testInvalidRemoveOrphanFilesCases() {\n \n     assertThatThrownBy(() -> sql(\"CALL %s.custom.remove_orphan_files('n', 't')\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\nindex 3d3a105a14be..efd8d03df52b 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\n@@ -721,6 +721,7 @@ public void testInvalidCasesForRewriteDataFiles() {\n \n     assertThatThrownBy(() -> sql(\"CALL %s.custom.rewrite_data_files('n', 't')\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java\nindex 5eebd9aeb711..8515bf347ebb 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteManifestsProcedure.java\n@@ -294,6 +294,7 @@ public void testInvalidRewriteManifestsCases() {\n \n     assertThatThrownBy(() -> sql(\"CALL %s.custom.rewrite_manifests('n', 't')\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\nindex 43df78bf766d..d9c1172f9cd2 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToSnapshotProcedure.java\n@@ -256,6 +256,7 @@ public void testInvalidRollbackToSnapshotCases() {\n \n     assertThatThrownBy(() -> sql(\"CALL %s.custom.rollback_to_snapshot('n', 't', 1L)\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\nindex ae35b9f1817c..8e75ba2548a3 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRollbackToTimestampProcedure.java\n@@ -297,6 +297,7 @@ public void testInvalidRollbackToTimestampCases() {\n     assertThatThrownBy(\n             () -> sql(\"CALL %s.custom.rollback_to_timestamp('n', 't', %s)\", catalogName, timestamp))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\nindex 4c34edef5d25..50a8760e8a41 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestSetCurrentSnapshotProcedure.java\n@@ -205,6 +205,7 @@ public void testInvalidRollbackToSnapshotCases() {\n \n     assertThatThrownBy(() -> sql(\"CALL %s.custom.set_current_snapshot('n', 't', 1L)\", catalogName))\n         .isInstanceOf(ParseException.class)\n+        .hasMessageContaining(\"Syntax error\")\n         .satisfies(\n             exception -> {\n               ParseException parseException = (ParseException) exception;\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\nindex 5f59c8eef4ba..7dbe99d64e51 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestReadProjection.java\n@@ -136,6 +136,7 @@ public void testReorderedProjection() throws Exception {\n   }\n \n   @TestTemplate\n+  @SuppressWarnings(\"checkstyle:AssertThatThrownByWithMessageCheck\")\n   public void testEmptyProjection() throws Exception {\n     Schema schema =\n         new Schema(\n@@ -150,6 +151,7 @@ public void testEmptyProjection() throws Exception {\n \n     assertThat(projected).as(\"Should read a non-null record\").isNotNull();\n     // this is expected because there are no values\n+    // no check on the underlying error msg as it might be missing based on the JDK version\n     assertThatThrownBy(() -> projected.get(0)).isInstanceOf(ArrayIndexOutOfBoundsException.class);\n   }\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12579",
    "pr_id": 12579,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 8,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java"
    ],
    "base_commit": "c8d8b8ca57fb158ecab6b98cd744588448af64ee",
    "head_commit": "3e40b1ffb0d2f9b058dc79819cfda148a594f799",
    "repo_url": "https://github.com/apache/iceberg/pull/12579",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12579",
    "dockerfile": "",
    "pr_merged_at": "2025-03-21T09:50:50.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java\nindex d322f1d67b03..8788940b09d1 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java\n@@ -21,6 +21,7 @@\n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.File;\n import java.io.IOException;\n@@ -32,12 +33,12 @@\n import java.util.List;\n import java.util.Map;\n import java.util.Set;\n+import java.util.UUID;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.concurrent.atomic.AtomicInteger;\n import java.util.stream.Collectors;\n-import java.util.stream.StreamSupport;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n@@ -45,6 +46,9 @@\n import org.apache.iceberg.Files;\n import org.apache.iceberg.GenericBlobMetadata;\n import org.apache.iceberg.GenericStatisticsFile;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Snapshot;\n@@ -64,12 +68,10 @@\n import org.apache.iceberg.puffin.PuffinWriter;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n-import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.spark.SparkSQLProperties;\n-import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.spark.actions.DeleteOrphanFilesSparkAction.StringToFileURI;\n import org.apache.iceberg.spark.source.FilePathLastModifiedRecord;\n import org.apache.iceberg.spark.source.ThreeColumnRecord;\n@@ -80,13 +82,13 @@\n import org.apache.spark.sql.RowFactory;\n import org.apache.spark.sql.types.DataTypes;\n import org.apache.spark.sql.types.StructType;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n \n-public abstract class TestRemoveOrphanFilesAction extends SparkTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public abstract class TestRemoveOrphanFilesAction extends TestBase {\n \n   private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n   protected static final Schema SCHEMA =\n@@ -97,17 +99,23 @@ public abstract class TestRemoveOrphanFilesAction extends SparkTestBase {\n   protected static final PartitionSpec SPEC =\n       PartitionSpec.builderFor(SCHEMA).truncate(\"c2\", 2).identity(\"c3\").build();\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n-  private File tableDir = null;\n+  @TempDir private File tableDir = null;\n   protected String tableLocation = null;\n+  protected Map<String, String> properties;\n+  @Parameter private int formatVersion;\n \n-  @Before\n+  @Parameters(name = \"formatVersion = {0}\")\n+  protected static List<Object> parameters() {\n+    return Arrays.asList(2, 3);\n+  }\n+\n+  @BeforeEach\n   public void setupTableLocation() throws Exception {\n-    this.tableDir = temp.newFolder();\n     this.tableLocation = tableDir.toURI().toString();\n+    properties = ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDryRun() throws IOException, InterruptedException {\n     Table table =\n         TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap(), tableLocation);\n@@ -129,7 +137,7 @@ public void testDryRun() throws IOException, InterruptedException {\n             .select(\"file_path\")\n             .as(Encoders.STRING())\n             .collectAsList();\n-    Assert.assertEquals(\"Should be 2 valid files\", 2, validFiles.size());\n+    assertThat(validFiles).as(\"Should be 2 valid files\").hasSize(2);\n \n     df.write().mode(\"append\").parquet(tableLocation + \"/data\");\n \n@@ -140,11 +148,11 @@ public void testDryRun() throws IOException, InterruptedException {\n             .filter(FileStatus::isFile)\n             .map(file -> file.getPath().toString())\n             .collect(Collectors.toList());\n-    Assert.assertEquals(\"Should be 3 files\", 3, allFiles.size());\n+    assertThat(allFiles).as(\"Should be 3 valid files\").hasSize(3);\n \n     List<String> invalidFiles = Lists.newArrayList(allFiles);\n     invalidFiles.removeAll(validFiles);\n-    Assert.assertEquals(\"Should be 1 invalid file\", 1, invalidFiles.size());\n+    assertThat(invalidFiles).as(\"Should be 1 invalid file\").hasSize(1);\n \n     waitUntilAfter(System.currentTimeMillis());\n \n@@ -152,9 +160,9 @@ public void testDryRun() throws IOException, InterruptedException {\n \n     DeleteOrphanFiles.Result result1 =\n         actions.deleteOrphanFiles(table).deleteWith(s -> {}).execute();\n-    Assert.assertTrue(\n-        \"Default olderThan interval should be safe\",\n-        Iterables.isEmpty(result1.orphanFileLocations()));\n+    assertThat(result1.orphanFileLocations())\n+        .as(\"Default olderThan interval should be safe\")\n+        .isEmpty();\n \n     DeleteOrphanFiles.Result result2 =\n         actions\n@@ -162,14 +170,21 @@ public void testDryRun() throws IOException, InterruptedException {\n             .olderThan(System.currentTimeMillis())\n             .deleteWith(s -> {})\n             .execute();\n-    Assert.assertEquals(\"Action should find 1 file\", invalidFiles, result2.orphanFileLocations());\n-    Assert.assertTrue(\"Invalid file should be present\", fs.exists(new Path(invalidFiles.get(0))));\n+    assertThat(result2.orphanFileLocations())\n+        .as(\"Action should find 1 file\")\n+        .isEqualTo(invalidFiles);\n+    assertThat(fs.exists(new Path(invalidFiles.get(0))))\n+        .as(\"Invalid file should be present\")\n+        .isTrue();\n \n     DeleteOrphanFiles.Result result3 =\n         actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n-    Assert.assertEquals(\"Action should delete 1 file\", invalidFiles, result3.orphanFileLocations());\n-    Assert.assertFalse(\n-        \"Invalid file should not be present\", fs.exists(new Path(invalidFiles.get(0))));\n+    assertThat(result3.orphanFileLocations())\n+        .as(\"Action should delete 1 file\")\n+        .isEqualTo(invalidFiles);\n+    assertThat(fs.exists(new Path(invalidFiles.get(0))))\n+        .as(\"Invalid file should not be present\")\n+        .isFalse();\n \n     List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n     expectedRecords.addAll(records);\n@@ -178,10 +193,10 @@ public void testDryRun() throws IOException, InterruptedException {\n     Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n     List<ThreeColumnRecord> actualRecords =\n         resultDF.as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+    assertThat(actualRecords).isEqualTo(expectedRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAllValidFilesAreKept() throws IOException, InterruptedException {\n     Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n \n@@ -205,13 +220,13 @@ public void testAllValidFilesAreKept() throws IOException, InterruptedException\n     List<Snapshot> snapshots = Lists.newArrayList(table.snapshots());\n \n     List<String> snapshotFiles1 = snapshotFiles(snapshots.get(0).snapshotId());\n-    Assert.assertEquals(1, snapshotFiles1.size());\n+    assertThat(snapshotFiles1).hasSize(1);\n \n     List<String> snapshotFiles2 = snapshotFiles(snapshots.get(1).snapshotId());\n-    Assert.assertEquals(1, snapshotFiles2.size());\n+    assertThat(snapshotFiles2).hasSize(1);\n \n     List<String> snapshotFiles3 = snapshotFiles(snapshots.get(2).snapshotId());\n-    Assert.assertEquals(2, snapshotFiles3.size());\n+    assertThat(snapshotFiles3).hasSize(2);\n \n     df2.coalesce(1).write().mode(\"append\").parquet(tableLocation + \"/data\");\n     df2.coalesce(1).write().mode(\"append\").parquet(tableLocation + \"/data/c2_trunc=AA\");\n@@ -225,25 +240,25 @@ public void testAllValidFilesAreKept() throws IOException, InterruptedException\n     DeleteOrphanFiles.Result result =\n         actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n \n-    Assert.assertEquals(\"Should delete 4 files\", 4, Iterables.size(result.orphanFileLocations()));\n+    assertThat(result.orphanFileLocations()).as(\"Should delete 4 files\").hasSize(4);\n \n     Path dataPath = new Path(tableLocation + \"/data\");\n     FileSystem fs = dataPath.getFileSystem(spark.sessionState().newHadoopConf());\n \n     for (String fileLocation : snapshotFiles1) {\n-      Assert.assertTrue(\"All snapshot files must remain\", fs.exists(new Path(fileLocation)));\n+      assertThat(fs.exists(new Path(fileLocation))).as(\"All snapshot files must remain\").isTrue();\n     }\n \n     for (String fileLocation : snapshotFiles2) {\n-      Assert.assertTrue(\"All snapshot files must remain\", fs.exists(new Path(fileLocation)));\n+      assertThat(fs.exists(new Path(fileLocation))).as(\"All snapshot files must remain\").isTrue();\n     }\n \n     for (String fileLocation : snapshotFiles3) {\n-      Assert.assertTrue(\"All snapshot files must remain\", fs.exists(new Path(fileLocation)));\n+      assertThat(fs.exists(new Path(fileLocation))).as(\"All snapshot files must remain\").isTrue();\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void orphanedFileRemovedWithParallelTasks() throws InterruptedException, IOException {\n     Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n \n@@ -299,16 +314,15 @@ public void orphanedFileRemovedWithParallelTasks() throws InterruptedException,\n \n     // Verifies that the delete methods ran in the threads created by the provided ExecutorService\n     // ThreadFactory\n-    Assert.assertEquals(\n-        deleteThreads,\n-        Sets.newHashSet(\n-            \"remove-orphan-0\", \"remove-orphan-1\", \"remove-orphan-2\", \"remove-orphan-3\"));\n-\n-    Assert.assertEquals(\"Should delete 4 files\", 4, deletedFiles.size());\n+    assertThat(deleteThreads)\n+        .containsExactlyInAnyOrder(\n+            \"remove-orphan-0\", \"remove-orphan-1\", \"remove-orphan-2\", \"remove-orphan-3\");\n+    assertThat(deletedFiles).hasSize(4);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWapFilesAreKept() throws InterruptedException {\n+    assumeThat(formatVersion).as(\"currently fails with DVs\").isEqualTo(2);\n     Map<String, String> props = Maps.newHashMap();\n     props.put(TableProperties.WRITE_AUDIT_PUBLISH_ENABLED, \"true\");\n     Table table = TABLES.create(SCHEMA, SPEC, props, tableLocation);\n@@ -328,7 +342,9 @@ public void testWapFilesAreKept() throws InterruptedException {\n     Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n     List<ThreeColumnRecord> actualRecords =\n         resultDF.as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Should not return data from the staged snapshot\", records, actualRecords);\n+    assertThat(actualRecords)\n+        .as(\"Should not return data from the staged snapshot\")\n+        .isEqualTo(records);\n \n     waitUntilAfter(System.currentTimeMillis());\n \n@@ -337,11 +353,10 @@ public void testWapFilesAreKept() throws InterruptedException {\n     DeleteOrphanFiles.Result result =\n         actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n \n-    Assert.assertTrue(\n-        \"Should not delete any files\", Iterables.isEmpty(result.orphanFileLocations()));\n+    assertThat(result.orphanFileLocations()).as(\"Should not delete any files\").isEmpty();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMetadataFolderIsIntact() throws InterruptedException {\n     // write data directly to the table location\n     Map<String, String> props = Maps.newHashMap();\n@@ -363,15 +378,15 @@ public void testMetadataFolderIsIntact() throws InterruptedException {\n     DeleteOrphanFiles.Result result =\n         actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n \n-    Assert.assertEquals(\"Should delete 1 file\", 1, Iterables.size(result.orphanFileLocations()));\n+    assertThat(result.orphanFileLocations()).as(\"Should delete 1 file\").hasSize(1);\n \n     Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n     List<ThreeColumnRecord> actualRecords =\n         resultDF.as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n+    assertThat(actualRecords).as(\"Rows must match\").isEqualTo(records);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testOlderThanTimestamp() throws InterruptedException {\n     Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n \n@@ -397,11 +412,10 @@ public void testOlderThanTimestamp() throws InterruptedException {\n     DeleteOrphanFiles.Result result =\n         actions.deleteOrphanFiles(table).olderThan(timestamp).execute();\n \n-    Assert.assertEquals(\n-        \"Should delete only 2 files\", 2, Iterables.size(result.orphanFileLocations()));\n+    assertThat(result.orphanFileLocations()).as(\"Should delete only 2 files\").hasSize(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveUnreachableMetadataVersionFiles() throws InterruptedException {\n     Map<String, String> props = Maps.newHashMap();\n     props.put(TableProperties.WRITE_DATA_LOCATION, tableLocation);\n@@ -423,11 +437,8 @@ public void testRemoveUnreachableMetadataVersionFiles() throws InterruptedExcept\n     DeleteOrphanFiles.Result result =\n         actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n \n-    Assert.assertEquals(\"Should delete 1 file\", 1, Iterables.size(result.orphanFileLocations()));\n-    Assert.assertTrue(\n-        \"Should remove v1 file\",\n-        StreamSupport.stream(result.orphanFileLocations().spliterator(), false)\n-            .anyMatch(file -> file.contains(\"v1.metadata.json\")));\n+    assertThat(result.orphanFileLocations())\n+        .containsExactly(tableLocation + \"metadata/v1.metadata.json\");\n \n     List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n     expectedRecords.addAll(records);\n@@ -436,10 +447,10 @@ public void testRemoveUnreachableMetadataVersionFiles() throws InterruptedExcept\n     Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n     List<ThreeColumnRecord> actualRecords =\n         resultDF.as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+    assertThat(actualRecords).as(\"Rows must match\").isEqualTo(expectedRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testManyTopLevelPartitions() throws InterruptedException {\n     Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n \n@@ -459,14 +470,13 @@ public void testManyTopLevelPartitions() throws InterruptedException {\n     DeleteOrphanFiles.Result result =\n         actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n \n-    Assert.assertTrue(\n-        \"Should not delete any files\", Iterables.isEmpty(result.orphanFileLocations()));\n+    assertThat(result.orphanFileLocations()).as(\"Should not delete any files\").isEmpty();\n \n     Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n-    Assert.assertEquals(\"Rows count must match\", records.size(), resultDF.count());\n+    assertThat(resultDF.count()).as(\"Rows count must match\").isEqualTo(records.size());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testManyLeafPartitions() throws InterruptedException {\n     Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n \n@@ -486,14 +496,13 @@ public void testManyLeafPartitions() throws InterruptedException {\n     DeleteOrphanFiles.Result result =\n         actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n \n-    Assert.assertTrue(\n-        \"Should not delete any files\", Iterables.isEmpty(result.orphanFileLocations()));\n+    assertThat(result.orphanFileLocations()).as(\"Should not delete any files\").isEmpty();\n \n     Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n-    Assert.assertEquals(\"Row count must match\", records.size(), resultDF.count());\n+    assertThat(resultDF.count()).as(\"Row count must match\").isEqualTo(records.size());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHiddenPartitionPaths() throws InterruptedException {\n     Schema schema =\n         new Schema(\n@@ -523,10 +532,10 @@ public void testHiddenPartitionPaths() throws InterruptedException {\n     DeleteOrphanFiles.Result result =\n         actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n \n-    Assert.assertEquals(\"Should delete 2 files\", 2, Iterables.size(result.orphanFileLocations()));\n+    assertThat(result.orphanFileLocations()).as(\"Should delete 2 files\").hasSize(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHiddenPartitionPathsWithPartitionEvolution() throws InterruptedException {\n     Schema schema =\n         new Schema(\n@@ -559,10 +568,10 @@ public void testHiddenPartitionPathsWithPartitionEvolution() throws InterruptedE\n     DeleteOrphanFiles.Result result =\n         actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n \n-    Assert.assertEquals(\"Should delete 2 files\", 2, Iterables.size(result.orphanFileLocations()));\n+    assertThat(result.orphanFileLocations()).as(\"Should delete 2 files\").hasSize(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHiddenPathsStartingWithPartitionNamesAreIgnored()\n       throws InterruptedException, IOException {\n     Schema schema =\n@@ -595,8 +604,8 @@ public void testHiddenPathsStartingWithPartitionNamesAreIgnored()\n     DeleteOrphanFiles.Result result =\n         actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n \n-    Assert.assertEquals(\"Should delete 0 files\", 0, Iterables.size(result.orphanFileLocations()));\n-    Assert.assertTrue(fs.exists(pathToFileInHiddenFolder));\n+    assertThat(result.orphanFileLocations()).as(\"Should delete 0 files\").isEmpty();\n+    assertThat(fs.exists(pathToFileInHiddenFolder)).isTrue();\n   }\n \n   private List<String> snapshotFiles(long snapshotId) {\n@@ -610,7 +619,7 @@ private List<String> snapshotFiles(long snapshotId) {\n         .collectAsList();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, InterruptedException {\n     Table table =\n         TABLES.create(\n@@ -635,7 +644,7 @@ public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, Inte\n             .select(\"file_path\")\n             .as(Encoders.STRING())\n             .collectAsList();\n-    Assert.assertEquals(\"Should be 1 valid files\", 1, validFiles.size());\n+    assertThat(validFiles).as(\"Should be 1 valid file\").hasSize(1);\n     String validFile = validFiles.get(0);\n \n     df.write().mode(\"append\").parquet(tableLocation + \"/data\");\n@@ -647,11 +656,11 @@ public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, Inte\n             .filter(FileStatus::isFile)\n             .map(file -> file.getPath().toString())\n             .collect(Collectors.toList());\n-    Assert.assertEquals(\"Should be 2 files\", 2, allFiles.size());\n+    assertThat(allFiles).as(\"Should be 2 files\").hasSize(2);\n \n     List<String> invalidFiles = Lists.newArrayList(allFiles);\n     invalidFiles.removeIf(file -> file.contains(validFile));\n-    Assert.assertEquals(\"Should be 1 invalid file\", 1, invalidFiles.size());\n+    assertThat(invalidFiles).as(\"Should be 1 invalid file\").hasSize(1);\n \n     waitUntilAfter(System.currentTimeMillis());\n \n@@ -662,11 +671,15 @@ public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, Inte\n             .olderThan(System.currentTimeMillis())\n             .deleteWith(s -> {})\n             .execute();\n-    Assert.assertEquals(\"Action should find 1 file\", invalidFiles, result.orphanFileLocations());\n-    Assert.assertTrue(\"Invalid file should be present\", fs.exists(new Path(invalidFiles.get(0))));\n+    assertThat(result.orphanFileLocations())\n+        .as(\"Action should find 1 file\")\n+        .isEqualTo(invalidFiles);\n+    assertThat(fs.exists(new Path(invalidFiles.get(0))))\n+        .as(\"Invalid file should be present\")\n+        .isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException {\n     HadoopCatalog catalog = new HadoopCatalog(new Configuration(), tableLocation);\n     String namespaceName = \"testDb\";\n@@ -693,24 +706,18 @@ public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException\n     DeleteOrphanFiles.Result result =\n         SparkActions.get().deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n \n-    Assert.assertEquals(\n-        \"Should delete only 1 files\", 1, Iterables.size(result.orphanFileLocations()));\n+    assertThat(result.orphanFileLocations()).as(\"Should delete only 1 file\").hasSize(1);\n \n     Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(table.location());\n     List<ThreeColumnRecord> actualRecords =\n         resultDF.as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Rows must match\", records, actualRecords);\n+    assertThat(actualRecords).as(\"Rows must match\").isEqualTo(records);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHiveCatalogTable() throws IOException {\n-    Table table =\n-        catalog.createTable(\n-            TableIdentifier.of(\"default\", \"hivetestorphan\"),\n-            SCHEMA,\n-            SPEC,\n-            tableLocation,\n-            Maps.newHashMap());\n+    TableIdentifier identifier = TableIdentifier.of(\"default\", randomName(\"hivetestorphan\"));\n+    Table table = catalog.createTable(identifier, SCHEMA, SPEC, tableLocation, properties);\n \n     List<ThreeColumnRecord> records =\n         Lists.newArrayList(new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"));\n@@ -721,7 +728,7 @@ public void testHiveCatalogTable() throws IOException {\n         .write()\n         .format(\"iceberg\")\n         .mode(\"append\")\n-        .save(\"default.hivetestorphan\");\n+        .save(identifier.toString());\n \n     String location = table.location().replaceFirst(\"file:\", \"\");\n     new File(location + \"/data/trashfile\").createNewFile();\n@@ -731,13 +738,12 @@ public void testHiveCatalogTable() throws IOException {\n             .deleteOrphanFiles(table)\n             .olderThan(System.currentTimeMillis() + 1000)\n             .execute();\n-    Assert.assertTrue(\n-        \"trash file should be removed\",\n-        StreamSupport.stream(result.orphanFileLocations().spliterator(), false)\n-            .anyMatch(file -> file.contains(\"file:\" + location + \"/data/trashfile\")));\n+    assertThat(result.orphanFileLocations())\n+        .as(\"trash file should be removed\")\n+        .contains(\"file:\" + location + \"/data/trashfile\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testGarbageCollectionDisabled() {\n     Table table =\n         TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap(), tableLocation);\n@@ -757,7 +763,7 @@ public void testGarbageCollectionDisabled() {\n             \"Cannot delete orphan files: GC is disabled (deleting files may corrupt other tables)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCompareToFileList() throws IOException, InterruptedException {\n     Table table =\n         TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap(), tableLocation);\n@@ -782,7 +788,7 @@ public void testCompareToFileList() throws IOException, InterruptedException {\n                         file.getPath().toString(), new Timestamp(file.getModificationTime())))\n             .collect(Collectors.toList());\n \n-    Assert.assertEquals(\"Should be 2 valid files\", 2, validFiles.size());\n+    assertThat(validFiles).as(\"Should be 2 valid files\").hasSize(2);\n \n     df.write().mode(\"append\").parquet(tableLocation + \"/data\");\n \n@@ -795,7 +801,7 @@ public void testCompareToFileList() throws IOException, InterruptedException {\n                         file.getPath().toString(), new Timestamp(file.getModificationTime())))\n             .collect(Collectors.toList());\n \n-    Assert.assertEquals(\"Should be 3 files\", 3, allFiles.size());\n+    assertThat(allFiles).as(\"Should be 3 files\").hasSize(3);\n \n     List<FilePathLastModifiedRecord> invalidFiles = Lists.newArrayList(allFiles);\n     invalidFiles.removeAll(validFiles);\n@@ -803,7 +809,7 @@ public void testCompareToFileList() throws IOException, InterruptedException {\n         invalidFiles.stream()\n             .map(FilePathLastModifiedRecord::getFilePath)\n             .collect(Collectors.toList());\n-    Assert.assertEquals(\"Should be 1 invalid file\", 1, invalidFiles.size());\n+    assertThat(invalidFiles).as(\"Should be 1 invalid file\").hasSize(1);\n \n     // sleep for 1 second to ensure files will be old enough\n     waitUntilAfter(System.currentTimeMillis());\n@@ -822,9 +828,9 @@ public void testCompareToFileList() throws IOException, InterruptedException {\n             .compareToFileList(compareToFileList)\n             .deleteWith(s -> {})\n             .execute();\n-    Assert.assertTrue(\n-        \"Default olderThan interval should be safe\",\n-        Iterables.isEmpty(result1.orphanFileLocations()));\n+    assertThat(result1.orphanFileLocations())\n+        .as(\"Default olderThan interval should be safe\")\n+        .isEmpty();\n \n     DeleteOrphanFiles.Result result2 =\n         actions\n@@ -833,10 +839,12 @@ public void testCompareToFileList() throws IOException, InterruptedException {\n             .olderThan(System.currentTimeMillis())\n             .deleteWith(s -> {})\n             .execute();\n-    Assert.assertEquals(\n-        \"Action should find 1 file\", invalidFilePaths, result2.orphanFileLocations());\n-    Assert.assertTrue(\n-        \"Invalid file should be present\", fs.exists(new Path(invalidFilePaths.get(0))));\n+    assertThat(result2.orphanFileLocations())\n+        .as(\"Action should find 1 file\")\n+        .isEqualTo(invalidFilePaths);\n+    assertThat(fs.exists(new Path(invalidFilePaths.get(0))))\n+        .as(\"Invalid file should be present\")\n+        .isTrue();\n \n     DeleteOrphanFiles.Result result3 =\n         actions\n@@ -844,10 +852,12 @@ public void testCompareToFileList() throws IOException, InterruptedException {\n             .compareToFileList(compareToFileList)\n             .olderThan(System.currentTimeMillis())\n             .execute();\n-    Assert.assertEquals(\n-        \"Action should delete 1 file\", invalidFilePaths, result3.orphanFileLocations());\n-    Assert.assertFalse(\n-        \"Invalid file should not be present\", fs.exists(new Path(invalidFilePaths.get(0))));\n+    assertThat(result3.orphanFileLocations())\n+        .as(\"Action should delete 1 file\")\n+        .isEqualTo(invalidFilePaths);\n+    assertThat(fs.exists(new Path(invalidFilePaths.get(0))))\n+        .as(\"Invalid file should not be present\")\n+        .isFalse();\n \n     List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n     expectedRecords.addAll(records);\n@@ -856,7 +866,7 @@ public void testCompareToFileList() throws IOException, InterruptedException {\n     Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n     List<ThreeColumnRecord> actualRecords =\n         resultDF.as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n-    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+    assertThat(actualRecords).as(\"Rows must match\").isEqualTo(expectedRecords);\n \n     List<FilePathLastModifiedRecord> outsideLocationMockFiles =\n         Lists.newArrayList(new FilePathLastModifiedRecord(\"/tmp/mock1\", new Timestamp(0L)));\n@@ -873,8 +883,7 @@ public void testCompareToFileList() throws IOException, InterruptedException {\n             .compareToFileList(compareToFileListWithOutsideLocation)\n             .deleteWith(s -> {})\n             .execute();\n-    Assert.assertEquals(\n-        \"Action should find nothing\", Lists.newArrayList(), result4.orphanFileLocations());\n+    assertThat(result4.orphanFileLocations()).as(\"Action should find nothing\").isEmpty();\n   }\n \n   protected long waitUntilAfter(long timestampMillis) {\n@@ -885,7 +894,7 @@ protected long waitUntilAfter(long timestampMillis) {\n     return current;\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesWithStatisticFiles() throws Exception {\n     Table table =\n         TABLES.create(\n@@ -954,35 +963,32 @@ public void testRemoveOrphanFilesWithStatisticFiles() throws Exception {\n             .olderThan(System.currentTimeMillis() + 1000)\n             .execute();\n     Iterable<String> orphanFileLocations = result.orphanFileLocations();\n-    assertThat(orphanFileLocations).as(\"Should be orphan files\").hasSize(1);\n-    assertThat(Iterables.getOnlyElement(orphanFileLocations))\n-        .as(\"Deleted file\")\n-        .isEqualTo(statsLocation.toURI().toString());\n-    assertThat(statsLocation.exists()).as(\"stats file should be deleted\").isFalse();\n+    assertThat(orphanFileLocations).hasSize(1).containsExactly(statsLocation.toURI().toString());\n+    assertThat(statsLocation).as(\"stats file should be deleted\").doesNotExist();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPathsWithExtraSlashes() {\n     List<String> validFiles = Lists.newArrayList(\"file:///dir1/dir2/file1\");\n     List<String> actualFiles = Lists.newArrayList(\"file:///dir1/////dir2///file1\");\n     executeTest(validFiles, actualFiles, Lists.newArrayList());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPathsWithValidFileHavingNoAuthority() {\n     List<String> validFiles = Lists.newArrayList(\"hdfs:///dir1/dir2/file1\");\n     List<String> actualFiles = Lists.newArrayList(\"hdfs://servicename/dir1/dir2/file1\");\n     executeTest(validFiles, actualFiles, Lists.newArrayList());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPathsWithActualFileHavingNoAuthority() {\n     List<String> validFiles = Lists.newArrayList(\"hdfs://servicename/dir1/dir2/file1\");\n     List<String> actualFiles = Lists.newArrayList(\"hdfs:///dir1/dir2/file1\");\n     executeTest(validFiles, actualFiles, Lists.newArrayList());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPathsWithEqualSchemes() {\n     List<String> validFiles = Lists.newArrayList(\"scheme1://bucket1/dir1/dir2/file1\");\n     List<String> actualFiles = Lists.newArrayList(\"scheme2://bucket1/dir1/dir2/file1\");\n@@ -1011,7 +1017,7 @@ public void testPathsWithEqualSchemes() {\n         DeleteOrphanFiles.PrefixMismatchMode.ERROR);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPathsWithEqualAuthorities() {\n     List<String> validFiles = Lists.newArrayList(\"hdfs://servicename1/dir1/dir2/file1\");\n     List<String> actualFiles = Lists.newArrayList(\"hdfs://servicename2/dir1/dir2/file1\");\n@@ -1040,7 +1046,7 @@ public void testPathsWithEqualAuthorities() {\n         DeleteOrphanFiles.PrefixMismatchMode.ERROR);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFileActionWithDeleteMode() {\n     List<String> validFiles = Lists.newArrayList(\"hdfs://servicename1/dir1/dir2/file1\");\n     List<String> actualFiles = Lists.newArrayList(\"hdfs://servicename2/dir1/dir2/file1\");\n@@ -1054,6 +1060,10 @@ public void testRemoveOrphanFileActionWithDeleteMode() {\n         DeleteOrphanFiles.PrefixMismatchMode.DELETE);\n   }\n \n+  protected String randomName(String prefix) {\n+    return prefix + UUID.randomUUID().toString().replace(\"-\", \"\");\n+  }\n+\n   private void executeTest(\n       List<String> validFiles, List<String> actualFiles, List<String> expectedOrphanFiles) {\n     executeTest(\n@@ -1081,6 +1091,6 @@ private void executeTest(\n     List<String> orphanFiles =\n         DeleteOrphanFilesSparkAction.findOrphanFiles(\n             spark, toFileUri.apply(actualFileDS), toFileUri.apply(validFileDS), mode);\n-    Assert.assertEquals(expectedOrphanFiles, orphanFiles);\n+    assertThat(orphanFiles).isEqualTo(expectedOrphanFiles);\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java\nindex 0abfd79d5ddb..646e5f8e70d4 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java\n@@ -18,23 +18,21 @@\n  */\n package org.apache.iceberg.spark.actions;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+\n import java.io.File;\n-import java.util.Map;\n-import java.util.stream.StreamSupport;\n import org.apache.iceberg.actions.DeleteOrphanFiles;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.spark.SparkCatalog;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.iceberg.spark.SparkSessionCatalog;\n import org.apache.iceberg.spark.source.SparkTable;\n import org.apache.spark.sql.connector.catalog.Identifier;\n import org.apache.spark.sql.connector.expressions.Transform;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.TestTemplate;\n \n public class TestRemoveOrphanFilesAction3 extends TestRemoveOrphanFilesAction {\n-  @Test\n+  @TestTemplate\n   public void testSparkCatalogTable() throws Exception {\n     spark.conf().set(\"spark.sql.catalog.mycat\", \"org.apache.iceberg.spark.SparkCatalog\");\n     spark.conf().set(\"spark.sql.catalog.mycat.type\", \"hadoop\");\n@@ -42,29 +40,28 @@ public void testSparkCatalogTable() throws Exception {\n     SparkCatalog cat = (SparkCatalog) spark.sessionState().catalogManager().catalog(\"mycat\");\n \n     String[] database = {\"default\"};\n-    Identifier id = Identifier.of(database, \"table\");\n-    Map<String, String> options = Maps.newHashMap();\n+    Identifier id = Identifier.of(database, randomName(\"table\"));\n     Transform[] transforms = {};\n-    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, properties);\n     SparkTable table = (SparkTable) cat.loadTable(id);\n \n-    spark.sql(\"INSERT INTO mycat.default.table VALUES (1,1,1)\");\n+    sql(\"INSERT INTO mycat.default.%s VALUES (1,1,1)\", id.name());\n \n     String location = table.table().location().replaceFirst(\"file:\", \"\");\n-    new File(location + \"/data/trashfile\").createNewFile();\n+    String trashFile = randomName(\"/data/trashfile\");\n+    new File(location + trashFile).createNewFile();\n \n     DeleteOrphanFiles.Result results =\n         SparkActions.get()\n             .deleteOrphanFiles(table.table())\n             .olderThan(System.currentTimeMillis() + 1000)\n             .execute();\n-    Assert.assertTrue(\n-        \"trash file should be removed\",\n-        StreamSupport.stream(results.orphanFileLocations().spliterator(), false)\n-            .anyMatch(file -> file.contains(\"file:\" + location + \"/data/trashfile\")));\n+    assertThat(results.orphanFileLocations())\n+        .as(\"trash file should be removed\")\n+        .contains(\"file:\" + location + trashFile);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkCatalogNamedHadoopTable() throws Exception {\n     spark.conf().set(\"spark.sql.catalog.hadoop\", \"org.apache.iceberg.spark.SparkCatalog\");\n     spark.conf().set(\"spark.sql.catalog.hadoop.type\", \"hadoop\");\n@@ -72,29 +69,28 @@ public void testSparkCatalogNamedHadoopTable() throws Exception {\n     SparkCatalog cat = (SparkCatalog) spark.sessionState().catalogManager().catalog(\"hadoop\");\n \n     String[] database = {\"default\"};\n-    Identifier id = Identifier.of(database, \"table\");\n-    Map<String, String> options = Maps.newHashMap();\n+    Identifier id = Identifier.of(database, randomName(\"table\"));\n     Transform[] transforms = {};\n-    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, properties);\n     SparkTable table = (SparkTable) cat.loadTable(id);\n \n-    spark.sql(\"INSERT INTO hadoop.default.table VALUES (1,1,1)\");\n+    sql(\"INSERT INTO hadoop.default.%s VALUES (1,1,1)\", id.name());\n \n     String location = table.table().location().replaceFirst(\"file:\", \"\");\n-    new File(location + \"/data/trashfile\").createNewFile();\n+    String trashFile = randomName(\"/data/trashfile\");\n+    new File(location + trashFile).createNewFile();\n \n     DeleteOrphanFiles.Result results =\n         SparkActions.get()\n             .deleteOrphanFiles(table.table())\n             .olderThan(System.currentTimeMillis() + 1000)\n             .execute();\n-    Assert.assertTrue(\n-        \"trash file should be removed\",\n-        StreamSupport.stream(results.orphanFileLocations().spliterator(), false)\n-            .anyMatch(file -> file.contains(\"file:\" + location + \"/data/trashfile\")));\n+    assertThat(results.orphanFileLocations())\n+        .as(\"trash file should be removed\")\n+        .contains(\"file:\" + location + trashFile);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkCatalogNamedHiveTable() throws Exception {\n     spark.conf().set(\"spark.sql.catalog.hive\", \"org.apache.iceberg.spark.SparkCatalog\");\n     spark.conf().set(\"spark.sql.catalog.hive.type\", \"hadoop\");\n@@ -102,29 +98,28 @@ public void testSparkCatalogNamedHiveTable() throws Exception {\n     SparkCatalog cat = (SparkCatalog) spark.sessionState().catalogManager().catalog(\"hive\");\n \n     String[] database = {\"default\"};\n-    Identifier id = Identifier.of(database, \"table\");\n-    Map<String, String> options = Maps.newHashMap();\n+    Identifier id = Identifier.of(database, randomName(\"table\"));\n     Transform[] transforms = {};\n-    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, properties);\n     SparkTable table = (SparkTable) cat.loadTable(id);\n \n-    spark.sql(\"INSERT INTO hive.default.table VALUES (1,1,1)\");\n+    sql(\"INSERT INTO hive.default.%s VALUES (1,1,1)\", id.name());\n \n     String location = table.table().location().replaceFirst(\"file:\", \"\");\n-    new File(location + \"/data/trashfile\").createNewFile();\n+    String trashFile = randomName(\"/data/trashfile\");\n+    new File(location + trashFile).createNewFile();\n \n     DeleteOrphanFiles.Result results =\n         SparkActions.get()\n             .deleteOrphanFiles(table.table())\n             .olderThan(System.currentTimeMillis() + 1000)\n             .execute();\n-    Assert.assertTrue(\n-        \"trash file should be removed\",\n-        StreamSupport.stream(results.orphanFileLocations().spliterator(), false)\n-            .anyMatch(file -> file.contains(\"file:\" + location + \"/data/trashfile\")));\n+    assertThat(results.orphanFileLocations())\n+        .as(\"trash file should be removed\")\n+        .contains(\"file:\" + location + trashFile);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkSessionCatalogHadoopTable() throws Exception {\n     spark\n         .conf()\n@@ -135,29 +130,28 @@ public void testSparkSessionCatalogHadoopTable() throws Exception {\n         (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n \n     String[] database = {\"default\"};\n-    Identifier id = Identifier.of(database, \"table\");\n-    Map<String, String> options = Maps.newHashMap();\n+    Identifier id = Identifier.of(database, randomName(\"table\"));\n     Transform[] transforms = {};\n-    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, properties);\n     SparkTable table = (SparkTable) cat.loadTable(id);\n \n-    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n+    sql(\"INSERT INTO default.%s VALUES (1,1,1)\", id.name());\n \n     String location = table.table().location().replaceFirst(\"file:\", \"\");\n-    new File(location + \"/data/trashfile\").createNewFile();\n+    String trashFile = randomName(\"/data/trashfile\");\n+    new File(location + trashFile).createNewFile();\n \n     DeleteOrphanFiles.Result results =\n         SparkActions.get()\n             .deleteOrphanFiles(table.table())\n             .olderThan(System.currentTimeMillis() + 1000)\n             .execute();\n-    Assert.assertTrue(\n-        \"trash file should be removed\",\n-        StreamSupport.stream(results.orphanFileLocations().spliterator(), false)\n-            .anyMatch(file -> file.contains(\"file:\" + location + \"/data/trashfile\")));\n+    assertThat(results.orphanFileLocations())\n+        .as(\"trash file should be removed\")\n+        .contains(\"file:\" + location + trashFile);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkSessionCatalogHiveTable() throws Exception {\n     spark\n         .conf()\n@@ -168,30 +162,29 @@ public void testSparkSessionCatalogHiveTable() throws Exception {\n \n     String[] database = {\"default\"};\n     Identifier id = Identifier.of(database, \"sessioncattest\");\n-    Map<String, String> options = Maps.newHashMap();\n     Transform[] transforms = {};\n     cat.dropTable(id);\n-    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, properties);\n     SparkTable table = (SparkTable) cat.loadTable(id);\n \n-    spark.sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n+    sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n \n     String location = table.table().location().replaceFirst(\"file:\", \"\");\n-    new File(location + \"/data/trashfile\").createNewFile();\n+    String trashFile = randomName(\"/data/trashfile\");\n+    new File(location + trashFile).createNewFile();\n \n     DeleteOrphanFiles.Result results =\n         SparkActions.get()\n             .deleteOrphanFiles(table.table())\n             .olderThan(System.currentTimeMillis() + 1000)\n             .execute();\n-    Assert.assertTrue(\n-        \"trash file should be removed\",\n-        StreamSupport.stream(results.orphanFileLocations().spliterator(), false)\n-            .anyMatch(file -> file.contains(\"file:\" + location + \"/data/trashfile\")));\n+    assertThat(results.orphanFileLocations())\n+        .as(\"trash file should be removed\")\n+        .contains(\"file:\" + location + trashFile);\n   }\n \n-  @After\n-  public void resetSparkSessionCatalog() throws Exception {\n+  @AfterEach\n+  public void resetSparkSessionCatalog() {\n     spark.conf().unset(\"spark.sql.catalog.spark_catalog\");\n     spark.conf().unset(\"spark.sql.catalog.spark_catalog.type\");\n     spark.conf().unset(\"spark.sql.catalog.spark_catalog.warehouse\");\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\nindex 490c7119304c..43e5cb37da11 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\n@@ -38,6 +38,7 @@\n import java.io.File;\n import java.io.IOException;\n import java.io.UncheckedIOException;\n+import java.util.Arrays;\n import java.util.Collections;\n import java.util.Comparator;\n import java.util.List;\n@@ -55,6 +56,9 @@\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionData;\n import org.apache.iceberg.PartitionKey;\n import org.apache.iceberg.PartitionSpec;\n@@ -90,7 +94,6 @@\n import org.apache.iceberg.io.OutputFile;\n import org.apache.iceberg.io.OutputFileFactory;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n@@ -100,8 +103,8 @@\n import org.apache.iceberg.spark.FileRewriteCoordinator;\n import org.apache.iceberg.spark.ScanTaskSetManager;\n import org.apache.iceberg.spark.SparkTableUtil;\n-import org.apache.iceberg.spark.SparkTestBase;\n import org.apache.iceberg.spark.SparkWriteOptions;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.spark.actions.RewriteDataFilesSparkAction.RewriteExecutionContext;\n import org.apache.iceberg.spark.data.TestHelpers;\n import org.apache.iceberg.spark.source.ThreeColumnRecord;\n@@ -116,17 +119,18 @@\n import org.apache.spark.sql.Encoders;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.internal.SQLConf;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.BeforeClass;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n import org.mockito.ArgumentMatcher;\n import org.mockito.Mockito;\n \n-public class TestRewriteDataFilesAction extends SparkTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRewriteDataFilesAction extends TestBase {\n \n+  @TempDir private File tableDir;\n   private static final int SCALE = 400000;\n \n   private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n@@ -138,21 +142,25 @@ public class TestRewriteDataFilesAction extends SparkTestBase {\n \n   private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @Parameter private int formatVersion;\n+\n+  @Parameters(name = \"formatVersion = {0}\")\n+  protected static List<Object> parameters() {\n+    return Arrays.asList(2, 3);\n+  }\n \n   private final FileRewriteCoordinator coordinator = FileRewriteCoordinator.get();\n   private final ScanTaskSetManager manager = ScanTaskSetManager.get();\n   private String tableLocation = null;\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void setupSpark() {\n     // disable AQE as tests assume that writes generate a particular number of files\n     spark.conf().set(SQLConf.ADAPTIVE_EXECUTION_ENABLED().key(), \"false\");\n   }\n \n-  @Before\n-  public void setupTableLocation() throws Exception {\n-    File tableDir = temp.newFolder();\n+  @BeforeEach\n+  public void setupTableLocation() {\n     this.tableLocation = tableDir.toURI().toString();\n   }\n \n@@ -162,20 +170,20 @@ private RewriteDataFilesSparkAction basicRewrite(Table table) {\n     return actions().rewriteDataFiles(table).option(SizeBasedFileRewriter.MIN_INPUT_FILES, \"1\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testEmptyTable() {\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n     Map<String, String> options = Maps.newHashMap();\n     Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n \n-    Assert.assertNull(\"Table must be empty\", table.currentSnapshot());\n+    assertThat(table.currentSnapshot()).as(\"Table must be empty\").isNull();\n \n     basicRewrite(table).execute();\n \n-    Assert.assertNull(\"Table must stay empty\", table.currentSnapshot());\n+    assertThat(table.currentSnapshot()).as(\"Table must stay empty\").isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackUnpartitionedTable() {\n     Table table = createTable(4);\n     shouldHaveFiles(table, 4);\n@@ -183,8 +191,10 @@ public void testBinPackUnpartitionedTable() {\n     long dataSizeBefore = testDataSize(table);\n \n     Result result = basicRewrite(table).execute();\n-    Assert.assertEquals(\"Action should rewrite 4 data files\", 4, result.rewrittenDataFilesCount());\n-    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFilesCount());\n+    assertThat(result.rewrittenDataFilesCount())\n+        .as(\"Action should rewrite 4 data files\")\n+        .isEqualTo(4);\n+    assertThat(result.addedDataFilesCount()).as(\"Action should add 1 data file\").isOne();\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     shouldHaveFiles(table, 1);\n@@ -193,7 +203,7 @@ public void testBinPackUnpartitionedTable() {\n     assertEquals(\"Rows must match\", expectedRecords, actual);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackPartitionedTable() {\n     Table table = createTablePartitioned(4, 2);\n     shouldHaveFiles(table, 8);\n@@ -201,8 +211,10 @@ public void testBinPackPartitionedTable() {\n     long dataSizeBefore = testDataSize(table);\n \n     Result result = basicRewrite(table).execute();\n-    Assert.assertEquals(\"Action should rewrite 8 data files\", 8, result.rewrittenDataFilesCount());\n-    Assert.assertEquals(\"Action should add 4 data file\", 4, result.addedDataFilesCount());\n+    assertThat(result.rewrittenDataFilesCount())\n+        .as(\"Action should rewrite 8 data files\")\n+        .isEqualTo(8);\n+    assertThat(result.addedDataFilesCount()).as(\"Action should add 4 data file\").isEqualTo(4);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     shouldHaveFiles(table, 4);\n@@ -211,7 +223,7 @@ public void testBinPackPartitionedTable() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackWithFilter() {\n     Table table = createTablePartitioned(4, 2);\n     shouldHaveFiles(table, 8);\n@@ -224,8 +236,10 @@ public void testBinPackWithFilter() {\n             .filter(Expressions.startsWith(\"c2\", \"foo\"))\n             .execute();\n \n-    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.rewrittenDataFilesCount());\n-    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFilesCount());\n+    assertThat(result.rewrittenDataFilesCount())\n+        .as(\"Action should rewrite 2 data files\")\n+        .isEqualTo(2);\n+    assertThat(result.addedDataFilesCount()).as(\"Action should add 1 data file\").isOne();\n     assertThat(result.rewrittenBytesCount()).isGreaterThan(0L).isLessThan(dataSizeBefore);\n \n     shouldHaveFiles(table, 7);\n@@ -234,7 +248,7 @@ public void testBinPackWithFilter() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackWithFilterOnBucketExpression() {\n     Table table = createTablePartitioned(4, 2);\n \n@@ -260,7 +274,7 @@ public void testBinPackWithFilterOnBucketExpression() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackAfterPartitionChange() {\n     Table table = createTable();\n \n@@ -282,10 +296,9 @@ public void testBinPackAfterPartitionChange() {\n                 Integer.toString(averageFileSize(table) + 1001))\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Should have 1 fileGroup because all files were not correctly partitioned\",\n-        1,\n-        result.rewriteResults().size());\n+    assertThat(result.rewriteResults())\n+        .as(\"Should have 1 fileGroup because all files were not correctly partitioned\")\n+        .hasSize(1);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     List<Object[]> postRewriteData = currentData();\n@@ -296,7 +309,7 @@ public void testBinPackAfterPartitionChange() {\n     shouldHaveFiles(table, 20);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackWithDeletes() {\n     Table table = createTablePartitioned(4, 2);\n     table.updateProperties().set(TableProperties.FORMAT_VERSION, \"2\").commit();\n@@ -331,15 +344,15 @@ public void testBinPackWithDeletes() {\n             .option(SizeBasedFileRewriter.MAX_FILE_SIZE_BYTES, Long.toString(Long.MAX_VALUE))\n             .option(SizeBasedDataRewriter.DELETE_FILE_THRESHOLD, \"2\")\n             .execute();\n-    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.rewrittenDataFilesCount());\n+    assertThat(result.rewrittenDataFilesCount()).isEqualTo(2);\n     assertThat(result.rewrittenBytesCount()).isGreaterThan(0L).isLessThan(dataSizeBefore);\n \n     List<Object[]> actualRecords = currentData();\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n-    Assert.assertEquals(\"7 rows are removed\", total - 7, actualRecords.size());\n+    assertThat(actualRecords).hasSize(total - 7);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveDangledEqualityDeletesPartitionEvolution() {\n     Table table =\n         TABLES.create(\n@@ -398,7 +411,7 @@ public void testRemoveDangledEqualityDeletesPartitionEvolution() {\n     shouldHaveFiles(table, 5);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveDangledPositionDeletesPartitionEvolution() {\n     Table table =\n         TABLES.create(\n@@ -437,11 +450,11 @@ public void testRemoveDangledPositionDeletesPartitionEvolution() {\n         .containsExactly(1, 2, 1);\n     shouldHaveMinSequenceNumberInPartition(table, \"data_file.partition.c1 == 1\", 3);\n     shouldHaveSnapshots(table, 5);\n-    assertThat(table.currentSnapshot().summary().get(\"total-position-deletes\")).isEqualTo(\"0\");\n+    assertThat(table.currentSnapshot().summary()).containsEntry(\"total-position-deletes\", \"0\");\n     assertEquals(\"Rows must match\", expectedRecords, currentData());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackWithDeleteAllData() {\n     Map<String, String> options = Maps.newHashMap();\n     options.put(TableProperties.FORMAT_VERSION, \"2\");\n@@ -466,26 +479,25 @@ public void testBinPackWithDeleteAllData() {\n             .rewriteDataFiles(table)\n             .option(SizeBasedDataRewriter.DELETE_FILE_THRESHOLD, \"1\")\n             .execute();\n-    Assert.assertEquals(\"Action should rewrite 1 data files\", 1, result.rewrittenDataFilesCount());\n+    assertThat(result.rewrittenDataFilesCount()).as(\"Action should rewrite 1 data files\").isOne();\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     List<Object[]> actualRecords = currentData();\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n-    Assert.assertEquals(\n-        \"Data manifest should not have existing data file\",\n-        0,\n-        (long) table.currentSnapshot().dataManifests(table.io()).get(0).existingFilesCount());\n-    Assert.assertEquals(\n-        \"Data manifest should have 1 delete data file\",\n-        1L,\n-        (long) table.currentSnapshot().dataManifests(table.io()).get(0).deletedFilesCount());\n-    Assert.assertEquals(\n-        \"Delete manifest added row count should equal total count\",\n-        total,\n-        (long) table.currentSnapshot().deleteManifests(table.io()).get(0).addedRowsCount());\n-  }\n-\n-  @Test\n+    assertThat(table.currentSnapshot().dataManifests(table.io()).get(0).existingFilesCount())\n+        .as(\"Data manifest should not have existing data file\")\n+        .isZero();\n+\n+    assertThat((long) table.currentSnapshot().dataManifests(table.io()).get(0).deletedFilesCount())\n+        .as(\"Data manifest should have 1 delete data file\")\n+        .isEqualTo(1L);\n+\n+    assertThat(table.currentSnapshot().deleteManifests(table.io()).get(0).addedRowsCount())\n+        .as(\"Delete manifest added row count should equal total count\")\n+        .isEqualTo(total);\n+  }\n+\n+  @TestTemplate\n   public void testBinPackWithStartingSequenceNumber() {\n     Table table = createTablePartitioned(4, 2);\n     shouldHaveFiles(table, 8);\n@@ -497,8 +509,10 @@ public void testBinPackWithStartingSequenceNumber() {\n \n     Result result =\n         basicRewrite(table).option(RewriteDataFiles.USE_STARTING_SEQUENCE_NUMBER, \"true\").execute();\n-    Assert.assertEquals(\"Action should rewrite 8 data files\", 8, result.rewrittenDataFilesCount());\n-    Assert.assertEquals(\"Action should add 4 data file\", 4, result.addedDataFilesCount());\n+    assertThat(result.rewrittenDataFilesCount())\n+        .as(\"Action should rewrite 8 data files\")\n+        .isEqualTo(8);\n+    assertThat(result.addedDataFilesCount()).as(\"Action should add 4 data files\").isEqualTo(4);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     shouldHaveFiles(table, 4);\n@@ -506,20 +520,21 @@ public void testBinPackWithStartingSequenceNumber() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n \n     table.refresh();\n-    Assert.assertTrue(\n-        \"Table sequence number should be incremented\",\n-        oldSequenceNumber < table.currentSnapshot().sequenceNumber());\n+    assertThat(table.currentSnapshot().sequenceNumber())\n+        .as(\"Table sequence number should be incremented\")\n+        .isGreaterThan(oldSequenceNumber);\n \n     Dataset<Row> rows = SparkTableUtil.loadMetadataTable(spark, table, MetadataTableType.ENTRIES);\n     for (Row row : rows.collectAsList()) {\n       if (row.getInt(0) == 1) {\n-        Assert.assertEquals(\n-            \"Expect old sequence number for added entries\", oldSequenceNumber, row.getLong(2));\n+        assertThat(row.getLong(2))\n+            .as(\"Expect old sequence number for added entries\")\n+            .isEqualTo(oldSequenceNumber);\n       }\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackWithStartingSequenceNumberV1Compatibility() {\n     Map<String, String> properties = ImmutableMap.of(TableProperties.FORMAT_VERSION, \"1\");\n     Table table = createTablePartitioned(4, 2, SCALE, properties);\n@@ -527,13 +542,15 @@ public void testBinPackWithStartingSequenceNumberV1Compatibility() {\n     List<Object[]> expectedRecords = currentData();\n     table.refresh();\n     long oldSequenceNumber = table.currentSnapshot().sequenceNumber();\n-    Assert.assertEquals(\"Table sequence number should be 0\", 0, oldSequenceNumber);\n+    assertThat(oldSequenceNumber).as(\"Table sequence number should be 0\").isZero();\n     long dataSizeBefore = testDataSize(table);\n \n     Result result =\n         basicRewrite(table).option(RewriteDataFiles.USE_STARTING_SEQUENCE_NUMBER, \"true\").execute();\n-    Assert.assertEquals(\"Action should rewrite 8 data files\", 8, result.rewrittenDataFilesCount());\n-    Assert.assertEquals(\"Action should add 4 data file\", 4, result.addedDataFilesCount());\n+    assertThat(result.rewrittenDataFilesCount())\n+        .as(\"Action should rewrite 8 data files\")\n+        .isEqualTo(8);\n+    assertThat(result.addedDataFilesCount()).as(\"Action should add 4 data files\").isEqualTo(4);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     shouldHaveFiles(table, 4);\n@@ -541,19 +558,19 @@ public void testBinPackWithStartingSequenceNumberV1Compatibility() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n \n     table.refresh();\n-    Assert.assertEquals(\n-        \"Table sequence number should still be 0\",\n-        oldSequenceNumber,\n-        table.currentSnapshot().sequenceNumber());\n+    assertThat(table.currentSnapshot().sequenceNumber())\n+        .as(\"Table sequence number should still be 0\")\n+        .isEqualTo(oldSequenceNumber);\n \n     Dataset<Row> rows = SparkTableUtil.loadMetadataTable(spark, table, MetadataTableType.ENTRIES);\n     for (Row row : rows.collectAsList()) {\n-      Assert.assertEquals(\n-          \"Expect sequence number 0 for all entries\", oldSequenceNumber, row.getLong(2));\n+      assertThat(row.getLong(2))\n+          .as(\"Expect sequence number 0 for all entries\")\n+          .isEqualTo(oldSequenceNumber);\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteLargeTableHasResiduals() {\n     PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).build();\n     Map<String, String> options = Maps.newHashMap();\n@@ -575,15 +592,19 @@ public void testRewriteLargeTableHasResiduals() {\n     CloseableIterable<FileScanTask> tasks =\n         table.newScan().ignoreResiduals().filter(Expressions.equal(\"c3\", \"0\")).planFiles();\n     for (FileScanTask task : tasks) {\n-      Assert.assertEquals(\"Residuals must be ignored\", Expressions.alwaysTrue(), task.residual());\n+      assertThat(task.residual())\n+          .as(\"Residuals must be ignored\")\n+          .isEqualTo(Expressions.alwaysTrue());\n     }\n \n     shouldHaveFiles(table, 2);\n \n     long dataSizeBefore = testDataSize(table);\n     Result result = basicRewrite(table).filter(Expressions.equal(\"c3\", \"0\")).execute();\n-    Assert.assertEquals(\"Action should rewrite 2 data files\", 2, result.rewrittenDataFilesCount());\n-    Assert.assertEquals(\"Action should add 1 data file\", 1, result.addedDataFilesCount());\n+    assertThat(result.rewrittenDataFilesCount())\n+        .as(\"Action should rewrite 2 data files\")\n+        .isEqualTo(2);\n+    assertThat(result.addedDataFilesCount()).as(\"Action should add 1 data file\").isOne();\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     List<Object[]> actualRecords = currentData();\n@@ -591,7 +612,7 @@ public void testRewriteLargeTableHasResiduals() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackSplitLargeFile() {\n     Table table = createTable(1);\n     shouldHaveFiles(table, 1);\n@@ -606,8 +627,8 @@ public void testBinPackSplitLargeFile() {\n             .option(SizeBasedFileRewriter.MAX_FILE_SIZE_BYTES, Long.toString(targetSize * 2 - 2000))\n             .execute();\n \n-    Assert.assertEquals(\"Action should delete 1 data files\", 1, result.rewrittenDataFilesCount());\n-    Assert.assertEquals(\"Action should add 2 data files\", 2, result.addedDataFilesCount());\n+    assertThat(result.rewrittenDataFilesCount()).as(\"Action should delete 1 data files\").isOne();\n+    assertThat(result.addedDataFilesCount()).as(\"Action should add 2 data files\").isEqualTo(2);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     shouldHaveFiles(table, 2);\n@@ -616,7 +637,7 @@ public void testBinPackSplitLargeFile() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackCombineMixedFiles() {\n     Table table = createTable(1); // 400000\n     shouldHaveFiles(table, 1);\n@@ -638,10 +659,12 @@ public void testBinPackCombineMixedFiles() {\n             .option(SizeBasedFileRewriter.MIN_FILE_SIZE_BYTES, Integer.toString(targetSize - 1000))\n             .execute();\n \n-    Assert.assertEquals(\"Action should delete 3 data files\", 3, result.rewrittenDataFilesCount());\n+    assertThat(result.rewrittenDataFilesCount())\n+        .as(\"Action should delete 3 data files\")\n+        .isEqualTo(3);\n     // Should Split the big files into 3 pieces, one of which should be combined with the two\n     // smaller files\n-    Assert.assertEquals(\"Action should add 3 data files\", 3, result.addedDataFilesCount());\n+    assertThat(result.addedDataFilesCount()).as(\"Action should add 3 data files\").isEqualTo(3);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     shouldHaveFiles(table, 3);\n@@ -650,7 +673,7 @@ public void testBinPackCombineMixedFiles() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackCombineMediumFiles() {\n     Table table = createTable(4);\n     shouldHaveFiles(table, 4);\n@@ -671,8 +694,10 @@ public void testBinPackCombineMediumFiles() {\n                 Integer.toString(targetSize - 100)) // All files too small\n             .execute();\n \n-    Assert.assertEquals(\"Action should delete 4 data files\", 4, result.rewrittenDataFilesCount());\n-    Assert.assertEquals(\"Action should add 3 data files\", 3, result.addedDataFilesCount());\n+    assertThat(result.rewrittenDataFilesCount())\n+        .as(\"Action should delete 4 data files\")\n+        .isEqualTo(4);\n+    assertThat(result.addedDataFilesCount()).as(\"Action should add 3 data files\").isEqualTo(3);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     shouldHaveFiles(table, 3);\n@@ -681,7 +706,7 @@ public void testBinPackCombineMediumFiles() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartialProgressEnabled() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -700,7 +725,7 @@ public void testPartialProgressEnabled() {\n             .option(RewriteDataFiles.PARTIAL_PROGRESS_MAX_COMMITS, \"10\")\n             .execute();\n \n-    Assert.assertEquals(\"Should have 10 fileGroups\", result.rewriteResults().size(), 10);\n+    assertThat(result.rewriteResults()).as(\"Should have 10 fileGroups\").hasSize(10);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     table.refresh();\n@@ -712,7 +737,7 @@ public void testPartialProgressEnabled() {\n     assertEquals(\"We shouldn't have changed the data\", originalData, postRewriteData);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMultipleGroups() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -728,7 +753,7 @@ public void testMultipleGroups() {\n             .option(SizeBasedFileRewriter.MIN_INPUT_FILES, \"1\")\n             .execute();\n \n-    Assert.assertEquals(\"Should have 10 fileGroups\", result.rewriteResults().size(), 10);\n+    assertThat(result.rewriteResults()).as(\"Should have 10 fileGroups\").hasSize(10);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     table.refresh();\n@@ -740,7 +765,7 @@ public void testMultipleGroups() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartialProgressMaxCommits() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -757,7 +782,7 @@ public void testPartialProgressMaxCommits() {\n             .option(RewriteDataFiles.PARTIAL_PROGRESS_MAX_COMMITS, \"3\")\n             .execute();\n \n-    Assert.assertEquals(\"Should have 10 fileGroups\", result.rewriteResults().size(), 10);\n+    assertThat(result.rewriteResults()).as(\"Should have 10 fileGroups\").hasSize(10);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     table.refresh();\n@@ -769,7 +794,7 @@ public void testPartialProgressMaxCommits() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSingleCommitWithRewriteFailure() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -803,7 +828,7 @@ public void testSingleCommitWithRewriteFailure() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSingleCommitWithCommitFailure() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -837,7 +862,7 @@ public void testSingleCommitWithCommitFailure() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCommitFailsWithUncleanableFailure() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -871,7 +896,7 @@ public void testCommitFailsWithUncleanableFailure() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testParallelSingleCommitWithRewriteFailure() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -906,7 +931,7 @@ public void testParallelSingleCommitWithRewriteFailure() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartialProgressWithRewriteFailure() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -948,7 +973,7 @@ public void testPartialProgressWithRewriteFailure() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testParallelPartialProgressWithRewriteFailure() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -991,7 +1016,7 @@ public void testParallelPartialProgressWithRewriteFailure() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testParallelPartialProgressWithCommitFailure() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -1021,8 +1046,8 @@ public void testParallelPartialProgressWithCommitFailure() {\n \n     RewriteDataFiles.Result result = spyRewrite.execute();\n \n-    // Commit 1: 4/4 + Commit 2 failed 0/4 + Commit 3: 2/2 == 6 out of 10 total groups comitted\n-    Assert.assertEquals(\"Should have 6 fileGroups\", 6, result.rewriteResults().size());\n+    // Commit 1: 4/4 + Commit 2 failed 0/4 + Commit 3: 2/2 == 6 out of 10 total groups committed\n+    assertThat(result.rewriteResults()).as(\"Should have 6 fileGroups\").hasSize(6);\n     assertThat(result.rewrittenBytesCount()).isGreaterThan(0L).isLessThan(dataSizeBefore);\n \n     table.refresh();\n@@ -1036,7 +1061,7 @@ public void testParallelPartialProgressWithCommitFailure() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidOptions() {\n     Table table = createTable(20);\n \n@@ -1080,7 +1105,7 @@ public void testInvalidOptions() {\n         .hasMessageContaining(\"requires enabling Iceberg Spark session extensions\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortMultipleGroups() {\n     Table table = createTable(20);\n     shouldHaveFiles(table, 20);\n@@ -1100,7 +1125,7 @@ public void testSortMultipleGroups() {\n                 RewriteDataFiles.MAX_FILE_GROUP_SIZE_BYTES, Integer.toString(fileSize * 2 + 1000))\n             .execute();\n \n-    Assert.assertEquals(\"Should have 10 fileGroups\", result.rewriteResults().size(), 10);\n+    assertThat(result.rewriteResults()).as(\"Should have 10 fileGroups\").hasSize(10);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     table.refresh();\n@@ -1112,7 +1137,7 @@ public void testSortMultipleGroups() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSimpleSort() {\n     Table table = createTable(20);\n     shouldHaveFiles(table, 20);\n@@ -1131,7 +1156,7 @@ public void testSimpleSort() {\n                 RewriteDataFiles.TARGET_FILE_SIZE_BYTES, Integer.toString(averageFileSize(table)))\n             .execute();\n \n-    Assert.assertEquals(\"Should have 1 fileGroups\", result.rewriteResults().size(), 1);\n+    assertThat(result.rewriteResults()).as(\"Should have 1 fileGroups\").hasSize(1);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     table.refresh();\n@@ -1145,7 +1170,7 @@ public void testSimpleSort() {\n     shouldHaveLastCommitSorted(table, \"c2\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortAfterPartitionChange() {\n     Table table = createTable(20);\n     shouldHaveFiles(table, 20);\n@@ -1165,10 +1190,9 @@ public void testSortAfterPartitionChange() {\n                 RewriteDataFiles.TARGET_FILE_SIZE_BYTES, Integer.toString(averageFileSize(table)))\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Should have 1 fileGroup because all files were not correctly partitioned\",\n-        result.rewriteResults().size(),\n-        1);\n+    assertThat(result.rewriteResults())\n+        .as(\"Should have 1 fileGroups because all files were not correctly partitioned\")\n+        .hasSize(1);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     table.refresh();\n@@ -1182,7 +1206,7 @@ public void testSortAfterPartitionChange() {\n     shouldHaveLastCommitSorted(table, \"c2\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortCustomSortOrder() {\n     Table table = createTable(20);\n     shouldHaveLastCommitUnsorted(table, \"c2\");\n@@ -1199,7 +1223,7 @@ public void testSortCustomSortOrder() {\n                 RewriteDataFiles.TARGET_FILE_SIZE_BYTES, Integer.toString(averageFileSize(table)))\n             .execute();\n \n-    Assert.assertEquals(\"Should have 1 fileGroups\", result.rewriteResults().size(), 1);\n+    assertThat(result.rewriteResults()).as(\"Should have 1 fileGroups\").hasSize(1);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     table.refresh();\n@@ -1213,7 +1237,7 @@ public void testSortCustomSortOrder() {\n     shouldHaveLastCommitSorted(table, \"c2\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortCustomSortOrderRequiresRepartition() {\n     int partitions = 4;\n     Table table = createTable();\n@@ -1238,7 +1262,7 @@ public void testSortCustomSortOrderRequiresRepartition() {\n                 Integer.toString(averageFileSize(table) / partitions))\n             .execute();\n \n-    Assert.assertEquals(\"Should have 1 fileGroups\", result.rewriteResults().size(), 1);\n+    assertThat(result.rewriteResults()).as(\"Should have 1 fileGroups\").hasSize(1);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n \n     table.refresh();\n@@ -1253,7 +1277,7 @@ public void testSortCustomSortOrderRequiresRepartition() {\n     shouldHaveLastCommitSorted(table, \"c3\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAutoSortShuffleOutput() {\n     Table table = createTable(20);\n     shouldHaveLastCommitUnsorted(table, \"c2\");\n@@ -1275,11 +1299,12 @@ public void testAutoSortShuffleOutput() {\n             .option(SizeBasedFileRewriter.MIN_INPUT_FILES, \"1\")\n             .execute();\n \n-    Assert.assertEquals(\"Should have 1 fileGroups\", result.rewriteResults().size(), 1);\n+    assertThat(result.rewriteResults()).as(\"Should have 1 fileGroups\").hasSize(1);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n-    Assert.assertTrue(\n-        \"Should have written 40+ files\",\n-        Iterables.size(table.currentSnapshot().addedDataFiles(table.io())) >= 40);\n+    assertThat(result.rewriteResults()).as(\"Should have 1 fileGroups\").hasSize(1);\n+    assertThat(table.currentSnapshot().addedDataFiles(table.io()))\n+        .as(\"Should have written 40+ files\")\n+        .hasSizeGreaterThanOrEqualTo(40);\n \n     table.refresh();\n \n@@ -1292,7 +1317,7 @@ public void testAutoSortShuffleOutput() {\n     shouldHaveLastCommitSorted(table, \"c2\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCommitStateUnknownException() {\n     Table table = createTable(20);\n     shouldHaveFiles(table, 20);\n@@ -1324,7 +1349,7 @@ public void testCommitStateUnknownException() {\n     shouldHaveSnapshots(table, 2); // Commit actually Succeeded\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testZOrderSort() {\n     int originalFiles = 20;\n     Table table = createTable(originalFiles);\n@@ -1337,8 +1362,8 @@ public void testZOrderSort() {\n     double originalFilesC2C3 =\n         percentFilesRequired(table, new String[] {\"c2\", \"c3\"}, new String[] {\"foo23\", \"bar23\"});\n \n-    Assert.assertTrue(\"Should require all files to scan c2\", originalFilesC2 > 0.99);\n-    Assert.assertTrue(\"Should require all files to scan c3\", originalFilesC3 > 0.99);\n+    assertThat(originalFilesC2).as(\"Should require all files to scan c2\").isGreaterThan(0.99);\n+    assertThat(originalFilesC3).as(\"Should require all files to scan c3\").isGreaterThan(0.99);\n \n     long dataSizeBefore = testDataSize(table);\n     RewriteDataFiles.Result result =\n@@ -1354,10 +1379,11 @@ public void testZOrderSort() {\n             .option(SizeBasedFileRewriter.MIN_INPUT_FILES, \"1\")\n             .execute();\n \n-    Assert.assertEquals(\"Should have 1 fileGroups\", 1, result.rewriteResults().size());\n+    assertThat(result.rewriteResults()).as(\"Should have 1 fileGroups\").hasSize(1);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n-    int zOrderedFilesTotal = Iterables.size(table.currentSnapshot().addedDataFiles(table.io()));\n-    Assert.assertTrue(\"Should have written 40+ files\", zOrderedFilesTotal >= 40);\n+    assertThat(table.currentSnapshot().addedDataFiles(table.io()))\n+        .as(\"Should have written 40+ files\")\n+        .hasSizeGreaterThanOrEqualTo(40);\n \n     table.refresh();\n \n@@ -1372,18 +1398,18 @@ public void testZOrderSort() {\n     double filesScannedC2C3 =\n         percentFilesRequired(table, new String[] {\"c2\", \"c3\"}, new String[] {\"foo23\", \"bar23\"});\n \n-    Assert.assertTrue(\n-        \"Should have reduced the number of files required for c2\",\n-        filesScannedC2 < originalFilesC2);\n-    Assert.assertTrue(\n-        \"Should have reduced the number of files required for c3\",\n-        filesScannedC3 < originalFilesC3);\n-    Assert.assertTrue(\n-        \"Should have reduced the number of files required for a c2,c3 predicate\",\n-        filesScannedC2C3 < originalFilesC2C3);\n+    assertThat(originalFilesC2)\n+        .as(\"Should have reduced the number of files required for c2\")\n+        .isGreaterThan(filesScannedC2);\n+    assertThat(originalFilesC3)\n+        .as(\"Should have reduced the number of files required for c3\")\n+        .isGreaterThan(filesScannedC3);\n+    assertThat(originalFilesC2C3)\n+        .as(\"Should have reduced the number of files required for c2,c3 predicate\")\n+        .isGreaterThan(filesScannedC2C3);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testZOrderAllTypesSort() {\n     Table table = createTypeTestTable();\n     shouldHaveFiles(table, 10);\n@@ -1410,10 +1436,12 @@ public void testZOrderAllTypesSort() {\n             .option(SizeBasedFileRewriter.REWRITE_ALL, \"true\")\n             .execute();\n \n-    Assert.assertEquals(\"Should have 1 fileGroups\", 1, result.rewriteResults().size());\n+    assertThat(result.rewriteResults()).as(\"Should have 1 fileGroups\").hasSize(1);\n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n     int zOrderedFilesTotal = Iterables.size(table.currentSnapshot().addedDataFiles(table.io()));\n-    Assert.assertEquals(\"Should have written 1 file\", 1, zOrderedFilesTotal);\n+    assertThat(table.currentSnapshot().addedDataFiles(table.io()))\n+        .as(\"Should have written 1 file\")\n+        .hasSize(1);\n \n     table.refresh();\n \n@@ -1426,7 +1454,7 @@ public void testZOrderAllTypesSort() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidAPIUsage() {\n     Table table = createTable(1);\n \n@@ -1445,7 +1473,7 @@ public void testInvalidAPIUsage() {\n         .hasMessage(\"Must use only one rewriter type (bin-pack, sort, zorder)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteJobOrderBytesAsc() {\n     Table table = createTablePartitioned(4, 2);\n     writeRecords(1, SCALE, 1);\n@@ -1472,12 +1500,12 @@ public void testRewriteJobOrderBytesAsc() {\n             .collect(Collectors.toList());\n \n     expected.sort(Comparator.naturalOrder());\n-    Assert.assertEquals(\"Size in bytes order should be ascending\", actual, expected);\n+    assertThat(actual).as(\"Size in bytes order should be ascending\").isEqualTo(expected);\n     Collections.reverse(expected);\n-    Assert.assertNotEquals(\"Size in bytes order should not be descending\", actual, expected);\n+    assertThat(actual).as(\"Size in bytes order should not be descending\").isNotEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteJobOrderBytesDesc() {\n     Table table = createTablePartitioned(4, 2);\n     writeRecords(1, SCALE, 1);\n@@ -1504,12 +1532,12 @@ public void testRewriteJobOrderBytesDesc() {\n             .collect(Collectors.toList());\n \n     expected.sort(Comparator.reverseOrder());\n-    Assert.assertEquals(\"Size in bytes order should be descending\", actual, expected);\n+    assertThat(actual).as(\"Size in bytes order should be descending\").isEqualTo(expected);\n     Collections.reverse(expected);\n-    Assert.assertNotEquals(\"Size in bytes order should not be ascending\", actual, expected);\n+    assertThat(actual).as(\"Size in bytes order should not be ascending\").isNotEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteJobOrderFilesAsc() {\n     Table table = createTablePartitioned(4, 2);\n     writeRecords(1, SCALE, 1);\n@@ -1536,12 +1564,12 @@ public void testRewriteJobOrderFilesAsc() {\n             .collect(Collectors.toList());\n \n     expected.sort(Comparator.naturalOrder());\n-    Assert.assertEquals(\"Number of files order should be ascending\", actual, expected);\n+    assertThat(actual).as(\"Number of files order should be ascending\").isEqualTo(expected);\n     Collections.reverse(expected);\n-    Assert.assertNotEquals(\"Number of files order should not be descending\", actual, expected);\n+    assertThat(actual).as(\"Number of files order should not be descending\").isNotEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteJobOrderFilesDesc() {\n     Table table = createTablePartitioned(4, 2);\n     writeRecords(1, SCALE, 1);\n@@ -1568,12 +1596,12 @@ public void testRewriteJobOrderFilesDesc() {\n             .collect(Collectors.toList());\n \n     expected.sort(Comparator.reverseOrder());\n-    Assert.assertEquals(\"Number of files order should be descending\", actual, expected);\n+    assertThat(actual).as(\"Number of files order should be descending\").isEqualTo(expected);\n     Collections.reverse(expected);\n-    Assert.assertNotEquals(\"Number of files order should not be ascending\", actual, expected);\n+    assertThat(actual).as(\"Number of files order should not be ascending\").isNotEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackRewriterWithSpecificUnparitionedOutputSpec() {\n     Table table = createTable(10);\n     shouldHaveFiles(table, 10);\n@@ -1591,11 +1619,11 @@ public void testBinPackRewriterWithSpecificUnparitionedOutputSpec() {\n             .execute();\n \n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n-    assertThat(currentData().size()).isEqualTo(count);\n+    assertThat(currentData()).hasSize((int) count);\n     shouldRewriteDataFilesWithPartitionSpec(table, outputSpecId);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackRewriterWithSpecificOutputSpec() {\n     Table table = createTable(10);\n     shouldHaveFiles(table, 10);\n@@ -1614,11 +1642,11 @@ public void testBinPackRewriterWithSpecificOutputSpec() {\n             .execute();\n \n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n-    assertThat(currentData().size()).isEqualTo(count);\n+    assertThat(currentData()).hasSize((int) count);\n     shouldRewriteDataFilesWithPartitionSpec(table, outputSpecId);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinpackRewriteWithInvalidOutputSpecId() {\n     Table table = createTable(10);\n     shouldHaveFiles(table, 10);\n@@ -1634,7 +1662,7 @@ public void testBinpackRewriteWithInvalidOutputSpecId() {\n             \"Cannot use output spec id 1234 because the table does not contain a reference to this spec-id.\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortRewriterWithSpecificOutputSpecId() {\n     Table table = createTable(10);\n     shouldHaveFiles(table, 10);\n@@ -1653,11 +1681,11 @@ public void testSortRewriterWithSpecificOutputSpecId() {\n             .execute();\n \n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n-    assertThat(currentData().size()).isEqualTo(count);\n+    assertThat(currentData()).hasSize((int) count);\n     shouldRewriteDataFilesWithPartitionSpec(table, outputSpecId);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testZOrderRewriteWithSpecificOutputSpecId() {\n     Table table = createTable(10);\n     shouldHaveFiles(table, 10);\n@@ -1676,7 +1704,7 @@ public void testZOrderRewriteWithSpecificOutputSpecId() {\n             .execute();\n \n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n-    assertThat(currentData().size()).isEqualTo(count);\n+    assertThat(currentData()).hasSize((int) count);\n     shouldRewriteDataFilesWithPartitionSpec(table, outputSpecId);\n   }\n \n@@ -1718,13 +1746,15 @@ protected long testDataSize(Table table) {\n   protected void shouldHaveMultipleFiles(Table table) {\n     table.refresh();\n     int numFiles = Iterables.size(table.newScan().planFiles());\n-    Assert.assertTrue(String.format(\"Should have multiple files, had %d\", numFiles), numFiles > 1);\n+    assertThat(numFiles)\n+        .as(String.format(\"Should have multiple files, had %d\", numFiles))\n+        .isGreaterThan(1);\n   }\n \n   protected void shouldHaveFiles(Table table, int numExpected) {\n     table.refresh();\n     int numFiles = Iterables.size(table.newScan().planFiles());\n-    Assert.assertEquals(\"Did not have the expected number of files\", numExpected, numFiles);\n+    assertThat(numFiles).as(\"Did not have the expected number of files\").isEqualTo(numExpected);\n   }\n \n   protected long shouldHaveMinSequenceNumberInPartition(\n@@ -1744,20 +1774,20 @@ protected long shouldHaveMinSequenceNumberInPartition(\n \n   protected void shouldHaveSnapshots(Table table, int expectedSnapshots) {\n     table.refresh();\n-    int actualSnapshots = Iterables.size(table.snapshots());\n-    Assert.assertEquals(\n-        \"Table did not have the expected number of snapshots\", expectedSnapshots, actualSnapshots);\n+    assertThat(table.snapshots())\n+        .as(\"Table did not have the expected number of snapshots\")\n+        .hasSize(expectedSnapshots);\n   }\n \n   protected void shouldHaveNoOrphans(Table table) {\n-    Assert.assertEquals(\n-        \"Should not have found any orphan files\",\n-        ImmutableList.of(),\n-        actions()\n-            .deleteOrphanFiles(table)\n-            .olderThan(System.currentTimeMillis())\n-            .execute()\n-            .orphanFileLocations());\n+    assertThat(\n+            actions()\n+                .deleteOrphanFiles(table)\n+                .olderThan(System.currentTimeMillis())\n+                .execute()\n+                .orphanFileLocations())\n+        .as(\"Should not have found any orphan files\")\n+        .isEmpty();\n   }\n \n   protected void shouldHaveOrphans(Table table) {\n@@ -1772,20 +1802,19 @@ protected void shouldHaveOrphans(Table table) {\n   }\n \n   protected void shouldHaveACleanCache(Table table) {\n-    Assert.assertEquals(\n-        \"Should not have any entries in cache\", ImmutableSet.of(), cacheContents(table));\n+    assertThat(cacheContents(table)).as(\"Should not have any entries in cache\").isEmpty();\n   }\n \n   protected <T> void shouldHaveLastCommitSorted(Table table, String column) {\n     List<Pair<Pair<T, T>, Pair<T, T>>> overlappingFiles = checkForOverlappingFiles(table, column);\n \n-    Assert.assertEquals(\"Found overlapping files\", Collections.emptyList(), overlappingFiles);\n+    assertThat(overlappingFiles).as(\"Found overlapping files\").isEmpty();\n   }\n \n   protected <T> void shouldHaveLastCommitUnsorted(Table table, String column) {\n     List<Pair<Pair<T, T>, Pair<T, T>>> overlappingFiles = checkForOverlappingFiles(table, column);\n \n-    Assert.assertNotEquals(\"Found no overlapping files\", Collections.emptyList(), overlappingFiles);\n+    assertThat(overlappingFiles).as(\"Found no overlapping files\").isNotEmpty();\n   }\n \n   private <T> Pair<T, T> boundsOf(DataFile file, NestedField field, Class<T> javaClass) {\n@@ -1864,7 +1893,7 @@ protected Table createTable() {\n         .updateProperties()\n         .set(TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES, Integer.toString(20 * 1024))\n         .commit();\n-    Assert.assertNull(\"Table must be empty\", table.currentSnapshot());\n+    assertThat(table.currentSnapshot()).as(\"Table must be empty\").isNull();\n     return table;\n   }\n \n@@ -1884,7 +1913,7 @@ protected Table createTablePartitioned(\n       int partitions, int files, int numRecords, Map<String, String> options) {\n     PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").truncate(\"c2\", 2).build();\n     Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n-    Assert.assertNull(\"Table must be empty\", table.currentSnapshot());\n+    assertThat(table.currentSnapshot()).as(\"Table must be empty\").isNull();\n \n     writeRecords(files, numRecords, partitions);\n     return table;\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\nindex bdc830e94610..8a98a1170491 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\n@@ -33,6 +33,7 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n import java.util.UUID;\n@@ -46,6 +47,9 @@\n import org.apache.iceberg.ManifestFile;\n import org.apache.iceberg.ManifestFiles;\n import org.apache.iceberg.ManifestWriter;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.RowDelta;\n import org.apache.iceberg.Schema;\n@@ -66,8 +70,8 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.spark.SparkTableUtil;\n-import org.apache.iceberg.spark.SparkTestBase;\n import org.apache.iceberg.spark.SparkWriteOptions;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.spark.source.ThreeColumnRecord;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.CharSequenceSet;\n@@ -76,17 +80,13 @@\n import org.apache.spark.sql.Encoders;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.TableIdentifier;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n-import org.junit.runner.RunWith;\n-import org.junit.runners.Parameterized;\n-import org.junit.runners.Parameterized.Parameters;\n-\n-@RunWith(Parameterized.class)\n-public class TestRewriteManifestsAction extends SparkTestBase {\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRewriteManifestsAction extends TestBase {\n \n   private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n   private static final Schema SCHEMA =\n@@ -98,36 +98,37 @@ public class TestRewriteManifestsAction extends SparkTestBase {\n   @Parameters(name = \"snapshotIdInheritanceEnabled = {0}, useCaching = {1}, formatVersion = {2}\")\n   public static Object[] parameters() {\n     return new Object[][] {\n-      new Object[] {\"true\", \"true\", 1},\n-      new Object[] {\"false\", \"true\", 1},\n-      new Object[] {\"true\", \"false\", 2},\n-      new Object[] {\"false\", \"false\", 2}\n+      new Object[] {\"true\", \"true\", false, 1},\n+      new Object[] {\"false\", \"true\", true, 1},\n+      new Object[] {\"true\", \"false\", false, 2},\n+      new Object[] {\"false\", \"false\", false, 2},\n+      new Object[] {\"true\", \"false\", false, 3},\n+      new Object[] {\"false\", \"false\", false, 3}\n     };\n   }\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @Parameter private String snapshotIdInheritanceEnabled;\n+\n+  @Parameter(index = 1)\n+  private String useCaching;\n+\n+  @Parameter(index = 2)\n+  private boolean shouldStageManifests;\n+\n+  @Parameter(index = 3)\n+  private int formatVersion;\n \n-  private final String snapshotIdInheritanceEnabled;\n-  private final String useCaching;\n-  private final int formatVersion;\n-  private final boolean shouldStageManifests;\n   private String tableLocation = null;\n \n-  public TestRewriteManifestsAction(\n-      String snapshotIdInheritanceEnabled, String useCaching, int formatVersion) {\n-    this.snapshotIdInheritanceEnabled = snapshotIdInheritanceEnabled;\n-    this.useCaching = useCaching;\n-    this.formatVersion = formatVersion;\n-    this.shouldStageManifests = formatVersion == 1 && snapshotIdInheritanceEnabled.equals(\"false\");\n-  }\n+  @TempDir private Path temp;\n+  @TempDir private File tableDir;\n \n-  @Before\n+  @BeforeEach\n   public void setupTableLocation() throws Exception {\n-    File tableDir = temp.newFolder();\n     this.tableLocation = tableDir.toURI().toString();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteManifestsEmptyTable() throws IOException {\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n     Map<String, String> options = Maps.newHashMap();\n@@ -135,7 +136,7 @@ public void testRewriteManifestsEmptyTable() throws IOException {\n     options.put(TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED, snapshotIdInheritanceEnabled);\n     Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n \n-    Assert.assertNull(\"Table must be empty\", table.currentSnapshot());\n+    assertThat(table.currentSnapshot()).as(\"Table must be empty\").isNull();\n \n     SparkActions actions = SparkActions.get();\n \n@@ -143,13 +144,13 @@ public void testRewriteManifestsEmptyTable() throws IOException {\n         .rewriteManifests(table)\n         .rewriteIf(manifest -> true)\n         .option(RewriteManifestsSparkAction.USE_CACHING, useCaching)\n-        .stagingLocation(temp.newFolder().toString())\n+        .stagingLocation(java.nio.file.Files.createTempDirectory(temp, \"junit\").toString())\n         .execute();\n \n-    Assert.assertNull(\"Table must stay empty\", table.currentSnapshot());\n+    assertThat(table.currentSnapshot()).as(\"Table must stay empty\").isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteSmallManifestsNonPartitionedTable() {\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n     Map<String, String> options = Maps.newHashMap();\n@@ -171,7 +172,7 @@ public void testRewriteSmallManifestsNonPartitionedTable() {\n     table.refresh();\n \n     List<ManifestFile> manifests = table.currentSnapshot().allManifests(table.io());\n-    Assert.assertEquals(\"Should have 2 manifests before rewrite\", 2, manifests.size());\n+    assertThat(manifests).as(\"Should have 2 manifests before rewrite\").hasSize(2);\n \n     SparkActions actions = SparkActions.get();\n \n@@ -182,20 +183,18 @@ public void testRewriteSmallManifestsNonPartitionedTable() {\n             .option(RewriteManifestsSparkAction.USE_CACHING, useCaching)\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Action should rewrite 2 manifests\", 2, Iterables.size(result.rewrittenManifests()));\n-    Assert.assertEquals(\n-        \"Action should add 1 manifests\", 1, Iterables.size(result.addedManifests()));\n+    assertThat(result.rewrittenManifests()).as(\"Action should rewrite 2 manifests\").hasSize(2);\n+    assertThat(result.addedManifests()).as(\"Action should add 1 manifests\").hasSize(1);\n     assertManifestsLocation(result.addedManifests());\n \n     table.refresh();\n \n     List<ManifestFile> newManifests = table.currentSnapshot().allManifests(table.io());\n-    Assert.assertEquals(\"Should have 1 manifests after rewrite\", 1, newManifests.size());\n+    assertThat(newManifests).as(\"Should have 1 manifests after rewrite\").hasSize(1);\n \n-    Assert.assertEquals(4, (long) newManifests.get(0).existingFilesCount());\n-    Assert.assertFalse(newManifests.get(0).hasAddedFiles());\n-    Assert.assertFalse(newManifests.get(0).hasDeletedFiles());\n+    assertThat(newManifests.get(0).existingFilesCount()).isEqualTo(4);\n+    assertThat(newManifests.get(0).hasAddedFiles()).isFalse();\n+    assertThat(newManifests.get(0).hasDeletedFiles()).isFalse();\n \n     List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n     expectedRecords.addAll(records1);\n@@ -205,10 +204,10 @@ public void testRewriteSmallManifestsNonPartitionedTable() {\n     List<ThreeColumnRecord> actualRecords =\n         resultDF.sort(\"c1\", \"c2\").as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n \n-    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+    assertThat(actualRecords).as(\"Rows must match\").isEqualTo(expectedRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteManifestsWithCommitStateUnknownException() {\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n     Map<String, String> options = Maps.newHashMap();\n@@ -230,7 +229,7 @@ public void testRewriteManifestsWithCommitStateUnknownException() {\n     table.refresh();\n \n     List<ManifestFile> manifests = table.currentSnapshot().allManifests(table.io());\n-    Assert.assertEquals(\"Should have 2 manifests before rewrite\", 2, manifests.size());\n+    assertThat(manifests).as(\"Should have 2 manifests before rewrite\").hasSize(2);\n \n     SparkActions actions = SparkActions.get();\n \n@@ -258,11 +257,11 @@ public void testRewriteManifestsWithCommitStateUnknownException() {\n \n     // table should reflect the changes, since the commit was successful\n     List<ManifestFile> newManifests = table.currentSnapshot().allManifests(table.io());\n-    Assert.assertEquals(\"Should have 1 manifests after rewrite\", 1, newManifests.size());\n+    assertThat(newManifests).as(\"Should have 1 manifests after rewrite\").hasSize(1);\n \n-    Assert.assertEquals(4, (long) newManifests.get(0).existingFilesCount());\n-    Assert.assertFalse(newManifests.get(0).hasAddedFiles());\n-    Assert.assertFalse(newManifests.get(0).hasDeletedFiles());\n+    assertThat(newManifests.get(0).existingFilesCount()).isEqualTo(4);\n+    assertThat(newManifests.get(0).hasAddedFiles()).isFalse();\n+    assertThat(newManifests.get(0).hasDeletedFiles()).isFalse();\n \n     List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n     expectedRecords.addAll(records1);\n@@ -272,10 +271,10 @@ public void testRewriteManifestsWithCommitStateUnknownException() {\n     List<ThreeColumnRecord> actualRecords =\n         resultDF.sort(\"c1\", \"c2\").as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n \n-    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+    assertThat(actualRecords).as(\"Rows must match\").isEqualTo(expectedRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteSmallManifestsPartitionedTable() {\n     PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").truncate(\"c2\", 2).build();\n     Map<String, String> options = Maps.newHashMap();\n@@ -309,7 +308,7 @@ public void testRewriteSmallManifestsPartitionedTable() {\n     table.refresh();\n \n     List<ManifestFile> manifests = table.currentSnapshot().allManifests(table.io());\n-    Assert.assertEquals(\"Should have 4 manifests before rewrite\", 4, manifests.size());\n+    assertThat(manifests).as(\"Should have 4 manifests before rewrite\").hasSize(4);\n \n     SparkActions actions = SparkActions.get();\n \n@@ -329,24 +328,22 @@ public void testRewriteSmallManifestsPartitionedTable() {\n             .option(RewriteManifestsSparkAction.USE_CACHING, useCaching)\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Action should rewrite 4 manifests\", 4, Iterables.size(result.rewrittenManifests()));\n-    Assert.assertEquals(\n-        \"Action should add 2 manifests\", 2, Iterables.size(result.addedManifests()));\n+    assertThat(result.rewrittenManifests()).as(\"Action should rewrite 4 manifests\").hasSize(4);\n+    assertThat(result.addedManifests()).as(\"Action should add 2 manifests\").hasSize(2);\n     assertManifestsLocation(result.addedManifests());\n \n     table.refresh();\n \n     List<ManifestFile> newManifests = table.currentSnapshot().allManifests(table.io());\n-    Assert.assertEquals(\"Should have 2 manifests after rewrite\", 2, newManifests.size());\n+    assertThat(newManifests).as(\"Should have 2 manifests after rewrite\").hasSize(2);\n \n-    Assert.assertEquals(4, (long) newManifests.get(0).existingFilesCount());\n-    Assert.assertFalse(newManifests.get(0).hasAddedFiles());\n-    Assert.assertFalse(newManifests.get(0).hasDeletedFiles());\n+    assertThat(newManifests.get(0).existingFilesCount()).isEqualTo(4);\n+    assertThat(newManifests.get(0).hasAddedFiles()).isFalse();\n+    assertThat(newManifests.get(0).hasDeletedFiles()).isFalse();\n \n-    Assert.assertEquals(4, (long) newManifests.get(1).existingFilesCount());\n-    Assert.assertFalse(newManifests.get(1).hasAddedFiles());\n-    Assert.assertFalse(newManifests.get(1).hasDeletedFiles());\n+    assertThat(newManifests.get(1).existingFilesCount()).isEqualTo(4);\n+    assertThat(newManifests.get(1).hasAddedFiles()).isFalse();\n+    assertThat(newManifests.get(1).hasDeletedFiles()).isFalse();\n \n     List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n     expectedRecords.addAll(records1);\n@@ -358,10 +355,10 @@ public void testRewriteSmallManifestsPartitionedTable() {\n     List<ThreeColumnRecord> actualRecords =\n         resultDF.sort(\"c1\", \"c2\").as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n \n-    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+    assertThat(actualRecords).as(\"Rows must match\").isEqualTo(expectedRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteImportedManifests() throws IOException {\n     PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"c3\").build();\n     Map<String, String> options = Maps.newHashMap();\n@@ -372,7 +369,7 @@ public void testRewriteImportedManifests() throws IOException {\n     List<ThreeColumnRecord> records =\n         Lists.newArrayList(\n             new ThreeColumnRecord(1, null, \"AAAA\"), new ThreeColumnRecord(1, \"BBBBBBBBBB\", \"BBBB\"));\n-    File parquetTableDir = temp.newFolder(\"parquet_table\");\n+    File parquetTableDir = temp.resolve(\"parquet_table\").toFile();\n     String parquetTableLocation = parquetTableDir.toURI().toString();\n \n     try {\n@@ -386,7 +383,7 @@ public void testRewriteImportedManifests() throws IOException {\n           .partitionBy(\"c3\")\n           .saveAsTable(\"parquet_table\");\n \n-      File stagingDir = temp.newFolder(\"staging-dir\");\n+      File stagingDir = temp.resolve(\"staging-dir\").toFile();\n       SparkTableUtil.importSparkTable(\n           spark, new TableIdentifier(\"parquet_table\"), table, stagingDir.toString());\n \n@@ -398,7 +395,8 @@ public void testRewriteImportedManifests() throws IOException {\n \n       SparkActions actions = SparkActions.get();\n \n-      String rewriteStagingLocation = temp.newFolder().toString();\n+      String rewriteStagingLocation =\n+          java.nio.file.Files.createTempDirectory(temp, \"junit\").toString();\n \n       RewriteManifests.Result result =\n           actions\n@@ -408,12 +406,10 @@ public void testRewriteImportedManifests() throws IOException {\n               .stagingLocation(rewriteStagingLocation)\n               .execute();\n \n-      Assert.assertEquals(\n-          \"Action should rewrite all manifests\",\n-          snapshot.allManifests(table.io()),\n-          result.rewrittenManifests());\n-      Assert.assertEquals(\n-          \"Action should add 1 manifest\", 1, Iterables.size(result.addedManifests()));\n+      assertThat(result.rewrittenManifests())\n+          .as(\"Action should rewrite all manifests\")\n+          .isEqualTo(snapshot.allManifests(table.io()));\n+      assertThat(result.addedManifests()).as(\"Action should add 1 manifest\").hasSize(1);\n       assertManifestsLocation(result.addedManifests(), rewriteStagingLocation);\n \n     } finally {\n@@ -421,7 +417,7 @@ public void testRewriteImportedManifests() throws IOException {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteLargeManifestsPartitionedTable() throws IOException {\n     PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"c3\").build();\n     Map<String, String> options = Maps.newHashMap();\n@@ -437,7 +433,7 @@ public void testRewriteLargeManifestsPartitionedTable() throws IOException {\n     table.newFastAppend().appendManifest(appendManifest).commit();\n \n     List<ManifestFile> manifests = table.currentSnapshot().allManifests(table.io());\n-    Assert.assertEquals(\"Should have 1 manifests before rewrite\", 1, manifests.size());\n+    assertThat(manifests).as(\"Should have 1 manifests before rewrite\").hasSize(1);\n \n     // set the target manifest size to a small value to force splitting records into multiple files\n     table\n@@ -449,7 +445,7 @@ public void testRewriteLargeManifestsPartitionedTable() throws IOException {\n \n     SparkActions actions = SparkActions.get();\n \n-    String stagingLocation = temp.newFolder().toString();\n+    String stagingLocation = java.nio.file.Files.createTempDirectory(temp, \"junit\").toString();\n \n     RewriteManifests.Result result =\n         actions\n@@ -469,7 +465,7 @@ public void testRewriteLargeManifestsPartitionedTable() throws IOException {\n     assertThat(newManifests).hasSizeGreaterThanOrEqualTo(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteManifestsWithPredicate() throws IOException {\n     PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").truncate(\"c2\", 2).build();\n     Map<String, String> options = Maps.newHashMap();\n@@ -493,11 +489,11 @@ public void testRewriteManifestsWithPredicate() throws IOException {\n     table.refresh();\n \n     List<ManifestFile> manifests = table.currentSnapshot().allManifests(table.io());\n-    Assert.assertEquals(\"Should have 3 manifests before rewrite\", 3, manifests.size());\n+    assertThat(manifests).as(\"Should have 3 manifests before rewrite\").hasSize(3);\n \n     SparkActions actions = SparkActions.get();\n \n-    String stagingLocation = temp.newFolder().toString();\n+    String stagingLocation = java.nio.file.Files.createTempDirectory(temp, \"junit\").toString();\n \n     // rewrite only the first manifest\n     RewriteManifests.Result result =\n@@ -511,22 +507,22 @@ public void testRewriteManifestsWithPredicate() throws IOException {\n             .option(RewriteManifestsSparkAction.USE_CACHING, useCaching)\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Action should rewrite 2 manifest\", 2, Iterables.size(result.rewrittenManifests()));\n-    Assert.assertEquals(\n-        \"Action should add 1 manifests\", 1, Iterables.size(result.addedManifests()));\n+    assertThat(result.rewrittenManifests()).as(\"Action should rewrite 2 manifest\").hasSize(2);\n+    assertThat(result.addedManifests()).as(\"Action should add 1 manifests\").hasSize(1);\n     assertManifestsLocation(result.addedManifests(), stagingLocation);\n \n     table.refresh();\n \n     List<ManifestFile> newManifests = table.currentSnapshot().allManifests(table.io());\n-    Assert.assertEquals(\"Should have 2 manifests after rewrite\", 2, newManifests.size());\n-\n-    Assert.assertFalse(\"First manifest must be rewritten\", newManifests.contains(manifests.get(0)));\n-    Assert.assertFalse(\n-        \"Second manifest must be rewritten\", newManifests.contains(manifests.get(1)));\n-    Assert.assertTrue(\n-        \"Third manifest must not be rewritten\", newManifests.contains(manifests.get(2)));\n+    assertThat(newManifests)\n+        .as(\"Should have 2 manifests after rewrite\")\n+        .hasSize(2)\n+        .as(\"First manifest must be rewritten\")\n+        .doesNotContain(manifests.get(0))\n+        .as(\"Second manifest must be rewritten\")\n+        .doesNotContain(manifests.get(1))\n+        .as(\"Third manifest must not be rewritten\")\n+        .contains(manifests.get(2));\n \n     List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n     expectedRecords.add(records1.get(0));\n@@ -539,10 +535,10 @@ public void testRewriteManifestsWithPredicate() throws IOException {\n     List<ThreeColumnRecord> actualRecords =\n         resultDF.sort(\"c1\", \"c2\").as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n \n-    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+    assertThat(actualRecords).as(\"Rows must match\").isEqualTo(expectedRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteSmallManifestsNonPartitionedV2Table() {\n     assumeThat(formatVersion).isGreaterThan(1);\n \n@@ -567,7 +563,7 @@ public void testRewriteSmallManifestsNonPartitionedV2Table() {\n     DataFile file2 = Iterables.getOnlyElement(snapshot2.addedDataFiles(table.io()));\n \n     List<ManifestFile> manifests = table.currentSnapshot().allManifests(table.io());\n-    Assert.assertEquals(\"Should have 2 manifests before rewrite\", 2, manifests.size());\n+    assertThat(manifests).as(\"Should have 2 manifests before rewrite\").hasSize(2);\n \n     SparkActions actions = SparkActions.get();\n     RewriteManifests.Result result =\n@@ -575,21 +571,19 @@ public void testRewriteSmallManifestsNonPartitionedV2Table() {\n             .rewriteManifests(table)\n             .option(RewriteManifestsSparkAction.USE_CACHING, useCaching)\n             .execute();\n-    Assert.assertEquals(\n-        \"Action should rewrite 2 manifests\", 2, Iterables.size(result.rewrittenManifests()));\n-    Assert.assertEquals(\n-        \"Action should add 1 manifests\", 1, Iterables.size(result.addedManifests()));\n+    assertThat(result.rewrittenManifests()).as(\"Action should rewrite 2 manifests\").hasSize(2);\n+    assertThat(result.addedManifests()).as(\"Action should add 1 manifests\").hasSize(1);\n     assertManifestsLocation(result.addedManifests());\n \n     table.refresh();\n \n     List<ManifestFile> newManifests = table.currentSnapshot().allManifests(table.io());\n-    Assert.assertEquals(\"Should have 1 manifests after rewrite\", 1, newManifests.size());\n+    assertThat(newManifests).as(\"Should have 1 manifests after rewrite\").hasSize(1);\n \n     ManifestFile newManifest = Iterables.getOnlyElement(newManifests);\n-    Assert.assertEquals(2, (long) newManifest.existingFilesCount());\n-    Assert.assertFalse(newManifest.hasAddedFiles());\n-    Assert.assertFalse(newManifest.hasDeletedFiles());\n+    assertThat(newManifest.existingFilesCount()).isEqualTo(2);\n+    assertThat(newManifest.hasAddedFiles()).isFalse();\n+    assertThat(newManifest.hasDeletedFiles()).isFalse();\n \n     validateDataManifest(\n         table,\n@@ -607,10 +601,10 @@ public void testRewriteSmallManifestsNonPartitionedV2Table() {\n     List<ThreeColumnRecord> actualRecords =\n         resultDF.sort(\"c1\", \"c2\").as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n \n-    Assert.assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n+    assertThat(actualRecords).as(\"Rows must match\").isEqualTo(expectedRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteLargeManifestsEvolvedUnpartitionedV1Table() throws IOException {\n     assumeThat(formatVersion).isEqualTo(1);\n \n@@ -659,9 +653,9 @@ public void testRewriteLargeManifestsEvolvedUnpartitionedV1Table() throws IOExce\n     assertThat(manifests).hasSizeGreaterThanOrEqualTo(2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteSmallDeleteManifestsNonPartitionedTable() throws IOException {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n \n     PartitionSpec spec = PartitionSpec.unpartitioned();\n     Map<String, String> options = Maps.newHashMap();\n@@ -732,9 +726,9 @@ public void testRewriteSmallDeleteManifestsNonPartitionedTable() throws IOExcept\n     assertThat(actualRecords()).isEqualTo(expectedRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteSmallDeleteManifestsPartitionedTable() throws IOException {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n \n     PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"c3\").build();\n     Map<String, String> options = Maps.newHashMap();\n@@ -835,9 +829,9 @@ public void testRewriteSmallDeleteManifestsPartitionedTable() throws IOException\n     assertThat(actualRecords()).isEqualTo(expectedRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteLargeDeleteManifestsPartitionedTable() throws IOException {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n \n     PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"c3\").build();\n     Map<String, String> options = Maps.newHashMap();\n@@ -874,7 +868,7 @@ public void testRewriteLargeDeleteManifestsPartitionedTable() throws IOException\n \n     SparkActions actions = SparkActions.get();\n \n-    String stagingLocation = temp.newFolder().toString();\n+    String stagingLocation = java.nio.file.Files.createTempDirectory(temp, \"junit\").toString();\n \n     RewriteManifests.Result result =\n         actions\n@@ -948,8 +942,8 @@ private void assertManifestsLocation(Iterable<ManifestFile> manifests, String st\n   }\n \n   private ManifestFile writeManifest(Table table, List<DataFile> files) throws IOException {\n-    File manifestFile = temp.newFile(\"generated-manifest.avro\");\n-    Assert.assertTrue(manifestFile.delete());\n+    File manifestFile = File.createTempFile(\"generated-manifest\", \".avro\", temp.toFile());\n+    assertThat(manifestFile.delete()).isTrue();\n     OutputFile outputFile = table.io().newOutputFile(manifestFile.getCanonicalPath());\n \n     ManifestWriter<DataFile> writer =\n@@ -1018,7 +1012,7 @@ private Pair<DeleteFile, CharSequenceSet> writePosDeletes(\n   private Pair<DeleteFile, CharSequenceSet> writePosDeletes(\n       Table table, StructLike partition, List<Pair<CharSequence, Long>> deletes)\n       throws IOException {\n-    OutputFile outputFile = Files.localOutput(temp.newFile());\n+    OutputFile outputFile = Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile()));\n     return FileHelpers.writeDeleteFile(table, outputFile, partition, deletes);\n   }\n \n@@ -1036,7 +1030,7 @@ private DeleteFile writeEqDeletes(Table table, StructLike partition, String key,\n       deletes.add(delete.copy(key, value));\n     }\n \n-    OutputFile outputFile = Files.localOutput(temp.newFile());\n+    OutputFile outputFile = Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile()));\n     return FileHelpers.writeDeleteFile(table, outputFile, partition, deletes, deleteSchema);\n   }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java\nindex 022dfa1592f6..d3508283dd8c 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java\n@@ -39,7 +39,6 @@\n import java.util.concurrent.Executors;\n import java.util.concurrent.atomic.AtomicInteger;\n import java.util.stream.Collectors;\n-import java.util.stream.StreamSupport;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n@@ -69,10 +68,8 @@\n import org.apache.iceberg.puffin.PuffinWriter;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n-import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.spark.SparkSQLProperties;\n import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.spark.actions.DeleteOrphanFilesSparkAction.StringToFileURI;\n@@ -317,10 +314,8 @@ public void orphanedFileRemovedWithParallelTasks() {\n     // Verifies that the delete methods ran in the threads created by the provided ExecutorService\n     // ThreadFactory\n     assertThat(deleteThreads)\n-        .isEqualTo(\n-            Sets.newHashSet(\n-                \"remove-orphan-0\", \"remove-orphan-1\", \"remove-orphan-2\", \"remove-orphan-3\"));\n-\n+        .containsExactlyInAnyOrder(\n+            \"remove-orphan-0\", \"remove-orphan-1\", \"remove-orphan-2\", \"remove-orphan-3\");\n     assertThat(deletedFiles).hasSize(4);\n   }\n \n@@ -446,10 +441,8 @@ public void testRemoveUnreachableMetadataVersionFiles() {\n     DeleteOrphanFiles.Result result =\n         actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n \n-    assertThat(result.orphanFileLocations()).as(\"Should delete 1 file\").hasSize(1);\n-    assertThat(StreamSupport.stream(result.orphanFileLocations().spliterator(), false))\n-        .as(\"Should remove v1 file\")\n-        .anyMatch(file -> file.contains(\"v1.metadata.json\"));\n+    assertThat(result.orphanFileLocations())\n+        .containsExactly(tableLocation + \"metadata/v1.metadata.json\");\n \n     List<ThreeColumnRecord> expectedRecords = Lists.newArrayList();\n     expectedRecords.addAll(records);\n@@ -748,9 +741,9 @@ public void testHiveCatalogTable() throws IOException {\n             .deleteOrphanFiles(table)\n             .olderThan(System.currentTimeMillis() + 1000)\n             .execute();\n-    assertThat(StreamSupport.stream(result.orphanFileLocations().spliterator(), false))\n+    assertThat(result.orphanFileLocations())\n         .as(\"trash file should be removed\")\n-        .anyMatch(file -> file.contains(\"file:\" + location + \"/data/trashfile\"));\n+        .contains(\"file:\" + location + \"/data/trashfile\");\n   }\n \n   @TestTemplate\n@@ -967,11 +960,8 @@ public void testRemoveOrphanFilesWithStatisticFiles() throws Exception {\n             .olderThan(System.currentTimeMillis() + 1000)\n             .execute();\n     Iterable<String> orphanFileLocations = result.orphanFileLocations();\n-    assertThat(orphanFileLocations).as(\"Should be orphan file\").hasSize(1);\n-    assertThat(Iterables.getOnlyElement(orphanFileLocations))\n-        .as(\"Deleted file\")\n-        .isEqualTo(statsLocation.toURI().toString());\n-    assertThat(statsLocation.exists()).as(\"stats file should be deleted\").isFalse();\n+    assertThat(orphanFileLocations).hasSize(1).containsExactly(statsLocation.toURI().toString());\n+    assertThat(statsLocation).as(\"stats file should be deleted\").doesNotExist();\n   }\n \n   @TestTemplate\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java\nindex 35d86b0a44b0..5f98287951f1 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java\n@@ -57,9 +57,9 @@ public void testSparkCatalogTable() throws Exception {\n             .deleteOrphanFiles(table.table())\n             .olderThan(System.currentTimeMillis() + 1000)\n             .execute();\n-    assertThat(StreamSupport.stream(results.orphanFileLocations().spliterator(), false))\n+    assertThat(results.orphanFileLocations())\n         .as(\"trash file should be removed\")\n-        .anyMatch(file -> file.contains(\"file:\" + location + trashFile));\n+        .contains(\"file:\" + location + trashFile);\n   }\n \n   @TestTemplate\n@@ -86,9 +86,9 @@ public void testSparkCatalogNamedHadoopTable() throws Exception {\n             .deleteOrphanFiles(table.table())\n             .olderThan(System.currentTimeMillis() + 1000)\n             .execute();\n-    assertThat(StreamSupport.stream(results.orphanFileLocations().spliterator(), false))\n+    assertThat(results.orphanFileLocations())\n         .as(\"trash file should be removed\")\n-        .anyMatch(file -> file.contains(\"file:\" + location + trashFile));\n+        .contains(\"file:\" + location + trashFile);\n   }\n \n   @TestTemplate\n@@ -148,9 +148,9 @@ public void testSparkSessionCatalogHadoopTable() throws Exception {\n             .deleteOrphanFiles(table.table())\n             .olderThan(System.currentTimeMillis() + 1000)\n             .execute();\n-    assertThat(StreamSupport.stream(results.orphanFileLocations().spliterator(), false))\n+    assertThat(results.orphanFileLocations())\n         .as(\"trash file should be removed\")\n-        .anyMatch(file -> file.contains(\"file:\" + location + trashFile));\n+        .contains(\"file:\" + location + trashFile);\n   }\n \n   @TestTemplate\n@@ -169,7 +169,7 @@ public void testSparkSessionCatalogHiveTable() throws Exception {\n     cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, properties);\n     SparkTable table = (SparkTable) cat.loadTable(id);\n \n-    spark.sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n+    sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n \n     String location = table.table().location().replaceFirst(\"file:\", \"\");\n     String trashFile = randomName(\"/data/trashfile\");\n@@ -180,13 +180,13 @@ public void testSparkSessionCatalogHiveTable() throws Exception {\n             .deleteOrphanFiles(table.table())\n             .olderThan(System.currentTimeMillis() + 1000)\n             .execute();\n-    assertThat(StreamSupport.stream(results.orphanFileLocations().spliterator(), false))\n+    assertThat(results.orphanFileLocations())\n         .as(\"trash file should be removed\")\n-        .anyMatch(file -> file.contains(\"file:\" + location + trashFile));\n+        .contains(\"file:\" + location + trashFile);\n   }\n \n   @AfterEach\n-  public void resetSparkSessionCatalog() throws Exception {\n+  public void resetSparkSessionCatalog() {\n     spark.conf().unset(\"spark.sql.catalog.spark_catalog\");\n     spark.conf().unset(\"spark.sql.catalog.spark_catalog.type\");\n     spark.conf().unset(\"spark.sql.catalog.spark_catalog.warehouse\");\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\nindex bdbb8c176812..b3af2b2b21cf 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\n@@ -658,7 +658,7 @@ public void testRemoveDangledPositionDeletesPartitionEvolution() throws IOExcept\n     shouldHaveMinSequenceNumberInPartition(table, \"data_file.partition.c1 == 1\", 3);\n \n     shouldHaveSnapshots(table, 5);\n-    assertThat(table.currentSnapshot().summary().get(\"total-position-deletes\")).isEqualTo(\"0\");\n+    assertThat(table.currentSnapshot().summary()).containsEntry(\"total-position-deletes\", \"0\");\n     assertEquals(\"Rows must match\", expectedRecords, currentData());\n   }\n \n@@ -1894,7 +1894,7 @@ public void testBinPackRewriterWithSpecificUnparitionedOutputSpec() {\n             .execute();\n \n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n-    assertThat(currentData().size()).isEqualTo(count);\n+    assertThat(currentData()).hasSize((int) count);\n     shouldRewriteDataFilesWithPartitionSpec(table, outputSpecId);\n   }\n \n@@ -1917,7 +1917,7 @@ public void testBinPackRewriterWithSpecificOutputSpec() {\n             .execute();\n \n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n-    assertThat(currentData().size()).isEqualTo(count);\n+    assertThat(currentData()).hasSize((int) count);\n     shouldRewriteDataFilesWithPartitionSpec(table, outputSpecId);\n   }\n \n@@ -1956,7 +1956,7 @@ public void testSortRewriterWithSpecificOutputSpecId() {\n             .execute();\n \n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n-    assertThat(currentData().size()).isEqualTo(count);\n+    assertThat(currentData()).hasSize((int) count);\n     shouldRewriteDataFilesWithPartitionSpec(table, outputSpecId);\n   }\n \n@@ -1979,7 +1979,7 @@ public void testZOrderRewriteWithSpecificOutputSpecId() {\n             .execute();\n \n     assertThat(result.rewrittenBytesCount()).isEqualTo(dataSizeBefore);\n-    assertThat(currentData().size()).isEqualTo(count);\n+    assertThat(currentData()).hasSize((int) count);\n     shouldRewriteDataFilesWithPartitionSpec(table, outputSpecId);\n   }\n \n@@ -2049,10 +2049,9 @@ protected long shouldHaveMinSequenceNumberInPartition(\n \n   protected void shouldHaveSnapshots(Table table, int expectedSnapshots) {\n     table.refresh();\n-    int actualSnapshots = Iterables.size(table.snapshots());\n-    assertThat(actualSnapshots)\n+    assertThat(table.snapshots())\n         .as(\"Table did not have the expected number of snapshots\")\n-        .isEqualTo(expectedSnapshots);\n+        .hasSize(expectedSnapshots);\n   }\n \n   protected void shouldHaveNoOrphans(Table table) {\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\nindex 44971843547b..76b201aa5649 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\n@@ -400,7 +400,6 @@ public void testRewriteSmallManifestsPartitionedTable() {\n     table.refresh();\n \n     List<ManifestFile> newManifests = table.currentSnapshot().allManifests(table.io());\n-\n     assertThat(newManifests).as(\"Should have 2 manifests after rewrite\").hasSize(2);\n \n     assertThat(newManifests.get(0).existingFilesCount()).isEqualTo(4);\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12552",
    "pr_id": 12552,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 11,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkSessionCatalog.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveDanglingDeleteAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteTablePathsAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkSessionCatalog.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkSessionCatalog.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveDanglingDeleteAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteTablePathsAction.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkSessionCatalog.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java"
    ],
    "base_commit": "bf2f552722b8f29b5a6c5c0a4b98c08bb400a0eb",
    "head_commit": "1957e9d7f16b7552bff934b2df70c0a4dc3e53df",
    "repo_url": "https://github.com/apache/iceberg/pull/12552",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12552",
    "dockerfile": "",
    "pr_merged_at": "2025-03-17T14:51:00.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java\nindex 744660073954..6f900ffebb10 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java\n@@ -46,10 +46,9 @@\n import org.apache.iceberg.catalog.Catalog;\n import org.apache.iceberg.expressions.Expression;\n import org.apache.iceberg.types.Types;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.Test;\n \n-public class TestSpark3Util extends SparkTestBase {\n+public class TestSpark3Util extends TestBase {\n   @Test\n   public void testDescribeSortOrder() {\n     Schema schema =\n@@ -57,46 +56,44 @@ public void testDescribeSortOrder() {\n             required(1, \"data\", Types.StringType.get()),\n             required(2, \"time\", Types.TimestampType.withoutZone()));\n \n-    Assert.assertEquals(\n-        \"Sort order isn't correct.\",\n-        \"data DESC NULLS FIRST\",\n-        Spark3Util.describe(buildSortOrder(\"Identity\", schema, 1)));\n-    Assert.assertEquals(\n-        \"Sort order isn't correct.\",\n-        \"bucket(1, data) DESC NULLS FIRST\",\n-        Spark3Util.describe(buildSortOrder(\"bucket[1]\", schema, 1)));\n-    Assert.assertEquals(\n-        \"Sort order isn't correct.\",\n-        \"truncate(data, 3) DESC NULLS FIRST\",\n-        Spark3Util.describe(buildSortOrder(\"truncate[3]\", schema, 1)));\n-    Assert.assertEquals(\n-        \"Sort order isn't correct.\",\n-        \"years(time) DESC NULLS FIRST\",\n-        Spark3Util.describe(buildSortOrder(\"year\", schema, 2)));\n-    Assert.assertEquals(\n-        \"Sort order isn't correct.\",\n-        \"months(time) DESC NULLS FIRST\",\n-        Spark3Util.describe(buildSortOrder(\"month\", schema, 2)));\n-    Assert.assertEquals(\n-        \"Sort order isn't correct.\",\n-        \"days(time) DESC NULLS FIRST\",\n-        Spark3Util.describe(buildSortOrder(\"day\", schema, 2)));\n-    Assert.assertEquals(\n-        \"Sort order isn't correct.\",\n-        \"hours(time) DESC NULLS FIRST\",\n-        Spark3Util.describe(buildSortOrder(\"hour\", schema, 2)));\n-    Assert.assertEquals(\n-        \"Sort order isn't correct.\",\n-        \"unknown(data) DESC NULLS FIRST\",\n-        Spark3Util.describe(buildSortOrder(\"unknown\", schema, 1)));\n+    assertThat(Spark3Util.describe(buildSortOrder(\"Identity\", schema, 1)))\n+        .as(\"Sort order isn't correct.\")\n+        .isEqualTo(\"data DESC NULLS FIRST\");\n+\n+    assertThat(Spark3Util.describe(buildSortOrder(\"bucket[1]\", schema, 1)))\n+        .as(\"Sort order isn't correct.\")\n+        .isEqualTo(\"bucket(1, data) DESC NULLS FIRST\");\n+\n+    assertThat(Spark3Util.describe(buildSortOrder(\"truncate[3]\", schema, 1)))\n+        .as(\"Sort order isn't correct.\")\n+        .isEqualTo(\"truncate(data, 3) DESC NULLS FIRST\");\n+\n+    assertThat(Spark3Util.describe(buildSortOrder(\"year\", schema, 2)))\n+        .as(\"Sort order isn't correct.\")\n+        .isEqualTo(\"years(time) DESC NULLS FIRST\");\n+\n+    assertThat(Spark3Util.describe(buildSortOrder(\"month\", schema, 2)))\n+        .as(\"Sort order isn't correct.\")\n+        .isEqualTo(\"months(time) DESC NULLS FIRST\");\n+\n+    assertThat(Spark3Util.describe(buildSortOrder(\"day\", schema, 2)))\n+        .as(\"Sort order isn't correct.\")\n+        .isEqualTo(\"days(time) DESC NULLS FIRST\");\n+\n+    assertThat(Spark3Util.describe(buildSortOrder(\"hour\", schema, 2)))\n+        .as(\"Sort order isn't correct.\")\n+        .isEqualTo(\"hours(time) DESC NULLS FIRST\");\n+\n+    assertThat(Spark3Util.describe(buildSortOrder(\"unknown\", schema, 1)))\n+        .as(\"Sort order isn't correct.\")\n+        .isEqualTo(\"unknown(data) DESC NULLS FIRST\");\n \n     // multiple sort orders\n     SortOrder multiOrder =\n         SortOrder.builderFor(schema).asc(\"time\", NULLS_FIRST).asc(\"data\", NULLS_LAST).build();\n-    Assert.assertEquals(\n-        \"Sort order isn't correct.\",\n-        \"time ASC NULLS FIRST, data ASC NULLS LAST\",\n-        Spark3Util.describe(multiOrder));\n+    assertThat(Spark3Util.describe(multiOrder))\n+        .as(\"Sort order isn't correct.\")\n+        .isEqualTo(\"time ASC NULLS FIRST, data ASC NULLS LAST\");\n   }\n \n   @Test\n@@ -110,10 +107,10 @@ public void testDescribeSchema() {\n                 Types.MapType.ofOptional(4, 5, Types.StringType.get(), Types.LongType.get())),\n             required(6, \"time\", Types.TimestampType.withoutZone()));\n \n-    Assert.assertEquals(\n-        \"Schema description isn't correct.\",\n-        \"struct<data: list<string> not null,pairs: map<string, bigint>,time: timestamp not null>\",\n-        Spark3Util.describe(schema));\n+    assertThat(Spark3Util.describe(schema))\n+        .as(\"Schema description isn't correct.\")\n+        .isEqualTo(\n+            \"struct<data: list<string> not null,pairs: map<string, bigint>,time: timestamp not null>\");\n   }\n \n   @Test\n@@ -126,7 +123,7 @@ public void testLoadIcebergTable() throws Exception {\n     sql(\"CREATE TABLE %s (c1 bigint, c2 string, c3 string) USING iceberg\", tableFullName);\n \n     Table table = Spark3Util.loadIcebergTable(spark, tableFullName);\n-    Assert.assertTrue(table.name().equals(tableFullName));\n+    assertThat(table.name()).isEqualTo(tableFullName);\n   }\n \n   @Test\n@@ -134,8 +131,9 @@ public void testLoadIcebergCatalog() throws Exception {\n     spark.conf().set(\"spark.sql.catalog.test_cat\", SparkCatalog.class.getName());\n     spark.conf().set(\"spark.sql.catalog.test_cat.type\", \"hive\");\n     Catalog catalog = Spark3Util.loadIcebergCatalog(spark, \"test_cat\");\n-    Assert.assertTrue(\n-        \"Should retrieve underlying catalog class\", catalog instanceof CachingCatalog);\n+    assertThat(catalog)\n+        .as(\"Should retrieve underlying catalog class\")\n+        .isInstanceOf(CachingCatalog.class);\n   }\n \n   @Test\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkSessionCatalog.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkSessionCatalog.java\nindex 82a2fb473360..85408ffff1d9 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkSessionCatalog.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestSparkSessionCatalog.java\n@@ -19,18 +19,19 @@\n package org.apache.iceberg.spark;\n \n import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.METASTOREURIS;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.BeforeClass;\n-import org.junit.Test;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n \n-public class TestSparkSessionCatalog extends SparkTestBase {\n+public class TestSparkSessionCatalog extends TestBase {\n   private final String envHmsUriKey = \"spark.hadoop.\" + METASTOREURIS.varname;\n   private final String catalogHmsUriKey = \"spark.sql.catalog.spark_catalog.uri\";\n   private final String hmsUri = hiveConf.get(METASTOREURIS.varname);\n \n-  @BeforeClass\n+  @BeforeAll\n   public static void setUpCatalog() {\n     spark\n         .conf()\n@@ -38,7 +39,7 @@ public static void setUpCatalog() {\n     spark.conf().set(\"spark.sql.catalog.spark_catalog.type\", \"hive\");\n   }\n \n-  @Before\n+  @BeforeEach\n   public void setupHmsUri() {\n     spark.sessionState().catalogManager().reset();\n     spark.conf().set(envHmsUriKey, hmsUri);\n@@ -48,52 +49,35 @@ public void setupHmsUri() {\n   @Test\n   public void testValidateHmsUri() {\n     // HMS uris match\n-    Assert.assertTrue(\n-        spark\n-            .sessionState()\n-            .catalogManager()\n-            .v2SessionCatalog()\n-            .defaultNamespace()[0]\n-            .equals(\"default\"));\n+    assertThat(spark.sessionState().catalogManager().v2SessionCatalog().defaultNamespace())\n+        .containsExactly(\"default\");\n \n     // HMS uris doesn't match\n     spark.sessionState().catalogManager().reset();\n     String catalogHmsUri = \"RandomString\";\n     spark.conf().set(envHmsUriKey, hmsUri);\n     spark.conf().set(catalogHmsUriKey, catalogHmsUri);\n-    IllegalArgumentException exception =\n-        Assert.assertThrows(\n-            IllegalArgumentException.class,\n-            () -> spark.sessionState().catalogManager().v2SessionCatalog());\n-    String errorMessage =\n-        String.format(\n-            \"Inconsistent Hive metastore URIs: %s (Spark session) != %s (spark_catalog)\",\n-            hmsUri, catalogHmsUri);\n-    Assert.assertEquals(errorMessage, exception.getMessage());\n+\n+    assertThatThrownBy(() -> spark.sessionState().catalogManager().v2SessionCatalog())\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\n+            String.format(\n+                \"Inconsistent Hive metastore URIs: %s (Spark session) != %s (spark_catalog)\",\n+                hmsUri, catalogHmsUri));\n \n     // no env HMS uri, only catalog HMS uri\n     spark.sessionState().catalogManager().reset();\n     spark.conf().set(catalogHmsUriKey, hmsUri);\n     spark.conf().unset(envHmsUriKey);\n-    Assert.assertTrue(\n-        spark\n-            .sessionState()\n-            .catalogManager()\n-            .v2SessionCatalog()\n-            .defaultNamespace()[0]\n-            .equals(\"default\"));\n+    assertThat(spark.sessionState().catalogManager().v2SessionCatalog().defaultNamespace())\n+        .containsExactly(\"default\");\n \n     // no catalog HMS uri, only env HMS uri\n     spark.sessionState().catalogManager().reset();\n     spark.conf().set(envHmsUriKey, hmsUri);\n     spark.conf().unset(catalogHmsUriKey);\n-    Assert.assertTrue(\n-        spark\n-            .sessionState()\n-            .catalogManager()\n-            .v2SessionCatalog()\n-            .defaultNamespace()[0]\n-            .equals(\"default\"));\n+    assertThat(spark.sessionState().catalogManager().v2SessionCatalog().defaultNamespace())\n+        .containsExactly(\"default\");\n   }\n \n   @Test\n@@ -102,11 +86,15 @@ public void testLoadFunction() {\n \n     // load permanent UDF in Hive via FunctionCatalog\n     spark.sql(String.format(\"CREATE FUNCTION perm_upper AS '%s'\", functionClass));\n-    Assert.assertEquals(\"Load permanent UDF in Hive\", \"XYZ\", scalarSql(\"SELECT perm_upper('xyz')\"));\n+    assertThat(scalarSql(\"SELECT perm_upper('xyz')\"))\n+        .as(\"Load permanent UDF in Hive\")\n+        .isEqualTo(\"XYZ\");\n \n     // load temporary UDF in Hive via FunctionCatalog\n     spark.sql(String.format(\"CREATE TEMPORARY FUNCTION temp_upper AS '%s'\", functionClass));\n-    Assert.assertEquals(\"Load temporary UDF in Hive\", \"XYZ\", scalarSql(\"SELECT temp_upper('xyz')\"));\n+    assertThat(scalarSql(\"SELECT temp_upper('xyz')\"))\n+        .as(\"Load temporary UDF in Hive\")\n+        .isEqualTo(\"XYZ\");\n \n     // TODO: fix loading Iceberg built-in functions in SessionCatalog\n   }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java\nindex f4b270528bb1..e17c72d90963 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java\n@@ -19,9 +19,13 @@\n package org.apache.iceberg.spark.actions;\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.File;\n+import java.util.Arrays;\n+import java.util.List;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.Executors;\n@@ -33,6 +37,9 @@\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileMetadata;\n import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -45,19 +52,16 @@\n import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n-import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n-import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n-import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.types.Types;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n \n-public class TestDeleteReachableFilesAction extends SparkTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestDeleteReachableFilesAction extends TestBase {\n   private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n   private static final Schema SCHEMA =\n       new Schema(\n@@ -113,15 +117,25 @@ public class TestDeleteReachableFilesAction extends SparkTestBase {\n           .withRecordCount(1)\n           .build();\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private File tableDir;\n+  @Parameter private int formatVersion;\n+\n+  @Parameters(name = \"formatVersion = {0}\")\n+  protected static List<Object> parameters() {\n+    return Arrays.asList(2, 3);\n+  }\n \n   private Table table;\n \n-  @Before\n+  @BeforeEach\n   public void setupTableLocation() throws Exception {\n-    File tableDir = temp.newFolder();\n     String tableLocation = tableDir.toURI().toString();\n-    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+    this.table =\n+        TABLES.create(\n+            SCHEMA,\n+            SPEC,\n+            ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion)),\n+            tableLocation);\n     spark.conf().set(\"spark.sql.shuffle.partitions\", SHUFFLE_PARTITIONS);\n   }\n \n@@ -133,33 +147,32 @@ private void checkRemoveFilesResults(\n       long expectedManifestListsDeleted,\n       long expectedOtherFilesDeleted,\n       DeleteReachableFiles.Result results) {\n-    Assert.assertEquals(\n-        \"Incorrect number of manifest files deleted\",\n-        expectedManifestsDeleted,\n-        results.deletedManifestsCount());\n-    Assert.assertEquals(\n-        \"Incorrect number of datafiles deleted\",\n-        expectedDatafiles,\n-        results.deletedDataFilesCount());\n-    Assert.assertEquals(\n-        \"Incorrect number of position delete files deleted\",\n-        expectedPosDeleteFiles,\n-        results.deletedPositionDeleteFilesCount());\n-    Assert.assertEquals(\n-        \"Incorrect number of equality delete files deleted\",\n-        expectedEqDeleteFiles,\n-        results.deletedEqualityDeleteFilesCount());\n-    Assert.assertEquals(\n-        \"Incorrect number of manifest lists deleted\",\n-        expectedManifestListsDeleted,\n-        results.deletedManifestListsCount());\n-    Assert.assertEquals(\n-        \"Incorrect number of other lists deleted\",\n-        expectedOtherFilesDeleted,\n-        results.deletedOtherFilesCount());\n+    assertThat(results.deletedManifestsCount())\n+        .as(\"Incorrect number of manifest files deleted\")\n+        .isEqualTo(expectedManifestsDeleted);\n+\n+    assertThat(results.deletedDataFilesCount())\n+        .as(\"Incorrect number of datafiles deleted\")\n+        .isEqualTo(expectedDatafiles);\n+\n+    assertThat(results.deletedPositionDeleteFilesCount())\n+        .as(\"Incorrect number of position delete files deleted\")\n+        .isEqualTo(expectedPosDeleteFiles);\n+\n+    assertThat(results.deletedEqualityDeleteFilesCount())\n+        .as(\"Incorrect number of equality delete files deleted\")\n+        .isEqualTo(expectedEqDeleteFiles);\n+\n+    assertThat(results.deletedManifestListsCount())\n+        .as(\"Incorrect number of manifest lists deleted\")\n+        .isEqualTo(expectedManifestListsDeleted);\n+\n+    assertThat(results.deletedOtherFilesCount())\n+        .as(\"Incorrect number of other lists deleted\")\n+        .isEqualTo(expectedOtherFilesDeleted);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void dataFilesCleanupWithParallelTasks() {\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n@@ -196,19 +209,20 @@ public void dataFilesCleanupWithParallelTasks() {\n \n     // Verifies that the delete methods ran in the threads created by the provided ExecutorService\n     // ThreadFactory\n-    Assert.assertEquals(\n-        deleteThreads,\n-        Sets.newHashSet(\"remove-files-0\", \"remove-files-1\", \"remove-files-2\", \"remove-files-3\"));\n+    assertThat(deleteThreads)\n+        .containsExactlyInAnyOrder(\n+            \"remove-files-0\", \"remove-files-1\", \"remove-files-2\", \"remove-files-3\");\n \n     Lists.newArrayList(FILE_A, FILE_B, FILE_C, FILE_D)\n         .forEach(\n             file ->\n-                Assert.assertTrue(\n-                    \"FILE_A should be deleted\", deletedFiles.contains(FILE_A.location())));\n+                assertThat(deletedFiles)\n+                    .as(\"FILE_A should be deleted\")\n+                    .contains(FILE_A.location()));\n     checkRemoveFilesResults(4L, 0, 0, 6L, 4L, 6, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWithExpiringDanglingStageCommit() {\n     table.location();\n     // `A` commit\n@@ -226,7 +240,7 @@ public void testWithExpiringDanglingStageCommit() {\n     checkRemoveFilesResults(3L, 0, 0, 3L, 3L, 5, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveFileActionOnEmptyTable() {\n     DeleteReachableFiles.Result result =\n         sparkActions().deleteReachableFiles(metadataLocation(table)).io(table.io()).execute();\n@@ -234,7 +248,7 @@ public void testRemoveFileActionOnEmptyTable() {\n     checkRemoveFilesResults(0, 0, 0, 0, 0, 2, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveFilesActionWithReducedVersionsTable() {\n     table.updateProperties().set(TableProperties.METADATA_PREVIOUS_VERSIONS_MAX, \"2\").commit();\n     table.newAppend().appendFile(FILE_A).commit();\n@@ -254,7 +268,7 @@ public void testRemoveFilesActionWithReducedVersionsTable() {\n     checkRemoveFilesResults(4, 0, 0, 5, 5, 8, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveFilesAction() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n@@ -265,8 +279,9 @@ public void testRemoveFilesAction() {\n     checkRemoveFilesResults(2, 0, 0, 2, 2, 4, baseRemoveFilesSparkAction.execute());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPositionDeleteFiles() {\n+    assumeThat(formatVersion).as(\"DV is not supported in Spark 3.4\").isEqualTo(2);\n     table.newAppend().appendFile(FILE_A).commit();\n \n     table.newAppend().appendFile(FILE_B).commit();\n@@ -278,7 +293,7 @@ public void testPositionDeleteFiles() {\n     checkRemoveFilesResults(2, 1, 0, 3, 3, 5, baseRemoveFilesSparkAction.execute());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testEqualityDeleteFiles() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n@@ -291,7 +306,7 @@ public void testEqualityDeleteFiles() {\n     checkRemoveFilesResults(2, 0, 1, 3, 3, 5, baseRemoveFilesSparkAction.execute());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveFilesActionWithDefaultIO() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n@@ -304,7 +319,7 @@ public void testRemoveFilesActionWithDefaultIO() {\n     checkRemoveFilesResults(2, 0, 0, 2, 2, 4, baseRemoveFilesSparkAction.execute());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUseLocalIterator() {\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n@@ -329,14 +344,13 @@ public void testUseLocalIterator() {\n \n           checkRemoveFilesResults(3L, 0, 0, 4L, 3L, 5, results);\n \n-          Assert.assertEquals(\n-              \"Expected total jobs to be equal to total number of shuffle partitions\",\n-              totalJobsRun,\n-              SHUFFLE_PARTITIONS);\n+          assertThat(totalJobsRun)\n+              .as(\"Expected total jobs to be equal to total number of shuffle partitions\")\n+              .isEqualTo(SHUFFLE_PARTITIONS);\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testIgnoreMetadataFilesNotFound() {\n     table.updateProperties().set(TableProperties.METADATA_PREVIOUS_VERSIONS_MAX, \"1\").commit();\n \n@@ -345,11 +359,10 @@ public void testIgnoreMetadataFilesNotFound() {\n     DeleteOrphanFiles.Result result =\n         sparkActions().deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n \n-    Assert.assertEquals(\"Should delete 1 file\", 1, Iterables.size(result.orphanFileLocations()));\n-    Assert.assertTrue(\n-        \"Should remove v1 file\",\n-        StreamSupport.stream(result.orphanFileLocations().spliterator(), false)\n-            .anyMatch(file -> file.contains(\"v1.metadata.json\")));\n+    assertThat(result.orphanFileLocations()).as(\"Should delete 1 file\").hasSize(1);\n+    assertThat(StreamSupport.stream(result.orphanFileLocations().spliterator(), false))\n+        .as(\"Should remove v1 file\")\n+        .anyMatch(file -> file.contains(\"v1.metadata.json\"));\n \n     DeleteReachableFiles baseRemoveFilesSparkAction =\n         sparkActions().deleteReachableFiles(metadataLocation(table)).io(table.io());\n@@ -358,7 +371,7 @@ public void testIgnoreMetadataFilesNotFound() {\n     checkRemoveFilesResults(1, 0, 0, 1, 1, 4, res);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testEmptyIOThrowsException() {\n     DeleteReachableFiles baseRemoveFilesSparkAction =\n         sparkActions().deleteReachableFiles(metadataLocation(table)).io(null);\n@@ -368,7 +381,7 @@ public void testEmptyIOThrowsException() {\n         .hasMessage(\"File IO cannot be null\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveFilesActionWhenGarbageCollectionDisabled() {\n     table.updateProperties().set(TableProperties.GC_ENABLED, \"false\").commit();\n \n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java\nindex 03ae730312fb..9d4fff4957ae 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java\n@@ -19,10 +19,14 @@\n package org.apache.iceberg.spark.actions;\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Path;\n+import java.util.Arrays;\n import java.util.List;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n@@ -36,6 +40,9 @@\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileMetadata;\n import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.ReachableFileUtil;\n import org.apache.iceberg.Schema;\n@@ -51,19 +58,19 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n-import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.spark.data.TestHelpers;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.Dataset;\n import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n \n-public class TestExpireSnapshotsAction extends SparkTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestExpireSnapshotsAction extends TestBase {\n   private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n   private static final Schema SCHEMA =\n       new Schema(\n@@ -119,17 +126,27 @@ public class TestExpireSnapshotsAction extends SparkTestBase {\n           .withRecordCount(1)\n           .build();\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n+  @Parameter private int formatVersion;\n \n-  private File tableDir;\n+  @Parameters(name = \"formatVersion = {0}\")\n+  protected static List<Object> parameters() {\n+    return Arrays.asList(2, 3);\n+  }\n+\n+  @TempDir private File tableDir;\n   private String tableLocation;\n   private Table table;\n \n-  @Before\n+  @BeforeEach\n   public void setupTableLocation() throws Exception {\n-    this.tableDir = temp.newFolder();\n     this.tableLocation = tableDir.toURI().toString();\n-    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+    this.table =\n+        TABLES.create(\n+            SCHEMA,\n+            SPEC,\n+            ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion)),\n+            tableLocation);\n     spark.conf().set(\"spark.sql.shuffle.partitions\", SHUFFLE_PARTITIONS);\n   }\n \n@@ -153,29 +170,28 @@ private void checkExpirationResults(\n       long expectedManifestListsDeleted,\n       ExpireSnapshots.Result results) {\n \n-    Assert.assertEquals(\n-        \"Incorrect number of manifest files deleted\",\n-        expectedManifestsDeleted,\n-        results.deletedManifestsCount());\n-    Assert.assertEquals(\n-        \"Incorrect number of datafiles deleted\",\n-        expectedDatafiles,\n-        results.deletedDataFilesCount());\n-    Assert.assertEquals(\n-        \"Incorrect number of pos deletefiles deleted\",\n-        expectedPosDeleteFiles,\n-        results.deletedPositionDeleteFilesCount());\n-    Assert.assertEquals(\n-        \"Incorrect number of eq deletefiles deleted\",\n-        expectedEqDeleteFiles,\n-        results.deletedEqualityDeleteFilesCount());\n-    Assert.assertEquals(\n-        \"Incorrect number of manifest lists deleted\",\n-        expectedManifestListsDeleted,\n-        results.deletedManifestListsCount());\n+    assertThat(results.deletedManifestsCount())\n+        .as(\"Incorrect number of manifest files deleted\")\n+        .isEqualTo(expectedManifestsDeleted);\n+\n+    assertThat(results.deletedDataFilesCount())\n+        .as(\"Incorrect number of datafiles deleted\")\n+        .isEqualTo(expectedDatafiles);\n+\n+    assertThat(results.deletedPositionDeleteFilesCount())\n+        .as(\"Incorrect number of pos deletefiles deleted\")\n+        .isEqualTo(expectedPosDeleteFiles);\n+\n+    assertThat(results.deletedEqualityDeleteFilesCount())\n+        .as(\"Incorrect number of eq deletefiles deleted\")\n+        .isEqualTo(expectedEqDeleteFiles);\n+\n+    assertThat(results.deletedManifestListsCount())\n+        .as(\"Incorrect number of manifest lists deleted\")\n+        .isEqualTo(expectedManifestListsDeleted);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilesCleaned() throws Exception {\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n@@ -188,13 +204,12 @@ public void testFilesCleaned() throws Exception {\n     ExpireSnapshots.Result results =\n         SparkActions.get().expireSnapshots(table).expireOlderThan(end).execute();\n \n-    Assert.assertEquals(\n-        \"Table does not have 1 snapshot after expiration\", 1, Iterables.size(table.snapshots()));\n+    assertThat(table.snapshots()).as(\"Table does not have 1 snapshot after expiration\").hasSize(1);\n \n     checkExpirationResults(1L, 0L, 0L, 1L, 2L, results);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void dataFilesCleanupWithParallelTasks() throws IOException {\n \n     table.newFastAppend().appendFile(FILE_A).commit();\n@@ -234,18 +249,17 @@ public void dataFilesCleanupWithParallelTasks() throws IOException {\n \n     // Verifies that the delete methods ran in the threads created by the provided ExecutorService\n     // ThreadFactory\n-    Assert.assertEquals(\n-        deleteThreads,\n-        Sets.newHashSet(\n-            \"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\"));\n+    assertThat(deleteThreads)\n+        .containsExactlyInAnyOrder(\n+            \"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\");\n \n-    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.location()));\n-    Assert.assertTrue(\"FILE_B should be deleted\", deletedFiles.contains(FILE_B.location()));\n+    assertThat(deletedFiles).as(\"FILE_A should be deleted\").contains(FILE_A.location());\n+    assertThat(deletedFiles).as(\"FILE_B should be deleted\").contains(FILE_B.location());\n \n     checkExpirationResults(2L, 0L, 0L, 3L, 3L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n@@ -253,7 +267,7 @@ public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n     checkExpirationResults(0L, 0L, 0L, 0L, 0L, results);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCleanupRepeatedOverwrites() throws Exception {\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n@@ -269,7 +283,7 @@ public void testCleanupRepeatedOverwrites() throws Exception {\n     checkExpirationResults(1L, 0L, 0L, 39L, 20L, results);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRetainLastWithExpireOlderThan() {\n     table\n         .newAppend()\n@@ -296,13 +310,11 @@ public void testRetainLastWithExpireOlderThan() {\n     // Retain last 2 snapshots\n     SparkActions.get().expireSnapshots(table).expireOlderThan(t3).retainLast(2).execute();\n \n-    Assert.assertEquals(\n-        \"Should have two snapshots.\", 2, Lists.newArrayList(table.snapshots()).size());\n-    Assert.assertEquals(\n-        \"First snapshot should not present.\", null, table.snapshot(firstSnapshotId));\n+    assertThat(table.snapshots()).as(\"Should have two snapshots.\").hasSize(2);\n+    assertThat(table.snapshot(firstSnapshotId)).as(\"First snapshot should not present.\").isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireTwoSnapshotsById() throws Exception {\n     table\n         .newAppend()\n@@ -330,17 +342,14 @@ public void testExpireTwoSnapshotsById() throws Exception {\n             .expireSnapshotId(secondSnapshotID)\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Should have one snapshots.\", 1, Lists.newArrayList(table.snapshots()).size());\n-    Assert.assertEquals(\n-        \"First snapshot should not present.\", null, table.snapshot(firstSnapshotId));\n-    Assert.assertEquals(\n-        \"Second snapshot should not be present.\", null, table.snapshot(secondSnapshotID));\n+    assertThat(table.snapshots()).as(\"Should have one snapshot.\").hasSize(1);\n+    assertThat(table.snapshot(firstSnapshotId)).as(\"First snapshot should not present.\").isNull();\n+    assertThat(table.snapshot(secondSnapshotID)).as(\"Second snapshot should not present.\").isNull();\n \n     checkExpirationResults(0L, 0L, 0L, 0L, 2L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRetainLastWithExpireById() {\n     table\n         .newAppend()\n@@ -366,14 +375,12 @@ public void testRetainLastWithExpireById() {\n             .retainLast(3)\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Should have two snapshots.\", 2, Lists.newArrayList(table.snapshots()).size());\n-    Assert.assertEquals(\n-        \"First snapshot should not present.\", null, table.snapshot(firstSnapshotId));\n+    assertThat(table.snapshots()).as(\"Should have 2 snapshots.\").hasSize(2);\n+    assertThat(table.snapshot(firstSnapshotId)).as(\"First snapshot should not present.\").isNull();\n     checkExpirationResults(0L, 0L, 0L, 0L, 1L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRetainLastWithTooFewSnapshots() {\n     table\n         .newAppend()\n@@ -393,16 +400,14 @@ public void testRetainLastWithTooFewSnapshots() {\n     ExpireSnapshots.Result result =\n         SparkActions.get().expireSnapshots(table).expireOlderThan(t2).retainLast(3).execute();\n \n-    Assert.assertEquals(\n-        \"Should have two snapshots\", 2, Lists.newArrayList(table.snapshots()).size());\n-    Assert.assertEquals(\n-        \"First snapshot should still present\",\n-        firstSnapshotId,\n-        table.snapshot(firstSnapshotId).snapshotId());\n+    assertThat(table.snapshots()).as(\"Should have two snapshots.\").hasSize(2);\n+    assertThat(table.snapshot(firstSnapshotId))\n+        .as(\"First snapshot should still be present.\")\n+        .isNotNull();\n     checkExpirationResults(0L, 0L, 0L, 0L, 0L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRetainLastKeepsExpiringSnapshot() {\n     table\n         .newAppend()\n@@ -434,14 +439,14 @@ public void testRetainLastKeepsExpiringSnapshot() {\n             .retainLast(2)\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Should have three snapshots.\", 3, Lists.newArrayList(table.snapshots()).size());\n-    Assert.assertNotNull(\n-        \"Second snapshot should present.\", table.snapshot(secondSnapshot.snapshotId()));\n+    assertThat(table.snapshots()).as(\"Should have three snapshots.\").hasSize(3);\n+    assertThat(table.snapshot(secondSnapshot.snapshotId()))\n+        .as(\"First snapshot should be present.\")\n+        .isNotNull();\n     checkExpirationResults(0L, 0L, 0L, 0L, 1L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireSnapshotsWithDisabledGarbageCollection() {\n     table.updateProperties().set(TableProperties.GC_ENABLED, \"false\").commit();\n \n@@ -453,7 +458,7 @@ public void testExpireSnapshotsWithDisabledGarbageCollection() {\n             \"Cannot expire snapshots: GC is disabled (deleting files may corrupt other tables)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOlderThanMultipleCalls() {\n     table\n         .newAppend()\n@@ -482,14 +487,14 @@ public void testExpireOlderThanMultipleCalls() {\n             .expireOlderThan(thirdSnapshot.timestampMillis())\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Should have one snapshots.\", 1, Lists.newArrayList(table.snapshots()).size());\n-    Assert.assertNull(\n-        \"Second snapshot should not present.\", table.snapshot(secondSnapshot.snapshotId()));\n+    assertThat(table.snapshots()).as(\"Should have one snapshot.\").hasSize(1);\n+    assertThat(table.snapshot(secondSnapshot.snapshotId()))\n+        .as(\"Second snapshot should not present.\")\n+        .isNull();\n     checkExpirationResults(0L, 0L, 0L, 0L, 2L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRetainLastMultipleCalls() {\n     table\n         .newAppend()\n@@ -519,21 +524,21 @@ public void testRetainLastMultipleCalls() {\n             .retainLast(1)\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Should have one snapshots.\", 1, Lists.newArrayList(table.snapshots()).size());\n-    Assert.assertNull(\n-        \"Second snapshot should not present.\", table.snapshot(secondSnapshot.snapshotId()));\n+    assertThat(table.snapshots()).as(\"Should have one snapshot.\").hasSize(1);\n+    assertThat(table.snapshot(secondSnapshot.snapshotId()))\n+        .as(\"Second snapshot should not present.\")\n+        .isNull();\n     checkExpirationResults(0L, 0L, 0L, 0L, 2L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRetainZeroSnapshots() {\n     assertThatThrownBy(() -> SparkActions.get().expireSnapshots(table).retainLast(0).execute())\n         .isInstanceOf(IllegalArgumentException.class)\n         .hasMessage(\"Number of snapshots to retain must be at least 1, cannot be: 0\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testScanExpiredManifestInValidSnapshotAppend() {\n     table.newAppend().appendFile(FILE_A).appendFile(FILE_B).commit();\n \n@@ -552,11 +557,11 @@ public void testScanExpiredManifestInValidSnapshotAppend() {\n             .deleteWith(deletedFiles::add)\n             .execute();\n \n-    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.location()));\n+    assertThat(deletedFiles).as(\"FILE_A should be deleted\").contains(FILE_A.location());\n     checkExpirationResults(1L, 0L, 0L, 1L, 2L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testScanExpiredManifestInValidSnapshotFastAppend() {\n     table\n         .updateProperties()\n@@ -581,7 +586,7 @@ public void testScanExpiredManifestInValidSnapshotFastAppend() {\n             .deleteWith(deletedFiles::add)\n             .execute();\n \n-    Assert.assertTrue(\"FILE_A should be deleted\", deletedFiles.contains(FILE_A.location()));\n+    assertThat(deletedFiles).as(\"FILE_A should be deleted\").contains(FILE_A.location());\n     checkExpirationResults(1L, 0L, 0L, 1L, 2L, result);\n   }\n \n@@ -589,7 +594,7 @@ public void testScanExpiredManifestInValidSnapshotFastAppend() {\n    * Test on table below, and expiring the staged commit `B` using `expireOlderThan` API. Table: A -\n    * C ` B (staged)\n    */\n-  @Test\n+  @TestTemplate\n   public void testWithExpiringDanglingStageCommit() {\n     // `A` commit\n     table.newAppend().appendFile(FILE_A).commit();\n@@ -638,8 +643,9 @@ public void testWithExpiringDanglingStageCommit() {\n                 expectedDeletes.add(file.path());\n               }\n             });\n-    Assert.assertSame(\n-        \"Files deleted count should be expected\", expectedDeletes.size(), deletedFiles.size());\n+    assertThat(expectedDeletes)\n+        .as(\"Files deleted count should be expected\")\n+        .hasSameSizeAs(deletedFiles);\n     // Take the diff\n     expectedDeletes.removeAll(deletedFiles);\n     Assert.assertTrue(\"Exactly same files should be deleted\", expectedDeletes.isEmpty());\n@@ -649,7 +655,7 @@ public void testWithExpiringDanglingStageCommit() {\n    * Expire cherry-pick the commit as shown below, when `B` is in table's current state Table: A - B\n    * - C <--current snapshot `- D (source=B)\n    */\n-  @Test\n+  @TestTemplate\n   public void testWithCherryPickTableSnapshot() {\n     // `A` commit\n     table.newAppend().appendFile(FILE_A).commit();\n@@ -704,7 +710,7 @@ public void testWithCherryPickTableSnapshot() {\n    * Test on table below, and expiring `B` which is not in current table state. 1) Expire `B` 2) All\n    * commit Table: A - C - D (B) ` B (staged)\n    */\n-  @Test\n+  @TestTemplate\n   public void testWithExpiringStagedThenCherrypick() {\n     // `A` commit\n     table.newAppend().appendFile(FILE_A).commit();\n@@ -768,7 +774,7 @@ public void testWithExpiringStagedThenCherrypick() {\n     checkExpirationResults(0L, 0L, 0L, 0L, 2L, secondResult);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOlderThan() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n@@ -791,37 +797,34 @@ public void testExpireOlderThan() {\n             .deleteWith(deletedFiles::add)\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Expire should not change current snapshot\",\n-        snapshotId,\n-        table.currentSnapshot().snapshotId());\n-    Assert.assertNull(\n-        \"Expire should remove the oldest snapshot\", table.snapshot(firstSnapshot.snapshotId()));\n-    Assert.assertEquals(\n-        \"Should remove only the expired manifest list location\",\n-        Sets.newHashSet(firstSnapshot.manifestListLocation()),\n-        deletedFiles);\n+    assertThat(table.currentSnapshot().snapshotId())\n+        .as(\"Expire should not change current snapshot.\")\n+        .isEqualTo(snapshotId);\n+    assertThat(table.snapshot(firstSnapshot.snapshotId()))\n+        .as(\"Expire should remove the oldest snapshot.\")\n+        .isNull();\n+    assertThat(deletedFiles)\n+        .as(\"Should remove only the expired manifest list location.\")\n+        .containsExactly(firstSnapshot.manifestListLocation());\n \n     checkExpirationResults(0, 0, 0, 0, 1, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOlderThanWithDelete() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n     Snapshot firstSnapshot = table.currentSnapshot();\n-    Assert.assertEquals(\n-        \"Should create one manifest\", 1, firstSnapshot.allManifests(table.io()).size());\n+    assertThat(firstSnapshot.allManifests(table.io())).as(\"Should create one manifest\").hasSize(1);\n \n     rightAfterSnapshot();\n \n     table.newDelete().deleteFile(FILE_A).commit();\n \n     Snapshot secondSnapshot = table.currentSnapshot();\n-    Assert.assertEquals(\n-        \"Should create replace manifest with a rewritten manifest\",\n-        1,\n-        secondSnapshot.allManifests(table.io()).size());\n+    assertThat(secondSnapshot.allManifests(table.io()))\n+        .as(\"Should create replace manifest with a rewritten manifest\")\n+        .hasSize(1);\n \n     table.newAppend().appendFile(FILE_B).commit();\n \n@@ -840,19 +843,18 @@ public void testExpireOlderThanWithDelete() {\n             .deleteWith(deletedFiles::add)\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Expire should not change current snapshot\",\n-        snapshotId,\n-        table.currentSnapshot().snapshotId());\n-    Assert.assertNull(\n-        \"Expire should remove the oldest snapshot\", table.snapshot(firstSnapshot.snapshotId()));\n-    Assert.assertNull(\n-        \"Expire should remove the second oldest snapshot\",\n-        table.snapshot(secondSnapshot.snapshotId()));\n-\n-    Assert.assertEquals(\n-        \"Should remove expired manifest lists and deleted data file\",\n-        Sets.newHashSet(\n+    assertThat(table.currentSnapshot().snapshotId())\n+        .as(\"Expire should not change current snapshot.\")\n+        .isEqualTo(snapshotId);\n+    assertThat(table.snapshot(firstSnapshot.snapshotId()))\n+        .as(\"Expire should remove the oldest snapshot.\")\n+        .isNull();\n+    assertThat(table.snapshot(secondSnapshot.snapshotId()))\n+        .as(\"Expire should remove the second oldest snapshot.\")\n+        .isNull();\n+    assertThat(deletedFiles)\n+        .as(\"Should remove expired manifest lists and deleted data file.\")\n+        .containsExactlyInAnyOrder(\n             firstSnapshot.manifestListLocation(), // snapshot expired\n             firstSnapshot\n                 .allManifests(table.io())\n@@ -863,13 +865,12 @@ public void testExpireOlderThanWithDelete() {\n                 .allManifests(table.io())\n                 .get(0)\n                 .path(), // manifest contained only deletes, was dropped\n-            FILE_A.location()), // deleted\n-        deletedFiles);\n+            FILE_A.location());\n \n     checkExpirationResults(1, 0, 0, 2, 2, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOlderThanWithDeleteInMergedManifests() {\n     // merge every commit\n     table.updateProperties().set(TableProperties.MANIFEST_MIN_MERGE_COUNT, \"0\").commit();\n@@ -877,8 +878,7 @@ public void testExpireOlderThanWithDeleteInMergedManifests() {\n     table.newAppend().appendFile(FILE_A).appendFile(FILE_B).commit();\n \n     Snapshot firstSnapshot = table.currentSnapshot();\n-    Assert.assertEquals(\n-        \"Should create one manifest\", 1, firstSnapshot.allManifests(table.io()).size());\n+    assertThat(firstSnapshot.allManifests(table.io())).as(\"Should create one manifest\").hasSize(1);\n \n     rightAfterSnapshot();\n \n@@ -888,10 +888,9 @@ public void testExpireOlderThanWithDeleteInMergedManifests() {\n         .commit();\n \n     Snapshot secondSnapshot = table.currentSnapshot();\n-    Assert.assertEquals(\n-        \"Should replace manifest with a rewritten manifest\",\n-        1,\n-        secondSnapshot.allManifests(table.io()).size());\n+    assertThat(secondSnapshot.allManifests(table.io()))\n+        .as(\"Should replace manifest with a rewritten manifest\")\n+        .hasSize(1);\n \n     table\n         .newFastAppend() // do not merge to keep the last snapshot's manifest valid\n@@ -913,32 +912,30 @@ public void testExpireOlderThanWithDeleteInMergedManifests() {\n             .deleteWith(deletedFiles::add)\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Expire should not change current snapshot\",\n-        snapshotId,\n-        table.currentSnapshot().snapshotId());\n-    Assert.assertNull(\n-        \"Expire should remove the oldest snapshot\", table.snapshot(firstSnapshot.snapshotId()));\n-    Assert.assertNull(\n-        \"Expire should remove the second oldest snapshot\",\n-        table.snapshot(secondSnapshot.snapshotId()));\n-\n-    Assert.assertEquals(\n-        \"Should remove expired manifest lists and deleted data file\",\n-        Sets.newHashSet(\n+    assertThat(table.currentSnapshot().snapshotId())\n+        .as(\"Expire should not change current snapshot.\")\n+        .isEqualTo(snapshotId);\n+    assertThat(table.snapshot(firstSnapshot.snapshotId()))\n+        .as(\"Expire should remove the oldest snapshot.\")\n+        .isNull();\n+    assertThat(table.snapshot(secondSnapshot.snapshotId()))\n+        .as(\"Expire should remove the second oldest snapshot.\")\n+        .isNull();\n+\n+    assertThat(deletedFiles)\n+        .as(\"Should remove expired manifest lists and deleted data file.\")\n+        .containsExactlyInAnyOrder(\n             firstSnapshot.manifestListLocation(), // snapshot expired\n             firstSnapshot\n                 .allManifests(table.io())\n                 .get(0)\n                 .path(), // manifest was rewritten for delete\n             secondSnapshot.manifestListLocation(), // snapshot expired\n-            FILE_A.location()), // deleted\n-        deletedFiles);\n-\n+            FILE_A.location());\n     checkExpirationResults(1, 0, 0, 1, 2, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOlderThanWithRollback() {\n     // merge every commit\n     table.updateProperties().set(TableProperties.MANIFEST_MIN_MERGE_COUNT, \"0\").commit();\n@@ -946,8 +943,7 @@ public void testExpireOlderThanWithRollback() {\n     table.newAppend().appendFile(FILE_A).appendFile(FILE_B).commit();\n \n     Snapshot firstSnapshot = table.currentSnapshot();\n-    Assert.assertEquals(\n-        \"Should create one manifest\", 1, firstSnapshot.allManifests(table.io()).size());\n+    assertThat(firstSnapshot.allManifests(table.io())).as(\"Should create one manifest\").hasSize(1);\n \n     rightAfterSnapshot();\n \n@@ -957,8 +953,7 @@ public void testExpireOlderThanWithRollback() {\n     Set<ManifestFile> secondSnapshotManifests =\n         Sets.newHashSet(secondSnapshot.allManifests(table.io()));\n     secondSnapshotManifests.removeAll(firstSnapshot.allManifests(table.io()));\n-    Assert.assertEquals(\n-        \"Should add one new manifest for append\", 1, secondSnapshotManifests.size());\n+    assertThat(secondSnapshotManifests).as(\"Should add one new manifest for append\").hasSize(1);\n \n     table.manageSnapshots().rollbackTo(firstSnapshot.snapshotId()).commit();\n \n@@ -975,34 +970,32 @@ public void testExpireOlderThanWithRollback() {\n             .deleteWith(deletedFiles::add)\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Expire should not change current snapshot\",\n-        snapshotId,\n-        table.currentSnapshot().snapshotId());\n-    Assert.assertNotNull(\n-        \"Expire should keep the oldest snapshot, current\",\n-        table.snapshot(firstSnapshot.snapshotId()));\n-    Assert.assertNull(\n-        \"Expire should remove the orphaned snapshot\", table.snapshot(secondSnapshot.snapshotId()));\n-\n-    Assert.assertEquals(\n-        \"Should remove expired manifest lists and reverted appended data file\",\n-        Sets.newHashSet(\n+    assertThat(table.currentSnapshot().snapshotId())\n+        .as(\"Expire should not change current snapshot.\")\n+        .isEqualTo(snapshotId);\n+\n+    assertThat(table.snapshot(firstSnapshot.snapshotId()))\n+        .as(\"Expire should keep the oldest snapshot, current.\")\n+        .isNotNull();\n+    assertThat(table.snapshot(secondSnapshot.snapshotId()))\n+        .as(\"Expire should remove the orphaned snapshot.\")\n+        .isNull();\n+\n+    assertThat(deletedFiles)\n+        .as(\"Should remove expired manifest lists and reverted appended data file\")\n+        .containsExactlyInAnyOrder(\n             secondSnapshot.manifestListLocation(), // snapshot expired\n-            Iterables.getOnlyElement(secondSnapshotManifests)\n-                .path()), // manifest is no longer referenced\n-        deletedFiles);\n+            Iterables.getOnlyElement(secondSnapshotManifests).path());\n \n     checkExpirationResults(0, 0, 0, 1, 1, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOlderThanWithRollbackAndMergedManifests() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n     Snapshot firstSnapshot = table.currentSnapshot();\n-    Assert.assertEquals(\n-        \"Should create one manifest\", 1, firstSnapshot.allManifests(table.io()).size());\n+    assertThat(firstSnapshot.allManifests(table.io())).as(\"Should create one manifest\").hasSize(1);\n \n     rightAfterSnapshot();\n \n@@ -1012,8 +1005,7 @@ public void testExpireOlderThanWithRollbackAndMergedManifests() {\n     Set<ManifestFile> secondSnapshotManifests =\n         Sets.newHashSet(secondSnapshot.allManifests(table.io()));\n     secondSnapshotManifests.removeAll(firstSnapshot.allManifests(table.io()));\n-    Assert.assertEquals(\n-        \"Should add one new manifest for append\", 1, secondSnapshotManifests.size());\n+    assertThat(secondSnapshotManifests).as(\"Should add one new manifest for append\").hasSize(1);\n \n     table.manageSnapshots().rollbackTo(firstSnapshot.snapshotId()).commit();\n \n@@ -1030,35 +1022,32 @@ public void testExpireOlderThanWithRollbackAndMergedManifests() {\n             .deleteWith(deletedFiles::add)\n             .execute();\n \n-    Assert.assertEquals(\n-        \"Expire should not change current snapshot\",\n-        snapshotId,\n-        table.currentSnapshot().snapshotId());\n-    Assert.assertNotNull(\n-        \"Expire should keep the oldest snapshot, current\",\n-        table.snapshot(firstSnapshot.snapshotId()));\n-    Assert.assertNull(\n-        \"Expire should remove the orphaned snapshot\", table.snapshot(secondSnapshot.snapshotId()));\n-\n-    Assert.assertEquals(\n-        \"Should remove expired manifest lists and reverted appended data file\",\n-        Sets.newHashSet(\n+    assertThat(table.currentSnapshot().snapshotId())\n+        .as(\"Expire should not change current snapshot.\")\n+        .isEqualTo(snapshotId);\n+\n+    assertThat(table.snapshot(firstSnapshot.snapshotId()))\n+        .as(\"Expire should keep the oldest snapshot, current.\")\n+        .isNotNull();\n+    assertThat(table.snapshot(secondSnapshot.snapshotId()))\n+        .as(\"Expire should remove the orphaned snapshot.\")\n+        .isNull();\n+\n+    assertThat(deletedFiles)\n+        .as(\"Should remove expired manifest lists and reverted appended data file\")\n+        .containsExactlyInAnyOrder(\n             secondSnapshot.manifestListLocation(), // snapshot expired\n             Iterables.getOnlyElement(secondSnapshotManifests)\n                 .path(), // manifest is no longer referenced\n-            FILE_B.location()), // added, but rolled back\n-        deletedFiles);\n+            FILE_B.location());\n \n     checkExpirationResults(1, 0, 0, 1, 1, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOlderThanWithDeleteFile() {\n-    table\n-        .updateProperties()\n-        .set(TableProperties.FORMAT_VERSION, \"2\")\n-        .set(TableProperties.MANIFEST_MERGE_ENABLED, \"false\")\n-        .commit();\n+    assumeThat(formatVersion).as(\"DV is not supported in Spark 3.4\").isEqualTo(2);\n+    table.updateProperties().set(TableProperties.MANIFEST_MERGE_ENABLED, \"false\").commit();\n \n     // Add Data File\n     table.newAppend().appendFile(FILE_A).commit();\n@@ -1111,15 +1100,14 @@ public void testExpireOlderThanWithDeleteFile() {\n             .map(CharSequence::toString)\n             .collect(Collectors.toSet()));\n \n-    Assert.assertEquals(\n-        \"Should remove expired manifest lists and deleted data file\",\n-        expectedDeletes,\n-        deletedFiles);\n+    assertThat(deletedFiles)\n+        .as(\"Should remove expired manifest lists and deleted data file\")\n+        .isEqualTo(expectedDeletes);\n \n     checkExpirationResults(1, 1, 1, 6, 4, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOnEmptyTable() {\n     Set<String> deletedFiles = Sets.newHashSet();\n \n@@ -1134,7 +1122,7 @@ public void testExpireOnEmptyTable() {\n     checkExpirationResults(0, 0, 0, 0, 0, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireAction() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n@@ -1159,28 +1147,30 @@ public void testExpireAction() {\n \n     List<FileInfo> pending = pendingDeletes.collectAsList();\n \n-    Assert.assertEquals(\n-        \"Should not change current snapshot\", snapshotId, table.currentSnapshot().snapshotId());\n-    Assert.assertNull(\n-        \"Should remove the oldest snapshot\", table.snapshot(firstSnapshot.snapshotId()));\n-\n-    Assert.assertEquals(\"Pending deletes should contain one row\", 1, pending.size());\n-    Assert.assertEquals(\n-        \"Pending delete should be the expired manifest list location\",\n-        firstSnapshot.manifestListLocation(),\n-        pending.get(0).getPath());\n-    Assert.assertEquals(\n-        \"Pending delete should be a manifest list\", \"Manifest List\", pending.get(0).getType());\n-\n-    Assert.assertEquals(\"Should not delete any files\", 0, deletedFiles.size());\n-\n-    Assert.assertEquals(\n-        \"Multiple calls to expire should return the same count of deleted files\",\n-        pendingDeletes.count(),\n-        action.expireFiles().count());\n+    assertThat(table.currentSnapshot().snapshotId())\n+        .as(\"Should not change current snapshot.\")\n+        .isEqualTo(snapshotId);\n+\n+    assertThat(table.snapshot(firstSnapshot.snapshotId()))\n+        .as(\"Should remove the oldest snapshot\")\n+        .isNull();\n+\n+    assertThat(pending.get(0).getPath())\n+        .as(\"Pending delete should be the expired manifest list location\")\n+        .isEqualTo(firstSnapshot.manifestListLocation());\n+\n+    assertThat(pending.get(0).getType())\n+        .as(\"Pending delete should be a manifest list\")\n+        .isEqualTo(\"Manifest List\");\n+\n+    assertThat(deletedFiles).as(\"Should not delete any files\").isEmpty();\n+\n+    assertThat(action.expireFiles().count())\n+        .as(\"Multiple calls to expire should return the same count of deleted files\")\n+        .isEqualTo(pendingDeletes.count());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUseLocalIterator() {\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n@@ -1207,14 +1197,14 @@ public void testUseLocalIterator() {\n \n           checkExpirationResults(1L, 0L, 0L, 1L, 2L, results);\n \n-          Assert.assertEquals(\n-              \"Expected total number of jobs with stream-results should match the expected number\",\n-              4L,\n-              jobsRunDuringStreamResults);\n+          assertThat(jobsRunDuringStreamResults)\n+              .as(\n+                  \"Expected total number of jobs with stream-results should match the expected number\")\n+              .isEqualTo(4L);\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireAfterExecute() {\n     table\n         .newAppend()\n@@ -1243,18 +1233,18 @@ public void testExpireAfterExecute() {\n     checkExpirationResults(0L, 0L, 0L, 0L, 1L, result);\n \n     List<FileInfo> typedExpiredFiles = action.expireFiles().collectAsList();\n-    Assert.assertEquals(\"Expired results must match\", 1, typedExpiredFiles.size());\n+    assertThat(typedExpiredFiles).as(\"Expired results must match\").hasSize(1);\n \n     List<FileInfo> untypedExpiredFiles = action.expireFiles().collectAsList();\n-    Assert.assertEquals(\"Expired results must match\", 1, untypedExpiredFiles.size());\n+    assertThat(untypedExpiredFiles).as(\"Expired results must match\").hasSize(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireFileDeletionMostExpired() {\n     textExpireAllCheckFilesDeleted(5, 2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireFileDeletionMostRetained() {\n     textExpireAllCheckFilesDeleted(2, 5);\n   }\n@@ -1311,11 +1301,12 @@ public void textExpireAllCheckFilesDeleted(int dataFilesExpired, int dataFilesRe\n         .deleteWith(deletedFiles::add)\n         .execute();\n \n-    Assert.assertEquals(\n-        \"All reachable files before expiration should be deleted\", expectedDeletes, deletedFiles);\n+    assertThat(deletedFiles)\n+        .as(\"All reachable files before expiration should be deleted\")\n+        .isEqualTo(expectedDeletes);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireSomeCheckFilesDeleted() {\n \n     table.newAppend().appendFile(FILE_A).commit();\n@@ -1343,9 +1334,8 @@ public void testExpireSomeCheckFilesDeleted() {\n     // C, D should be retained (live)\n     // B should be retained (previous snapshot points to it)\n     // A should be deleted\n-    Assert.assertTrue(deletedFiles.contains(FILE_A.location()));\n-    Assert.assertFalse(deletedFiles.contains(FILE_B.location()));\n-    Assert.assertFalse(deletedFiles.contains(FILE_C.location()));\n-    Assert.assertFalse(deletedFiles.contains(FILE_D.location()));\n+    assertThat(deletedFiles)\n+        .contains(FILE_A.location())\n+        .doesNotContain(FILE_B.location(), FILE_C.location(), FILE_D.location());\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveDanglingDeleteAction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveDanglingDeleteAction.java\nindex dd330e461b1d..fb07421fc5db 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveDanglingDeleteAction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveDanglingDeleteAction.java\n@@ -22,6 +22,7 @@\n import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.File;\n+import java.util.Arrays;\n import java.util.List;\n import java.util.Set;\n import java.util.stream.Collectors;\n@@ -31,6 +32,9 @@\n import org.apache.iceberg.DataFiles;\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileMetadata;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -39,17 +43,18 @@\n import org.apache.iceberg.hadoop.HadoopTables;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.Encoders;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n import scala.Tuple2;\n \n-public class TestRemoveDanglingDeleteAction extends SparkTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestRemoveDanglingDeleteAction extends TestBase {\n   private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n   private static final Schema SCHEMA =\n       new Schema(\n@@ -197,18 +202,24 @@ public class TestRemoveDanglingDeleteAction extends SparkTestBase {\n           .withFileSizeInBytes(10)\n           .withRecordCount(1)\n           .build();\n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+\n+  @TempDir private File tableDir;\n+  @Parameter private int formatVersion;\n+\n+  @Parameters(name = \"formatVersion = {0}\")\n+  protected static List<Object> parameters() {\n+    return Arrays.asList(2, 3);\n+  }\n \n   private String tableLocation = null;\n   private Table table;\n \n-  @Before\n+  @BeforeEach\n   public void before() throws Exception {\n-    File tableDir = temp.newFolder();\n     this.tableLocation = tableDir.toURI().toString();\n   }\n \n-  @After\n+  @AfterEach\n   public void after() {\n     TABLES.dropTable(tableLocation);\n   }\n@@ -228,7 +239,7 @@ private void setupUnpartitionedTable() {\n             tableLocation);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedDeletesWithLesserSeqNo() {\n     setupPartitionedTable();\n     // Add Data Files\n@@ -322,7 +333,7 @@ public void testPartitionedDeletesWithLesserSeqNo() {\n     assertThat(actualAfter).isEqualTo(expectedAfter);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedDeletesWithEqSeqNo() {\n     setupPartitionedTable();\n     // Add Data Files\n@@ -410,7 +421,7 @@ public void testPartitionedDeletesWithEqSeqNo() {\n     assertThat(actualAfter).isEqualTo(expectedAfter);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedTable() {\n     setupUnpartitionedTable();\n     table\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteTablePathsAction.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteTablePathsAction.java\nindex db1a068d4fd2..225217165ea9 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteTablePathsAction.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteTablePathsAction.java\n@@ -25,6 +25,7 @@\n import java.io.File;\n import java.io.IOException;\n import java.net.URI;\n+import java.nio.file.Path;\n import java.util.List;\n import java.util.Map;\n import java.util.stream.Collectors;\n@@ -59,7 +60,7 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.spark.SparkCatalog;\n-import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.spark.source.ThreeColumnRecord;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.Pair;\n@@ -73,16 +74,18 @@\n import org.apache.spark.storage.BlockInfoManager;\n import org.apache.spark.storage.BlockManager;\n import org.apache.spark.storage.BroadcastBlockId;\n-import org.junit.After;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n import scala.Tuple2;\n \n-public class TestRewriteTablePathsAction extends SparkTestBase {\n+public class TestRewriteTablePathsAction extends TestBase {\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path staging;\n+  @TempDir private Path tableDir;\n+  @TempDir private Path newTableDir;\n+  @TempDir private Path targetTableDir;\n \n   protected ActionsProvider actions() {\n     return SparkActions.get();\n@@ -96,27 +99,19 @@ protected ActionsProvider actions() {\n           optional(3, \"c3\", Types.StringType.get()));\n \n   protected String tableLocation = null;\n-  public String staging = null;\n-  public String tableDir = null;\n-  public String newTableDir = null;\n-  public String targetTableDir = null;\n   private Table table = null;\n \n   private final String ns = \"testns\";\n   private final String backupNs = \"backupns\";\n \n-  @Before\n-  public void setupTableLocation() throws Exception {\n-    this.tableLocation = temp.newFolder().toURI().toString();\n-    this.staging = temp.newFolder(\"staging\").toURI().toString();\n-    this.tableDir = temp.newFolder(\"table\").toURI().toString();\n-    this.newTableDir = temp.newFolder(\"newTable\").toURI().toString();\n-    this.targetTableDir = temp.newFolder(\"targetTable\").toURI().toString();\n+  @BeforeEach\n+  public void setupTableLocation() {\n+    this.tableLocation = tableDir.toFile().toURI().toString();\n     this.table = createATableWith2Snapshots(tableLocation);\n     createNameSpaces();\n   }\n \n-  @After\n+  @AfterEach\n   public void cleanupTableSetup() throws Exception {\n     dropNameSpaces();\n   }\n@@ -241,13 +236,14 @@ public void testStartVersion() throws Exception {\n   }\n \n   @Test\n-  public void testTableWith3Snapshots() throws Exception {\n+  public void testTableWith3Snapshots(@TempDir Path location1, @TempDir Path location2)\n+      throws Exception {\n     String location = newTableLocation();\n     Table tableWith3Snaps = createTableWithSnapshots(location, 3);\n     RewriteTablePath.Result result =\n         actions()\n             .rewriteTablePath(tableWith3Snaps)\n-            .rewriteLocationPrefix(location, temp.newFolder().toURI().toString())\n+            .rewriteLocationPrefix(location, toAbsolute(location1))\n             .startVersion(\"v2.metadata.json\")\n             .execute();\n \n@@ -257,7 +253,7 @@ public void testTableWith3Snapshots() throws Exception {\n     RewriteTablePath.Result result1 =\n         actions()\n             .rewriteTablePath(tableWith3Snaps)\n-            .rewriteLocationPrefix(location, temp.newFolder().toURI().toString())\n+            .rewriteLocationPrefix(location, toAbsolute(location2))\n             .startVersion(\"v1.metadata.json\")\n             .execute();\n \n@@ -956,15 +952,19 @@ protected void checkFileNum(\n   }\n \n   protected String newTableLocation() throws IOException {\n-    return newTableDir;\n+    return toAbsolute(newTableDir);\n   }\n \n   protected String targetTableLocation() throws IOException {\n-    return targetTableDir;\n+    return toAbsolute(targetTableDir);\n   }\n \n   protected String stagingLocation() throws IOException {\n-    return staging;\n+    return toAbsolute(staging);\n+  }\n+\n+  protected String toAbsolute(Path relative) throws IOException {\n+    return relative.toFile().toURI().toString();\n   }\n \n   private void copyTableFiles(RewriteTablePath.Result result) throws Exception {\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\nindex 7d728a912214..fc99a85e2745 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\n@@ -18,6 +18,7 @@\n  */\n package org.apache.iceberg.spark.actions;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.util.List;\n@@ -33,17 +34,15 @@\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n-import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.types.Types.IntegerType;\n import org.apache.iceberg.types.Types.NestedField;\n import org.apache.iceberg.types.Types.StringType;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Test;\n \n-public class TestSparkFileRewriter extends SparkTestBase {\n+public class TestSparkFileRewriter extends TestBase {\n \n   private static final TableIdentifier TABLE_IDENT = TableIdentifier.of(\"default\", \"tbl\");\n   private static final Schema SCHEMA =\n@@ -54,7 +53,7 @@ public class TestSparkFileRewriter extends SparkTestBase {\n       PartitionSpec.builderFor(SCHEMA).identity(\"dep\").build();\n   private static final SortOrder SORT_ORDER = SortOrder.builderFor(SCHEMA).asc(\"id\").build();\n \n-  @After\n+  @AfterEach\n   public void removeTable() {\n     catalog.dropTable(TABLE_IDENT);\n   }\n@@ -111,9 +110,9 @@ private void checkDataFileSizeFiltering(SizeBasedDataRewriter rewriter) {\n     rewriter.init(options);\n \n     Iterable<List<FileScanTask>> groups = rewriter.planFileGroups(tasks);\n-    Assert.assertEquals(\"Must have 1 group\", 1, Iterables.size(groups));\n+    assertThat(groups).as(\"Must have 1 group\").hasSize(1);\n     List<FileScanTask> group = Iterables.getOnlyElement(groups);\n-    Assert.assertEquals(\"Must rewrite 2 files\", 2, group.size());\n+    assertThat(group).as(\"Must rewrite 2 files\").hasSize(2);\n   }\n \n   private void checkDataFilesDeleteThreshold(SizeBasedDataRewriter rewriter) {\n@@ -130,9 +129,9 @@ private void checkDataFilesDeleteThreshold(SizeBasedDataRewriter rewriter) {\n     rewriter.init(options);\n \n     Iterable<List<FileScanTask>> groups = rewriter.planFileGroups(tasks);\n-    Assert.assertEquals(\"Must have 1 group\", 1, Iterables.size(groups));\n+    assertThat(groups).as(\"Must have 1 group\").hasSize(1);\n     List<FileScanTask> group = Iterables.getOnlyElement(groups);\n-    Assert.assertEquals(\"Must rewrite 1 file\", 1, group.size());\n+    assertThat(group).as(\"Must rewrite 1 file\").hasSize(1);\n   }\n \n   private void checkDataFileGroupWithEnoughFiles(SizeBasedDataRewriter rewriter) {\n@@ -153,9 +152,9 @@ private void checkDataFileGroupWithEnoughFiles(SizeBasedDataRewriter rewriter) {\n     rewriter.init(options);\n \n     Iterable<List<FileScanTask>> groups = rewriter.planFileGroups(tasks);\n-    Assert.assertEquals(\"Must have 1 group\", 1, Iterables.size(groups));\n+    assertThat(groups).as(\"Must have 1 group\").hasSize(1);\n     List<FileScanTask> group = Iterables.getOnlyElement(groups);\n-    Assert.assertEquals(\"Must rewrite 4 files\", 4, group.size());\n+    assertThat(group).as(\"Must rewrite 4 files\").hasSize(4);\n   }\n \n   private void checkDataFileGroupWithEnoughData(SizeBasedDataRewriter rewriter) {\n@@ -173,9 +172,9 @@ private void checkDataFileGroupWithEnoughData(SizeBasedDataRewriter rewriter) {\n     rewriter.init(options);\n \n     Iterable<List<FileScanTask>> groups = rewriter.planFileGroups(tasks);\n-    Assert.assertEquals(\"Must have 1 group\", 1, Iterables.size(groups));\n+    assertThat(groups).as(\"Must have 1 group\").hasSize(1);\n     List<FileScanTask> group = Iterables.getOnlyElement(groups);\n-    Assert.assertEquals(\"Must rewrite 3 files\", 3, group.size());\n+    assertThat(group).as(\"Must rewrite 3 files\").hasSize(3);\n   }\n \n   private void checkDataFileGroupWithTooMuchData(SizeBasedDataRewriter rewriter) {\n@@ -191,9 +190,9 @@ private void checkDataFileGroupWithTooMuchData(SizeBasedDataRewriter rewriter) {\n     rewriter.init(options);\n \n     Iterable<List<FileScanTask>> groups = rewriter.planFileGroups(tasks);\n-    Assert.assertEquals(\"Must have 1 group\", 1, Iterables.size(groups));\n+    assertThat(groups).as(\"Must have 1 group\").hasSize(1);\n     List<FileScanTask> group = Iterables.getOnlyElement(groups);\n-    Assert.assertEquals(\"Must rewrite big file\", 1, group.size());\n+    assertThat(group).as(\"Must rewrite big file\").hasSize(1);\n   }\n \n   @Test\n@@ -237,9 +236,9 @@ public void testBinPackDataValidOptions() {\n     Table table = catalog.createTable(TABLE_IDENT, SCHEMA);\n     SparkBinPackDataRewriter rewriter = new SparkBinPackDataRewriter(spark, table);\n \n-    Assert.assertEquals(\n-        \"Rewriter must report all supported options\",\n-        ImmutableSet.of(\n+    assertThat(rewriter.validOptions())\n+        .as(\"Rewriter must report all supported options\")\n+        .containsExactlyInAnyOrder(\n             SparkBinPackDataRewriter.TARGET_FILE_SIZE_BYTES,\n             SparkBinPackDataRewriter.MIN_FILE_SIZE_BYTES,\n             SparkBinPackDataRewriter.MAX_FILE_SIZE_BYTES,\n@@ -247,8 +246,7 @@ public void testBinPackDataValidOptions() {\n             SparkBinPackDataRewriter.REWRITE_ALL,\n             SparkBinPackDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n             SparkBinPackDataRewriter.DELETE_FILE_THRESHOLD,\n-            SparkBinPackDataRewriter.DELETE_RATIO_THRESHOLD),\n-        rewriter.validOptions());\n+            SparkBinPackDataRewriter.DELETE_RATIO_THRESHOLD);\n   }\n \n   @Test\n@@ -256,9 +254,9 @@ public void testSortDataValidOptions() {\n     Table table = catalog.createTable(TABLE_IDENT, SCHEMA);\n     SparkSortDataRewriter rewriter = new SparkSortDataRewriter(spark, table, SORT_ORDER);\n \n-    Assert.assertEquals(\n-        \"Rewriter must report all supported options\",\n-        ImmutableSet.of(\n+    assertThat(rewriter.validOptions())\n+        .as(\"Rewriter must report all supported options\")\n+        .containsExactlyInAnyOrder(\n             SparkSortDataRewriter.SHUFFLE_PARTITIONS_PER_FILE,\n             SparkSortDataRewriter.TARGET_FILE_SIZE_BYTES,\n             SparkSortDataRewriter.MIN_FILE_SIZE_BYTES,\n@@ -268,8 +266,7 @@ public void testSortDataValidOptions() {\n             SparkSortDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n             SparkSortDataRewriter.DELETE_FILE_THRESHOLD,\n             SparkSortDataRewriter.DELETE_RATIO_THRESHOLD,\n-            SparkSortDataRewriter.COMPRESSION_FACTOR),\n-        rewriter.validOptions());\n+            SparkSortDataRewriter.COMPRESSION_FACTOR);\n   }\n \n   @Test\n@@ -278,9 +275,9 @@ public void testZOrderDataValidOptions() {\n     ImmutableList<String> zOrderCols = ImmutableList.of(\"id\");\n     SparkZOrderDataRewriter rewriter = new SparkZOrderDataRewriter(spark, table, zOrderCols);\n \n-    Assert.assertEquals(\n-        \"Rewriter must report all supported options\",\n-        ImmutableSet.of(\n+    assertThat(rewriter.validOptions())\n+        .as(\"Rewriter must report all supported options\")\n+        .containsExactlyInAnyOrder(\n             SparkZOrderDataRewriter.SHUFFLE_PARTITIONS_PER_FILE,\n             SparkZOrderDataRewriter.TARGET_FILE_SIZE_BYTES,\n             SparkZOrderDataRewriter.MIN_FILE_SIZE_BYTES,\n@@ -292,8 +289,7 @@ public void testZOrderDataValidOptions() {\n             SparkZOrderDataRewriter.DELETE_RATIO_THRESHOLD,\n             SparkZOrderDataRewriter.COMPRESSION_FACTOR,\n             SparkZOrderDataRewriter.MAX_OUTPUT_SIZE,\n-            SparkZOrderDataRewriter.VAR_LENGTH_CONTRIBUTION),\n-        rewriter.validOptions());\n+            SparkZOrderDataRewriter.VAR_LENGTH_CONTRIBUTION);\n   }\n \n   @Test\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkSessionCatalog.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkSessionCatalog.java\nindex b8062a4a49fe..85408ffff1d9 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkSessionCatalog.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkSessionCatalog.java\n@@ -49,8 +49,8 @@ public void setupHmsUri() {\n   @Test\n   public void testValidateHmsUri() {\n     // HMS uris match\n-    assertThat(spark.sessionState().catalogManager().v2SessionCatalog().defaultNamespace()[0])\n-        .isEqualTo(\"default\");\n+    assertThat(spark.sessionState().catalogManager().v2SessionCatalog().defaultNamespace())\n+        .containsExactly(\"default\");\n \n     // HMS uris doesn't match\n     spark.sessionState().catalogManager().reset();\n@@ -69,15 +69,15 @@ public void testValidateHmsUri() {\n     spark.sessionState().catalogManager().reset();\n     spark.conf().set(catalogHmsUriKey, hmsUri);\n     spark.conf().unset(envHmsUriKey);\n-    assertThat(spark.sessionState().catalogManager().v2SessionCatalog().defaultNamespace()[0])\n-        .isEqualTo(\"default\");\n+    assertThat(spark.sessionState().catalogManager().v2SessionCatalog().defaultNamespace())\n+        .containsExactly(\"default\");\n \n     // no catalog HMS uri, only env HMS uri\n     spark.sessionState().catalogManager().reset();\n     spark.conf().set(envHmsUriKey, hmsUri);\n     spark.conf().unset(catalogHmsUriKey);\n-    assertThat(spark.sessionState().catalogManager().v2SessionCatalog().defaultNamespace()[0])\n-        .isEqualTo(\"default\");\n+    assertThat(spark.sessionState().catalogManager().v2SessionCatalog().defaultNamespace())\n+        .containsExactly(\"default\");\n   }\n \n   @Test\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java\nindex d5bb63b2d88a..85fbf7152b19 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java\n@@ -53,7 +53,6 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.types.Types;\n import org.junit.jupiter.api.BeforeEach;\n@@ -211,9 +210,8 @@ public void dataFilesCleanupWithParallelTasks() {\n     // Verifies that the delete methods ran in the threads created by the provided ExecutorService\n     // ThreadFactory\n     assertThat(deleteThreads)\n-        .isEqualTo(\n-            Sets.newHashSet(\n-                \"remove-files-0\", \"remove-files-1\", \"remove-files-2\", \"remove-files-3\"));\n+        .containsExactlyInAnyOrder(\n+            \"remove-files-0\", \"remove-files-1\", \"remove-files-2\", \"remove-files-3\");\n \n     Lists.newArrayList(FILE_A, FILE_B, FILE_C, FILE_D)\n         .forEach(\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java\nindex 6a1eb0ed9054..7aa569041d24 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java\n@@ -254,12 +254,8 @@ public void dataFilesCleanupWithParallelTasks() throws IOException {\n     // Verifies that the delete methods ran in the threads created by the provided ExecutorService\n     // ThreadFactory\n     assertThat(deleteThreads)\n-        .isEqualTo(\n-            Sets.newHashSet(\n-                \"remove-snapshot-0\",\n-                \"remove-snapshot-1\",\n-                \"remove-snapshot-2\",\n-                \"remove-snapshot-3\"));\n+        .containsExactlyInAnyOrder(\n+            \"remove-snapshot-0\", \"remove-snapshot-1\", \"remove-snapshot-2\", \"remove-snapshot-3\");\n \n     assertThat(deletedFiles).as(\"FILE_A should be deleted\").contains(FILE_A.location());\n     assertThat(deletedFiles).as(\"FILE_B should be deleted\").contains(FILE_B.location());\n@@ -813,7 +809,7 @@ public void testExpireOlderThan() {\n         .isNull();\n     assertThat(deletedFiles)\n         .as(\"Should remove only the expired manifest list location.\")\n-        .isEqualTo(Sets.newHashSet(firstSnapshot.manifestListLocation()));\n+        .containsExactly(firstSnapshot.manifestListLocation());\n \n     checkExpirationResults(0, 0, 0, 0, 1, result);\n   }\n@@ -862,20 +858,18 @@ public void testExpireOlderThanWithDelete() {\n         .isNull();\n     assertThat(deletedFiles)\n         .as(\"Should remove expired manifest lists and deleted data file.\")\n-        .isEqualTo(\n-            Sets.newHashSet(\n-                firstSnapshot.manifestListLocation(), // snapshot expired\n-                firstSnapshot\n-                    .allManifests(table.io())\n-                    .get(0)\n-                    .path(), // manifest was rewritten for delete\n-                secondSnapshot.manifestListLocation(), // snapshot expired\n-                secondSnapshot\n-                    .allManifests(table.io())\n-                    .get(0)\n-                    .path(), // manifest contained only deletes, was dropped\n-                FILE_A.location()) // deleted\n-            );\n+        .containsExactlyInAnyOrder(\n+            firstSnapshot.manifestListLocation(), // snapshot expired\n+            firstSnapshot\n+                .allManifests(table.io())\n+                .get(0)\n+                .path(), // manifest was rewritten for delete\n+            secondSnapshot.manifestListLocation(), // snapshot expired\n+            secondSnapshot\n+                .allManifests(table.io())\n+                .get(0)\n+                .path(), // manifest contained only deletes, was dropped\n+            FILE_A.location());\n \n     checkExpirationResults(1, 0, 0, 2, 2, result);\n   }\n@@ -933,16 +927,14 @@ public void testExpireOlderThanWithDeleteInMergedManifests() {\n \n     assertThat(deletedFiles)\n         .as(\"Should remove expired manifest lists and deleted data file.\")\n-        .isEqualTo(\n-            Sets.newHashSet(\n-                firstSnapshot.manifestListLocation(), // snapshot expired\n-                firstSnapshot\n-                    .allManifests(table.io())\n-                    .get(0)\n-                    .path(), // manifest was rewritten for delete\n-                secondSnapshot.manifestListLocation(), // snapshot expired\n-                FILE_A.location()) // deleted\n-            );\n+        .containsExactlyInAnyOrder(\n+            firstSnapshot.manifestListLocation(), // snapshot expired\n+            firstSnapshot\n+                .allManifests(table.io())\n+                .get(0)\n+                .path(), // manifest was rewritten for delete\n+            secondSnapshot.manifestListLocation(), // snapshot expired\n+            FILE_A.location());\n     checkExpirationResults(1, 0, 0, 1, 2, result);\n   }\n \n@@ -994,12 +986,9 @@ public void testExpireOlderThanWithRollback() {\n \n     assertThat(deletedFiles)\n         .as(\"Should remove expired manifest lists and reverted appended data file\")\n-        .isEqualTo(\n-            Sets.newHashSet(\n-                secondSnapshot.manifestListLocation(), // snapshot expired\n-                Iterables.getOnlyElement(secondSnapshotManifests)\n-                    .path()) // manifest is no longer referenced\n-            );\n+        .containsExactlyInAnyOrder(\n+            secondSnapshot.manifestListLocation(), // snapshot expired\n+            Iterables.getOnlyElement(secondSnapshotManifests).path());\n \n     checkExpirationResults(0, 0, 0, 1, 1, result);\n   }\n@@ -1048,13 +1037,11 @@ public void testExpireOlderThanWithRollbackAndMergedManifests() {\n \n     assertThat(deletedFiles)\n         .as(\"Should remove expired manifest lists and reverted appended data file\")\n-        .isEqualTo(\n-            Sets.newHashSet(\n-                secondSnapshot.manifestListLocation(), // snapshot expired\n-                Iterables.getOnlyElement(secondSnapshotManifests)\n-                    .path(), // manifest is no longer referenced\n-                FILE_B.location()) // added, but rolled back\n-            );\n+        .containsExactlyInAnyOrder(\n+            secondSnapshot.manifestListLocation(), // snapshot expired\n+            Iterables.getOnlyElement(secondSnapshotManifests)\n+                .path(), // manifest is no longer referenced\n+            FILE_B.location());\n \n     checkExpirationResults(1, 0, 0, 1, 1, result);\n   }\n@@ -1180,7 +1167,7 @@ public void testExpireAction() {\n         .as(\"Pending delete should be a manifest list\")\n         .isEqualTo(\"Manifest List\");\n \n-    assertThat(deletedFiles).as(\"Should not delete any files\").hasSize(0);\n+    assertThat(deletedFiles).as(\"Should not delete any files\").isEmpty();\n \n     assertThat(action.expireFiles().count())\n         .as(\"Multiple calls to expire should return the same count of deleted files\")\n@@ -1351,9 +1338,8 @@ public void testExpireSomeCheckFilesDeleted() {\n     // C, D should be retained (live)\n     // B should be retained (previous snapshot points to it)\n     // A should be deleted\n-    assertThat(deletedFiles).contains(FILE_A.location());\n-    assertThat(deletedFiles).doesNotContain(FILE_B.location());\n-    assertThat(deletedFiles).doesNotContain(FILE_C.location());\n-    assertThat(deletedFiles).doesNotContain(FILE_D.location());\n+    assertThat(deletedFiles)\n+        .contains(FILE_A.location())\n+        .doesNotContain(FILE_B.location(), FILE_C.location(), FILE_D.location());\n   }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\nindex 42e008ef21d3..9ae5dafaa9b8 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\n@@ -34,7 +34,6 @@\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n-import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.types.Types.IntegerType;\n@@ -261,16 +260,15 @@ public void testBinPackDataValidOptions() {\n \n     assertThat(rewriter.validOptions())\n         .as(\"Rewriter must report all supported options\")\n-        .isEqualTo(\n-            ImmutableSet.of(\n-                SparkBinPackDataRewriter.TARGET_FILE_SIZE_BYTES,\n-                SparkBinPackDataRewriter.MIN_FILE_SIZE_BYTES,\n-                SparkBinPackDataRewriter.MAX_FILE_SIZE_BYTES,\n-                SparkBinPackDataRewriter.MIN_INPUT_FILES,\n-                SparkBinPackDataRewriter.REWRITE_ALL,\n-                SparkBinPackDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n-                SparkBinPackDataRewriter.DELETE_FILE_THRESHOLD,\n-                SparkBinPackDataRewriter.DELETE_RATIO_THRESHOLD));\n+        .containsExactlyInAnyOrder(\n+            SparkBinPackDataRewriter.TARGET_FILE_SIZE_BYTES,\n+            SparkBinPackDataRewriter.MIN_FILE_SIZE_BYTES,\n+            SparkBinPackDataRewriter.MAX_FILE_SIZE_BYTES,\n+            SparkBinPackDataRewriter.MIN_INPUT_FILES,\n+            SparkBinPackDataRewriter.REWRITE_ALL,\n+            SparkBinPackDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n+            SparkBinPackDataRewriter.DELETE_FILE_THRESHOLD,\n+            SparkBinPackDataRewriter.DELETE_RATIO_THRESHOLD);\n   }\n \n   @Test\n@@ -280,18 +278,17 @@ public void testSortDataValidOptions() {\n \n     assertThat(rewriter.validOptions())\n         .as(\"Rewriter must report all supported options\")\n-        .isEqualTo(\n-            ImmutableSet.of(\n-                SparkSortDataRewriter.SHUFFLE_PARTITIONS_PER_FILE,\n-                SparkSortDataRewriter.TARGET_FILE_SIZE_BYTES,\n-                SparkSortDataRewriter.MIN_FILE_SIZE_BYTES,\n-                SparkSortDataRewriter.MAX_FILE_SIZE_BYTES,\n-                SparkSortDataRewriter.MIN_INPUT_FILES,\n-                SparkSortDataRewriter.REWRITE_ALL,\n-                SparkSortDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n-                SparkSortDataRewriter.DELETE_FILE_THRESHOLD,\n-                SparkSortDataRewriter.DELETE_RATIO_THRESHOLD,\n-                SparkSortDataRewriter.COMPRESSION_FACTOR));\n+        .containsExactlyInAnyOrder(\n+            SparkSortDataRewriter.SHUFFLE_PARTITIONS_PER_FILE,\n+            SparkSortDataRewriter.TARGET_FILE_SIZE_BYTES,\n+            SparkSortDataRewriter.MIN_FILE_SIZE_BYTES,\n+            SparkSortDataRewriter.MAX_FILE_SIZE_BYTES,\n+            SparkSortDataRewriter.MIN_INPUT_FILES,\n+            SparkSortDataRewriter.REWRITE_ALL,\n+            SparkSortDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n+            SparkSortDataRewriter.DELETE_FILE_THRESHOLD,\n+            SparkSortDataRewriter.DELETE_RATIO_THRESHOLD,\n+            SparkSortDataRewriter.COMPRESSION_FACTOR);\n   }\n \n   @Test\n@@ -302,20 +299,19 @@ public void testZOrderDataValidOptions() {\n \n     assertThat(rewriter.validOptions())\n         .as(\"Rewriter must report all supported options\")\n-        .isEqualTo(\n-            ImmutableSet.of(\n-                SparkZOrderDataRewriter.SHUFFLE_PARTITIONS_PER_FILE,\n-                SparkZOrderDataRewriter.TARGET_FILE_SIZE_BYTES,\n-                SparkZOrderDataRewriter.MIN_FILE_SIZE_BYTES,\n-                SparkZOrderDataRewriter.MAX_FILE_SIZE_BYTES,\n-                SparkZOrderDataRewriter.MIN_INPUT_FILES,\n-                SparkZOrderDataRewriter.REWRITE_ALL,\n-                SparkZOrderDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n-                SparkZOrderDataRewriter.DELETE_FILE_THRESHOLD,\n-                SparkZOrderDataRewriter.DELETE_RATIO_THRESHOLD,\n-                SparkZOrderDataRewriter.COMPRESSION_FACTOR,\n-                SparkZOrderDataRewriter.MAX_OUTPUT_SIZE,\n-                SparkZOrderDataRewriter.VAR_LENGTH_CONTRIBUTION));\n+        .containsExactlyInAnyOrder(\n+            SparkZOrderDataRewriter.SHUFFLE_PARTITIONS_PER_FILE,\n+            SparkZOrderDataRewriter.TARGET_FILE_SIZE_BYTES,\n+            SparkZOrderDataRewriter.MIN_FILE_SIZE_BYTES,\n+            SparkZOrderDataRewriter.MAX_FILE_SIZE_BYTES,\n+            SparkZOrderDataRewriter.MIN_INPUT_FILES,\n+            SparkZOrderDataRewriter.REWRITE_ALL,\n+            SparkZOrderDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n+            SparkZOrderDataRewriter.DELETE_FILE_THRESHOLD,\n+            SparkZOrderDataRewriter.DELETE_RATIO_THRESHOLD,\n+            SparkZOrderDataRewriter.COMPRESSION_FACTOR,\n+            SparkZOrderDataRewriter.MAX_OUTPUT_SIZE,\n+            SparkZOrderDataRewriter.VAR_LENGTH_CONTRIBUTION);\n   }\n \n   @Test\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12528",
    "pr_id": 12528,
    "issue_id": 12529,
    "repo": "apache/iceberg",
    "problem_statement": "PartitionsTable#partitions returns incomplete list in case of partition evolution and null partition values\n### Apache Iceberg version\n\n1.8.1 (latest release)\n\n### Query engine\n\nNone\n\n### Please describe the bug üêû\n\nBelow 3 StructProjections are considered equal leading to key overwrite in PartitionMap;\n````\n\"ice_orc(company_id=null)\"\n\"ice_orc(company_id=null/dept_id=null)\"\n\"ice_orc(company_id=null/dept_id=null/team_id=null)\"\n````\n\n### Willingness to contribute\n\n- [x] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 97,
    "test_files_count": 4,
    "non_test_files_count": 4,
    "pr_changed_files": [
      "api/src/main/java/org/apache/iceberg/util/StructProjection.java",
      "core/src/main/java/org/apache/iceberg/PartitionsTable.java",
      "core/src/main/java/org/apache/iceberg/util/StructLikeMap.java",
      "core/src/main/java/org/apache/iceberg/util/StructLikeWrapper.java",
      "core/src/test/java/org/apache/iceberg/MetadataTableScanTestBase.java",
      "core/src/test/java/org/apache/iceberg/TestBase.java",
      "core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java",
      "core/src/test/java/org/apache/iceberg/TestTables.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/MetadataTableScanTestBase.java",
      "core/src/test/java/org/apache/iceberg/TestBase.java",
      "core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java",
      "core/src/test/java/org/apache/iceberg/TestTables.java"
    ],
    "base_commit": "78156e7f4c829880ad1c62f35588c0a57d5ff18b",
    "head_commit": "43acfdd44dddd1c170b4b3fb5b2aeee1ddf1699d",
    "repo_url": "https://github.com/apache/iceberg/pull/12528",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12528",
    "dockerfile": "",
    "pr_merged_at": "2025-06-18T18:02:11.000Z",
    "patch": "diff --git a/api/src/main/java/org/apache/iceberg/util/StructProjection.java b/api/src/main/java/org/apache/iceberg/util/StructProjection.java\nindex 8e2fc7a1418f..08dedf0fe18e 100644\n--- a/api/src/main/java/org/apache/iceberg/util/StructProjection.java\n+++ b/api/src/main/java/org/apache/iceberg/util/StructProjection.java\n@@ -23,6 +23,7 @@\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.StructLike;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.primitives.Ints;\n import org.apache.iceberg.types.TypeUtil;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.types.Types.ListType;\n@@ -173,6 +174,10 @@ private StructProjection(StructType structType, StructType projection, boolean a\n     }\n   }\n \n+  public int projectedFields() {\n+    return (int) Ints.asList(positionMap).stream().filter(val -> val != -1).count();\n+  }\n+\n   public StructProjection wrap(StructLike newStruct) {\n     this.struct = newStruct;\n     return this;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/PartitionsTable.java b/core/src/main/java/org/apache/iceberg/PartitionsTable.java\nindex 6d0fc8c235f9..09c6e7893b7e 100644\n--- a/core/src/main/java/org/apache/iceberg/PartitionsTable.java\n+++ b/core/src/main/java/org/apache/iceberg/PartitionsTable.java\n@@ -22,14 +22,17 @@\n import com.github.benmanes.caffeine.cache.LoadingCache;\n import java.io.IOException;\n import java.io.UncheckedIOException;\n+import java.util.Comparator;\n import java.util.List;\n import org.apache.iceberg.expressions.ManifestEvaluator;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n+import org.apache.iceberg.types.Comparators;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.ParallelIterable;\n import org.apache.iceberg.util.PartitionUtil;\n import org.apache.iceberg.util.StructLikeMap;\n+import org.apache.iceberg.util.StructProjection;\n \n /** A {@link Table} implementation that exposes a table's partitions as rows. */\n public class PartitionsTable extends BaseMetadataTable {\n@@ -165,21 +168,26 @@ private static StaticDataTask.Row convertPartition(Partition partition) {\n \n   private static Iterable<Partition> partitions(Table table, StaticTableScan scan) {\n     Types.StructType partitionType = Partitioning.partitionType(table);\n-    PartitionMap partitions = new PartitionMap(partitionType);\n+\n+    StructLikeMap<Partition> partitions =\n+        StructLikeMap.create(partitionType, new PartitionComparator(partitionType));\n+\n     try (CloseableIterable<ManifestEntry<? extends ContentFile<?>>> entries = planEntries(scan)) {\n       for (ManifestEntry<? extends ContentFile<?>> entry : entries) {\n         Snapshot snapshot = table.snapshot(entry.snapshotId());\n         ContentFile<?> file = entry.file();\n-        StructLike partition =\n+        StructLike key =\n             PartitionUtil.coercePartition(\n                 partitionType, table.specs().get(file.specId()), file.partition());\n-        partitions.get(partition).update(file, snapshot);\n+        partitions\n+            .computeIfAbsent(key, () -> new Partition(key, partitionType))\n+            .update(file, snapshot);\n       }\n     } catch (IOException e) {\n       throw new UncheckedIOException(e);\n     }\n \n-    return partitions.all();\n+    return partitions.values();\n   }\n \n   @VisibleForTesting\n@@ -238,26 +246,26 @@ private class PartitionsScan extends StaticTableScan {\n     }\n   }\n \n-  static class PartitionMap {\n-    private final StructLikeMap<Partition> partitions;\n-    private final Types.StructType keyType;\n+  private static class PartitionComparator implements Comparator<StructLike> {\n+    private Comparator<StructLike> comparator;\n \n-    PartitionMap(Types.StructType type) {\n-      this.partitions = StructLikeMap.create(type);\n-      this.keyType = type;\n+    private PartitionComparator(Types.StructType struct) {\n+      this.comparator = Comparators.forType(struct);\n     }\n \n-    Partition get(StructLike key) {\n-      Partition partition = partitions.get(key);\n-      if (partition == null) {\n-        partition = new Partition(key, keyType);\n-        partitions.put(key, partition);\n+    @Override\n+    public int compare(StructLike o1, StructLike o2) {\n+      if (o1 instanceof StructProjection && o2 instanceof StructProjection) {\n+        int cmp =\n+            Integer.compare(\n+                ((StructProjection) o1).projectedFields(),\n+                ((StructProjection) o2).projectedFields());\n+        if (cmp != 0) {\n+          return cmp;\n+        }\n       }\n-      return partition;\n-    }\n \n-    Iterable<Partition> all() {\n-      return partitions.values();\n+      return comparator.compare(o1, o2);\n     }\n   }\n \n@@ -290,6 +298,8 @@ void update(ContentFile<?> file, Snapshot snapshot) {\n       if (snapshot != null) {\n         long snapshotCommitTime = snapshot.timestampMillis() * 1000;\n         if (this.lastUpdatedAt == null || snapshotCommitTime > this.lastUpdatedAt) {\n+          this.specId = file.specId();\n+\n           this.lastUpdatedAt = snapshotCommitTime;\n           this.lastUpdatedSnapshotId = snapshot.snapshotId();\n         }\n@@ -299,18 +309,15 @@ void update(ContentFile<?> file, Snapshot snapshot) {\n         case DATA:\n           this.dataRecordCount += file.recordCount();\n           this.dataFileCount += 1;\n-          this.specId = file.specId();\n           this.dataFileSizeInBytes += file.fileSizeInBytes();\n           break;\n         case POSITION_DELETES:\n           this.posDeleteRecordCount += file.recordCount();\n           this.posDeleteFileCount += 1;\n-          this.specId = file.specId();\n           break;\n         case EQUALITY_DELETES:\n           this.eqDeleteRecordCount += file.recordCount();\n           this.eqDeleteFileCount += 1;\n-          this.specId = file.specId();\n           break;\n         default:\n           throw new UnsupportedOperationException(\n@@ -319,15 +326,9 @@ void update(ContentFile<?> file, Snapshot snapshot) {\n     }\n \n     /** Needed because StructProjection is not serializable */\n-    private PartitionData toPartitionData(StructLike key, Types.StructType keyType) {\n-      PartitionData data = new PartitionData(keyType);\n-      for (int i = 0; i < keyType.fields().size(); i++) {\n-        Object val = key.get(i, keyType.fields().get(i).type().typeId().javaClass());\n-        if (val != null) {\n-          data.set(i, val);\n-        }\n-      }\n-      return data;\n+    private static PartitionData toPartitionData(StructLike key, Types.StructType keyType) {\n+      PartitionData keyTemplate = new PartitionData(keyType);\n+      return keyTemplate.copyFor(key);\n     }\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/util/StructLikeMap.java b/core/src/main/java/org/apache/iceberg/util/StructLikeMap.java\nindex 2bb5fa1c9d40..e0d5c0c6f59b 100644\n--- a/core/src/main/java/org/apache/iceberg/util/StructLikeMap.java\n+++ b/core/src/main/java/org/apache/iceberg/util/StructLikeMap.java\n@@ -20,28 +20,49 @@\n \n import java.util.AbstractMap;\n import java.util.Collection;\n+import java.util.Comparator;\n import java.util.Map;\n import java.util.Set;\n import java.util.function.Function;\n+import java.util.function.Supplier;\n import org.apache.iceberg.StructLike;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.Comparators;\n import org.apache.iceberg.types.Types;\n \n public class StructLikeMap<T> extends AbstractMap<StructLike, T> implements Map<StructLike, T> {\n \n+  /**\n+   * Creates a new StructLikeMap with the specified type and comparator.\n+   *\n+   * @param type the struct type for the keys\n+   * @param comparator the comparator for comparing struct keys\n+   * @return a new StructLikeMap instance\n+   */\n+  public static <T> StructLikeMap<T> create(\n+      Types.StructType type, Comparator<StructLike> comparator) {\n+    return new StructLikeMap<>(type, comparator);\n+  }\n+\n+  /**\n+   * Creates a new StructLikeMap with the specified type using the default comparator for the type.\n+   *\n+   * @param type the struct type for the keys\n+   * @return a new StructLikeMap instance\n+   */\n   public static <T> StructLikeMap<T> create(Types.StructType type) {\n-    return new StructLikeMap<>(type);\n+    return create(type, Comparators.forType(type));\n   }\n \n   private final Types.StructType type;\n   private final Map<StructLikeWrapper, T> wrapperMap;\n   private final ThreadLocal<StructLikeWrapper> wrappers;\n \n-  private StructLikeMap(Types.StructType type) {\n+  private StructLikeMap(Types.StructType type, Comparator<StructLike> comparator) {\n     this.type = type;\n     this.wrapperMap = Maps.newHashMap();\n-    this.wrappers = ThreadLocal.withInitial(() -> StructLikeWrapper.forType(type));\n+    this.wrappers = ThreadLocal.withInitial(() -> StructLikeWrapper.forType(type, comparator));\n   }\n \n   @Override\n@@ -125,6 +146,10 @@ public Set<Entry<StructLike, T>> entrySet() {\n     return entrySet;\n   }\n \n+  public T computeIfAbsent(StructLike struct, Supplier<T> valueSupplier) {\n+    return wrapperMap.computeIfAbsent(wrappers.get().copyFor(struct), key -> valueSupplier.get());\n+  }\n+\n   private static class StructLikeEntry<R> implements Entry<StructLike, R> {\n \n     private final Entry<StructLikeWrapper, R> inner;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/util/StructLikeWrapper.java b/core/src/main/java/org/apache/iceberg/util/StructLikeWrapper.java\nindex e8cf0a8db76e..28629706bf5e 100644\n--- a/core/src/main/java/org/apache/iceberg/util/StructLikeWrapper.java\n+++ b/core/src/main/java/org/apache/iceberg/util/StructLikeWrapper.java\n@@ -27,8 +27,13 @@\n /** Wrapper to adapt StructLike for use in maps and sets by implementing equals and hashCode. */\n public class StructLikeWrapper {\n \n-  public static StructLikeWrapper forType(Types.StructType struct) {\n-    return new StructLikeWrapper(struct);\n+  public static StructLikeWrapper forType(\n+      Types.StructType type, Comparator<StructLike> comparator) {\n+    return new StructLikeWrapper(comparator, JavaHash.forType(type));\n+  }\n+\n+  public static StructLikeWrapper forType(Types.StructType type) {\n+    return forType(type, Comparators.forType(type));\n   }\n \n   private final Comparator<StructLike> comparator;\n@@ -36,10 +41,6 @@ public static StructLikeWrapper forType(Types.StructType struct) {\n   private Integer hashCode;\n   private StructLike struct;\n \n-  private StructLikeWrapper(Types.StructType type) {\n-    this(Comparators.forType(type), JavaHash.forType(type));\n-  }\n-\n   private StructLikeWrapper(Comparator<StructLike> comparator, JavaHash<StructLike> structHash) {\n     this.comparator = comparator;\n     this.structHash = structHash;\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/MetadataTableScanTestBase.java b/core/src/test/java/org/apache/iceberg/MetadataTableScanTestBase.java\nindex ff9dfd1afc7f..7eb2b9cefac0 100644\n--- a/core/src/test/java/org/apache/iceberg/MetadataTableScanTestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/MetadataTableScanTestBase.java\n@@ -79,7 +79,7 @@ protected void validateSingleFieldPartition(\n   protected void validatePartition(\n       CloseableIterable<ManifestEntry<? extends ContentFile<?>>> entries,\n       int position,\n-      int partitionValue) {\n+      Object partitionValue) {\n     assertThat(entries)\n         .as(\"File scan tasks do not include correct file\")\n         .anyMatch(\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestBase.java b/core/src/test/java/org/apache/iceberg/TestBase.java\nindex 5d0919c568f4..51a976612e13 100644\n--- a/core/src/test/java/org/apache/iceberg/TestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/TestBase.java\n@@ -655,13 +655,19 @@ private <T extends ContentFile<T>> void validateManifestSequenceNumbers(\n     }\n   }\n \n+  protected DataFile newDataFile(StructLike partition) {\n+    return newDataFileBuilder(table).withPartition(partition).build();\n+  }\n+\n   protected DataFile newDataFile(String partitionPath) {\n+    return newDataFileBuilder(table).withPartitionPath(partitionPath).build();\n+  }\n+\n+  private static DataFiles.Builder newDataFileBuilder(Table table) {\n     return DataFiles.builder(table.spec())\n         .withPath(\"/path/to/data-\" + UUID.randomUUID() + \".parquet\")\n         .withFileSizeInBytes(10)\n-        .withPartitionPath(partitionPath)\n-        .withRecordCount(1)\n-        .build();\n+        .withRecordCount(1);\n   }\n \n   protected DeleteFile fileADeletes() {\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java b/core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java\nindex 03338804d8bc..fe3a7c2686a3 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java\n@@ -31,9 +31,11 @@\n import org.apache.iceberg.expressions.Expression;\n import org.apache.iceberg.expressions.Expressions;\n import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.relocated.com.google.common.collect.FluentIterable;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.StructProjection;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n@@ -243,6 +245,90 @@ public void testPartitionSpecEvolutionToUnpartitioned() throws IOException {\n     }\n   }\n \n+  @TestTemplate\n+  public void testPartitionSpecEvolutionNullValues() throws IOException {\n+    Schema schema =\n+        new Schema(\n+            required(1, \"company_id\", Types.IntegerType.get()),\n+            required(2, \"dept_id\", Types.IntegerType.get()),\n+            required(3, \"team_id\", Types.IntegerType.get()));\n+\n+    table =\n+        TestTables.create(\n+            tableDir,\n+            metadataDir,\n+            \"nulltest\",\n+            schema,\n+            PartitionSpec.builderFor(schema).identity(\"company_id\").build(),\n+            SortOrder.unsorted(),\n+            formatVersion);\n+    table.newFastAppend().appendFile(newDataFile(TestHelpers.Row.of(new Object[] {null}))).commit();\n+\n+    table.updateSpec().addField(\"dept_id\").commit();\n+    table.newFastAppend().appendFile(newDataFile(TestHelpers.Row.of(null, null))).commit();\n+\n+    table.updateSpec().addField(\"team_id\").commit();\n+    table.newFastAppend().appendFile(newDataFile(TestHelpers.Row.of(null, null, null))).commit();\n+\n+    assertPartitions(\n+        \"company_id=null\",\n+        \"company_id=null/dept_id=null\",\n+        \"company_id=null/dept_id=null/team_id=null\");\n+  }\n+\n+  @TestTemplate\n+  public void testPartitionSpecRenameFields() throws IOException {\n+    Schema schema =\n+        new Schema(\n+            required(1, \"data\", Types.StringType.get()),\n+            required(2, \"category\", Types.StringType.get()));\n+\n+    table =\n+        TestTables.create(\n+            tableDir,\n+            metadataDir,\n+            \"renametest\",\n+            schema,\n+            PartitionSpec.builderFor(schema).identity(\"data\").identity(\"category\").build(),\n+            SortOrder.unsorted(),\n+            formatVersion);\n+    table\n+        .newFastAppend()\n+        .appendFile(newDataFile(TestHelpers.Row.of(\"c1\", \"d1\")))\n+        .appendFile(newDataFile(TestHelpers.Row.of(\"c2\", \"d2\")))\n+        .commit();\n+\n+    table.updateSpec().renameField(\"category\", \"category_another_name\").commit();\n+    table\n+        .newFastAppend()\n+        .appendFile(newDataFile(TestHelpers.Row.of(\"c1\", \"d1\")))\n+        .appendFile(newDataFile(TestHelpers.Row.of(\"c2\", \"d2\")))\n+        .commit();\n+\n+    assertPartitions(\"data=c1/category_another_name=d1\", \"data=c2/category_another_name=d2\");\n+  }\n+\n+  private void assertPartitions(String... expected) throws IOException {\n+    PartitionsTable partitionsTable = new PartitionsTable(table);\n+\n+    try (CloseableIterable<FileScanTask> fileScanTasks = partitionsTable.newScan().planFiles()) {\n+      List<String> partitions =\n+          FluentIterable.from(fileScanTasks)\n+              .transformAndConcat(task -> task.asDataTask().rows())\n+              .transform(\n+                  row -> {\n+                    StructLike data = row.get(0, StructProjection.class);\n+                    PartitionSpec spec = table.specs().get(row.get(1, Integer.class));\n+\n+                    PartitionData keyTemplate = new PartitionData(spec.partitionType());\n+                    return spec.partitionToPath(keyTemplate.copyFor((data)));\n+                  })\n+              .toList();\n+\n+      assertThat(partitions).containsExactlyInAnyOrder(expected);\n+    }\n+  }\n+\n   private Stream<StructLike> allRows(Iterable<FileScanTask> tasks) {\n     return Streams.stream(tasks).flatMap(task -> Streams.stream(task.asDataTask().rows()));\n   }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestTables.java b/core/src/test/java/org/apache/iceberg/TestTables.java\nindex ad5369ea5e1a..073a95fca2e7 100644\n--- a/core/src/test/java/org/apache/iceberg/TestTables.java\n+++ b/core/src/test/java/org/apache/iceberg/TestTables.java\n@@ -59,10 +59,21 @@ public static TestTable create(\n       PartitionSpec spec,\n       SortOrder sortOrder,\n       int formatVersion) {\n+    return create(temp, null, name, schema, spec, sortOrder, formatVersion);\n+  }\n+\n+  public static TestTable create(\n+      File temp,\n+      File metaTemp,\n+      String name,\n+      Schema schema,\n+      PartitionSpec spec,\n+      SortOrder sortOrder,\n+      int formatVersion) {\n     TestTableOperations ops = new TestTableOperations(name, temp);\n \n     return createTable(\n-        temp, name, schema, spec, formatVersion, ImmutableMap.of(), sortOrder, null, ops);\n+        temp, metaTemp, name, schema, spec, formatVersion, ImmutableMap.of(), sortOrder, null, ops);\n   }\n \n   public static TestTable create(\n@@ -74,7 +85,7 @@ public static TestTable create(\n       int formatVersion,\n       TestTableOperations ops) {\n     return createTable(\n-        temp, name, schema, spec, formatVersion, ImmutableMap.of(), sortOrder, null, ops);\n+        temp, null, name, schema, spec, formatVersion, ImmutableMap.of(), sortOrder, null, ops);\n   }\n \n   public static TestTable create(\n@@ -88,7 +99,7 @@ public static TestTable create(\n     TestTableOperations ops = new TestTableOperations(name, temp);\n \n     return createTable(\n-        temp, name, schema, spec, formatVersion, ImmutableMap.of(), sortOrder, reporter, ops);\n+        temp, null, name, schema, spec, formatVersion, ImmutableMap.of(), sortOrder, reporter, ops);\n   }\n \n   public static TestTable create(\n@@ -101,11 +112,12 @@ public static TestTable create(\n     TestTableOperations ops = new TestTableOperations(name, temp);\n \n     return createTable(\n-        temp, name, schema, spec, formatVersion, properties, SortOrder.unsorted(), null, ops);\n+        temp, null, name, schema, spec, formatVersion, properties, SortOrder.unsorted(), null, ops);\n   }\n \n   private static TestTable createTable(\n       File temp,\n+      File metaTemp,\n       String name,\n       Schema schema,\n       PartitionSpec spec,\n@@ -118,9 +130,18 @@ private static TestTable createTable(\n       throw new AlreadyExistsException(\"Table %s already exists at location: %s\", name, temp);\n     }\n \n-    ops.commit(\n-        null,\n-        newTableMetadata(schema, spec, sortOrder, temp.toString(), properties, formatVersion));\n+    TableMetadata metadata =\n+        newTableMetadata(schema, spec, sortOrder, temp.toString(), properties, formatVersion);\n+\n+    if (metaTemp != null) {\n+      metadata =\n+          TableMetadata.buildFrom(metadata)\n+              .discardChanges()\n+              .withMetadataLocation(metaTemp.toString())\n+              .build();\n+    }\n+\n+    ops.commit(null, metadata);\n \n     if (reporter != null) {\n       return new TestTable(ops, reporter);\n@@ -307,7 +328,11 @@ public void commit(TableMetadata base, TableMetadata updatedMetadata) {\n           }\n           Integer version = VERSIONS.get(tableName);\n           // remove changes from the committed metadata\n-          this.current = TableMetadata.buildFrom(updatedMetadata).discardChanges().build();\n+          this.current =\n+              TableMetadata.buildFrom(updatedMetadata)\n+                  .discardChanges()\n+                  .withMetadataLocation((current != null) ? current.metadataFileLocation() : null)\n+                  .build();\n           VERSIONS.put(tableName, version == null ? 0 : version + 1);\n           METADATA.put(tableName, current);\n         } else {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12526",
    "pr_id": 12526,
    "issue_id": 12525,
    "repo": "apache/iceberg",
    "problem_statement": "REST Catalog `createTransaction` throws with overridden table defaults\n### Apache Iceberg version\n\nNone\n\n### Query engine\n\nNone\n\n### Please describe the bug üêû\n\nSee https://github.com/apache/iceberg/pull/12523 for a failing test. When a table default property of `default-key2` is set on a REST catalog, the code below throws\n\n```java\ncatalog()\n  .buildTable(ident, SCHEMA)\n  .withProperty(\"default-key2\", \"catalog-overridden-key2\")\n  .withProperty(\"prop1\", \"val1\")\n  .createTransaction()\n  .commitTransaction();\n```\n\nwith\n\n```\nMultiple entries with same key: default-key2=catalog-overridden-key2 and default-key2=catalog-default-key2\njava.lang.IllegalArgumentException: Multiple entries with same key: default-key2=catalog-overridden-key2 and default-key2=catalog-default-key2\n```\n\nThis is because https://github.com/apache/iceberg/pull/11646 introduced the Catalog-configurable table-default property in the REST session catalog with the `create` method on the REST table builder therefore using `buildKeepingLast`:\n\nhttps://github.com/apache/iceberg/blob/c02ebe4740b22d6f5a78b636aea2d918037b2751/core/src/main/java/org/apache/iceberg/rest/RESTSessionCatalog.java#L859\n\nHowever, this wasn't done for the other builder terminal methods which throw due to duplicate keys.\n\n### Willingness to contribute\n\n- [x] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 211,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/rest/RESTSessionCatalog.java",
      "core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java"
    ],
    "base_commit": "3e3df8ebb4e6f11727e731809149d2a3c66726be",
    "head_commit": "fc99bbf24aea5ec561a87d6c3142ba392e3525d4",
    "repo_url": "https://github.com/apache/iceberg/pull/12526",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12526",
    "dockerfile": "",
    "pr_merged_at": "2025-03-14T18:24:47.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/rest/RESTSessionCatalog.java b/core/src/main/java/org/apache/iceberg/rest/RESTSessionCatalog.java\nindex e3badfccdc55..03a78d719aa2 100644\n--- a/core/src/main/java/org/apache/iceberg/rest/RESTSessionCatalog.java\n+++ b/core/src/main/java/org/apache/iceberg/rest/RESTSessionCatalog.java\n@@ -922,7 +922,7 @@ public Transaction replaceTransaction() {\n       AuthSession session = tableSession(response.config(), session(context));\n       TableMetadata base = response.tableMetadata();\n \n-      Map<String, String> tableProperties = propertiesBuilder.build();\n+      Map<String, String> tableProperties = propertiesBuilder.buildKeepingLast();\n       TableMetadata replacement =\n           base.buildReplacement(\n               schema,\n@@ -984,7 +984,7 @@ public Transaction createOrReplaceTransaction() {\n     }\n \n     private LoadTableResponse stageCreate() {\n-      Map<String, String> tableProperties = propertiesBuilder.build();\n+      Map<String, String> tableProperties = propertiesBuilder.buildKeepingLast();\n \n       CreateTableRequest request =\n           CreateTableRequest.builder()\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java b/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\nindex b8aea5e2a167..d21fa0c124a5 100644\n--- a/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\n+++ b/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\n@@ -685,6 +685,65 @@ public void testDefaultTableProperties() {\n     assertThat(catalog.dropTable(ident)).as(\"Should successfully drop table\").isTrue();\n   }\n \n+  @Test\n+  public void testDefaultTablePropertiesCreateTransaction() {\n+    C catalog = catalog();\n+\n+    TableIdentifier ident = TableIdentifier.of(\"ns\", \"table\");\n+\n+    if (requiresNamespaceCreate()) {\n+      catalog.createNamespace(ident.namespace());\n+    }\n+\n+    assertThat(catalog.tableExists(ident)).as(\"Table should not exist\").isFalse();\n+\n+    catalog()\n+        .buildTable(ident, SCHEMA)\n+        .withProperty(\"default-key2\", \"catalog-overridden-key2\")\n+        .withProperty(\"prop1\", \"val1\")\n+        .createTransaction()\n+        .commitTransaction();\n+\n+    Table table = catalog.loadTable(ident);\n+\n+    assertThat(table.properties())\n+        .containsEntry(\"default-key1\", \"catalog-default-key1\")\n+        .containsEntry(\"default-key2\", \"catalog-overridden-key2\")\n+        .containsEntry(\"prop1\", \"val1\");\n+\n+    assertThat(catalog.dropTable(ident)).as(\"Should successfully drop table\").isTrue();\n+  }\n+\n+  @Test\n+  public void testDefaultTablePropertiesReplaceTransaction() {\n+    C catalog = catalog();\n+\n+    TableIdentifier ident = TableIdentifier.of(\"ns\", \"table\");\n+\n+    if (requiresNamespaceCreate()) {\n+      catalog.createNamespace(ident.namespace());\n+    }\n+\n+    catalog.createTable(ident, SCHEMA);\n+    assertThat(catalog.tableExists(ident)).as(\"Table should exist\").isTrue();\n+\n+    catalog()\n+        .buildTable(ident, OTHER_SCHEMA)\n+        .withProperty(\"default-key2\", \"catalog-overridden-key2\")\n+        .withProperty(\"prop1\", \"val1\")\n+        .replaceTransaction()\n+        .commitTransaction();\n+\n+    Table table = catalog.loadTable(ident);\n+\n+    assertThat(table.properties())\n+        .containsEntry(\"default-key1\", \"catalog-default-key1\")\n+        .containsEntry(\"default-key2\", \"catalog-overridden-key2\")\n+        .containsEntry(\"prop1\", \"val1\");\n+\n+    assertThat(catalog.dropTable(ident)).as(\"Should successfully drop table\").isTrue();\n+  }\n+\n   @Test\n   public void testCreateTableWithDefaultColumnValue() {\n     C catalog = catalog();\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12520",
    "pr_id": 12520,
    "issue_id": 12495,
    "repo": "apache/iceberg",
    "problem_statement": "Unable to set `write-default` for a column\n### Apache Iceberg version\n\n1.8.1 (latest release)\n\n### Query engine\n\nNone\n\n### Please describe the bug üêû\n\n- I am attempting to create a table via the  Iceberg Java API with the `write-default` values set on some columns. After creating the table and reading the schema back, I expected the schema to include the default value definitions but they are missing.\n\n```\n// To create a new teable\nfinal TableIdentifier tableIdentifier = TableIdentifier.parse(\"MyNamespace.MyTable\");\nfinal Schema schema = new Schema(List.of(\n        Types.NestedField.required(1, \"intCol\", Types.IntegerType.get()),\n        Types.NestedField.required(2, \"doubleCol\", Types.DoubleType.get()),\n        Types.NestedField.required(3, \"longCol\", Types.LongType.get()),\n        Types.NestedField.required(\"newIntCol\")\n                .withId(4)\n                .ofType(Types.IntegerType.get())\n                .withWriteDefault(Integer.valueOf(\"10\"))\n                .build()\n));\n\ncatalog.createTable(tableIdentifier, schema);\n\n\n// To read back the schema\ncatalog.loadTable.loadTable(tableIdentifier).schema();\n// Schema doesn't have write default set for `newIntCol`\n```\nSame issue also exist for `initial-default` too.\n\n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [x] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 197,
    "test_files_count": 4,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java",
      "api/src/main/java/org/apache/iceberg/types/AssignIds.java",
      "api/src/main/java/org/apache/iceberg/types/ReassignDoc.java",
      "api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java",
      "core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java"
    ],
    "pr_changed_test_files": [
      "api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java",
      "core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java"
    ],
    "base_commit": "c02ebe4740b22d6f5a78b636aea2d918037b2751",
    "head_commit": "6eb83f886a634148c5aa9a4e258d8cb5e5f5ee23",
    "repo_url": "https://github.com/apache/iceberg/pull/12520",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12520",
    "dockerfile": "",
    "pr_merged_at": "2025-03-14T13:20:20.000Z",
    "patch": "diff --git a/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java b/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java\nindex 75055cddc197..f3759f1d72f3 100644\n--- a/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java\n+++ b/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java\n@@ -88,11 +88,7 @@ public Type struct(Types.StructType struct, Iterable<Type> futures) {\n     for (int i = 0; i < length; i += 1) {\n       Types.NestedField field = fields.get(i);\n       Type type = types.next();\n-      if (field.isOptional()) {\n-        newFields.add(Types.NestedField.optional(newIds.get(i), field.name(), type, field.doc()));\n-      } else {\n-        newFields.add(Types.NestedField.required(newIds.get(i), field.name(), type, field.doc()));\n-      }\n+      newFields.add(Types.NestedField.from(field).withId(newIds.get(i)).ofType(type).build());\n     }\n \n     return Types.StructType.of(newFields);\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/AssignIds.java b/api/src/main/java/org/apache/iceberg/types/AssignIds.java\nindex b2f72751eb89..fd5ac7ff67b9 100644\n--- a/api/src/main/java/org/apache/iceberg/types/AssignIds.java\n+++ b/api/src/main/java/org/apache/iceberg/types/AssignIds.java\n@@ -56,11 +56,7 @@ public Type struct(Types.StructType struct, Iterable<Type> futures) {\n     for (int i = 0; i < length; i += 1) {\n       Types.NestedField field = fields.get(i);\n       Type type = types.next();\n-      if (field.isOptional()) {\n-        newFields.add(Types.NestedField.optional(newIds.get(i), field.name(), type, field.doc()));\n-      } else {\n-        newFields.add(Types.NestedField.required(newIds.get(i), field.name(), type, field.doc()));\n-      }\n+      newFields.add(Types.NestedField.from(field).withId(newIds.get(i)).ofType(type).build());\n     }\n \n     return Types.StructType.of(newFields);\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java b/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java\nindex 328d81c42885..86527fb3897f 100644\n--- a/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java\n+++ b/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java\n@@ -50,13 +50,8 @@ public Type struct(Types.StructType struct, Iterable<Type> fieldTypes) {\n \n       Preconditions.checkNotNull(docField, \"Field \" + fieldId + \" not found in source schema\");\n \n-      if (field.isRequired()) {\n-        newFields.add(\n-            Types.NestedField.required(fieldId, field.name(), types.get(i), docField.doc()));\n-      } else {\n-        newFields.add(\n-            Types.NestedField.optional(fieldId, field.name(), types.get(i), docField.doc()));\n-      }\n+      newFields.add(\n+          Types.NestedField.from(field).ofType(types.get(i)).withDoc(docField.doc()).build());\n     }\n \n     return Types.StructType.of(newFields);\n",
    "test_patch": "diff --git a/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java b/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java\nindex 078c0180b5e7..9501d4e2503a 100644\n--- a/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java\n+++ b/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java\n@@ -653,6 +653,108 @@ public void testReassignOrRefreshIdsCaseInsensitive() {\n     assertThat(actualSchema.asStruct()).isEqualTo(expectedSchema.asStruct());\n   }\n \n+  @Test\n+  public void testAssignIds() {\n+    Schema schema =\n+        new Schema(\n+            Lists.newArrayList(\n+                required(0, \"a\", Types.IntegerType.get()),\n+                Types.NestedField.required(\"c\")\n+                    .withId(1)\n+                    .ofType(Types.IntegerType.get())\n+                    .withInitialDefault(Literal.of(23))\n+                    .withWriteDefault(Literal.of(34))\n+                    .build(),\n+                required(2, \"B\", Types.IntegerType.get())));\n+\n+    Type actualSchema = TypeUtil.assignIds(schema.asStruct(), oldId -> oldId + 10);\n+    Schema expectedSchema =\n+        new Schema(\n+            Lists.newArrayList(\n+                required(10, \"a\", Types.IntegerType.get()),\n+                Types.NestedField.required(\"c\")\n+                    .withId(11)\n+                    .ofType(Types.IntegerType.get())\n+                    .withInitialDefault(Literal.of(23))\n+                    .withWriteDefault(Literal.of(34))\n+                    .build(),\n+                required(12, \"B\", Types.IntegerType.get())));\n+\n+    assertThat(actualSchema).isEqualTo(expectedSchema.asStruct());\n+  }\n+\n+  @Test\n+  public void testAssignFreshIds() {\n+    Schema schema =\n+        new Schema(\n+            Lists.newArrayList(\n+                required(0, \"a\", Types.IntegerType.get()),\n+                Types.NestedField.required(\"c\")\n+                    .withId(1)\n+                    .ofType(Types.IntegerType.get())\n+                    .withInitialDefault(Literal.of(23))\n+                    .withWriteDefault(Literal.of(34))\n+                    .build(),\n+                required(2, \"B\", Types.IntegerType.get())));\n+\n+    Schema actualSchema = TypeUtil.assignFreshIds(schema, new AtomicInteger(10)::incrementAndGet);\n+    Schema expectedSchema =\n+        new Schema(\n+            Lists.newArrayList(\n+                required(11, \"a\", Types.IntegerType.get()),\n+                Types.NestedField.required(\"c\")\n+                    .withId(12)\n+                    .ofType(Types.IntegerType.get())\n+                    .withInitialDefault(Literal.of(23))\n+                    .withWriteDefault(Literal.of(34))\n+                    .build(),\n+                required(13, \"B\", Types.IntegerType.get())));\n+\n+    assertThat(actualSchema.asStruct()).isEqualTo(expectedSchema.asStruct());\n+  }\n+\n+  @Test\n+  public void testReassignDoc() {\n+    Schema schema =\n+        new Schema(\n+            Lists.newArrayList(\n+                required(0, \"a\", Types.IntegerType.get()),\n+                Types.NestedField.required(\"c\")\n+                    .withId(1)\n+                    .ofType(Types.IntegerType.get())\n+                    .withInitialDefault(Literal.of(23))\n+                    .withWriteDefault(Literal.of(34))\n+                    .build(),\n+                required(2, \"B\", Types.IntegerType.get())));\n+\n+    Schema docSchema =\n+        new Schema(\n+            Lists.newArrayList(\n+                required(0, \"a\", Types.IntegerType.get(), \"a_doc\"),\n+                Types.NestedField.required(\"c\")\n+                    .withId(1)\n+                    .ofType(Types.IntegerType.get())\n+                    .withDoc(\"c_doc\")\n+                    .build(),\n+                required(2, \"B\", Types.IntegerType.get(), \"b_doc\")));\n+\n+    Schema actualSchema = TypeUtil.reassignDoc(schema, docSchema);\n+    Schema expectedSchema =\n+        new Schema(\n+            Lists.newArrayList(\n+                required(0, \"a\", Types.IntegerType.get(), \"a_doc\"),\n+                Types.NestedField.required(\"c\")\n+                    .withId(1)\n+                    .ofType(Types.IntegerType.get())\n+                    .withInitialDefault(Literal.of(23))\n+                    .withWriteDefault(Literal.of(34))\n+                    .withDoc(\"c_doc\")\n+                    .build(),\n+                required(2, \"B\", Types.IntegerType.get(), \"b_doc\")));\n+\n+    assertThat(actualSchema.asStruct()).isEqualTo(expectedSchema.asStruct());\n+  }\n+\n   private static Stream<Arguments> testTypes() {\n     return Stream.of(\n         Arguments.of(Types.UnknownType.get()),\n\ndiff --git a/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java b/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\nindex a8afcc83c6d0..b8aea5e2a167 100644\n--- a/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\n+++ b/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\n@@ -62,6 +62,7 @@\n import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n import org.apache.iceberg.exceptions.NoSuchTableException;\n import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.expressions.Literal;\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.metrics.CommitReport;\n import org.apache.iceberg.metrics.MetricsReport;\n@@ -684,6 +685,38 @@ public void testDefaultTableProperties() {\n     assertThat(catalog.dropTable(ident)).as(\"Should successfully drop table\").isTrue();\n   }\n \n+  @Test\n+  public void testCreateTableWithDefaultColumnValue() {\n+    C catalog = catalog();\n+\n+    TableIdentifier ident = TableIdentifier.of(\"ns\", \"table\");\n+\n+    if (requiresNamespaceCreate()) {\n+      catalog.createNamespace(ident.namespace());\n+    }\n+\n+    assertThat(catalog.tableExists(ident)).as(\"Table should not exist\").isFalse();\n+\n+    Schema schemaWithDefault =\n+        new Schema(\n+            List.of(\n+                Types.NestedField.required(\"colWithDefault\")\n+                    .withId(1)\n+                    .ofType(Types.IntegerType.get())\n+                    .withWriteDefault(Literal.of(10))\n+                    .withInitialDefault(Literal.of(12))\n+                    .build()));\n+\n+    catalog\n+        .buildTable(ident, schemaWithDefault)\n+        .withLocation(\"file:/tmp/ns/table\")\n+        .withProperty(TableProperties.FORMAT_VERSION, \"3\")\n+        .create();\n+    assertThat(catalog.tableExists(ident)).as(\"Table should exist\").isTrue();\n+    assertThat(schemaWithDefault.asStruct())\n+        .isEqualTo(catalog.loadTable(ident).schema().asStruct());\n+  }\n+\n   @Test\n   public void testLoadTable() {\n     C catalog = catalog();\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\nindex 3a269740b709..06d5e0c44fb3 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\n@@ -24,6 +24,7 @@\n import java.io.IOException;\n import java.nio.file.Path;\n import java.util.List;\n+import java.util.Map;\n import org.apache.avro.generic.GenericData;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.BaseTable;\n@@ -32,7 +33,9 @@\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableMetadata;\n import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.spark.data.AvroDataTest;\n import org.apache.iceberg.spark.data.RandomData;\n import org.apache.iceberg.spark.data.TestHelpers;\n@@ -84,7 +87,15 @@ protected void writeAndValidate(Schema writeSchema, Schema expectedSchema) throw\n     File location = new File(parent, \"test\");\n \n     HadoopTables tables = new HadoopTables(CONF);\n-    Table table = tables.create(writeSchema, PartitionSpec.unpartitioned(), location.toString());\n+    // If V3 spec features are used, set the format version to 3\n+    Map<String, String> tableProperties =\n+        writeSchema.columns().stream()\n+                .anyMatch(f -> f.initialDefaultLiteral() != null || f.writeDefaultLiteral() != null)\n+            ? ImmutableMap.of(TableProperties.FORMAT_VERSION, \"3\")\n+            : ImmutableMap.of();\n+    Table table =\n+        tables.create(\n+            writeSchema, PartitionSpec.unpartitioned(), tableProperties, location.toString());\n \n     // Important: use the table's schema for the rest of the test\n     // When tables are created, the column ids are reassigned.\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\nindex 3a269740b709..06d5e0c44fb3 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/ScanTestBase.java\n@@ -24,6 +24,7 @@\n import java.io.IOException;\n import java.nio.file.Path;\n import java.util.List;\n+import java.util.Map;\n import org.apache.avro.generic.GenericData;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.BaseTable;\n@@ -32,7 +33,9 @@\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableMetadata;\n import org.apache.iceberg.TableOperations;\n+import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.spark.data.AvroDataTest;\n import org.apache.iceberg.spark.data.RandomData;\n import org.apache.iceberg.spark.data.TestHelpers;\n@@ -84,7 +87,15 @@ protected void writeAndValidate(Schema writeSchema, Schema expectedSchema) throw\n     File location = new File(parent, \"test\");\n \n     HadoopTables tables = new HadoopTables(CONF);\n-    Table table = tables.create(writeSchema, PartitionSpec.unpartitioned(), location.toString());\n+    // If V3 spec features are used, set the format version to 3\n+    Map<String, String> tableProperties =\n+        writeSchema.columns().stream()\n+                .anyMatch(f -> f.initialDefaultLiteral() != null || f.writeDefaultLiteral() != null)\n+            ? ImmutableMap.of(TableProperties.FORMAT_VERSION, \"3\")\n+            : ImmutableMap.of();\n+    Table table =\n+        tables.create(\n+            writeSchema, PartitionSpec.unpartitioned(), tableProperties, location.toString());\n \n     // Important: use the table's schema for the rest of the test\n     // When tables are created, the column ids are reassigned.\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12512",
    "pr_id": 12512,
    "issue_id": 12472,
    "repo": "apache/iceberg",
    "problem_statement": "Parquet: Support Variant Array read and write\n### Feature Request / Improvement\n\n#12139 implements the Variant readers and #12323 implements the Variant writers. The arrays currently are not supported yet. We need to add reading and writing arrays in a Variant for non-shredded and shredded arrays. The shredded arrays are stored in the format listed in https://github.com/apache/parquet-format/blob/master/VariantShredding.md.\n \n\n### Query engine\n\nNone\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [x] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [ ] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 103,
    "test_files_count": 3,
    "non_test_files_count": 5,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/variants/ValueArray.java",
      "core/src/main/java/org/apache/iceberg/variants/Variants.java",
      "core/src/test/java/org/apache/iceberg/variants/TestShreddedObject.java",
      "core/src/test/java/org/apache/iceberg/variants/TestValueArray.java",
      "parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantReaders.java",
      "parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantVisitor.java",
      "parquet/src/main/java/org/apache/iceberg/parquet/VariantReaderBuilder.java",
      "parquet/src/test/java/org/apache/iceberg/parquet/TestVariantReaders.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/variants/TestShreddedObject.java",
      "core/src/test/java/org/apache/iceberg/variants/TestValueArray.java",
      "parquet/src/test/java/org/apache/iceberg/parquet/TestVariantReaders.java"
    ],
    "base_commit": "01fe380d455949abb49ebfecd9509afce8764fae",
    "head_commit": "df4e4d7f2267dc700e7a6c632eb4cd8844c7c5e5",
    "repo_url": "https://github.com/apache/iceberg/pull/12512",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12512",
    "dockerfile": "",
    "pr_merged_at": "2025-04-25T23:17:10.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/variants/ValueArray.java b/core/src/main/java/org/apache/iceberg/variants/ValueArray.java\nnew file mode 100644\nindex 000000000000..3da79bcef106\n--- /dev/null\n+++ b/core/src/main/java/org/apache/iceberg/variants/ValueArray.java\n@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.variants;\n+\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.util.List;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+\n+public class ValueArray implements VariantArray {\n+  private SerializationState serializationState = null;\n+  private List<VariantValue> elements = Lists.newArrayList();\n+\n+  ValueArray() {}\n+\n+  @Override\n+  public VariantValue get(int index) {\n+    return elements.get(index);\n+  }\n+\n+  @Override\n+  public int numElements() {\n+    return elements.size();\n+  }\n+\n+  public void add(VariantValue value) {\n+    elements.add(value);\n+    this.serializationState = null;\n+  }\n+\n+  @Override\n+  public int sizeInBytes() {\n+    if (null == serializationState) {\n+      this.serializationState = new SerializationState(elements);\n+    }\n+\n+    return serializationState.size();\n+  }\n+\n+  @Override\n+  public int writeTo(ByteBuffer buffer, int offset) {\n+    Preconditions.checkArgument(\n+        buffer.order() == ByteOrder.LITTLE_ENDIAN, \"Invalid byte order: big endian\");\n+\n+    if (null == serializationState) {\n+      this.serializationState = new SerializationState(elements);\n+    }\n+\n+    return serializationState.writeTo(buffer, offset);\n+  }\n+\n+  /** Common state for {@link #size()} and {@link #writeTo(ByteBuffer, int)} */\n+  private static class SerializationState {\n+    private final List<VariantValue> elements;\n+    private final int numElements;\n+    private final boolean isLarge;\n+    private final int dataSize;\n+    private final int offsetSize;\n+\n+    private SerializationState(List<VariantValue> elements) {\n+      this.elements = elements;\n+      this.numElements = elements.size();\n+      this.isLarge = numElements > 0xFF;\n+\n+      int totalDataSize = 0;\n+      for (VariantValue value : elements) {\n+        totalDataSize += value.sizeInBytes();\n+      }\n+\n+      this.dataSize = totalDataSize;\n+      this.offsetSize = VariantUtil.sizeOf(totalDataSize);\n+    }\n+\n+    private int size() {\n+      return 1 /* header */\n+          + (isLarge ? 4 : 1) /* num elements size */\n+          + (1 + numElements) * offsetSize /* offset list size */\n+          + dataSize;\n+    }\n+\n+    private int writeTo(ByteBuffer buffer, int offset) {\n+      int offsetListOffset =\n+          offset + 1 /* header size */ + (isLarge ? 4 : 1) /* num elements size */;\n+      int dataOffset = offsetListOffset + ((1 + numElements) * offsetSize);\n+      byte header = VariantUtil.arrayHeader(isLarge, offsetSize);\n+\n+      VariantUtil.writeByte(buffer, header, offset);\n+      VariantUtil.writeLittleEndianUnsigned(buffer, numElements, offset + 1, isLarge ? 4 : 1);\n+\n+      // Insert element offsets\n+      int nextValueOffset = 0;\n+      int index = 0;\n+      for (VariantValue element : elements) {\n+        // write the data offset\n+        VariantUtil.writeLittleEndianUnsigned(\n+            buffer, nextValueOffset, offsetListOffset + (index * offsetSize), offsetSize);\n+\n+        // write the data\n+        int valueSize = element.writeTo(buffer, dataOffset + nextValueOffset);\n+\n+        nextValueOffset += valueSize;\n+        index += 1;\n+      }\n+\n+      // write the final size of the data section\n+      VariantUtil.writeLittleEndianUnsigned(\n+          buffer, nextValueOffset, offsetListOffset + (index * offsetSize), offsetSize);\n+\n+      // return the total size\n+      return (dataOffset - offset) + dataSize;\n+    }\n+  }\n+}\n\ndiff --git a/core/src/main/java/org/apache/iceberg/variants/Variants.java b/core/src/main/java/org/apache/iceberg/variants/Variants.java\nindex d5f8cb4ae67c..5591145ca603 100644\n--- a/core/src/main/java/org/apache/iceberg/variants/Variants.java\n+++ b/core/src/main/java/org/apache/iceberg/variants/Variants.java\n@@ -121,6 +121,10 @@ public static boolean isNull(ByteBuffer valueBuffer) {\n     return VariantUtil.readByte(valueBuffer, 0) == 0;\n   }\n \n+  public static ValueArray array() {\n+    return new ValueArray();\n+  }\n+\n   public static <T> VariantPrimitive<T> of(PhysicalType type, T value) {\n     return new PrimitiveWrapper<>(type, value);\n   }\n\ndiff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantReaders.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantReaders.java\nindex 3e5635958c0a..40b0aeecc3b5 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantReaders.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantReaders.java\n@@ -31,6 +31,7 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.variants.PhysicalType;\n import org.apache.iceberg.variants.ShreddedObject;\n+import org.apache.iceberg.variants.ValueArray;\n import org.apache.iceberg.variants.Variant;\n import org.apache.iceberg.variants.VariantMetadata;\n import org.apache.iceberg.variants.VariantObject;\n@@ -95,6 +96,14 @@ public static VariantValueReader objects(\n         fieldReaders);\n   }\n \n+  public static VariantValueReader array(\n+      int repeatedDefinitionLevel,\n+      int repeatedRepetitionLevel,\n+      ParquetValueReader<?> elementReader) {\n+    return new ArrayReader(\n+        repeatedDefinitionLevel, repeatedRepetitionLevel, (VariantValueReader) elementReader);\n+  }\n+\n   public static VariantValueReader asVariant(PhysicalType type, ParquetValueReader<?> reader) {\n     return new ValueAsVariantReader<>(type, reader);\n   }\n@@ -332,6 +341,58 @@ public void setPageSource(PageReadStore pageStore) {\n     }\n   }\n \n+  private static class ArrayReader implements VariantValueReader {\n+    private final int definitionLevel;\n+    private final int repetitionLevel;\n+    private final VariantValueReader reader;\n+    private final TripleIterator<?> column;\n+    private final List<TripleIterator<?>> children;\n+\n+    protected ArrayReader(int definitionLevel, int repetitionLevel, VariantValueReader reader) {\n+      this.definitionLevel = definitionLevel;\n+      this.repetitionLevel = repetitionLevel;\n+      this.reader = reader;\n+      this.column = reader.column();\n+      this.children = reader.columns();\n+    }\n+\n+    @Override\n+    public void setPageSource(PageReadStore pageStore) {\n+      reader.setPageSource(pageStore);\n+    }\n+\n+    @Override\n+    public ValueArray read(VariantMetadata metadata) {\n+      ValueArray arr = Variants.array();\n+      do {\n+        if (column.currentDefinitionLevel() > definitionLevel) {\n+          VariantValue value = reader.read(metadata);\n+          arr.add(value != null ? value : Variants.ofNull());\n+        } else {\n+          // consume the empty list triple\n+          for (TripleIterator<?> child : children) {\n+            child.nextNull();\n+          }\n+          // if the current definition level is equal to the definition level of this repeated type,\n+          // then the result is an empty list and the repetition level will always be <= rl.\n+          break;\n+        }\n+      } while (column.currentRepetitionLevel() > repetitionLevel);\n+\n+      return arr;\n+    }\n+\n+    @Override\n+    public TripleIterator<?> column() {\n+      return column;\n+    }\n+\n+    @Override\n+    public List<TripleIterator<?>> columns() {\n+      return children;\n+    }\n+  }\n+\n   private static class VariantReader implements ParquetValueReader<Variant> {\n     private final ParquetValueReader<VariantMetadata> metadataReader;\n     private final VariantValueReader valueReader;\n\ndiff --git a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantVisitor.java b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantVisitor.java\nindex 71d2eb26627b..d0ca00b19313 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantVisitor.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/ParquetVariantVisitor.java\n@@ -31,6 +31,7 @@ public abstract class ParquetVariantVisitor<R> {\n   static final String METADATA = \"metadata\";\n   static final String VALUE = \"value\";\n   static final String TYPED_VALUE = \"typed_value\";\n+  static final String LIST = \"list\";\n \n   /**\n    * Handles the root variant column group.\n\ndiff --git a/parquet/src/main/java/org/apache/iceberg/parquet/VariantReaderBuilder.java b/parquet/src/main/java/org/apache/iceberg/parquet/VariantReaderBuilder.java\nindex df41c5aa6067..29ca90034623 100644\n--- a/parquet/src/main/java/org/apache/iceberg/parquet/VariantReaderBuilder.java\n+++ b/parquet/src/main/java/org/apache/iceberg/parquet/VariantReaderBuilder.java\n@@ -66,8 +66,8 @@ private String[] currentPath() {\n     return Streams.concat(Streams.stream(basePath), fieldNames.stream()).toArray(String[]::new);\n   }\n \n-  private String[] path(String name) {\n-    return Streams.concat(Streams.stream(basePath), fieldNames.stream(), Stream.of(name))\n+  private String[] path(String... names) {\n+    return Streams.concat(Streams.stream(basePath), fieldNames.stream(), Stream.of(names))\n         .toArray(String[]::new);\n   }\n \n@@ -162,8 +162,16 @@ public VariantValueReader object(\n \n   @Override\n   public VariantValueReader array(\n-      GroupType array, ParquetValueReader<?> valueResult, ParquetValueReader<?> elementResult) {\n-    throw new UnsupportedOperationException(\"Array is not yet supported\");\n+      GroupType array, ParquetValueReader<?> valueReader, ParquetValueReader<?> elementReader) {\n+    int valueDL =\n+        valueReader != null ? schema.getMaxDefinitionLevel(path(VALUE)) - 1 : Integer.MAX_VALUE;\n+    int typedDL = schema.getMaxDefinitionLevel(path(TYPED_VALUE)) - 1;\n+    int repeatedDL = schema.getMaxDefinitionLevel(path(TYPED_VALUE, LIST)) - 1;\n+    int repeatedRL = schema.getMaxRepetitionLevel(path(TYPED_VALUE, LIST)) - 1;\n+    VariantValueReader typedReader =\n+        ParquetVariantReaders.array(repeatedDL, repeatedRL, elementReader);\n+\n+    return ParquetVariantReaders.shredded(valueDL, valueReader, typedDL, typedReader);\n   }\n \n   private static class LogicalTypeToVariantReader\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/variants/TestShreddedObject.java b/core/src/test/java/org/apache/iceberg/variants/TestShreddedObject.java\nindex 6707ae6651a0..66d5c9911a79 100644\n--- a/core/src/test/java/org/apache/iceberg/variants/TestShreddedObject.java\n+++ b/core/src/test/java/org/apache/iceberg/variants/TestShreddedObject.java\n@@ -217,11 +217,12 @@ public void testPartiallyShreddedObjectSerializationLargeBuffer() {\n         .isEqualTo(DateTimeUtil.isoDateToDays(\"2024-10-12\"));\n   }\n \n-  @Test\n-  public void testTwoByteOffsets() {\n-    // a string larger than 255 bytes to push the value offset size above 1 byte\n-    String randomString = RandomUtil.generateString(300, random);\n-    SerializedPrimitive bigString = VariantTestUtil.createString(randomString);\n+  @ParameterizedTest\n+  @ValueSource(ints = {300, 70_000, 16_777_300})\n+  public void testMultiByteOffsets(int len) {\n+    // Use a string exceeding 255 bytes to test value offset sizes of 2, 3, and 4 bytes\n+    String randomString = RandomUtil.generateString(len, random);\n+    VariantPrimitive<String> bigString = Variants.of(randomString);\n \n     Map<String, VariantValue> data = Maps.newHashMap();\n     data.putAll(FIELDS);\n@@ -244,60 +245,6 @@ public void testTwoByteOffsets() {\n     assertThat(object.get(\"big\").asPrimitive().get()).isEqualTo(randomString);\n   }\n \n-  @Test\n-  public void testThreeByteOffsets() {\n-    // a string larger than 65535 bytes to push the value offset size above 2 bytes\n-    String randomString = RandomUtil.generateString(70_000, random);\n-    SerializedPrimitive reallyBigString = VariantTestUtil.createString(randomString);\n-\n-    Map<String, VariantValue> data = Maps.newHashMap();\n-    data.putAll(FIELDS);\n-    data.put(\"really-big\", reallyBigString);\n-\n-    ShreddedObject shredded = createShreddedObject(data);\n-    VariantValue value = roundTripLargeBuffer(shredded, shredded.metadata());\n-\n-    assertThat(value.type()).isEqualTo(PhysicalType.OBJECT);\n-    SerializedObject object = (SerializedObject) value;\n-    assertThat(object.numFields()).isEqualTo(4);\n-\n-    assertThat(object.get(\"a\").type()).isEqualTo(PhysicalType.INT32);\n-    assertThat(object.get(\"a\").asPrimitive().get()).isEqualTo(34);\n-    assertThat(object.get(\"b\").type()).isEqualTo(PhysicalType.STRING);\n-    assertThat(object.get(\"b\").asPrimitive().get()).isEqualTo(\"iceberg\");\n-    assertThat(object.get(\"c\").type()).isEqualTo(PhysicalType.DECIMAL4);\n-    assertThat(object.get(\"c\").asPrimitive().get()).isEqualTo(new BigDecimal(\"12.21\"));\n-    assertThat(object.get(\"really-big\").type()).isEqualTo(PhysicalType.STRING);\n-    assertThat(object.get(\"really-big\").asPrimitive().get()).isEqualTo(randomString);\n-  }\n-\n-  @Test\n-  public void testFourByteOffsets() {\n-    // a string larger than 16777215 bytes to push the value offset size above 3 bytes\n-    String randomString = RandomUtil.generateString(16_777_300, random);\n-    SerializedPrimitive reallyBigString = VariantTestUtil.createString(randomString);\n-\n-    Map<String, VariantValue> data = Maps.newHashMap();\n-    data.putAll(FIELDS);\n-    data.put(\"really-big\", reallyBigString);\n-\n-    ShreddedObject shredded = createShreddedObject(data);\n-    VariantValue value = roundTripLargeBuffer(shredded, shredded.metadata());\n-\n-    assertThat(value.type()).isEqualTo(PhysicalType.OBJECT);\n-    SerializedObject object = (SerializedObject) value;\n-    assertThat(object.numFields()).isEqualTo(4);\n-\n-    assertThat(object.get(\"a\").type()).isEqualTo(PhysicalType.INT32);\n-    assertThat(object.get(\"a\").asPrimitive().get()).isEqualTo(34);\n-    assertThat(object.get(\"b\").type()).isEqualTo(PhysicalType.STRING);\n-    assertThat(object.get(\"b\").asPrimitive().get()).isEqualTo(\"iceberg\");\n-    assertThat(object.get(\"c\").type()).isEqualTo(PhysicalType.DECIMAL4);\n-    assertThat(object.get(\"c\").asPrimitive().get()).isEqualTo(new BigDecimal(\"12.21\"));\n-    assertThat(object.get(\"really-big\").type()).isEqualTo(PhysicalType.STRING);\n-    assertThat(object.get(\"really-big\").asPrimitive().get()).isEqualTo(randomString);\n-  }\n-\n   @ParameterizedTest\n   @ValueSource(booleans = {true, false})\n   @SuppressWarnings({\"unchecked\", \"rawtypes\"})\n\ndiff --git a/core/src/test/java/org/apache/iceberg/variants/TestValueArray.java b/core/src/test/java/org/apache/iceberg/variants/TestValueArray.java\nnew file mode 100644\nindex 000000000000..f500f6106573\n--- /dev/null\n+++ b/core/src/test/java/org/apache/iceberg/variants/TestValueArray.java\n@@ -0,0 +1,166 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.variants;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import java.math.BigDecimal;\n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n+import java.util.List;\n+import java.util.Random;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.util.RandomUtil;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.ValueSource;\n+\n+public class TestValueArray {\n+  private static final VariantMetadata EMPTY_METADATA = Variants.emptyMetadata();\n+  private static final List<VariantValue> ELEMENTS =\n+      ImmutableList.of(\n+          Variants.of(34), Variants.of(\"iceberg\"), Variants.of(new BigDecimal(\"12.21\")));\n+\n+  private final Random random = new Random(871925);\n+\n+  @Test\n+  public void testElementAccess() {\n+    ValueArray arr = createArray(ELEMENTS);\n+\n+    assertThat(arr.numElements()).isEqualTo(3);\n+    assertThat(arr.get(0)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(arr.get(0).asPrimitive().get()).isEqualTo(34);\n+    assertThat(arr.get(1)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(arr.get(1).asPrimitive().get()).isEqualTo(\"iceberg\");\n+    assertThat(arr.get(2)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(arr.get(2).asPrimitive().get()).isEqualTo(new BigDecimal(\"12.21\"));\n+  }\n+\n+  @Test\n+  public void testSerializationMinimalBuffer() {\n+    ValueArray arr = createArray(ELEMENTS);\n+\n+    VariantValue value = roundTripMinimalBuffer(arr);\n+\n+    assertThat(value).isInstanceOf(SerializedArray.class);\n+    SerializedArray actual = (SerializedArray) value;\n+\n+    assertThat(actual.numElements()).isEqualTo(3);\n+    assertThat(actual.get(0)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(actual.get(0).asPrimitive().get()).isEqualTo(34);\n+    assertThat(actual.get(1)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(actual.get(1).asPrimitive().get()).isEqualTo(\"iceberg\");\n+    assertThat(actual.get(2)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(actual.get(2).asPrimitive().get()).isEqualTo(new BigDecimal(\"12.21\"));\n+  }\n+\n+  @Test\n+  public void testSerializationLargeBuffer() {\n+    ValueArray arr = createArray(ELEMENTS);\n+\n+    VariantValue value = roundTripLargeBuffer(arr);\n+\n+    assertThat(value).isInstanceOf(SerializedArray.class);\n+    SerializedArray actual = (SerializedArray) value;\n+\n+    assertThat(actual.numElements()).isEqualTo(3);\n+    assertThat(actual.get(0)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(actual.get(0).asPrimitive().get()).isEqualTo(34);\n+    assertThat(actual.get(1)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(actual.get(1).asPrimitive().get()).isEqualTo(\"iceberg\");\n+    assertThat(actual.get(2)).isInstanceOf(VariantPrimitive.class);\n+    assertThat(actual.get(2).asPrimitive().get()).isEqualTo(new BigDecimal(\"12.21\"));\n+  }\n+\n+  @ParameterizedTest\n+  @ValueSource(ints = {300, 70_000, 16_777_300})\n+  public void testMultiByteOffsets(int len) {\n+    // Use a string exceeding 255 bytes to test value offset sizes of 2, 3, and 4 bytes\n+    String randomString = RandomUtil.generateString(len, random);\n+    VariantPrimitive<String> bigString = Variants.of(randomString);\n+\n+    List<VariantValue> data = Lists.newArrayList();\n+    data.addAll(ELEMENTS);\n+    data.add(bigString);\n+\n+    ValueArray shredded = createArray(data);\n+    VariantValue value = roundTripLargeBuffer(shredded);\n+\n+    assertThat(value.type()).isEqualTo(PhysicalType.ARRAY);\n+    SerializedArray actualArray = (SerializedArray) value;\n+    assertThat(actualArray.numElements()).isEqualTo(4);\n+\n+    assertThat(actualArray.get(0).type()).isEqualTo(PhysicalType.INT32);\n+    assertThat(actualArray.get(0).asPrimitive().get()).isEqualTo(34);\n+    assertThat(actualArray.get(1).type()).isEqualTo(PhysicalType.STRING);\n+    assertThat(actualArray.get(1).asPrimitive().get()).isEqualTo(\"iceberg\");\n+    assertThat(actualArray.get(2).type()).isEqualTo(PhysicalType.DECIMAL4);\n+    assertThat(actualArray.get(2).asPrimitive().get()).isEqualTo(new BigDecimal(\"12.21\"));\n+    assertThat(actualArray.get(3).type()).isEqualTo(PhysicalType.STRING);\n+    assertThat(actualArray.get(3).asPrimitive().get()).isEqualTo(randomString);\n+  }\n+\n+  @Test\n+  public void testLargeArray() {\n+    List<VariantValue> elements = Lists.newArrayList();\n+    for (int i = 0; i < 10_000; i += 1) {\n+      elements.add(Variants.of(RandomUtil.generateString(10, random)));\n+    }\n+\n+    ValueArray arr = createArray(elements);\n+    VariantValue value = roundTripLargeBuffer(arr);\n+\n+    assertThat(value.type()).isEqualTo(PhysicalType.ARRAY);\n+    SerializedArray actualArray = (SerializedArray) value;\n+    assertThat(actualArray.numElements()).isEqualTo(10_000);\n+\n+    for (int i = 0; i < 10_000; i++) {\n+      VariantTestUtil.assertEqual(elements.get(i), actualArray.get(i));\n+    }\n+  }\n+\n+  private static VariantValue roundTripMinimalBuffer(ValueArray arr) {\n+    ByteBuffer serialized = ByteBuffer.allocate(arr.sizeInBytes()).order(ByteOrder.LITTLE_ENDIAN);\n+    arr.writeTo(serialized, 0);\n+\n+    return Variants.value(EMPTY_METADATA, serialized);\n+  }\n+\n+  private static VariantValue roundTripLargeBuffer(ValueArray arr) {\n+    ByteBuffer serialized =\n+        ByteBuffer.allocate(1000 + arr.sizeInBytes()).order(ByteOrder.LITTLE_ENDIAN);\n+    arr.writeTo(serialized, 300);\n+\n+    ByteBuffer slice = serialized.duplicate().order(ByteOrder.LITTLE_ENDIAN);\n+    slice.position(300);\n+    slice.limit(300 + arr.sizeInBytes());\n+\n+    return Variants.value(EMPTY_METADATA, slice);\n+  }\n+\n+  private static ValueArray createArray(List<VariantValue> elements) {\n+    ValueArray arr = new ValueArray();\n+    for (VariantValue element : elements) {\n+      arr.add(element);\n+    }\n+\n+    return arr;\n+  }\n+}\n\ndiff --git a/parquet/src/test/java/org/apache/iceberg/parquet/TestVariantReaders.java b/parquet/src/test/java/org/apache/iceberg/parquet/TestVariantReaders.java\nindex b0299762f7a2..23c6e9b3282c 100644\n--- a/parquet/src/test/java/org/apache/iceberg/parquet/TestVariantReaders.java\n+++ b/parquet/src/test/java/org/apache/iceberg/parquet/TestVariantReaders.java\n@@ -48,6 +48,7 @@\n import org.apache.iceberg.types.Types.VariantType;\n import org.apache.iceberg.variants.PhysicalType;\n import org.apache.iceberg.variants.ShreddedObject;\n+import org.apache.iceberg.variants.ValueArray;\n import org.apache.iceberg.variants.Variant;\n import org.apache.iceberg.variants.VariantMetadata;\n import org.apache.iceberg.variants.VariantObject;\n@@ -57,6 +58,8 @@\n import org.apache.iceberg.variants.Variants;\n import org.apache.parquet.avro.AvroSchemaConverter;\n import org.apache.parquet.avro.AvroWriteSupport;\n+import org.apache.parquet.conf.ParquetConfiguration;\n+import org.apache.parquet.conf.PlainParquetConfiguration;\n import org.apache.parquet.hadoop.ParquetWriter;\n import org.apache.parquet.hadoop.api.WriteSupport;\n import org.apache.parquet.schema.GroupType;\n@@ -135,6 +138,15 @@ public class TestVariantReaders {\n         Variants.ofUUID(\"f24f9b64-81fa-49d1-b74e-8c09a6e31c56\"),\n       };\n \n+  // Required configuration to convert between Avro and Parquet schemas with 3-level list structure\n+  private static final ParquetConfiguration CONF =\n+      new PlainParquetConfiguration(\n+          Map.of(\n+              AvroWriteSupport.WRITE_OLD_LIST_STRUCTURE,\n+              \"false\",\n+              AvroSchemaConverter.ADD_LIST_ELEMENT_RECORDS,\n+              \"false\"));\n+\n   private static Stream<Arguments> metadataAndValues() {\n     Stream<Arguments> primitives =\n         Stream.of(PRIMITIVES).map(variant -> Arguments.of(EMPTY_METADATA, variant));\n@@ -255,7 +267,7 @@ public void testMissingValueColumn() throws IOException {\n   }\n \n   @Test\n-  public void testValueAndTypedValueConflict() throws IOException {\n+  public void testValueAndTypedValueConflict() {\n     GroupType variantType = variant(\"var\", 2, shreddedPrimitive(PrimitiveTypeName.INT32));\n     MessageType parquetSchema = parquetSchema(variantType);\n \n@@ -885,6 +897,460 @@ public void testMixedRecords() throws IOException {\n     VariantTestUtil.assertEqual(expectedThree, actualThreeVariant.value());\n   }\n \n+  @Test\n+  public void testSimpleArray() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType = element(shreddedType);\n+    GroupType variantType = variant(\"var\", 2, list(elementType));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    List<GenericRecord> arr =\n+        List.of(\n+            record(elementType, Map.of(\"typed_value\", \"comedy\")),\n+            record(elementType, Map.of(\"typed_value\", \"drama\")));\n+\n+    GenericRecord var =\n+        record(\n+            variantType, Map.of(\"metadata\", VariantTestUtil.emptyMetadata(), \"typed_value\", arr));\n+    GenericRecord row = record(parquetSchema, Map.of(\"id\", 1, \"var\", var));\n+\n+    ValueArray expectedArray = Variants.array();\n+    expectedArray.add(Variants.of(\"comedy\"));\n+    expectedArray.add(Variants.of(\"drama\"));\n+\n+    Record actual = writeAndRead(parquetSchema, row);\n+\n+    assertThat(actual.getField(\"id\")).isEqualTo(1);\n+    assertThat(actual.getField(\"var\")).isInstanceOf(Variant.class);\n+    Variant actualVariant = (Variant) actual.getField(\"var\");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+    VariantTestUtil.assertEqual(expectedArray, actualVariant.value());\n+  }\n+\n+  @Test\n+  public void testNullArray() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType variantType = variant(\"var\", 2, list(element(shreddedType)));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    GenericRecord var =\n+        record(\n+            variantType,\n+            Map.of(\n+                \"metadata\",\n+                VariantTestUtil.emptyMetadata(),\n+                \"value\",\n+                serialize(Variants.ofNull())));\n+    GenericRecord row = record(parquetSchema, Map.of(\"id\", 1, \"var\", var));\n+\n+    Record actual = writeAndRead(parquetSchema, row);\n+\n+    assertThat(actual.getField(\"id\")).isEqualTo(1);\n+    assertThat(actual.getField(\"var\")).isInstanceOf(Variant.class);\n+    Variant actualVariant = (Variant) actual.getField(\"var\");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+    VariantTestUtil.assertEqual(Variants.ofNull(), actualVariant.value());\n+  }\n+\n+  @Test\n+  public void testEmptyArray() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType variantType = variant(\"var\", 2, list(element(shreddedType)));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    List<GenericRecord> arr = List.of();\n+    GenericRecord var =\n+        record(\n+            variantType, Map.of(\"metadata\", VariantTestUtil.emptyMetadata(), \"typed_value\", arr));\n+    GenericRecord row = record(parquetSchema, Map.of(\"id\", 1, \"var\", var));\n+\n+    Record actual = writeAndRead(parquetSchema, row);\n+    assertThat(actual.getField(\"id\")).isEqualTo(1);\n+    assertThat(actual.getField(\"var\")).isInstanceOf(Variant.class);\n+    Variant actualVariant = (Variant) actual.getField(\"var\");\n+    assertThat(actualVariant.value().type()).isEqualTo(PhysicalType.ARRAY);\n+    assertThat(actualVariant.value().asArray().numElements()).isEqualTo(0);\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+  }\n+\n+  @Test\n+  public void testArrayWithNull() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType = element(shreddedType);\n+    GroupType variantType = variant(\"var\", 2, list(elementType));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    List<GenericRecord> arr =\n+        List.of(\n+            record(elementType, Map.of(\"typed_value\", \"comedy\")),\n+            record(elementType, Map.of(\"value\", serialize(Variants.ofNull()))),\n+            record(elementType, Map.of(\"typed_value\", \"drama\")));\n+\n+    GenericRecord var =\n+        record(\n+            variantType, Map.of(\"metadata\", VariantTestUtil.emptyMetadata(), \"typed_value\", arr));\n+    GenericRecord row = record(parquetSchema, Map.of(\"id\", 1, \"var\", var));\n+\n+    ValueArray expectedArray = Variants.array();\n+    expectedArray.add(Variants.of(\"comedy\"));\n+    expectedArray.add(Variants.ofNull());\n+    expectedArray.add(Variants.of(\"drama\"));\n+\n+    Record actual = writeAndRead(parquetSchema, row);\n+\n+    assertThat(actual.getField(\"id\")).isEqualTo(1);\n+    assertThat(actual.getField(\"var\")).isInstanceOf(Variant.class);\n+    Variant actualVariant = (Variant) actual.getField(\"var\");\n+    assertThat(actualVariant.value().type()).isEqualTo(PhysicalType.ARRAY);\n+    assertThat(actualVariant.value().asArray().numElements()).isEqualTo(3);\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+    VariantTestUtil.assertEqual(expectedArray, actualVariant.value());\n+  }\n+\n+  @Test\n+  public void testNestedArray() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType = element(shreddedType);\n+    GroupType outerElementType = element(list(elementType));\n+    GroupType variantType = variant(\"var\", 2, list(outerElementType));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    List<GenericRecord> inner1 =\n+        List.of(\n+            record(elementType, Map.of(\"typed_value\", \"comedy\")),\n+            record(elementType, Map.of(\"typed_value\", \"drama\")));\n+    List<GenericRecord> outer1 =\n+        List.of(\n+            record(outerElementType, Map.of(\"typed_value\", inner1)),\n+            record(outerElementType, Map.of(\"typed_value\", List.of())));\n+    GenericRecord var =\n+        record(\n+            variantType,\n+            Map.of(\"metadata\", VariantTestUtil.emptyMetadata(), \"typed_value\", outer1));\n+    GenericRecord row = record(parquetSchema, Map.of(\"id\", 1, \"var\", var));\n+\n+    ValueArray expectedArray = Variants.array();\n+    ValueArray expectedInner1 = Variants.array();\n+    expectedInner1.add(Variants.of(\"comedy\"));\n+    expectedInner1.add(Variants.of(\"drama\"));\n+    ValueArray expectedInner2 = Variants.array();\n+    expectedArray.add(expectedInner1);\n+    expectedArray.add(expectedInner2);\n+\n+    Record actual = writeAndRead(parquetSchema, row);\n+\n+    // Verify\n+    assertThat(actual.getField(\"id\")).isEqualTo(1);\n+    assertThat(actual.getField(\"var\")).isInstanceOf(Variant.class);\n+    Variant actualVariant = (Variant) actual.getField(\"var\");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+    VariantTestUtil.assertEqual(expectedArray, actualVariant.value());\n+  }\n+\n+  @Test\n+  public void testArrayWithNestedObject() throws IOException {\n+    GroupType fieldA = field(\"a\", shreddedPrimitive(PrimitiveTypeName.INT32));\n+    GroupType fieldB = field(\"b\", shreddedPrimitive(PrimitiveTypeName.BINARY, STRING));\n+    GroupType shreddedFields = objectFields(fieldA, fieldB);\n+    GroupType elementType = element(shreddedFields);\n+    GroupType listType = list(elementType);\n+    GroupType variantType = variant(\"var\", 2, listType);\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    // Row 1 with nested fully shredded object\n+    GenericRecord shredded1 =\n+        record(\n+            shreddedFields,\n+            Map.of(\n+                \"a\",\n+                record(fieldA, Map.of(\"typed_value\", 1)),\n+                \"b\",\n+                record(fieldB, Map.of(\"typed_value\", \"comedy\"))));\n+    GenericRecord shredded2 =\n+        record(\n+            shreddedFields,\n+            Map.of(\n+                \"a\",\n+                record(fieldA, Map.of(\"typed_value\", 2)),\n+                \"b\",\n+                record(fieldB, Map.of(\"typed_value\", \"drama\"))));\n+    List<GenericRecord> arr1 =\n+        List.of(\n+            record(elementType, Map.of(\"typed_value\", shredded1)),\n+            record(elementType, Map.of(\"typed_value\", shredded2)));\n+    GenericRecord var1 =\n+        record(variantType, Map.of(\"metadata\", TEST_METADATA_BUFFER, \"typed_value\", arr1));\n+    GenericRecord row1 = record(parquetSchema, Map.of(\"id\", 1, \"var\", var1));\n+\n+    ValueArray expected1 = Variants.array();\n+    ShreddedObject expectedElement1 = Variants.object(TEST_METADATA);\n+    expectedElement1.put(\"a\", Variants.of(1));\n+    expectedElement1.put(\"b\", Variants.of(\"comedy\"));\n+    expected1.add(expectedElement1);\n+    ShreddedObject expectedElement2 = Variants.object(TEST_METADATA);\n+    expectedElement2.put(\"a\", Variants.of(2));\n+    expectedElement2.put(\"b\", Variants.of(\"drama\"));\n+    expected1.add(expectedElement2);\n+\n+    // Row 2 with nested partially shredded object\n+    GenericRecord shredded3 =\n+        record(\n+            shreddedFields,\n+            Map.of(\n+                \"a\",\n+                record(fieldA, Map.of(\"typed_value\", 3)),\n+                \"b\",\n+                record(fieldB, Map.of(\"typed_value\", \"action\"))));\n+    ShreddedObject baseObject3 = Variants.object(TEST_METADATA);\n+    baseObject3.put(\"c\", Variants.of(\"str\"));\n+\n+    GenericRecord shredded4 =\n+        record(\n+            shreddedFields,\n+            Map.of(\n+                \"a\",\n+                record(fieldA, Map.of(\"typed_value\", 4)),\n+                \"b\",\n+                record(fieldB, Map.of(\"typed_value\", \"horror\"))));\n+    ShreddedObject baseObject4 = Variants.object(TEST_METADATA);\n+    baseObject4.put(\"d\", Variants.ofIsoDate(\"2024-01-30\"));\n+\n+    List<GenericRecord> arr2 =\n+        List.of(\n+            record(elementType, Map.of(\"value\", serialize(baseObject3), \"typed_value\", shredded3)),\n+            record(elementType, Map.of(\"value\", serialize(baseObject4), \"typed_value\", shredded4)));\n+    GenericRecord var2 =\n+        record(variantType, Map.of(\"metadata\", TEST_METADATA_BUFFER, \"typed_value\", arr2));\n+    GenericRecord row2 = record(parquetSchema, Map.of(\"id\", 2, \"var\", var2));\n+\n+    ValueArray expected2 = Variants.array();\n+    ShreddedObject expectedElement3 = Variants.object(TEST_METADATA);\n+    expectedElement3.put(\"a\", Variants.of(3));\n+    expectedElement3.put(\"b\", Variants.of(\"action\"));\n+    expectedElement3.put(\"c\", Variants.of(\"str\"));\n+    expected2.add(expectedElement3);\n+    ShreddedObject expectedElement4 = Variants.object(TEST_METADATA);\n+    expectedElement4.put(\"a\", Variants.of(4));\n+    expectedElement4.put(\"b\", Variants.of(\"horror\"));\n+    expectedElement4.put(\"d\", Variants.ofIsoDate(\"2024-01-30\"));\n+    expected2.add(expectedElement4);\n+\n+    // verify\n+    List<Record> actual = writeAndRead(parquetSchema, List.of(row1, row2));\n+    Record actual1 = actual.get(0);\n+    assertThat(actual1.getField(\"id\")).isEqualTo(1);\n+    assertThat(actual1.getField(\"var\")).isInstanceOf(Variant.class);\n+\n+    Variant actualVariant1 = (Variant) actual1.getField(\"var\");\n+    VariantTestUtil.assertEqual(TEST_METADATA, actualVariant1.metadata());\n+    VariantTestUtil.assertEqual(expected1, actualVariant1.value());\n+\n+    Record actual2 = actual.get(1);\n+    assertThat(actual2.getField(\"id\")).isEqualTo(2);\n+    assertThat(actual2.getField(\"var\")).isInstanceOf(Variant.class);\n+\n+    Variant actualVariant2 = (Variant) actual2.getField(\"var\");\n+    VariantTestUtil.assertEqual(TEST_METADATA, actualVariant2.metadata());\n+    VariantTestUtil.assertEqual(expected2, actualVariant2.value());\n+  }\n+\n+  @Test\n+  public void testArrayWithNonArray() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType = element(shreddedType);\n+    GroupType variantType = variant(\"var\", 2, list(elementType));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    List<GenericRecord> arr1 =\n+        List.of(\n+            record(elementType, Map.of(\"typed_value\", \"comedy\")),\n+            record(elementType, Map.of(\"typed_value\", \"drama\")));\n+    GenericRecord var1 =\n+        record(\n+            variantType, Map.of(\"metadata\", VariantTestUtil.emptyMetadata(), \"typed_value\", arr1));\n+    GenericRecord row1 = record(parquetSchema, Map.of(\"id\", 1, \"var\", var1));\n+\n+    ValueArray expectedArray1 = Variants.array();\n+    expectedArray1.add(Variants.of(\"comedy\"));\n+    expectedArray1.add(Variants.of(\"drama\"));\n+\n+    GenericRecord var2 =\n+        record(\n+            variantType,\n+            Map.of(\n+                \"metadata\", VariantTestUtil.emptyMetadata(), \"value\", serialize(Variants.of(34))));\n+    GenericRecord row2 = record(parquetSchema, Map.of(\"id\", 2, \"var\", var2));\n+\n+    VariantValue expectedValue2 = Variants.of(PhysicalType.INT32, 34);\n+\n+    GenericRecord var3 =\n+        record(variantType, Map.of(\"metadata\", TEST_METADATA_BUFFER, \"value\", TEST_OBJECT_BUFFER));\n+    GenericRecord row3 = record(parquetSchema, Map.of(\"id\", 3, \"var\", var3));\n+\n+    ShreddedObject expectedObject3 = Variants.object(TEST_METADATA);\n+    expectedObject3.put(\"a\", Variants.ofNull());\n+    expectedObject3.put(\"d\", Variants.of(\"iceberg\"));\n+\n+    // Test array is read properly after a non-array\n+    List<GenericRecord> arr4 =\n+        List.of(\n+            record(elementType, Map.of(\"typed_value\", \"action\")),\n+            record(elementType, Map.of(\"typed_value\", \"horror\")));\n+    GenericRecord var4 =\n+        record(variantType, Map.of(\"metadata\", TEST_METADATA_BUFFER, \"typed_value\", arr4));\n+    GenericRecord row4 = record(parquetSchema, Map.of(\"id\", 4, \"var\", var4));\n+\n+    ValueArray expectedArray4 = Variants.array();\n+    expectedArray4.add(Variants.of(\"action\"));\n+    expectedArray4.add(Variants.of(\"horror\"));\n+\n+    List<Record> actual = writeAndRead(parquetSchema, List.of(row1, row2, row3, row4));\n+\n+    // Verify\n+    Record actual1 = actual.get(0);\n+    assertThat(actual1.getField(\"id\")).isEqualTo(1);\n+    assertThat(actual1.getField(\"var\")).isInstanceOf(Variant.class);\n+    Variant actualVariant1 = (Variant) actual1.getField(\"var\");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant1.metadata());\n+    VariantTestUtil.assertEqual(expectedArray1, actualVariant1.value());\n+\n+    Record actual2 = actual.get(1);\n+    assertThat(actual2.getField(\"id\")).isEqualTo(2);\n+    assertThat(actual2.getField(\"var\")).isInstanceOf(Variant.class);\n+    Variant actualVariant2 = (Variant) actual2.getField(\"var\");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant2.metadata());\n+    VariantTestUtil.assertEqual(expectedValue2, actualVariant2.value());\n+\n+    Record actual3 = actual.get(2);\n+    assertThat(actual3.getField(\"id\")).isEqualTo(3);\n+    assertThat(actual3.getField(\"var\")).isInstanceOf(Variant.class);\n+    Variant actualVariant3 = (Variant) actual3.getField(\"var\");\n+    VariantTestUtil.assertEqual(TEST_METADATA, actualVariant3.metadata());\n+    VariantTestUtil.assertEqual(expectedObject3, actualVariant3.value());\n+\n+    Record actual4 = actual.get(3);\n+    assertThat(actual4.getField(\"id\")).isEqualTo(4);\n+    assertThat(actual4.getField(\"var\")).isInstanceOf(Variant.class);\n+    Variant actualVariant4 = (Variant) actual4.getField(\"var\");\n+    VariantTestUtil.assertEqual(TEST_METADATA, actualVariant4.metadata());\n+    VariantTestUtil.assertEqual(expectedArray4, actualVariant4.value());\n+  }\n+\n+  @Test\n+  public void testArrayMissingValueColumn() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType = element(shreddedType);\n+    GroupType variantType =\n+        Types.buildGroup(Type.Repetition.OPTIONAL)\n+            .id(2)\n+            .required(PrimitiveTypeName.BINARY)\n+            .named(\"metadata\")\n+            .addField(list(elementType))\n+            .named(\"var\");\n+\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    List<GenericRecord> arr =\n+        List.of(\n+            record(elementType, Map.of(\"typed_value\", \"comedy\")),\n+            record(elementType, Map.of(\"typed_value\", \"drama\")));\n+    GenericRecord var =\n+        record(\n+            variantType, Map.of(\"metadata\", VariantTestUtil.emptyMetadata(), \"typed_value\", arr));\n+    GenericRecord row = record(parquetSchema, Map.of(\"id\", 1, \"var\", var));\n+\n+    ValueArray expectedArray = Variants.array();\n+    expectedArray.add(Variants.of(\"comedy\"));\n+    expectedArray.add(Variants.of(\"drama\"));\n+\n+    Record actual = writeAndRead(parquetSchema, row);\n+\n+    assertThat(actual.getField(\"id\")).isEqualTo(1);\n+    assertThat(actual.getField(\"var\")).isInstanceOf(Variant.class);\n+    Variant actualVariant = (Variant) actual.getField(\"var\");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+    VariantTestUtil.assertEqual(expectedArray, actualVariant.value());\n+  }\n+\n+  @Test\n+  public void testArrayMissingElementValueColumn() throws IOException {\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType =\n+        Types.buildGroup(Type.Repetition.REQUIRED).addField(shreddedType).named(\"element\");\n+\n+    GroupType variantType = variant(\"var\", 2, list(elementType));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    List<GenericRecord> arr =\n+        List.of(\n+            record(elementType, Map.of(\"typed_value\", \"comedy\")),\n+            record(elementType, Map.of(\"typed_value\", \"drama\")));\n+    GenericRecord var =\n+        record(\n+            variantType, Map.of(\"metadata\", VariantTestUtil.emptyMetadata(), \"typed_value\", arr));\n+    GenericRecord row = record(parquetSchema, Map.of(\"id\", 1, \"var\", var));\n+\n+    ValueArray expectedArray = Variants.array();\n+    expectedArray.add(Variants.of(\"comedy\"));\n+    expectedArray.add(Variants.of(\"drama\"));\n+\n+    Record actual = writeAndRead(parquetSchema, row);\n+\n+    assertThat(actual.getField(\"id\")).isEqualTo(1);\n+    assertThat(actual.getField(\"var\")).isInstanceOf(Variant.class);\n+    Variant actualVariant = (Variant) actual.getField(\"var\");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+    VariantTestUtil.assertEqual(expectedArray, actualVariant.value());\n+  }\n+\n+  @Test\n+  public void testArrayWithElementNullValueAndNullTypedValue() throws IOException {\n+    // Test the invalid case that both value and typed_value of an element are null\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType = element(shreddedType);\n+    GroupType variantType = variant(\"var\", 2, list(elementType));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    GenericRecord element = record(elementType, Map.of());\n+    GenericRecord variant =\n+        record(\n+            variantType,\n+            Map.of(\"metadata\", VariantTestUtil.emptyMetadata(), \"typed_value\", List.of(element)));\n+    GenericRecord record = record(parquetSchema, Map.of(\"id\", 1, \"var\", variant));\n+\n+    Record actual = writeAndRead(parquetSchema, record);\n+    assertThat(actual.getField(\"id\")).isEqualTo(1);\n+    assertThat(actual.getField(\"var\")).isInstanceOf(Variant.class);\n+\n+    Variant actualVariant = (Variant) actual.getField(\"var\");\n+    VariantTestUtil.assertEqual(EMPTY_METADATA, actualVariant.metadata());\n+    VariantValue actualValue = actualVariant.value();\n+    assertThat(actualValue.type()).isEqualTo(PhysicalType.ARRAY);\n+    assertThat(actualValue.asArray().numElements()).isEqualTo(1);\n+    VariantTestUtil.assertEqual(Variants.ofNull(), actualValue.asArray().get(0));\n+  }\n+\n+  @Test\n+  public void testArrayWithElementValueTypedValueConflict() {\n+    // Test the invalid case that both value and typed_value of an element are not null\n+    Type shreddedType = shreddedPrimitive(PrimitiveTypeName.BINARY, STRING);\n+    GroupType elementType = element(shreddedType);\n+    GroupType variantType = variant(\"var\", 2, list(elementType));\n+    MessageType parquetSchema = parquetSchema(variantType);\n+\n+    GenericRecord element =\n+        record(elementType, Map.of(\"value\", serialize(Variants.of(3)), \"typed_value\", \"comedy\"));\n+    GenericRecord variant =\n+        record(\n+            variantType,\n+            Map.of(\"metadata\", VariantTestUtil.emptyMetadata(), \"typed_value\", List.of(element)));\n+    GenericRecord record = record(parquetSchema, Map.of(\"id\", 1, \"var\", variant));\n+\n+    assertThatThrownBy(() -> writeAndRead(parquetSchema, record))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"Invalid variant, conflicting value and typed_value\");\n+  }\n+\n   private static ByteBuffer serialize(VariantValue value) {\n     ByteBuffer buffer = ByteBuffer.allocate(value.sizeInBytes()).order(ByteOrder.LITTLE_ENDIAN);\n     value.writeTo(buffer, 0);\n@@ -943,7 +1409,7 @@ static List<Record> writeAndRead(MessageType parquetSchema, List<GenericRecord>\n     OutputFile outputFile = new InMemoryOutputFile();\n \n     try (ParquetWriter<GenericRecord> writer =\n-        new TestWriterBuilder(outputFile).withFileType(parquetSchema).build()) {\n+        new TestWriterBuilder(outputFile).withFileType(parquetSchema).withConf(CONF).build()) {\n       for (GenericRecord record : records) {\n         writer.write(record);\n       }\n@@ -1104,14 +1570,38 @@ private static GroupType field(String name, Type shreddedType) {\n         .named(name);\n   }\n \n+  private static GroupType element(Type shreddedType) {\n+    return field(\"element\", shreddedType);\n+  }\n+\n+  private static GroupType list(GroupType elementType) {\n+    return Types.optionalList().element(elementType).named(\"typed_value\");\n+  }\n+\n+  private static void checkListType(GroupType listType) {\n+    // Check the list is a 3-level structure\n+    Preconditions.checkArgument(\n+        listType.getFieldCount() == 1\n+            && listType.getFields().get(0).isRepetition(Type.Repetition.REPEATED),\n+        \"Invalid list type: does not contain single repeated field: %s\",\n+        listType);\n+\n+    GroupType repeated = listType.getFields().get(0).asGroupType();\n+    Preconditions.checkArgument(\n+        repeated.getFieldCount() == 1\n+            && repeated.getFields().get(0).isRepetition(Type.Repetition.REQUIRED),\n+        \"Invalid list type: does not contain single required subfield: %s\",\n+        listType);\n+  }\n+\n   private static org.apache.avro.Schema avroSchema(GroupType schema) {\n     if (schema instanceof MessageType) {\n-      return new AvroSchemaConverter().convert((MessageType) schema);\n+      return new AvroSchemaConverter(CONF).convert((MessageType) schema);\n \n     } else {\n       MessageType wrapped = Types.buildMessage().addField(schema).named(\"table\");\n       org.apache.avro.Schema avro =\n-          new AvroSchemaConverter().convert(wrapped).getFields().get(0).schema();\n+          new AvroSchemaConverter(CONF).convert(wrapped).getFields().get(0).schema();\n       switch (avro.getType()) {\n         case RECORD:\n           return avro;\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12501",
    "pr_id": 12501,
    "issue_id": 7160,
    "repo": "apache/iceberg",
    "problem_statement": "Move JUnit4 tests to JUnit5\n### Feature Request / Improvement\n\nIn our contributing guidelines we point people to writing new unit tests using JUni5. \r\n\r\nWe should consider/evaluate what the best approach would be to eventually move existing tests from JUnit4 tests to JUnit5. \n\n### Query engine\n\nNone",
    "issue_word_count": 45,
    "test_files_count": 9,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/CatalogTestBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBaseWithCatalog.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestFunctionCatalog.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPathIdentifier.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestAlterTable.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPathIdentifier.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestAlterTable.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/CatalogTestBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBase.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBaseWithCatalog.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestFunctionCatalog.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPathIdentifier.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestAlterTable.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestBase.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPathIdentifier.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestAlterTable.java"
    ],
    "base_commit": "3dba6afb789a420373e44ac29ecdef866bd7ebee",
    "head_commit": "f57511c94b060e0523266d28d682f5966fafdd5f",
    "repo_url": "https://github.com/apache/iceberg/pull/12501",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12501",
    "dockerfile": "",
    "pr_merged_at": "2025-03-14T08:09:52.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/CatalogTestBase.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/CatalogTestBase.java\nnew file mode 100644\nindex 000000000000..87a49b64441d\n--- /dev/null\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/CatalogTestBase.java\n@@ -0,0 +1,59 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.spark;\n+\n+import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public abstract class CatalogTestBase extends TestBaseWithCatalog {\n+\n+  // these parameters are broken out to avoid changes that need to modify lots of test suites\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  public static Object[][] parameters() {\n+    return new Object[][] {\n+      {\n+        SparkCatalogConfig.HIVE.catalogName(),\n+        SparkCatalogConfig.HIVE.implementation(),\n+        SparkCatalogConfig.HIVE.properties()\n+      },\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties()\n+      },\n+      {\n+        SparkCatalogConfig.SPARK.catalogName(),\n+        SparkCatalogConfig.SPARK.implementation(),\n+        SparkCatalogConfig.SPARK.properties()\n+      },\n+      {\n+        SparkCatalogConfig.REST.catalogName(),\n+        SparkCatalogConfig.REST.implementation(),\n+        ImmutableMap.builder()\n+            .putAll(SparkCatalogConfig.REST.properties())\n+            .put(CatalogProperties.URI, restCatalog.properties().get(CatalogProperties.URI))\n+            .build()\n+      }\n+    };\n+  }\n+}\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBase.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBase.java\nnew file mode 100644\nindex 000000000000..2d327519e027\n--- /dev/null\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBase.java\n@@ -0,0 +1,287 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.spark;\n+\n+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.METASTOREURIS;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import java.io.IOException;\n+import java.io.UncheckedIOException;\n+import java.net.URI;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.TimeZone;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicReference;\n+import org.apache.hadoop.hive.conf.HiveConf;\n+import org.apache.iceberg.CatalogUtil;\n+import org.apache.iceberg.ContentFile;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.exceptions.AlreadyExistsException;\n+import org.apache.iceberg.hive.HiveCatalog;\n+import org.apache.iceberg.hive.TestHiveMetastore;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Encoders;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.execution.QueryExecution;\n+import org.apache.spark.sql.execution.SparkPlan;\n+import org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec;\n+import org.apache.spark.sql.internal.SQLConf;\n+import org.apache.spark.sql.util.QueryExecutionListener;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+\n+public abstract class TestBase extends SparkTestHelperBase {\n+\n+  protected static TestHiveMetastore metastore = null;\n+  protected static HiveConf hiveConf = null;\n+  protected static SparkSession spark = null;\n+  protected static JavaSparkContext sparkContext = null;\n+  protected static HiveCatalog catalog = null;\n+\n+  @BeforeAll\n+  public static void startMetastoreAndSpark() {\n+    TestBase.metastore = new TestHiveMetastore();\n+    metastore.start();\n+    TestBase.hiveConf = metastore.hiveConf();\n+\n+    TestBase.spark =\n+        SparkSession.builder()\n+            .master(\"local[2]\")\n+            .config(SQLConf.PARTITION_OVERWRITE_MODE().key(), \"dynamic\")\n+            .config(\"spark.hadoop.\" + METASTOREURIS.varname, hiveConf.get(METASTOREURIS.varname))\n+            .config(\"spark.sql.legacy.respectNullabilityInTextDatasetConversion\", \"true\")\n+            .enableHiveSupport()\n+            .getOrCreate();\n+\n+    TestBase.sparkContext = JavaSparkContext.fromSparkContext(spark.sparkContext());\n+\n+    TestBase.catalog =\n+        (HiveCatalog)\n+            CatalogUtil.loadCatalog(\n+                HiveCatalog.class.getName(), \"hive\", ImmutableMap.of(), hiveConf);\n+\n+    try {\n+      catalog.createNamespace(Namespace.of(\"default\"));\n+    } catch (AlreadyExistsException ignored) {\n+      // the default namespace already exists. ignore the create error\n+    }\n+  }\n+\n+  @AfterAll\n+  public static void stopMetastoreAndSpark() throws Exception {\n+    TestBase.catalog = null;\n+    if (metastore != null) {\n+      metastore.stop();\n+      TestBase.metastore = null;\n+    }\n+    if (spark != null) {\n+      spark.stop();\n+      TestBase.spark = null;\n+      TestBase.sparkContext = null;\n+    }\n+  }\n+\n+  protected long waitUntilAfter(long timestampMillis) {\n+    long current = System.currentTimeMillis();\n+    while (current <= timestampMillis) {\n+      current = System.currentTimeMillis();\n+    }\n+    return current;\n+  }\n+\n+  protected List<Object[]> sql(String query, Object... args) {\n+    List<Row> rows = spark.sql(String.format(query, args)).collectAsList();\n+    if (rows.size() < 1) {\n+      return ImmutableList.of();\n+    }\n+\n+    return rowsToJava(rows);\n+  }\n+\n+  protected Object scalarSql(String query, Object... args) {\n+    List<Object[]> rows = sql(query, args);\n+    assertThat(rows).as(\"Scalar SQL should return one row\").hasSize(1);\n+    Object[] row = Iterables.getOnlyElement(rows);\n+    assertThat(row).as(\"Scalar SQL should return one value\").hasSize(1);\n+    return row[0];\n+  }\n+\n+  protected Object[] row(Object... values) {\n+    return values;\n+  }\n+\n+  protected static String dbPath(String dbName) {\n+    return metastore.getDatabasePath(dbName);\n+  }\n+\n+  protected void withUnavailableFiles(Iterable<? extends ContentFile<?>> files, Action action) {\n+    Iterable<String> fileLocations = Iterables.transform(files, ContentFile::location);\n+    withUnavailableLocations(fileLocations, action);\n+  }\n+\n+  private void move(String location, String newLocation) {\n+    Path path = Paths.get(URI.create(location));\n+    Path tempPath = Paths.get(URI.create(newLocation));\n+\n+    try {\n+      Files.move(path, tempPath);\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(\"Failed to move: \" + location, e);\n+    }\n+  }\n+\n+  protected void withUnavailableLocations(Iterable<String> locations, Action action) {\n+    for (String location : locations) {\n+      move(location, location + \"_temp\");\n+    }\n+\n+    try {\n+      action.invoke();\n+    } finally {\n+      for (String location : locations) {\n+        move(location + \"_temp\", location);\n+      }\n+    }\n+  }\n+\n+  protected void withDefaultTimeZone(String zoneId, Action action) {\n+    TimeZone currentZone = TimeZone.getDefault();\n+    try {\n+      TimeZone.setDefault(TimeZone.getTimeZone(zoneId));\n+      action.invoke();\n+    } finally {\n+      TimeZone.setDefault(currentZone);\n+    }\n+  }\n+\n+  protected void withSQLConf(Map<String, String> conf, Action action) {\n+    SQLConf sqlConf = SQLConf.get();\n+\n+    Map<String, String> currentConfValues = Maps.newHashMap();\n+    conf.keySet()\n+        .forEach(\n+            confKey -> {\n+              if (sqlConf.contains(confKey)) {\n+                String currentConfValue = sqlConf.getConfString(confKey);\n+                currentConfValues.put(confKey, currentConfValue);\n+              }\n+            });\n+\n+    conf.forEach(\n+        (confKey, confValue) -> {\n+          if (SQLConf.isStaticConfigKey(confKey)) {\n+            throw new RuntimeException(\"Cannot modify the value of a static config: \" + confKey);\n+          }\n+          sqlConf.setConfString(confKey, confValue);\n+        });\n+\n+    try {\n+      action.invoke();\n+    } finally {\n+      conf.forEach(\n+          (confKey, confValue) -> {\n+            if (currentConfValues.containsKey(confKey)) {\n+              sqlConf.setConfString(confKey, currentConfValues.get(confKey));\n+            } else {\n+              sqlConf.unsetConf(confKey);\n+            }\n+          });\n+    }\n+  }\n+\n+  protected Dataset<Row> jsonToDF(String schema, String... records) {\n+    Dataset<String> jsonDF = spark.createDataset(ImmutableList.copyOf(records), Encoders.STRING());\n+    return spark.read().schema(schema).json(jsonDF);\n+  }\n+\n+  protected void append(String table, String... jsonRecords) {\n+    try {\n+      String schema = spark.table(table).schema().toDDL();\n+      Dataset<Row> df = jsonToDF(schema, jsonRecords);\n+      df.coalesce(1).writeTo(table).append();\n+    } catch (NoSuchTableException e) {\n+      throw new RuntimeException(\"Failed to write data\", e);\n+    }\n+  }\n+\n+  protected String tablePropsAsString(Map<String, String> tableProps) {\n+    StringBuilder stringBuilder = new StringBuilder();\n+\n+    for (Map.Entry<String, String> property : tableProps.entrySet()) {\n+      if (stringBuilder.length() > 0) {\n+        stringBuilder.append(\", \");\n+      }\n+      stringBuilder.append(String.format(\"'%s' '%s'\", property.getKey(), property.getValue()));\n+    }\n+\n+    return stringBuilder.toString();\n+  }\n+\n+  protected SparkPlan executeAndKeepPlan(String query, Object... args) {\n+    return executeAndKeepPlan(() -> sql(query, args));\n+  }\n+\n+  protected SparkPlan executeAndKeepPlan(Action action) {\n+    AtomicReference<SparkPlan> executedPlanRef = new AtomicReference<>();\n+\n+    QueryExecutionListener listener =\n+        new QueryExecutionListener() {\n+          @Override\n+          public void onSuccess(String funcName, QueryExecution qe, long durationNs) {\n+            executedPlanRef.set(qe.executedPlan());\n+          }\n+\n+          @Override\n+          public void onFailure(String funcName, QueryExecution qe, Exception exception) {}\n+        };\n+\n+    spark.listenerManager().register(listener);\n+\n+    action.invoke();\n+\n+    try {\n+      spark.sparkContext().listenerBus().waitUntilEmpty();\n+    } catch (TimeoutException e) {\n+      throw new RuntimeException(\"Timeout while waiting for processing events\", e);\n+    }\n+\n+    SparkPlan executedPlan = executedPlanRef.get();\n+    if (executedPlan instanceof AdaptiveSparkPlanExec) {\n+      return ((AdaptiveSparkPlanExec) executedPlan).executedPlan();\n+    } else {\n+      return executedPlan;\n+    }\n+  }\n+\n+  @FunctionalInterface\n+  protected interface Action {\n+    void invoke();\n+  }\n+}\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBaseWithCatalog.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBaseWithCatalog.java\nnew file mode 100644\nindex 000000000000..8d3e65ce7c9b\n--- /dev/null\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestBaseWithCatalog.java\n@@ -0,0 +1,195 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.spark;\n+\n+import static org.apache.iceberg.CatalogProperties.CATALOG_IMPL;\n+import static org.apache.iceberg.CatalogUtil.ICEBERG_CATALOG_TYPE;\n+import static org.apache.iceberg.CatalogUtil.ICEBERG_CATALOG_TYPE_HADOOP;\n+import static org.apache.iceberg.CatalogUtil.ICEBERG_CATALOG_TYPE_HIVE;\n+import static org.apache.iceberg.CatalogUtil.ICEBERG_CATALOG_TYPE_REST;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.util.Map;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.iceberg.CatalogProperties;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n+import org.apache.iceberg.PlanningMode;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.catalog.Catalog;\n+import org.apache.iceberg.catalog.Namespace;\n+import org.apache.iceberg.catalog.SupportsNamespaces;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.hadoop.HadoopCatalog;\n+import org.apache.iceberg.inmemory.InMemoryCatalog;\n+import org.apache.iceberg.rest.RESTCatalog;\n+import org.apache.iceberg.rest.RESTCatalogServer;\n+import org.apache.iceberg.rest.RESTServerExtension;\n+import org.apache.iceberg.util.PropertyUtil;\n+import org.junit.jupiter.api.AfterAll;\n+import org.junit.jupiter.api.BeforeAll;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.extension.RegisterExtension;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public abstract class TestBaseWithCatalog extends TestBase {\n+  protected static File warehouse = null;\n+\n+  @RegisterExtension\n+  private static final RESTServerExtension REST_SERVER_EXTENSION =\n+      new RESTServerExtension(\n+          Map.of(\n+              RESTCatalogServer.REST_PORT,\n+              RESTServerExtension.FREE_PORT,\n+              // In-memory sqlite database by default is private to the connection that created it.\n+              // If more than 1 jdbc connection backed by in-memory sqlite is created behind one\n+              // JdbcCatalog, then different jdbc connections could provide different views of table\n+              // status even belonging to the same catalog. Reference:\n+              // https://www.sqlite.org/inmemorydb.html\n+              CatalogProperties.CLIENT_POOL_SIZE,\n+              \"1\"));\n+\n+  protected static RESTCatalog restCatalog;\n+\n+  @Parameters(name = \"catalogName = {0}, implementation = {1}, config = {2}\")\n+  protected static Object[][] parameters() {\n+    return new Object[][] {\n+      {\n+        SparkCatalogConfig.HADOOP.catalogName(),\n+        SparkCatalogConfig.HADOOP.implementation(),\n+        SparkCatalogConfig.HADOOP.properties()\n+      },\n+    };\n+  }\n+\n+  @BeforeAll\n+  public static void setUpAll() throws IOException {\n+    TestBaseWithCatalog.warehouse = File.createTempFile(\"warehouse\", null);\n+    assertThat(warehouse.delete()).isTrue();\n+    restCatalog = REST_SERVER_EXTENSION.client();\n+  }\n+\n+  @AfterAll\n+  public static void tearDownAll() throws IOException {\n+    if (warehouse != null && warehouse.exists()) {\n+      Path warehousePath = new Path(warehouse.getAbsolutePath());\n+      FileSystem fs = warehousePath.getFileSystem(hiveConf);\n+      assertThat(fs.delete(warehousePath, true)).as(\"Failed to delete \" + warehousePath).isTrue();\n+    }\n+  }\n+\n+  @TempDir protected java.nio.file.Path temp;\n+\n+  @Parameter(index = 0)\n+  protected String catalogName;\n+\n+  @Parameter(index = 1)\n+  protected String implementation;\n+\n+  @Parameter(index = 2)\n+  protected Map<String, String> catalogConfig;\n+\n+  protected Catalog validationCatalog;\n+  protected SupportsNamespaces validationNamespaceCatalog;\n+  protected TableIdentifier tableIdent = TableIdentifier.of(Namespace.of(\"default\"), \"table\");\n+  protected String tableName;\n+\n+  @BeforeEach\n+  public void before() {\n+    configureValidationCatalog();\n+\n+    spark.conf().set(\"spark.sql.catalog.\" + catalogName, implementation);\n+    catalogConfig.forEach(\n+        (key, value) -> spark.conf().set(\"spark.sql.catalog.\" + catalogName + \".\" + key, value));\n+\n+    if (\"hadoop\".equalsIgnoreCase(catalogConfig.get(\"type\"))) {\n+      spark.conf().set(\"spark.sql.catalog.\" + catalogName + \".warehouse\", \"file:\" + warehouse);\n+    }\n+\n+    this.tableName =\n+        (catalogName.equals(\"spark_catalog\") ? \"\" : catalogName + \".\") + \"default.table\";\n+\n+    sql(\"CREATE NAMESPACE IF NOT EXISTS default\");\n+  }\n+\n+  protected String tableName(String name) {\n+    return (catalogName.equals(\"spark_catalog\") ? \"\" : catalogName + \".\") + \"default.\" + name;\n+  }\n+\n+  protected String commitTarget() {\n+    return tableName;\n+  }\n+\n+  protected String selectTarget() {\n+    return tableName;\n+  }\n+\n+  protected boolean cachingCatalogEnabled() {\n+    return PropertyUtil.propertyAsBoolean(\n+        catalogConfig, CatalogProperties.CACHE_ENABLED, CatalogProperties.CACHE_ENABLED_DEFAULT);\n+  }\n+\n+  protected void configurePlanningMode(PlanningMode planningMode) {\n+    configurePlanningMode(tableName, planningMode);\n+  }\n+\n+  protected void configurePlanningMode(String table, PlanningMode planningMode) {\n+    sql(\n+        \"ALTER TABLE %s SET TBLPROPERTIES ('%s' '%s', '%s' '%s')\",\n+        table,\n+        TableProperties.DATA_PLANNING_MODE,\n+        planningMode.modeName(),\n+        TableProperties.DELETE_PLANNING_MODE,\n+        planningMode.modeName());\n+  }\n+\n+  private void configureValidationCatalog() {\n+    if (catalogConfig.containsKey(ICEBERG_CATALOG_TYPE)) {\n+      switch (catalogConfig.get(ICEBERG_CATALOG_TYPE)) {\n+        case ICEBERG_CATALOG_TYPE_HADOOP:\n+          this.validationCatalog =\n+              new HadoopCatalog(spark.sessionState().newHadoopConf(), \"file:\" + warehouse);\n+          break;\n+        case ICEBERG_CATALOG_TYPE_REST:\n+          this.validationCatalog = restCatalog;\n+          break;\n+        case ICEBERG_CATALOG_TYPE_HIVE:\n+          this.validationCatalog = catalog;\n+          break;\n+        default:\n+          throw new IllegalArgumentException(\"Unknown catalog type\");\n+      }\n+    } else if (catalogConfig.containsKey(CATALOG_IMPL)) {\n+      switch (catalogConfig.get(CATALOG_IMPL)) {\n+        case \"org.apache.iceberg.inmemory.InMemoryCatalog\":\n+          this.validationCatalog = new InMemoryCatalog();\n+          break;\n+        default:\n+          throw new IllegalArgumentException(\"Unknown catalog impl\");\n+      }\n+    }\n+    this.validationNamespaceCatalog = (SupportsNamespaces) validationCatalog;\n+  }\n+}\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestFunctionCatalog.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestFunctionCatalog.java\nindex b51861e4a2b3..a146d5b7020a 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestFunctionCatalog.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/TestFunctionCatalog.java\n@@ -22,6 +22,7 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import org.apache.iceberg.IcebergBuild;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.spark.functions.IcebergVersionFunction;\n import org.apache.spark.sql.AnalysisException;\n@@ -30,33 +31,32 @@\n import org.apache.spark.sql.connector.catalog.FunctionCatalog;\n import org.apache.spark.sql.connector.catalog.Identifier;\n import org.apache.spark.sql.connector.catalog.functions.UnboundFunction;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestFunctionCatalog extends SparkTestBaseWithCatalog {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestFunctionCatalog extends TestBaseWithCatalog {\n   private static final String[] EMPTY_NAMESPACE = new String[] {};\n   private static final String[] SYSTEM_NAMESPACE = new String[] {\"system\"};\n   private static final String[] DEFAULT_NAMESPACE = new String[] {\"default\"};\n   private static final String[] DB_NAMESPACE = new String[] {\"db\"};\n-  private final FunctionCatalog asFunctionCatalog;\n+  private FunctionCatalog asFunctionCatalog;\n \n-  public TestFunctionCatalog() {\n-    this.asFunctionCatalog = castToFunctionCatalog(catalogName);\n-  }\n-\n-  @Before\n+  @BeforeEach\n   public void createDefaultNamespace() {\n+    super.before();\n+    this.asFunctionCatalog = castToFunctionCatalog(catalogName);\n     sql(\"CREATE NAMESPACE IF NOT EXISTS %s\", catalogName + \".default\");\n   }\n \n-  @After\n+  @AfterEach\n   public void dropDefaultNamespace() {\n     sql(\"DROP NAMESPACE IF EXISTS %s\", catalogName + \".default\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testListFunctionsViaCatalog() throws NoSuchNamespaceException {\n     assertThat(asFunctionCatalog.listFunctions(EMPTY_NAMESPACE))\n         .anyMatch(func -> \"iceberg_version\".equals(func.name()));\n@@ -64,17 +64,16 @@ public void testListFunctionsViaCatalog() throws NoSuchNamespaceException {\n     assertThat(asFunctionCatalog.listFunctions(SYSTEM_NAMESPACE))\n         .anyMatch(func -> \"iceberg_version\".equals(func.name()));\n \n-    Assert.assertArrayEquals(\n-        \"Listing functions in an existing namespace that's not system should not throw\",\n-        new Identifier[0],\n-        asFunctionCatalog.listFunctions(DEFAULT_NAMESPACE));\n+    assertThat(asFunctionCatalog.listFunctions(DEFAULT_NAMESPACE))\n+        .as(\"Listing functions in an existing namespace that's not system should not throw\")\n+        .isEqualTo(new Identifier[0]);\n \n     assertThatThrownBy(() -> asFunctionCatalog.listFunctions(DB_NAMESPACE))\n         .isInstanceOf(NoSuchNamespaceException.class)\n         .hasMessageStartingWith(\"[SCHEMA_NOT_FOUND] The schema `db` cannot be found.\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testLoadFunctions() throws NoSuchFunctionException {\n     for (String[] namespace : ImmutableList.of(EMPTY_NAMESPACE, SYSTEM_NAMESPACE)) {\n       Identifier identifier = Identifier.of(namespace, \"iceberg_version\");\n@@ -105,31 +104,31 @@ public void testLoadFunctions() throws NoSuchFunctionException {\n             \"[UNRESOLVED_ROUTINE] Cannot resolve function `undefined_function` on search path\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCallingFunctionInSQLEndToEnd() {\n     String buildVersion = IcebergBuild.version();\n \n-    Assert.assertEquals(\n-        \"Should be able to use the Iceberg version function from the fully qualified system namespace\",\n-        buildVersion,\n-        scalarSql(\"SELECT %s.system.iceberg_version()\", catalogName));\n+    assertThat(scalarSql(\"SELECT %s.system.iceberg_version()\", catalogName))\n+        .as(\n+            \"Should be able to use the Iceberg version function from the fully qualified system namespace\")\n+        .isEqualTo(buildVersion);\n \n-    Assert.assertEquals(\n-        \"Should be able to use the Iceberg version function when fully qualified without specifying a namespace\",\n-        buildVersion,\n-        scalarSql(\"SELECT %s.iceberg_version()\", catalogName));\n+    assertThat(scalarSql(\"SELECT %s.iceberg_version()\", catalogName))\n+        .as(\n+            \"Should be able to use the Iceberg version function when fully qualified without specifying a namespace\")\n+        .isEqualTo(buildVersion);\n \n     sql(\"USE %s\", catalogName);\n \n-    Assert.assertEquals(\n-        \"Should be able to call iceberg_version from system namespace without fully qualified name when using Iceberg catalog\",\n-        buildVersion,\n-        scalarSql(\"SELECT system.iceberg_version()\"));\n+    assertThat(scalarSql(\"SELECT system.iceberg_version()\"))\n+        .as(\n+            \"Should be able to call iceberg_version from system namespace without fully qualified name when using Iceberg catalog\")\n+        .isEqualTo(buildVersion);\n \n-    Assert.assertEquals(\n-        \"Should be able to call iceberg_version from empty namespace without fully qualified name when using Iceberg catalog\",\n-        buildVersion,\n-        scalarSql(\"SELECT iceberg_version()\"));\n+    assertThat(scalarSql(\"SELECT iceberg_version()\"))\n+        .as(\n+            \"Should be able to call iceberg_version from empty namespace without fully qualified name when using Iceberg catalog\")\n+        .isEqualTo(buildVersion);\n   }\n \n   private FunctionCatalog castToFunctionCatalog(String name) {\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPathIdentifier.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPathIdentifier.java\nindex e18c4d32412f..aa8fe047b3f7 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPathIdentifier.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestPathIdentifier.java\n@@ -23,6 +23,7 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.nio.file.Path;\n import org.apache.iceberg.BaseTable;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.hadoop.HadoopTableOperations;\n@@ -30,41 +31,38 @@\n import org.apache.iceberg.spark.PathIdentifier;\n import org.apache.iceberg.spark.SparkCatalog;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n-import org.apache.iceberg.spark.SparkTestBase;\n+import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n import org.apache.spark.sql.catalyst.analysis.TableAlreadyExistsException;\n import org.apache.spark.sql.connector.expressions.Transform;\n import org.apache.spark.sql.util.CaseInsensitiveStringMap;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Rule;\n-import org.junit.Test;\n-import org.junit.rules.TemporaryFolder;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.io.TempDir;\n \n-public class TestPathIdentifier extends SparkTestBase {\n+public class TestPathIdentifier extends TestBase {\n \n   private static final Schema SCHEMA =\n       new Schema(\n           required(1, \"id\", Types.LongType.get()), required(2, \"data\", Types.StringType.get()));\n \n-  @Rule public TemporaryFolder temp = new TemporaryFolder();\n+  @TempDir private Path temp;\n   private File tableLocation;\n   private PathIdentifier identifier;\n   private SparkCatalog sparkCatalog;\n \n-  @Before\n+  @BeforeEach\n   public void before() throws IOException {\n-    tableLocation = temp.newFolder();\n+    tableLocation = temp.toFile();\n     identifier = new PathIdentifier(tableLocation.getAbsolutePath());\n     sparkCatalog = new SparkCatalog();\n     sparkCatalog.initialize(\"test\", new CaseInsensitiveStringMap(ImmutableMap.of()));\n   }\n \n-  @After\n+  @AfterEach\n   public void after() {\n-    tableLocation.delete();\n     sparkCatalog = null;\n   }\n \n@@ -75,11 +73,11 @@ public void testPathIdentifier() throws TableAlreadyExistsException, NoSuchTable\n             sparkCatalog.createTable(\n                 identifier, SparkSchemaUtil.convert(SCHEMA), new Transform[0], ImmutableMap.of());\n \n-    Assert.assertEquals(table.table().location(), tableLocation.getAbsolutePath());\n+    assertThat(tableLocation.getAbsolutePath()).isEqualTo(table.table().location());\n     assertThat(table.table()).isInstanceOf(BaseTable.class);\n     assertThat(((BaseTable) table.table()).operations()).isInstanceOf(HadoopTableOperations.class);\n \n-    Assert.assertEquals(sparkCatalog.loadTable(identifier), table);\n-    Assert.assertTrue(sparkCatalog.dropTable(identifier));\n+    assertThat(table).isEqualTo(sparkCatalog.loadTable(identifier));\n+    assertThat(sparkCatalog.dropTable(identifier)).isTrue();\n   }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestAlterTable.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestAlterTable.java\nindex e5c6fcd0aa34..f70df75d91ae 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestAlterTable.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/sql/TestAlterTable.java\n@@ -24,40 +24,37 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n import static org.assertj.core.api.Assumptions.assumeThat;\n \n-import java.util.Map;\n+import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.catalog.TableIdentifier;\n import org.apache.iceberg.hadoop.HadoopCatalog;\n-import org.apache.iceberg.spark.SparkCatalogTestBase;\n+import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.types.Types.NestedField;\n import org.apache.spark.SparkException;\n import org.apache.spark.sql.AnalysisException;\n-import org.junit.After;\n-import org.junit.Assert;\n-import org.junit.Before;\n-import org.junit.Test;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n \n-public class TestAlterTable extends SparkCatalogTestBase {\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestAlterTable extends CatalogTestBase {\n   private final TableIdentifier renamedIdent =\n       TableIdentifier.of(Namespace.of(\"default\"), \"table2\");\n \n-  public TestAlterTable(String catalogName, String implementation, Map<String, String> config) {\n-    super(catalogName, implementation, config);\n-  }\n-\n-  @Before\n+  @BeforeEach\n   public void createTable() {\n     sql(\"CREATE TABLE %s (id bigint NOT NULL, data string) USING iceberg\", tableName);\n   }\n \n-  @After\n+  @AfterEach\n   public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName);\n     sql(\"DROP TABLE IF EXISTS %s2\", tableName);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddColumnNotNull() {\n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s ADD COLUMN c3 INT NOT NULL\", tableName))\n         .isInstanceOf(SparkException.class)\n@@ -65,7 +62,7 @@ public void testAddColumnNotNull() {\n             \"Unsupported table change: Incompatible change: cannot add required column: c3\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddColumn() {\n     sql(\n         \"ALTER TABLE %s ADD COLUMN point struct<x: double NOT NULL, y: double NOT NULL> AFTER id\",\n@@ -82,10 +79,9 @@ public void testAddColumn() {\n                     NestedField.required(5, \"y\", Types.DoubleType.get()))),\n             NestedField.optional(2, \"data\", Types.StringType.get()));\n \n-    Assert.assertEquals(\n-        \"Schema should match expected\",\n-        expectedSchema,\n-        validationCatalog.loadTable(tableIdent).schema().asStruct());\n+    assertThat(validationCatalog.loadTable(tableIdent).schema().asStruct())\n+        .as(\"Schema should match expected\")\n+        .isEqualTo(expectedSchema);\n \n     sql(\"ALTER TABLE %s ADD COLUMN point.z double COMMENT 'May be null' FIRST\", tableName);\n \n@@ -101,13 +97,12 @@ public void testAddColumn() {\n                     NestedField.required(5, \"y\", Types.DoubleType.get()))),\n             NestedField.optional(2, \"data\", Types.StringType.get()));\n \n-    Assert.assertEquals(\n-        \"Schema should match expected\",\n-        expectedSchema2,\n-        validationCatalog.loadTable(tableIdent).schema().asStruct());\n+    assertThat(validationCatalog.loadTable(tableIdent).schema().asStruct())\n+        .as(\"Schema should match expected\")\n+        .isEqualTo(expectedSchema2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddColumnWithArray() {\n     sql(\"ALTER TABLE %s ADD COLUMN data2 array<struct<a:INT,b:INT,c:int>>\", tableName);\n     // use the implicit column name 'element' to access member of array and add column d to struct.\n@@ -126,13 +121,12 @@ public void testAddColumnWithArray() {\n                         NestedField.optional(6, \"b\", Types.IntegerType.get()),\n                         NestedField.optional(7, \"c\", Types.IntegerType.get()),\n                         NestedField.optional(8, \"d\", Types.IntegerType.get())))));\n-    Assert.assertEquals(\n-        \"Schema should match expected\",\n-        expectedSchema,\n-        validationCatalog.loadTable(tableIdent).schema().asStruct());\n+    assertThat(validationCatalog.loadTable(tableIdent).schema().asStruct())\n+        .as(\"Schema should match expected\")\n+        .isEqualTo(expectedSchema);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAddColumnWithMap() {\n     sql(\"ALTER TABLE %s ADD COLUMN data2 map<struct<x:INT>, struct<a:INT,b:INT>>\", tableName);\n     // use the implicit column name 'key' and 'value' to access member of map.\n@@ -153,10 +147,9 @@ public void testAddColumnWithMap() {\n                         NestedField.optional(7, \"a\", Types.IntegerType.get()),\n                         NestedField.optional(8, \"b\", Types.IntegerType.get()),\n                         NestedField.optional(9, \"c\", Types.IntegerType.get())))));\n-    Assert.assertEquals(\n-        \"Schema should match expected\",\n-        expectedSchema,\n-        validationCatalog.loadTable(tableIdent).schema().asStruct());\n+    assertThat(validationCatalog.loadTable(tableIdent).schema().asStruct())\n+        .as(\"Schema should match expected\")\n+        .isEqualTo(expectedSchema);\n \n     // should not allow changing map key column\n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s ADD COLUMN data2.key.y int\", tableName))\n@@ -164,20 +157,19 @@ public void testAddColumnWithMap() {\n         .hasMessageStartingWith(\"Unsupported table change: Cannot add fields to map keys:\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testDropColumn() {\n     sql(\"ALTER TABLE %s DROP COLUMN data\", tableName);\n \n     Types.StructType expectedSchema =\n         Types.StructType.of(NestedField.required(1, \"id\", Types.LongType.get()));\n \n-    Assert.assertEquals(\n-        \"Schema should match expected\",\n-        expectedSchema,\n-        validationCatalog.loadTable(tableIdent).schema().asStruct());\n+    assertThat(validationCatalog.loadTable(tableIdent).schema().asStruct())\n+        .as(\"Schema should match expected\")\n+        .isEqualTo(expectedSchema);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRenameColumn() {\n     sql(\"ALTER TABLE %s RENAME COLUMN id TO row_id\", tableName);\n \n@@ -186,13 +178,12 @@ public void testRenameColumn() {\n             NestedField.required(1, \"row_id\", Types.LongType.get()),\n             NestedField.optional(2, \"data\", Types.StringType.get()));\n \n-    Assert.assertEquals(\n-        \"Schema should match expected\",\n-        expectedSchema,\n-        validationCatalog.loadTable(tableIdent).schema().asStruct());\n+    assertThat(validationCatalog.loadTable(tableIdent).schema().asStruct())\n+        .as(\"Schema should match expected\")\n+        .isEqualTo(expectedSchema);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAlterColumnComment() {\n     sql(\"ALTER TABLE %s ALTER COLUMN id COMMENT 'Record id'\", tableName);\n \n@@ -201,13 +192,12 @@ public void testAlterColumnComment() {\n             NestedField.required(1, \"id\", Types.LongType.get(), \"Record id\"),\n             NestedField.optional(2, \"data\", Types.StringType.get()));\n \n-    Assert.assertEquals(\n-        \"Schema should match expected\",\n-        expectedSchema,\n-        validationCatalog.loadTable(tableIdent).schema().asStruct());\n+    assertThat(validationCatalog.loadTable(tableIdent).schema().asStruct())\n+        .as(\"Schema should match expected\")\n+        .isEqualTo(expectedSchema);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAlterColumnType() {\n     sql(\"ALTER TABLE %s ADD COLUMN count int\", tableName);\n     sql(\"ALTER TABLE %s ALTER COLUMN count TYPE bigint\", tableName);\n@@ -218,13 +208,12 @@ public void testAlterColumnType() {\n             NestedField.optional(2, \"data\", Types.StringType.get()),\n             NestedField.optional(3, \"count\", Types.LongType.get()));\n \n-    Assert.assertEquals(\n-        \"Schema should match expected\",\n-        expectedSchema,\n-        validationCatalog.loadTable(tableIdent).schema().asStruct());\n+    assertThat(validationCatalog.loadTable(tableIdent).schema().asStruct())\n+        .as(\"Schema should match expected\")\n+        .isEqualTo(expectedSchema);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAlterColumnDropNotNull() {\n     sql(\"ALTER TABLE %s ALTER COLUMN id DROP NOT NULL\", tableName);\n \n@@ -233,13 +222,12 @@ public void testAlterColumnDropNotNull() {\n             NestedField.optional(1, \"id\", Types.LongType.get()),\n             NestedField.optional(2, \"data\", Types.StringType.get()));\n \n-    Assert.assertEquals(\n-        \"Schema should match expected\",\n-        expectedSchema,\n-        validationCatalog.loadTable(tableIdent).schema().asStruct());\n+    assertThat(validationCatalog.loadTable(tableIdent).schema().asStruct())\n+        .as(\"Schema should match expected\")\n+        .isEqualTo(expectedSchema);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAlterColumnSetNotNull() {\n     // no-op changes are allowed\n     sql(\"ALTER TABLE %s ALTER COLUMN id SET NOT NULL\", tableName);\n@@ -249,17 +237,16 @@ public void testAlterColumnSetNotNull() {\n             NestedField.required(1, \"id\", Types.LongType.get()),\n             NestedField.optional(2, \"data\", Types.StringType.get()));\n \n-    Assert.assertEquals(\n-        \"Schema should match expected\",\n-        expectedSchema,\n-        validationCatalog.loadTable(tableIdent).schema().asStruct());\n+    assertThat(validationCatalog.loadTable(tableIdent).schema().asStruct())\n+        .as(\"Schema should match expected\")\n+        .isEqualTo(expectedSchema);\n \n     assertThatThrownBy(() -> sql(\"ALTER TABLE %s ALTER COLUMN data SET NOT NULL\", tableName))\n         .isInstanceOf(AnalysisException.class)\n         .hasMessageStartingWith(\"Cannot change nullable column to non-nullable: data\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAlterColumnPositionAfter() {\n     sql(\"ALTER TABLE %s ADD COLUMN count int\", tableName);\n     sql(\"ALTER TABLE %s ALTER COLUMN count AFTER id\", tableName);\n@@ -270,13 +257,12 @@ public void testAlterColumnPositionAfter() {\n             NestedField.optional(3, \"count\", Types.IntegerType.get()),\n             NestedField.optional(2, \"data\", Types.StringType.get()));\n \n-    Assert.assertEquals(\n-        \"Schema should match expected\",\n-        expectedSchema,\n-        validationCatalog.loadTable(tableIdent).schema().asStruct());\n+    assertThat(validationCatalog.loadTable(tableIdent).schema().asStruct())\n+        .as(\"Schema should match expected\")\n+        .isEqualTo(expectedSchema);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAlterColumnPositionFirst() {\n     sql(\"ALTER TABLE %s ADD COLUMN count int\", tableName);\n     sql(\"ALTER TABLE %s ALTER COLUMN count FIRST\", tableName);\n@@ -287,13 +273,12 @@ public void testAlterColumnPositionFirst() {\n             NestedField.required(1, \"id\", Types.LongType.get()),\n             NestedField.optional(2, \"data\", Types.StringType.get()));\n \n-    Assert.assertEquals(\n-        \"Schema should match expected\",\n-        expectedSchema,\n-        validationCatalog.loadTable(tableIdent).schema().asStruct());\n+    assertThat(validationCatalog.loadTable(tableIdent).schema().asStruct())\n+        .as(\"Schema should match expected\")\n+        .isEqualTo(expectedSchema);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testTableRename() {\n     assumeThat(catalogConfig.get(ICEBERG_CATALOG_TYPE))\n         .as(\n@@ -303,28 +288,32 @@ public void testTableRename() {\n         .as(\"Hadoop catalog does not support rename\")\n         .isNotInstanceOf(HadoopCatalog.class);\n \n-    Assert.assertTrue(\"Initial name should exist\", validationCatalog.tableExists(tableIdent));\n-    Assert.assertFalse(\"New name should not exist\", validationCatalog.tableExists(renamedIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent)).as(\"Initial name should exist\").isTrue();\n+    assertThat(validationCatalog.tableExists(renamedIdent))\n+        .as(\"New name should not exist\")\n+        .isFalse();\n \n     sql(\"ALTER TABLE %s RENAME TO %s2\", tableName, tableName);\n \n-    Assert.assertFalse(\"Initial name should not exist\", validationCatalog.tableExists(tableIdent));\n-    Assert.assertTrue(\"New name should exist\", validationCatalog.tableExists(renamedIdent));\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Initial name should not exist\")\n+        .isFalse();\n+    assertThat(validationCatalog.tableExists(renamedIdent)).as(\"New name should exist\").isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSetTableProperties() {\n     sql(\"ALTER TABLE %s SET TBLPROPERTIES ('prop'='value')\", tableName);\n \n-    assertThat(validationCatalog.loadTable(tableIdent).properties().get(\"prop\"))\n+    assertThat(validationCatalog.loadTable(tableIdent).properties())\n         .as(\"Should have the new table property\")\n-        .isEqualTo(\"value\");\n+        .containsEntry(\"prop\", \"value\");\n \n     sql(\"ALTER TABLE %s UNSET TBLPROPERTIES ('prop')\", tableName);\n \n-    assertThat(validationCatalog.loadTable(tableIdent).properties().get(\"prop\"))\n+    assertThat(validationCatalog.loadTable(tableIdent).properties())\n         .as(\"Should not have the removed table property\")\n-        .isNull();\n+        .doesNotContainKey(\"prop\");\n \n     String[] reservedProperties = new String[] {\"sort-order\", \"identifier-fields\"};\n     for (String reservedProp : reservedProperties) {\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestBase.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestBase.java\nindex de68351f6e39..3e9f3334ef6e 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestBase.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestBase.java\n@@ -127,9 +127,9 @@ protected List<Object[]> sql(String query, Object... args) {\n \n   protected Object scalarSql(String query, Object... args) {\n     List<Object[]> rows = sql(query, args);\n-    assertThat(rows.size()).as(\"Scalar SQL should return one row\").isEqualTo(1);\n+    assertThat(rows).as(\"Scalar SQL should return one row\").hasSize(1);\n     Object[] row = Iterables.getOnlyElement(rows);\n-    assertThat(row.length).as(\"Scalar SQL should return one value\").isEqualTo(1);\n+    assertThat(row).as(\"Scalar SQL should return one value\").hasSize(1);\n     return row[0];\n   }\n \n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPathIdentifier.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPathIdentifier.java\nindex bb026b2ab2da..aa8fe047b3f7 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPathIdentifier.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPathIdentifier.java\n@@ -63,7 +63,6 @@ public void before() throws IOException {\n \n   @AfterEach\n   public void after() {\n-    tableLocation.delete();\n     sparkCatalog = null;\n   }\n \n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestAlterTable.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestAlterTable.java\nindex 5abc72606f9f..a3515f446526 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestAlterTable.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestAlterTable.java\n@@ -302,15 +302,15 @@ public void testTableRename() {\n   public void testSetTableProperties() {\n     sql(\"ALTER TABLE %s SET TBLPROPERTIES ('prop'='value')\", tableName);\n \n-    assertThat(validationCatalog.loadTable(tableIdent).properties().get(\"prop\"))\n+    assertThat(validationCatalog.loadTable(tableIdent).properties())\n         .as(\"Should have the new table property\")\n-        .isEqualTo(\"value\");\n+        .containsEntry(\"prop\", \"value\");\n \n     sql(\"ALTER TABLE %s UNSET TBLPROPERTIES ('prop')\", tableName);\n \n-    assertThat(validationCatalog.loadTable(tableIdent).properties().get(\"prop\"))\n+    assertThat(validationCatalog.loadTable(tableIdent).properties())\n         .as(\"Should not have the removed table property\")\n-        .isNull();\n+        .doesNotContainKey(\"prop\");\n \n     String[] reservedProperties = new String[] {\"sort-order\", \"identifier-fields\"};\n     for (String reservedProp : reservedProperties) {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12401",
    "pr_id": 12401,
    "issue_id": 12113,
    "repo": "apache/iceberg",
    "problem_statement": "ViewMetadataBuilder does not always retain changes in current build\n### Apache Iceberg version\n\nmain (development)\n\n### Query engine\n\nNone\n\n### Please describe the bug üêû\n\nThere is one edge case we missed in https://github.com/apache/iceberg/pull/12067 that might lead to a ViewVersion, added in the current build, not beeing retained:\n\n1. Add view version 1 and set current\n2. Set retention of versions to 2\n3. Add view versions 2 & 3\n4. Build\n\nThe builder should keep all versions (1, 2, 3) because 1 is the current version and must always be retained and neither 2 nor 3 may be discarded as they where added in this build.\n\nA second build should retain 1 (current version) and 3 (highest id of the remaining) and discard 2.\n\nAs discussed CC @nastra \n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 170,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/view/ViewMetadata.java",
      "core/src/test/java/org/apache/iceberg/view/TestViewMetadata.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/view/TestViewMetadata.java"
    ],
    "base_commit": "a50ec923f3d928f67e2a4a361c0d1162341aa084",
    "head_commit": "faebb828466364a0817c38337a86fa5ff7122cd9",
    "repo_url": "https://github.com/apache/iceberg/pull/12401",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12401",
    "dockerfile": "",
    "pr_merged_at": "2025-03-05T16:18:01.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/view/ViewMetadata.java b/core/src/main/java/org/apache/iceberg/view/ViewMetadata.java\nindex cedbd31fb920..05c6ef375085 100644\n--- a/core/src/main/java/org/apache/iceberg/view/ViewMetadata.java\n+++ b/core/src/main/java/org/apache/iceberg/view/ViewMetadata.java\n@@ -35,6 +35,7 @@\n import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n@@ -462,9 +463,18 @@ public ViewMetadata build() {\n           ViewProperties.VERSION_HISTORY_SIZE,\n           historySize);\n \n-      // expire old versions, but keep at least the versions added in this builder\n-      int numAddedVersions = (int) changes(MetadataUpdate.AddViewVersion.class).count();\n-      int numVersionsToKeep = Math.max(numAddedVersions, historySize);\n+      // expire old versions, but keep at least the versions added in this builder and the current\n+      // version\n+      int numVersions =\n+          ImmutableSet.builder()\n+              .addAll(\n+                  changes(MetadataUpdate.AddViewVersion.class)\n+                      .map(v -> v.viewVersion().versionId())\n+                      .collect(Collectors.toSet()))\n+              .add(currentVersionId)\n+              .build()\n+              .size();\n+      int numVersionsToKeep = Math.max(numVersions, historySize);\n \n       List<ViewVersion> retainedVersions;\n       List<ViewHistoryEntry> retainedHistory;\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/view/TestViewMetadata.java b/core/src/test/java/org/apache/iceberg/view/TestViewMetadata.java\nindex fc22a2d89144..3a713d061b0e 100644\n--- a/core/src/test/java/org/apache/iceberg/view/TestViewMetadata.java\n+++ b/core/src/test/java/org/apache/iceberg/view/TestViewMetadata.java\n@@ -396,6 +396,31 @@ public void viewVersionHistoryIsCorrectlyRetained() {\n         .hasMessage(\"Cannot set current version to unknown version: 1\");\n   }\n \n+  @Test\n+  public void versionsAddedInCurrentBuildAreRetained() {\n+    ViewVersion v1 = newViewVersion(1, \"select 1 as count\");\n+    ViewVersion v2 = newViewVersion(2, \"select count from t1\");\n+    ViewVersion v3 = newViewVersion(3, \"select count(1) as count from t2\");\n+\n+    ViewMetadata metadata =\n+        ViewMetadata.builder()\n+            .setLocation(\"location\")\n+            .addSchema(new Schema(Types.NestedField.required(1, \"x\", Types.LongType.get())))\n+            .addVersion(v1)\n+            .setCurrentVersionId(v1.versionId())\n+            .setProperties(ImmutableMap.of(ViewProperties.VERSION_HISTORY_SIZE, \"2\"))\n+            .build();\n+    assertThat(metadata.versions()).containsOnly(v1);\n+\n+    // make sure all currently added versions are retained\n+    ViewMetadata updated = ViewMetadata.buildFrom(metadata).addVersion(v2).addVersion(v3).build();\n+    assertThat(updated.versions()).containsExactlyInAnyOrder(v1, v2, v3);\n+\n+    // rebuild the metadata to expire older versions\n+    updated = ViewMetadata.buildFrom(updated).build();\n+    assertThat(updated.versions()).containsExactlyInAnyOrder(v1, v3);\n+  }\n+\n   @Test\n   public void viewMetadataAndMetadataChanges() {\n     Map<String, String> properties = ImmutableMap.of(\"key1\", \"prop1\", \"key2\", \"prop2\");\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12396",
    "pr_id": 12396,
    "issue_id": 11945,
    "repo": "apache/iceberg",
    "problem_statement": "Remove workaround for TimestampNTZType in TestHelpers\n### Apache Iceberg version\n\n1.7.1 (latest release)\n\n### Query engine\n\nSpark\n\n### Please describe the bug üêû\n\nOnce https://issues.apache.org/jira/browse/SPARK-50624 has been released, the workaround in https://github.com/apache/iceberg/blob/cd187c5718ba1eb0c8853d595eed82fa7230bc3d/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java#L787-L791 can be removed\n\n### Willingness to contribute\n\n- [X] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 103,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "gradle/libs.versions.toml",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java"
    ],
    "base_commit": "a50ec923f3d928f67e2a4a361c0d1162341aa084",
    "head_commit": "7b5096ca847f1142135796e361cfd7d9ed127869",
    "repo_url": "https://github.com/apache/iceberg/pull/12396",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12396",
    "dockerfile": "",
    "pr_merged_at": "2025-02-28T06:57:37.000Z",
    "patch": "diff --git a/gradle/libs.versions.toml b/gradle/libs.versions.toml\nindex 60206e1b3f01..553dc53dad9d 100644\n--- a/gradle/libs.versions.toml\n+++ b/gradle/libs.versions.toml\n@@ -79,7 +79,7 @@ scala-collection-compat = \"2.13.0\"\n slf4j = \"2.0.16\"\n snowflake-jdbc = \"3.22.0\"\n spark-hive34 = \"3.4.4\"\n-spark-hive35 = \"3.5.4\"\n+spark-hive35 = \"3.5.5\"\n sqlite-jdbc = \"3.49.1.0\"\n testcontainers = \"1.20.5\"\n tez08 = { strictly = \"0.8.4\"}  # see rich version usage explanation above\n",
    "test_patch": "diff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java\nindex a0e77e2acbae..9b496ea52c15 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/TestHelpers.java\n@@ -79,8 +79,6 @@\n import org.apache.spark.sql.types.MapType;\n import org.apache.spark.sql.types.StructField;\n import org.apache.spark.sql.types.StructType;\n-import org.apache.spark.sql.types.TimestampNTZType;\n-import org.apache.spark.sql.types.TimestampType$;\n import org.apache.spark.sql.vectorized.ColumnarBatch;\n import org.apache.spark.unsafe.types.UTF8String;\n import scala.collection.Seq;\n@@ -784,11 +782,6 @@ private static void assertEquals(\n     for (int i = 0; i < actual.numFields(); i += 1) {\n       StructField field = struct.fields()[i];\n       DataType type = field.dataType();\n-      // ColumnarRow.get doesn't support TimestampNTZType, causing tests to fail. the representation\n-      // is identical to TimestampType so this uses that type to validate.\n-      if (type instanceof TimestampNTZType) {\n-        type = TimestampType$.MODULE$;\n-      }\n \n       assertEquals(\n           context + \".\" + field.name(),\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12390",
    "pr_id": 12390,
    "issue_id": 12373,
    "repo": "apache/iceberg",
    "problem_statement": "Iceberg 1.8.0 Breaking oauth2 authentication\n### Apache Iceberg version\n\n1.8.0 (latest release)\n\n### Query engine\n\nNone\n\n### Please describe the bug üêû\n\nVersion 1.8.0 removes the trailing slash \"/\" for every URL.\nThis has a negative side effect when using Authentik for oauth2 authentication.\n\nthe Authentic token url is for example : https://authentik.domain.tld/application/o/token/\nand is changed to: https://authentik.domain.tld/application/o/token\n\nwithout the trailing slash, Authentication will return a http 405 \n\nwhy is Iceberg removing trailing slash? this may also break rother stuff?\nand maybe if not required, then revert this change and keep the url as-is\n\ncode is in class  org.apache.iceberg.rest.HTTPRequest and the problem is with the \"RESTUtil.stripTrailingSlash(fullPath)\" part.\n\n```\n  @Value.Lazy\n  default URI requestUri() {\n    // if full path is provided, use the input path as path\n    String fullPath =\n        (path().startsWith(\"https://\") || path().startsWith(\"http://\"))\n            ? path()\n            : String.format(\"%s/%s\", baseUri(), path());\n    try {\n      URIBuilder builder = new URIBuilder(RESTUtil.stripTrailingSlash(fullPath));\n      queryParameters().forEach(builder::addParameter);\n      return builder.build();\n    } catch (URISyntaxException e) {\n      throw new RESTException(\n          \"Failed to create request URI from base %s, params %s\", fullPath, queryParameters());\n    }\n  }\n```\n\n\n\n\n\n\n### Willingness to contribute\n\n- [x] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 231,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/rest/HTTPRequest.java",
      "core/src/test/java/org/apache/iceberg/rest/TestHTTPRequest.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/rest/TestHTTPRequest.java"
    ],
    "base_commit": "3c6bf24a86d005895a3fdb95d820a4019990fefb",
    "head_commit": "5507fe6dc0e2c91ddb3349eb2a5f97a0ffd9261c",
    "repo_url": "https://github.com/apache/iceberg/pull/12390",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12390",
    "dockerfile": "",
    "pr_merged_at": "2025-02-24T08:14:06.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/rest/HTTPRequest.java b/core/src/main/java/org/apache/iceberg/rest/HTTPRequest.java\nindex 41921d946ca8..ce4b65b23efb 100644\n--- a/core/src/main/java/org/apache/iceberg/rest/HTTPRequest.java\n+++ b/core/src/main/java/org/apache/iceberg/rest/HTTPRequest.java\n@@ -53,13 +53,17 @@ enum HTTPMethod {\n    */\n   @Value.Lazy\n   default URI requestUri() {\n-    // if full path is provided, use the input path as path\n-    String fullPath =\n-        (path().startsWith(\"https://\") || path().startsWith(\"http://\"))\n-            ? path()\n-            : String.format(\"%s/%s\", baseUri(), path());\n+    String fullPath;\n+    if (path().startsWith(\"https://\") || path().startsWith(\"http://\")) {\n+      // if path is an absolute URI, use it as is\n+      fullPath = path();\n+    } else {\n+      String baseUri = RESTUtil.stripTrailingSlash(baseUri().toString());\n+      fullPath = RESTUtil.stripTrailingSlash(String.format(\"%s/%s\", baseUri, path()));\n+    }\n+\n     try {\n-      URIBuilder builder = new URIBuilder(RESTUtil.stripTrailingSlash(fullPath));\n+      URIBuilder builder = new URIBuilder(fullPath);\n       queryParameters().forEach(builder::addParameter);\n       return builder.build();\n     } catch (URISyntaxException e) {\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/rest/TestHTTPRequest.java b/core/src/test/java/org/apache/iceberg/rest/TestHTTPRequest.java\nindex 84e1b0830c9b..87bc2506633d 100644\n--- a/core/src/test/java/org/apache/iceberg/rest/TestHTTPRequest.java\n+++ b/core/src/test/java/org/apache/iceberg/rest/TestHTTPRequest.java\n@@ -58,6 +58,17 @@ public static Stream<Arguments> validRequestUris() {\n                 .build(),\n             URI.create(\n                 \"http://localhost:8080/foo/v1/namespaces/ns/tables?pageToken=1234&pageSize=10\")),\n+        Arguments.of(\n+            ImmutableHTTPRequest.builder()\n+                .baseUri(\n+                    URI.create(\"http://localhost:8080/foo/\")) // trailing slash should be removed\n+                .method(HTTPRequest.HTTPMethod.GET)\n+                .path(\"v1/namespaces/ns/tables/\") // trailing slash should be removed\n+                .putQueryParameter(\"pageToken\", \"1234\")\n+                .putQueryParameter(\"pageSize\", \"10\")\n+                .build(),\n+            URI.create(\n+                \"http://localhost:8080/foo/v1/namespaces/ns/tables?pageToken=1234&pageSize=10\")),\n         Arguments.of(\n             ImmutableHTTPRequest.builder()\n                 .baseUri(URI.create(\"http://localhost:8080/foo\"))\n@@ -71,7 +82,15 @@ public static Stream<Arguments> validRequestUris() {\n                 .method(HTTPRequest.HTTPMethod.GET)\n                 .path(\"http://authserver.com/token\") // absolute path HTTP\n                 .build(),\n-            URI.create(\"http://authserver.com/token\")));\n+            URI.create(\"http://authserver.com/token\")),\n+        Arguments.of(\n+            ImmutableHTTPRequest.builder()\n+                .baseUri(URI.create(\"http://localhost:8080/foo\"))\n+                .method(HTTPRequest.HTTPMethod.GET)\n+                // absolute path with trailing slash: should be preserved\n+                .path(\"http://authserver.com/token/\")\n+                .build(),\n+            URI.create(\"http://authserver.com/token/\")));\n   }\n \n   @Test\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12389",
    "pr_id": 12389,
    "issue_id": 12373,
    "repo": "apache/iceberg",
    "problem_statement": "Iceberg 1.8.0 Breaking oauth2 authentication\n### Apache Iceberg version\n\n1.8.0 (latest release)\n\n### Query engine\n\nNone\n\n### Please describe the bug üêû\n\nVersion 1.8.0 removes the trailing slash \"/\" for every URL.\nThis has a negative side effect when using Authentik for oauth2 authentication.\n\nthe Authentic token url is for example : https://authentik.domain.tld/application/o/token/\nand is changed to: https://authentik.domain.tld/application/o/token\n\nwithout the trailing slash, Authentication will return a http 405 \n\nwhy is Iceberg removing trailing slash? this may also break rother stuff?\nand maybe if not required, then revert this change and keep the url as-is\n\ncode is in class  org.apache.iceberg.rest.HTTPRequest and the problem is with the \"RESTUtil.stripTrailingSlash(fullPath)\" part.\n\n```\n  @Value.Lazy\n  default URI requestUri() {\n    // if full path is provided, use the input path as path\n    String fullPath =\n        (path().startsWith(\"https://\") || path().startsWith(\"http://\"))\n            ? path()\n            : String.format(\"%s/%s\", baseUri(), path());\n    try {\n      URIBuilder builder = new URIBuilder(RESTUtil.stripTrailingSlash(fullPath));\n      queryParameters().forEach(builder::addParameter);\n      return builder.build();\n    } catch (URISyntaxException e) {\n      throw new RESTException(\n          \"Failed to create request URI from base %s, params %s\", fullPath, queryParameters());\n    }\n  }\n```\n\n\n\n\n\n\n### Willingness to contribute\n\n- [x] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 231,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/rest/HTTPRequest.java",
      "core/src/test/java/org/apache/iceberg/rest/TestHTTPRequest.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/rest/TestHTTPRequest.java"
    ],
    "base_commit": "dc1e0b25ad1be42a431f4991381b16bf572a459e",
    "head_commit": "e2308e7a7efce8a3c31cd017d50f475be85a2e6d",
    "repo_url": "https://github.com/apache/iceberg/pull/12389",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12389",
    "dockerfile": "",
    "pr_merged_at": "2025-02-24T08:03:59.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/rest/HTTPRequest.java b/core/src/main/java/org/apache/iceberg/rest/HTTPRequest.java\nindex 41921d946ca8..ce4b65b23efb 100644\n--- a/core/src/main/java/org/apache/iceberg/rest/HTTPRequest.java\n+++ b/core/src/main/java/org/apache/iceberg/rest/HTTPRequest.java\n@@ -53,13 +53,17 @@ enum HTTPMethod {\n    */\n   @Value.Lazy\n   default URI requestUri() {\n-    // if full path is provided, use the input path as path\n-    String fullPath =\n-        (path().startsWith(\"https://\") || path().startsWith(\"http://\"))\n-            ? path()\n-            : String.format(\"%s/%s\", baseUri(), path());\n+    String fullPath;\n+    if (path().startsWith(\"https://\") || path().startsWith(\"http://\")) {\n+      // if path is an absolute URI, use it as is\n+      fullPath = path();\n+    } else {\n+      String baseUri = RESTUtil.stripTrailingSlash(baseUri().toString());\n+      fullPath = RESTUtil.stripTrailingSlash(String.format(\"%s/%s\", baseUri, path()));\n+    }\n+\n     try {\n-      URIBuilder builder = new URIBuilder(RESTUtil.stripTrailingSlash(fullPath));\n+      URIBuilder builder = new URIBuilder(fullPath);\n       queryParameters().forEach(builder::addParameter);\n       return builder.build();\n     } catch (URISyntaxException e) {\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/rest/TestHTTPRequest.java b/core/src/test/java/org/apache/iceberg/rest/TestHTTPRequest.java\nindex 84e1b0830c9b..87bc2506633d 100644\n--- a/core/src/test/java/org/apache/iceberg/rest/TestHTTPRequest.java\n+++ b/core/src/test/java/org/apache/iceberg/rest/TestHTTPRequest.java\n@@ -58,6 +58,17 @@ public static Stream<Arguments> validRequestUris() {\n                 .build(),\n             URI.create(\n                 \"http://localhost:8080/foo/v1/namespaces/ns/tables?pageToken=1234&pageSize=10\")),\n+        Arguments.of(\n+            ImmutableHTTPRequest.builder()\n+                .baseUri(\n+                    URI.create(\"http://localhost:8080/foo/\")) // trailing slash should be removed\n+                .method(HTTPRequest.HTTPMethod.GET)\n+                .path(\"v1/namespaces/ns/tables/\") // trailing slash should be removed\n+                .putQueryParameter(\"pageToken\", \"1234\")\n+                .putQueryParameter(\"pageSize\", \"10\")\n+                .build(),\n+            URI.create(\n+                \"http://localhost:8080/foo/v1/namespaces/ns/tables?pageToken=1234&pageSize=10\")),\n         Arguments.of(\n             ImmutableHTTPRequest.builder()\n                 .baseUri(URI.create(\"http://localhost:8080/foo\"))\n@@ -71,7 +82,15 @@ public static Stream<Arguments> validRequestUris() {\n                 .method(HTTPRequest.HTTPMethod.GET)\n                 .path(\"http://authserver.com/token\") // absolute path HTTP\n                 .build(),\n-            URI.create(\"http://authserver.com/token\")));\n+            URI.create(\"http://authserver.com/token\")),\n+        Arguments.of(\n+            ImmutableHTTPRequest.builder()\n+                .baseUri(URI.create(\"http://localhost:8080/foo\"))\n+                .method(HTTPRequest.HTTPMethod.GET)\n+                // absolute path with trailing slash: should be preserved\n+                .path(\"http://authserver.com/token/\")\n+                .build(),\n+            URI.create(\"http://authserver.com/token/\")));\n   }\n \n   @Test\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12348",
    "pr_id": 12348,
    "issue_id": 10940,
    "repo": "apache/iceberg",
    "problem_statement": "Remove Hadoop 2\n### Feature Request / Improvement\n\nHadoop 2 does not support Java 11+ https://github.com/apache/iceberg/pull/10518#issuecomment-2285844068\r\n\r\n@steveloughran took a stab at it here: https://github.com/apache/iceberg/pull/10932#issuecomment-2286877905 but he's open for someone else to take it over.\n\n### Query engine\n\nHive\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [X] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 87,
    "test_files_count": 4,
    "non_test_files_count": 8,
    "pr_changed_files": [
      "aliyun/src/test/java/org/apache/iceberg/aliyun/oss/mock/AliyunOSSMockLocalStore.java",
      "build.gradle",
      "flink/v1.18/build.gradle",
      "flink/v1.19/build.gradle",
      "flink/v1.20/build.gradle",
      "gradle/libs.versions.toml",
      "mr/build.gradle",
      "spark/v3.4/build.gradle",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestCompressionSettings.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java",
      "spark/v3.5/build.gradle",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java"
    ],
    "pr_changed_test_files": [
      "aliyun/src/test/java/org/apache/iceberg/aliyun/oss/mock/AliyunOSSMockLocalStore.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestCompressionSettings.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java"
    ],
    "base_commit": "de644158a20c4a8d1b79fbc7773ae5a58dd316ac",
    "head_commit": "0464cdc2c1eddb9ed3fea32c99bffa3133bb7344",
    "repo_url": "https://github.com/apache/iceberg/pull/12348",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12348",
    "dockerfile": "",
    "pr_merged_at": "2025-02-26T11:53:33.000Z",
    "patch": "diff --git a/build.gradle b/build.gradle\nindex 099192abaf84..7fd4c5106568 100644\n--- a/build.gradle\n+++ b/build.gradle\n@@ -348,7 +348,7 @@ project(':iceberg-core') {\n     implementation libs.jackson.databind\n     implementation libs.caffeine\n     implementation libs.roaringbitmap\n-    compileOnly(libs.hadoop2.client) {\n+    compileOnly(libs.hadoop3.client) {\n       exclude group: 'org.apache.avro', module: 'avro'\n       exclude group: 'org.slf4j', module: 'slf4j-log4j12'\n     }\n@@ -373,7 +373,7 @@ project(':iceberg-data') {\n     implementation project(':iceberg-core')\n     compileOnly project(':iceberg-parquet')\n     compileOnly project(':iceberg-orc')\n-    compileOnly(libs.hadoop2.common) {\n+    compileOnly(libs.hadoop3.common) {\n       exclude group: 'commons-beanutils'\n       exclude group: 'org.apache.avro', module: 'avro'\n       exclude group: 'org.slf4j', module: 'slf4j-log4j12'\n@@ -396,7 +396,7 @@ project(':iceberg-data') {\n \n     compileOnly libs.avro.avro\n \n-    testImplementation(libs.hadoop2.client) {\n+    testImplementation(libs.hadoop3.client) {\n       exclude group: 'org.apache.avro', module: 'avro'\n       exclude group: 'org.slf4j', module: 'slf4j-log4j12'\n     }\n@@ -427,7 +427,7 @@ project(':iceberg-aliyun') {\n     compileOnly libs.jaxb.api\n     compileOnly libs.activation\n     compileOnly libs.jaxb.runtime\n-    compileOnly(libs.hadoop2.common) {\n+    compileOnly(libs.hadoop3.common) {\n       exclude group: 'org.apache.avro', module: 'avro'\n       exclude group: 'org.slf4j', module: 'slf4j-log4j12'\n       exclude group: 'javax.servlet', module: 'servlet-api'\n@@ -470,7 +470,7 @@ project(':iceberg-aws') {\n     compileOnly(\"software.amazon.awssdk:dynamodb\")\n     compileOnly(\"software.amazon.awssdk:lakeformation\")\n \n-    compileOnly(libs.hadoop2.common) {\n+    compileOnly(libs.hadoop3.common) {\n       exclude group: 'org.apache.avro', module: 'avro'\n       exclude group: 'org.slf4j', module: 'slf4j-log4j12'\n       exclude group: 'javax.servlet', module: 'servlet-api'\n@@ -572,7 +572,7 @@ project(':iceberg-delta-lake') {\n \n     compileOnly \"io.delta:delta-standalone_${scalaVersion}:${libs.versions.delta.standalone.get()}\"\n \n-    compileOnly(libs.hadoop2.common) {\n+    compileOnly(libs.hadoop3.common) {\n       exclude group: 'org.apache.avro', module: 'avro'\n       exclude group: 'org.slf4j', module: 'slf4j-log4j12'\n       exclude group: 'javax.servlet', module: 'servlet-api'\n@@ -584,7 +584,7 @@ project(':iceberg-delta-lake') {\n     if (sparkVersions.contains(\"3.5\")) {\n       integrationImplementation \"io.delta:delta-spark_${scalaVersion}:${libs.versions.delta.spark.get()}\"\n       integrationImplementation project(path: \":iceberg-spark:iceberg-spark-3.5_${scalaVersion}\")\n-      integrationImplementation(libs.hadoop2.minicluster) {\n+      integrationImplementation(libs.hadoop3.minicluster) {\n         exclude group: 'org.apache.avro', module: 'avro'\n         // to make sure netty libs only come from project(':iceberg-arrow')\n         exclude group: 'io.netty', module: 'netty-buffer'\n@@ -645,7 +645,7 @@ project(':iceberg-gcp') {\n     testImplementation project(path: ':iceberg-api', configuration: 'testArtifacts')\n     testImplementation project(path: ':iceberg-core', configuration: 'testArtifacts')\n \n-    testImplementation(libs.hadoop2.common) {\n+    testImplementation(libs.hadoop3.common) {\n       exclude group: 'org.apache.avro', module: 'avro'\n       exclude group: 'org.slf4j', module: 'slf4j-log4j12'\n       exclude group: 'javax.servlet', module: 'servlet-api'\n@@ -722,7 +722,7 @@ project(':iceberg-hive-metastore') {\n       exclude group: 'com.zaxxer', module: 'HikariCP'\n     }\n \n-    compileOnly(libs.hadoop2.client) {\n+    compileOnly(libs.hadoop3.client) {\n       exclude group: 'org.apache.avro', module: 'avro'\n       exclude group: 'org.slf4j', module: 'slf4j-log4j12'\n     }\n@@ -754,12 +754,12 @@ project(':iceberg-orc') {\n       exclude group: 'org.apache.hive', module: 'hive-storage-api'\n     }\n \n-    compileOnly(libs.hadoop2.common) {\n+    compileOnly(libs.hadoop3.common) {\n       exclude group: 'commons-beanutils'\n       exclude group: 'org.apache.avro', module: 'avro'\n       exclude group: 'org.slf4j', module: 'slf4j-log4j12'\n     }\n-    compileOnly(libs.hadoop2.client) {\n+    compileOnly(libs.hadoop3.client) {\n       exclude group: 'org.apache.avro', module: 'avro'\n     }\n \n@@ -788,7 +788,7 @@ project(':iceberg-parquet') {\n     }\n \n     compileOnly libs.avro.avro\n-    compileOnly(libs.hadoop2.client) {\n+    compileOnly(libs.hadoop3.client) {\n       exclude group: 'org.apache.avro', module: 'avro'\n     }\n \n@@ -832,8 +832,8 @@ project(':iceberg-arrow') {\n     // We import :netty-common through :arrow-memory-netty\n     // so that the same version as used by the :arrow-memory-netty module is picked.\n     testImplementation libs.arrow.memory.netty\n-    testImplementation libs.hadoop2.common\n-    testImplementation libs.hadoop2.mapreduce.client.core\n+    testImplementation libs.hadoop3.common\n+    testImplementation libs.hadoop3.mapreduce.client.core\n   }\n }\n \n@@ -854,7 +854,7 @@ project(':iceberg-nessie') {\n     implementation libs.jackson.core\n     implementation libs.jackson.databind\n \n-    compileOnly libs.hadoop2.common\n+    compileOnly libs.hadoop3.common\n     // Only there to prevent \"warning: unknown enum constant SchemaType.OBJECT\" compile messages\n     compileOnly libs.microprofile.openapi.api\n \n\ndiff --git a/flink/v1.18/build.gradle b/flink/v1.18/build.gradle\nindex 83dc07523a3c..d6e27514cb4a 100644\n--- a/flink/v1.18/build.gradle\n+++ b/flink/v1.18/build.gradle\n@@ -42,9 +42,9 @@ project(\":iceberg-flink:iceberg-flink-${flinkMajorVersion}\") {\n     compileOnly libs.flink118.connector.base\n     compileOnly libs.flink118.connector.files\n \n-    compileOnly libs.hadoop2.hdfs\n-    compileOnly libs.hadoop2.common\n-    compileOnly(libs.hadoop2.minicluster) {\n+    compileOnly libs.hadoop3.hdfs\n+    compileOnly libs.hadoop3.common\n+    compileOnly(libs.hadoop3.minicluster) {\n       exclude group: 'org.apache.avro', module: 'avro'\n     }\n \n@@ -186,9 +186,9 @@ project(\":iceberg-flink:iceberg-flink-runtime-${flinkMajorVersion}\") {\n     integrationImplementation libs.flink118.table.api.java.bridge\n     integrationImplementation \"org.apache.flink:flink-table-planner_${scalaVersion}:${libs.versions.flink118.get()}\"\n \n-    integrationImplementation libs.hadoop2.common\n-    integrationImplementation libs.hadoop2.hdfs\n-    integrationImplementation(libs.hadoop2.minicluster) {\n+    integrationImplementation libs.hadoop3.common\n+    integrationImplementation libs.hadoop3.hdfs\n+    integrationImplementation(libs.hadoop3.minicluster) {\n       exclude group: 'org.apache.avro', module: 'avro'\n     }\n \n\ndiff --git a/flink/v1.19/build.gradle b/flink/v1.19/build.gradle\nindex 50bcadb618e4..599ba085e4c4 100644\n--- a/flink/v1.19/build.gradle\n+++ b/flink/v1.19/build.gradle\n@@ -42,9 +42,9 @@ project(\":iceberg-flink:iceberg-flink-${flinkMajorVersion}\") {\n     compileOnly libs.flink119.connector.base\n     compileOnly libs.flink119.connector.files\n \n-    compileOnly libs.hadoop2.hdfs\n-    compileOnly libs.hadoop2.common\n-    compileOnly(libs.hadoop2.minicluster) {\n+    compileOnly libs.hadoop3.hdfs\n+    compileOnly libs.hadoop3.common\n+    compileOnly(libs.hadoop3.minicluster) {\n       exclude group: 'org.apache.avro', module: 'avro'\n     }\n \n@@ -187,9 +187,9 @@ project(\":iceberg-flink:iceberg-flink-runtime-${flinkMajorVersion}\") {\n     integrationImplementation libs.flink119.table.api.java.bridge\n     integrationImplementation \"org.apache.flink:flink-table-planner_${scalaVersion}:${libs.versions.flink119.get()}\"\n \n-    integrationImplementation libs.hadoop2.common\n-    integrationImplementation libs.hadoop2.hdfs\n-    integrationImplementation(libs.hadoop2.minicluster) {\n+    integrationImplementation libs.hadoop3.common\n+    integrationImplementation libs.hadoop3.hdfs\n+    integrationImplementation(libs.hadoop3.minicluster) {\n       exclude group: 'org.apache.avro', module: 'avro'\n     }\n \n\ndiff --git a/flink/v1.20/build.gradle b/flink/v1.20/build.gradle\nindex 4a1bae660bdb..3e308d22b021 100644\n--- a/flink/v1.20/build.gradle\n+++ b/flink/v1.20/build.gradle\n@@ -42,9 +42,9 @@ project(\":iceberg-flink:iceberg-flink-${flinkMajorVersion}\") {\n     compileOnly libs.flink120.connector.base\n     compileOnly libs.flink120.connector.files\n \n-    compileOnly libs.hadoop2.hdfs\n-    compileOnly libs.hadoop2.common\n-    compileOnly(libs.hadoop2.minicluster) {\n+    compileOnly libs.hadoop3.hdfs\n+    compileOnly libs.hadoop3.common\n+    compileOnly(libs.hadoop3.minicluster) {\n       exclude group: 'org.apache.avro', module: 'avro'\n     }\n \n@@ -187,9 +187,9 @@ project(\":iceberg-flink:iceberg-flink-runtime-${flinkMajorVersion}\") {\n     integrationImplementation libs.flink120.table.api.java.bridge\n     integrationImplementation \"org.apache.flink:flink-table-planner_${scalaVersion}:${libs.versions.flink120.get()}\"\n \n-    integrationImplementation libs.hadoop2.common\n-    integrationImplementation libs.hadoop2.hdfs\n-    integrationImplementation(libs.hadoop2.minicluster) {\n+    integrationImplementation libs.hadoop3.common\n+    integrationImplementation libs.hadoop3.hdfs\n+    integrationImplementation(libs.hadoop3.minicluster) {\n       exclude group: 'org.apache.avro', module: 'avro'\n     }\n \n\ndiff --git a/gradle/libs.versions.toml b/gradle/libs.versions.toml\nindex 5cb71c6cfba3..04c7bf649297 100644\n--- a/gradle/libs.versions.toml\n+++ b/gradle/libs.versions.toml\n@@ -46,7 +46,6 @@ flink119 =  { strictly = \"1.19.1\"}\n flink120 = { strictly = \"1.20.0\"}\n google-libraries-bom = \"26.54.0\"\n guava = \"33.4.0-jre\"\n-hadoop2 = \"2.7.3\"\n hadoop3 = \"3.4.1\"\n httpcomponents-httpclient5 = \"5.4.2\"\n hive2 = { strictly = \"2.3.10\"} # see rich version usage explanation above\n@@ -124,13 +123,11 @@ flink120-streaming-java = { module = \"org.apache.flink:flink-streaming-java\", ve\n flink120-table-api-java-bridge = { module = \"org.apache.flink:flink-table-api-java-bridge\", version.ref = \"flink120\" }\n google-libraries-bom = { module = \"com.google.cloud:libraries-bom\", version.ref = \"google-libraries-bom\" }\n guava-guava = { module = \"com.google.guava:guava\", version.ref = \"guava\" }\n-hadoop2-client = { module = \"org.apache.hadoop:hadoop-client\", version.ref = \"hadoop2\" }\n-hadoop2-common = { module = \"org.apache.hadoop:hadoop-common\", version.ref = \"hadoop2\" }\n-hadoop2-hdfs = { module = \"org.apache.hadoop:hadoop-hdfs\", version.ref = \"hadoop2\" }\n-hadoop2-mapreduce-client-core = { module = \"org.apache.hadoop:hadoop-mapreduce-client-core\", version.ref = \"hadoop2\" }\n-hadoop2-minicluster = { module = \"org.apache.hadoop:hadoop-minicluster\", version.ref = \"hadoop2\" }\n hadoop3-client = { module = \"org.apache.hadoop:hadoop-client\", version.ref = \"hadoop3\" }\n hadoop3-common = { module = \"org.apache.hadoop:hadoop-common\", version.ref = \"hadoop3\" }\n+hadoop3-hdfs = { module = \"org.apache.hadoop:hadoop-hdfs\", version.ref = \"hadoop3\" }\n+hadoop3-mapreduce-client-core = { module = \"org.apache.hadoop:hadoop-mapreduce-client-core\", version.ref = \"hadoop3\" }\n+hadoop3-minicluster = { module = \"org.apache.hadoop:hadoop-minicluster\", version.ref = \"hadoop3\" }\n hive2-exec = { module = \"org.apache.hive:hive-exec\", version.ref = \"hive2\" }\n hive2-metastore = { module = \"org.apache.hive:hive-metastore\", version.ref = \"hive2\" }\n hive2-service = { module = \"org.apache.hive:hive-service\", version.ref = \"hive2\" }\n\ndiff --git a/mr/build.gradle b/mr/build.gradle\nindex f41c5410a93f..dac7e3d4542b 100644\n--- a/mr/build.gradle\n+++ b/mr/build.gradle\n@@ -37,7 +37,7 @@ project(':iceberg-mr') {\n     implementation project(':iceberg-orc')\n     implementation project(':iceberg-parquet')\n \n-    compileOnly(libs.hadoop2.client) {\n+    compileOnly(libs.hadoop3.client) {\n       exclude group: 'org.apache.avro', module: 'avro'\n     }\n \n\ndiff --git a/spark/v3.4/build.gradle b/spark/v3.4/build.gradle\nindex 6eb26e8b73c1..e9f310ef7b36 100644\n--- a/spark/v3.4/build.gradle\n+++ b/spark/v3.4/build.gradle\n@@ -96,7 +96,7 @@ project(\":iceberg-spark:iceberg-spark-${sparkMajorVersion}_${scalaVersion}\") {\n \n     implementation libs.caffeine\n \n-    testImplementation(libs.hadoop2.minicluster) {\n+    testImplementation(libs.hadoop3.minicluster) {\n       exclude group: 'org.apache.avro', module: 'avro'\n       // to make sure netty libs only come from project(':iceberg-arrow')\n       exclude group: 'io.netty', module: 'netty-buffer'\n\ndiff --git a/spark/v3.5/build.gradle b/spark/v3.5/build.gradle\nindex e2d2c7a7ac07..52d9ce0348f6 100644\n--- a/spark/v3.5/build.gradle\n+++ b/spark/v3.5/build.gradle\n@@ -96,7 +96,7 @@ project(\":iceberg-spark:iceberg-spark-${sparkMajorVersion}_${scalaVersion}\") {\n \n     implementation libs.caffeine\n \n-    testImplementation(libs.hadoop2.minicluster) {\n+    testImplementation(libs.hadoop3.minicluster) {\n       exclude group: 'org.apache.avro', module: 'avro'\n       // to make sure netty libs only come from project(':iceberg-arrow')\n       exclude group: 'io.netty', module: 'netty-buffer'\n",
    "test_patch": "diff --git a/aliyun/src/test/java/org/apache/iceberg/aliyun/oss/mock/AliyunOSSMockLocalStore.java b/aliyun/src/test/java/org/apache/iceberg/aliyun/oss/mock/AliyunOSSMockLocalStore.java\nindex 521b87e31e80..f49697522dca 100644\n--- a/aliyun/src/test/java/org/apache/iceberg/aliyun/oss/mock/AliyunOSSMockLocalStore.java\n+++ b/aliyun/src/test/java/org/apache/iceberg/aliyun/oss/mock/AliyunOSSMockLocalStore.java\n@@ -37,10 +37,9 @@\n import java.security.NoSuchAlgorithmException;\n import java.util.Comparator;\n import java.util.List;\n-import java.util.Locale;\n import java.util.Map;\n import java.util.stream.Stream;\n-import org.apache.directory.api.util.Hex;\n+import org.apache.commons.codec.binary.Hex;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.io.ByteStreams;\n@@ -87,7 +86,7 @@ static String md5sum(InputStream is) throws IOException {\n     while ((numBytes = is.read(bytes)) != -1) {\n       md.update(bytes, 0, numBytes);\n     }\n-    return new String(Hex.encodeHex(md.digest())).toUpperCase(Locale.ROOT);\n+    return Hex.encodeHexString(md.digest(), false);\n   }\n \n   private static void inputStreamToFile(InputStream inputStream, File targetFile)\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestCompressionSettings.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestCompressionSettings.java\nindex 724c6edde26a..26cb4dcc9508 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestCompressionSettings.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestCompressionSettings.java\n@@ -70,6 +70,7 @@\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.SparkSession;\n import org.junit.AfterClass;\n+import org.junit.Before;\n import org.junit.BeforeClass;\n import org.junit.Rule;\n import org.junit.Test;\n@@ -106,6 +107,13 @@ public static void startSpark() {\n     TestCompressionSettings.spark = SparkSession.builder().master(\"local[2]\").getOrCreate();\n   }\n \n+  @Before\n+  public void resetSpecificConfigurations() {\n+    spark.conf().unset(COMPRESSION_CODEC);\n+    spark.conf().unset(COMPRESSION_LEVEL);\n+    spark.conf().unset(COMPRESSION_STRATEGY);\n+  }\n+\n   @Parameterized.AfterParam\n   public static void clearSourceCache() {\n     spark.sql(String.format(\"DROP TABLE IF EXISTS %s\", TABLE_NAME));\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java\nindex f420f1b955c0..961d69b72127 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java\n@@ -22,6 +22,8 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.io.File;\n+import java.nio.file.Files;\n+import java.nio.file.Paths;\n import java.util.List;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.PartitionSpec;\n@@ -118,6 +120,7 @@ public void testStreamingWriteAppendMode() throws Exception {\n       // remove the last commit to force Spark to reprocess batch #1\n       File lastCommitFile = new File(checkpoint + \"/commits/1\");\n       Assert.assertTrue(\"The commit file must be deleted\", lastCommitFile.delete());\n+      Files.deleteIfExists(Paths.get(checkpoint + \"/commits/.1.crc\"));\n \n       // restart the query from the checkpoint\n       StreamingQuery restartedQuery = streamWriter.start();\n@@ -178,6 +181,7 @@ public void testStreamingWriteCompleteMode() throws Exception {\n       // remove the last commit to force Spark to reprocess batch #1\n       File lastCommitFile = new File(checkpoint + \"/commits/1\");\n       Assert.assertTrue(\"The commit file must be deleted\", lastCommitFile.delete());\n+      Files.deleteIfExists(Paths.get(checkpoint + \"/commits/.1.crc\"));\n \n       // restart the query from the checkpoint\n       StreamingQuery restartedQuery = streamWriter.start();\n@@ -238,6 +242,7 @@ public void testStreamingWriteCompleteModeWithProjection() throws Exception {\n       // remove the last commit to force Spark to reprocess batch #1\n       File lastCommitFile = new File(checkpoint + \"/commits/1\");\n       Assert.assertTrue(\"The commit file must be deleted\", lastCommitFile.delete());\n+      Files.deleteIfExists(Paths.get(checkpoint + \"/commits/.1.crc\"));\n \n       // restart the query from the checkpoint\n       StreamingQuery restartedQuery = streamWriter.start();\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java\nindex 17db46b85c35..c84a65cbe951 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestStructuredStreaming.java\n@@ -23,7 +23,9 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.io.File;\n+import java.nio.file.Files;\n import java.nio.file.Path;\n+import java.nio.file.Paths;\n import java.util.List;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.PartitionSpec;\n@@ -117,6 +119,7 @@ public void testStreamingWriteAppendMode() throws Exception {\n       // remove the last commit to force Spark to reprocess batch #1\n       File lastCommitFile = new File(checkpoint + \"/commits/1\");\n       assertThat(lastCommitFile.delete()).as(\"The commit file must be deleted\").isTrue();\n+      Files.deleteIfExists(Paths.get(checkpoint + \"/commits/.1.crc\"));\n \n       // restart the query from the checkpoint\n       StreamingQuery restartedQuery = streamWriter.start();\n@@ -178,6 +181,7 @@ public void testStreamingWriteCompleteMode() throws Exception {\n       // remove the last commit to force Spark to reprocess batch #1\n       File lastCommitFile = new File(checkpoint + \"/commits/1\");\n       assertThat(lastCommitFile.delete()).as(\"The commit file must be deleted\").isTrue();\n+      Files.deleteIfExists(Paths.get(checkpoint + \"/commits/.1.crc\"));\n \n       // restart the query from the checkpoint\n       StreamingQuery restartedQuery = streamWriter.start();\n@@ -239,6 +243,7 @@ public void testStreamingWriteCompleteModeWithProjection() throws Exception {\n       // remove the last commit to force Spark to reprocess batch #1\n       File lastCommitFile = new File(checkpoint + \"/commits/1\");\n       assertThat(lastCommitFile.delete()).as(\"The commit file must be deleted\").isTrue();\n+      Files.deleteIfExists(Paths.get(checkpoint + \"/commits/.1.crc\"));\n \n       // restart the query from the checkpoint\n       StreamingQuery restartedQuery = streamWriter.start();\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12335",
    "pr_id": 12335,
    "issue_id": 12310,
    "repo": "apache/iceberg",
    "problem_statement": "Serialize `null` for `current-snapshot-id` when there is no current snapshot for ‚â•V3\n### Apache Iceberg version\n\nhttps://lists.apache.org/thread/gqqsnww6nqc50pddwn29blzghmb0m0h3\n\n### Query engine\n\nNone\n\n### Please describe the bug üêû\n\nActually reinstating https://github.com/apache/iceberg/pull/11560 for 2.0\n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 82,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/TableMetadataParser.java",
      "core/src/test/java/org/apache/iceberg/rest/responses/TestLoadTableResponseParser.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/rest/responses/TestLoadTableResponseParser.java"
    ],
    "base_commit": "25e7897b1161d5548ff428e0ec9016d5934635a1",
    "head_commit": "8915f380bb1fc2ce62f975e89a57492582445f03",
    "repo_url": "https://github.com/apache/iceberg/pull/12335",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12335",
    "dockerfile": "",
    "pr_merged_at": "2025-03-06T07:20:34.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/TableMetadataParser.java b/core/src/main/java/org/apache/iceberg/TableMetadataParser.java\nindex 4ac0364a15f2..5dfa715cd016 100644\n--- a/core/src/main/java/org/apache/iceberg/TableMetadataParser.java\n+++ b/core/src/main/java/org/apache/iceberg/TableMetadataParser.java\n@@ -112,6 +112,7 @@ private TableMetadataParser() {}\n   static final String PARTITION_STATISTICS = \"partition-statistics\";\n   static final String ROW_LINEAGE = \"row-lineage\";\n   static final String NEXT_ROW_ID = \"next-row-id\";\n+  static final int MIN_NULL_CURRENT_SNAPSHOT_VERSION = 3;\n \n   public static void overwrite(TableMetadata metadata, OutputFile outputFile) {\n     internalWrite(metadata, outputFile, true);\n@@ -216,9 +217,15 @@ public static void toJson(TableMetadata metadata, JsonGenerator generator) throw\n     // write properties map\n     JsonUtil.writeStringMap(PROPERTIES, metadata.properties(), generator);\n \n-    generator.writeNumberField(\n-        CURRENT_SNAPSHOT_ID,\n-        metadata.currentSnapshot() != null ? metadata.currentSnapshot().snapshotId() : -1);\n+    if (metadata.currentSnapshot() != null) {\n+      generator.writeNumberField(CURRENT_SNAPSHOT_ID, metadata.currentSnapshot().snapshotId());\n+    } else {\n+      if (metadata.formatVersion() >= MIN_NULL_CURRENT_SNAPSHOT_VERSION) {\n+        generator.writeNullField(CURRENT_SNAPSHOT_ID);\n+      } else {\n+        generator.writeNumberField(CURRENT_SNAPSHOT_ID, -1L);\n+      }\n+    }\n \n     if (metadata.rowLineageEnabled()) {\n       generator.writeBooleanField(ROW_LINEAGE, metadata.rowLineageEnabled());\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/rest/responses/TestLoadTableResponseParser.java b/core/src/test/java/org/apache/iceberg/rest/responses/TestLoadTableResponseParser.java\nindex cc6f4cfc74d7..2f3c4380f8ac 100644\n--- a/core/src/test/java/org/apache/iceberg/rest/responses/TestLoadTableResponseParser.java\n+++ b/core/src/test/java/org/apache/iceberg/rest/responses/TestLoadTableResponseParser.java\n@@ -18,6 +18,7 @@\n  */\n package org.apache.iceberg.rest.responses;\n \n+import static org.apache.iceberg.TestHelpers.MAX_FORMAT_VERSION;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n@@ -30,6 +31,8 @@\n import org.apache.iceberg.rest.credentials.ImmutableCredential;\n import org.apache.iceberg.types.Types;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.ValueSource;\n \n public class TestLoadTableResponseParser {\n \n@@ -57,10 +60,10 @@ public void missingFields() {\n   }\n \n   @Test\n-  public void roundTripSerde() {\n+  public void roundTripSerdeV1() {\n     String uuid = \"386b9f01-002b-4d8c-b77f-42c3fd3b7c9b\";\n     TableMetadata metadata =\n-        TableMetadata.buildFromEmpty()\n+        TableMetadata.buildFromEmpty(1)\n             .assignUUID(uuid)\n             .setLocation(\"location\")\n             .setCurrentSchema(\n@@ -78,12 +81,21 @@ public void roundTripSerde() {\n             \"{\\n\"\n                 + \"  \\\"metadata-location\\\" : \\\"metadata-location\\\",\\n\"\n                 + \"  \\\"metadata\\\" : {\\n\"\n-                + \"    \\\"format-version\\\" : 2,\\n\"\n+                + \"    \\\"format-version\\\" : 1,\\n\"\n                 + \"    \\\"table-uuid\\\" : \\\"386b9f01-002b-4d8c-b77f-42c3fd3b7c9b\\\",\\n\"\n                 + \"    \\\"location\\\" : \\\"location\\\",\\n\"\n-                + \"    \\\"last-sequence-number\\\" : 0,\\n\"\n-                + \"    \\\"last-updated-ms\\\" : %d,\\n\"\n+                + \"    \\\"last-updated-ms\\\" : %s,\\n\"\n                 + \"    \\\"last-column-id\\\" : 1,\\n\"\n+                + \"    \\\"schema\\\" : {\\n\"\n+                + \"      \\\"type\\\" : \\\"struct\\\",\\n\"\n+                + \"      \\\"schema-id\\\" : 0,\\n\"\n+                + \"      \\\"fields\\\" : [ {\\n\"\n+                + \"        \\\"id\\\" : 1,\\n\"\n+                + \"        \\\"name\\\" : \\\"x\\\",\\n\"\n+                + \"        \\\"required\\\" : true,\\n\"\n+                + \"        \\\"type\\\" : \\\"long\\\"\\n\"\n+                + \"      } ]\\n\"\n+                + \"    },\\n\"\n                 + \"    \\\"current-schema-id\\\" : 0,\\n\"\n                 + \"    \\\"schemas\\\" : [ {\\n\"\n                 + \"      \\\"type\\\" : \\\"struct\\\",\\n\"\n@@ -95,6 +107,7 @@ public void roundTripSerde() {\n                 + \"        \\\"type\\\" : \\\"long\\\"\\n\"\n                 + \"      } ]\\n\"\n                 + \"    } ],\\n\"\n+                + \"    \\\"partition-spec\\\" : [ ],\\n\"\n                 + \"    \\\"default-spec-id\\\" : 0,\\n\"\n                 + \"    \\\"partition-specs\\\" : [ {\\n\"\n                 + \"      \\\"spec-id\\\" : 0,\\n\"\n@@ -117,6 +130,75 @@ public void roundTripSerde() {\n                 + \"  }\\n\"\n                 + \"}\",\n             metadata.lastUpdatedMillis());\n+    String json = LoadTableResponseParser.toJson(response, true);\n+    assertThat(json).isEqualTo(expectedJson);\n+    // can't do an equality comparison because Schema doesn't implement equals/hashCode\n+    assertThat(LoadTableResponseParser.toJson(LoadTableResponseParser.fromJson(json), true))\n+        .isEqualTo(expectedJson);\n+  }\n+\n+  @ParameterizedTest\n+  @ValueSource(ints = {2, MAX_FORMAT_VERSION})\n+  public void roundTripSerdeV2andHigher(int formatVersion) {\n+    String uuid = \"386b9f01-002b-4d8c-b77f-42c3fd3b7c9b\";\n+    TableMetadata metadata =\n+        TableMetadata.buildFromEmpty(formatVersion)\n+            .assignUUID(uuid)\n+            .setLocation(\"location\")\n+            .setCurrentSchema(\n+                new Schema(Types.NestedField.required(1, \"x\", Types.LongType.get())), 1)\n+            .addPartitionSpec(PartitionSpec.unpartitioned())\n+            .addSortOrder(SortOrder.unsorted())\n+            .discardChanges()\n+            .withMetadataLocation(\"metadata-location\")\n+            .build();\n+\n+    LoadTableResponse response = LoadTableResponse.builder().withTableMetadata(metadata).build();\n+\n+    String expectedJson =\n+        String.format(\n+            \"{\\n\"\n+                + \"  \\\"metadata-location\\\" : \\\"metadata-location\\\",\\n\"\n+                + \"  \\\"metadata\\\" : {\\n\"\n+                + \"    \\\"format-version\\\" : %s,\\n\"\n+                + \"    \\\"table-uuid\\\" : \\\"386b9f01-002b-4d8c-b77f-42c3fd3b7c9b\\\",\\n\"\n+                + \"    \\\"location\\\" : \\\"location\\\",\\n\"\n+                + \"    \\\"last-sequence-number\\\" : 0,\\n\"\n+                + \"    \\\"last-updated-ms\\\" : %d,\\n\"\n+                + \"    \\\"last-column-id\\\" : 1,\\n\"\n+                + \"    \\\"current-schema-id\\\" : 0,\\n\"\n+                + \"    \\\"schemas\\\" : [ {\\n\"\n+                + \"      \\\"type\\\" : \\\"struct\\\",\\n\"\n+                + \"      \\\"schema-id\\\" : 0,\\n\"\n+                + \"      \\\"fields\\\" : [ {\\n\"\n+                + \"        \\\"id\\\" : 1,\\n\"\n+                + \"        \\\"name\\\" : \\\"x\\\",\\n\"\n+                + \"        \\\"required\\\" : true,\\n\"\n+                + \"        \\\"type\\\" : \\\"long\\\"\\n\"\n+                + \"      } ]\\n\"\n+                + \"    } ],\\n\"\n+                + \"    \\\"default-spec-id\\\" : 0,\\n\"\n+                + \"    \\\"partition-specs\\\" : [ {\\n\"\n+                + \"      \\\"spec-id\\\" : 0,\\n\"\n+                + \"      \\\"fields\\\" : [ ]\\n\"\n+                + \"    } ],\\n\"\n+                + \"    \\\"last-partition-id\\\" : 999,\\n\"\n+                + \"    \\\"default-sort-order-id\\\" : 0,\\n\"\n+                + \"    \\\"sort-orders\\\" : [ {\\n\"\n+                + \"      \\\"order-id\\\" : 0,\\n\"\n+                + \"      \\\"fields\\\" : [ ]\\n\"\n+                + \"    } ],\\n\"\n+                + \"    \\\"properties\\\" : { },\\n\"\n+                + \"    \\\"current-snapshot-id\\\" : %s,\\n\"\n+                + \"    \\\"refs\\\" : { },\\n\"\n+                + \"    \\\"snapshots\\\" : [ ],\\n\"\n+                + \"    \\\"statistics\\\" : [ ],\\n\"\n+                + \"    \\\"partition-statistics\\\" : [ ],\\n\"\n+                + \"    \\\"snapshot-log\\\" : [ ],\\n\"\n+                + \"    \\\"metadata-log\\\" : [ ]\\n\"\n+                + \"  }\\n\"\n+                + \"}\",\n+            formatVersion, metadata.lastUpdatedMillis(), formatVersion >= 3 ? \"null\" : \"-1\");\n \n     String json = LoadTableResponseParser.toJson(response, true);\n     assertThat(json).isEqualTo(expectedJson);\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12327",
    "pr_id": 12327,
    "issue_id": 12325,
    "repo": "apache/iceberg",
    "problem_statement": "Add option to provide partition spec in spark ADD_FILES procedure\n### Feature Request / Improvement\n\nCurrently, the ADD_FILES API in Apache Iceberg does not support specifying a partition spec, meaning that the API always operates on the latest table spec when adding files, as shown [in the implementation](https://github.com/apache/iceberg/blob/main/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/procedures/AddFilesProcedure.java#L172). \n\nThis can become problematic when the table or partition spec has evolved over time. For instance, in an archival and restore tool where data was archived before the partition spec changed, it would be beneficial to restore archived data using the older partition spec, rather than the current one. \n\n I am working on a PR to address this and would appreciate any specific suggestions or concerns from the community on making this change.\n\n### Query engine\n\nSpark\n\n### Willingness to contribute\n\n- [x] I can contribute this improvement/feature independently\n- [x] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [ ] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 180,
    "test_files_count": 1,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java",
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java",
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java",
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/AddFilesProcedure.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java"
    ],
    "base_commit": "88445086d039a85aba0530c89b7e9ae4f1f2eaa3",
    "head_commit": "62a720b55e3f506426d3c4bd622035ac08b66aa1",
    "repo_url": "https://github.com/apache/iceberg/pull/12327",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12327",
    "dockerfile": "",
    "pr_merged_at": "2025-03-05T00:08:26.000Z",
    "patch": "diff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex ad8a4beb55d0..dda22cd7e3a1 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n@@ -863,6 +863,23 @@ public static String quotedFullIdentifier(String catalogName, Identifier identif\n         .quoted();\n   }\n \n+  public static org.apache.spark.sql.execution.datasources.PartitionSpec getInferredSpec(\n+      SparkSession spark, Path rootPath) {\n+    FileStatusCache fileStatusCache = FileStatusCache.getOrCreate(spark);\n+    InMemoryFileIndex fileIndex =\n+        new InMemoryFileIndex(\n+            spark,\n+            JavaConverters.collectionAsScalaIterableConverter(ImmutableList.of(rootPath))\n+                .asScala()\n+                .toSeq(),\n+            scala.collection.immutable.Map$.MODULE$.empty(),\n+            Option.empty(), // Pass empty so that automatic schema inference is used\n+            fileStatusCache,\n+            Option.empty(),\n+            Option.empty());\n+    return fileIndex.partitionSpec();\n+  }\n+\n   /**\n    * Use Spark to list all partitions in the table.\n    *\n\ndiff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java\nindex fcf05f542c1c..efa5c834f644 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java\n@@ -29,6 +29,7 @@\n import java.util.List;\n import java.util.Locale;\n import java.util.Map;\n+import java.util.Set;\n import java.util.UUID;\n import java.util.concurrent.Callable;\n import java.util.concurrent.ExecutionException;\n@@ -545,6 +546,8 @@ public static void importSparkTable(\n       PartitionSpec spec =\n           findCompatibleSpec(targetTable, spark, sourceTableIdentWithDB.unquotedString());\n \n+      validatePartitionFilter(spec, partitionFilter, targetTable.name());\n+\n       if (Objects.equal(spec, PartitionSpec.unpartitioned())) {\n         importUnpartitionedSparkTable(\n             spark, sourceTableIdentWithDB, targetTable, checkDuplicateFiles, service);\n@@ -1091,21 +1094,13 @@ private ExecutorService getService() {\n \n   /**\n    * Returns the first partition spec in an IcebergTable that shares the same names and ordering as\n-   * the partition columns in a given Spark Table. Throws an error if not found\n+   * the partition columns provided. Throws an error if not found\n    */\n-  private static PartitionSpec findCompatibleSpec(\n-      Table icebergTable, SparkSession spark, String sparkTable) throws AnalysisException {\n-    List<String> parts = Lists.newArrayList(Splitter.on('.').limit(2).split(sparkTable));\n-    String db = parts.size() == 1 ? \"default\" : parts.get(0);\n-    String table = parts.get(parts.size() == 1 ? 0 : 1);\n-\n-    List<String> sparkPartNames =\n-        spark.catalog().listColumns(db, table).collectAsList().stream()\n-            .filter(org.apache.spark.sql.catalog.Column::isPartition)\n-            .map(org.apache.spark.sql.catalog.Column::name)\n+  public static PartitionSpec findCompatibleSpec(List<String> partitionNames, Table icebergTable) {\n+    List<String> partitionNamesLower =\n+        partitionNames.stream()\n             .map(name -> name.toLowerCase(Locale.ROOT))\n             .collect(Collectors.toList());\n-\n     for (PartitionSpec icebergSpec : icebergTable.specs().values()) {\n       boolean allIdentity =\n           icebergSpec.fields().stream().allMatch(field -> field.transform().isIdentity());\n@@ -1115,7 +1110,7 @@ private static PartitionSpec findCompatibleSpec(\n                 .map(PartitionField::name)\n                 .map(name -> name.toLowerCase(Locale.ROOT))\n                 .collect(Collectors.toList());\n-        if (icebergPartNames.equals(sparkPartNames)) {\n+        if (icebergPartNames.equals(partitionNamesLower)) {\n           return icebergSpec;\n         }\n       }\n@@ -1124,7 +1119,66 @@ private static PartitionSpec findCompatibleSpec(\n     throw new IllegalArgumentException(\n         String.format(\n             \"Cannot find a partition spec in Iceberg table %s that matches the partition\"\n-                + \" columns (%s) in Spark table %s\",\n-            icebergTable, sparkPartNames, sparkTable));\n+                + \" columns (%s) in input table\",\n+            icebergTable, partitionNames));\n+  }\n+\n+  /**\n+   * Returns the first partition spec in an IcebergTable that shares the same names and ordering as\n+   * the partition columns in a given Spark Table. Throws an error if not found\n+   */\n+  private static PartitionSpec findCompatibleSpec(\n+      Table icebergTable, SparkSession spark, String sparkTable) throws AnalysisException {\n+    List<String> parts = Lists.newArrayList(Splitter.on('.').limit(2).split(sparkTable));\n+    String db = parts.size() == 1 ? \"default\" : parts.get(0);\n+    String table = parts.get(parts.size() == 1 ? 0 : 1);\n+\n+    List<String> sparkPartNames =\n+        spark.catalog().listColumns(db, table).collectAsList().stream()\n+            .filter(org.apache.spark.sql.catalog.Column::isPartition)\n+            .map(org.apache.spark.sql.catalog.Column::name)\n+            .collect(Collectors.toList());\n+    return findCompatibleSpec(sparkPartNames, icebergTable);\n+  }\n+\n+  public static void validatePartitionFilter(\n+      PartitionSpec spec, Map<String, String> partitionFilter, String tableName) {\n+    List<PartitionField> partitionFields = spec.fields();\n+    Set<String> partitionNames =\n+        spec.fields().stream().map(PartitionField::name).collect(Collectors.toSet());\n+\n+    boolean tablePartitioned = !partitionFields.isEmpty();\n+    boolean partitionFilterPassed = !partitionFilter.isEmpty();\n+\n+    if (tablePartitioned && partitionFilterPassed) {\n+      // Check to see there are sufficient partition columns to satisfy the filter\n+      Preconditions.checkArgument(\n+          partitionFields.size() >= partitionFilter.size(),\n+          \"Cannot add data files to target table %s because that table is partitioned, \"\n+              + \"but the number of columns in the provided partition filter (%s) \"\n+              + \"is greater than the number of partitioned columns in table (%s)\",\n+          tableName,\n+          partitionFilter.size(),\n+          partitionFields.size());\n+\n+      // Check for any filters of non-existent columns\n+      List<String> unMatchedFilters =\n+          partitionFilter.keySet().stream()\n+              .filter(filterName -> !partitionNames.contains(filterName))\n+              .collect(Collectors.toList());\n+      Preconditions.checkArgument(\n+          unMatchedFilters.isEmpty(),\n+          \"Cannot add files to target table %s. %s is partitioned but the specified partition filter \"\n+              + \"refers to columns that are not partitioned: '%s' . Valid partition columns %s\",\n+          tableName,\n+          tableName,\n+          unMatchedFilters,\n+          String.join(\",\", partitionNames));\n+    } else {\n+      Preconditions.checkArgument(\n+          !partitionFilterPassed,\n+          \"Cannot use partition filter with an unpartitioned table %s\",\n+          tableName);\n+    }\n   }\n }\n\ndiff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/AddFilesProcedure.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/AddFilesProcedure.java\nindex 40a343b55b80..1e14b748c0f7 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/AddFilesProcedure.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/AddFilesProcedure.java\n@@ -21,10 +21,8 @@\n import java.util.Collections;\n import java.util.List;\n import java.util.Map;\n-import java.util.Set;\n import java.util.stream.Collectors;\n import org.apache.hadoop.fs.Path;\n-import org.apache.iceberg.PartitionField;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.SnapshotSummary;\n@@ -51,6 +49,7 @@\n import org.apache.spark.sql.types.Metadata;\n import org.apache.spark.sql.types.StructField;\n import org.apache.spark.sql.types.StructType;\n+import scala.collection.JavaConverters;\n \n class AddFilesProcedure extends BaseProcedure {\n \n@@ -62,7 +61,6 @@ class AddFilesProcedure extends BaseProcedure {\n       ProcedureParameter.optional(\"partition_filter\", STRING_MAP);\n   private static final ProcedureParameter CHECK_DUPLICATE_FILES_PARAM =\n       ProcedureParameter.optional(\"check_duplicate_files\", DataTypes.BooleanType);\n-\n   private static final ProcedureParameter PARALLELISM =\n       ProcedureParameter.optional(\"parallelism\", DataTypes.IntegerType);\n \n@@ -157,20 +155,13 @@ private InternalRow[] importToIceberg(\n     return modifyIcebergTable(\n         destIdent,\n         table -> {\n-          validatePartitionSpec(table, partitionFilter);\n           ensureNameMappingPresent(table);\n \n           if (isFileIdentifier(sourceIdent)) {\n             Path sourcePath = new Path(sourceIdent.name());\n             String format = sourceIdent.namespace()[0];\n             importFileTable(\n-                table,\n-                sourcePath,\n-                format,\n-                partitionFilter,\n-                checkDuplicateFiles,\n-                table.spec(),\n-                parallelism);\n+                table, sourcePath, format, partitionFilter, checkDuplicateFiles, parallelism);\n           } else {\n             importCatalogTable(\n                 table, sourceIdent, partitionFilter, checkDuplicateFiles, parallelism);\n@@ -196,11 +187,22 @@ private void importFileTable(\n       String format,\n       Map<String, String> partitionFilter,\n       boolean checkDuplicateFiles,\n-      PartitionSpec spec,\n       int parallelism) {\n+\n+    org.apache.spark.sql.execution.datasources.PartitionSpec inferredSpec =\n+        Spark3Util.getInferredSpec(spark(), tableLocation);\n+\n+    List<String> sparkPartNames =\n+        JavaConverters.seqAsJavaList(inferredSpec.partitionColumns()).stream()\n+            .map(StructField::name)\n+            .collect(Collectors.toList());\n+    PartitionSpec compatibleSpec = SparkTableUtil.findCompatibleSpec(sparkPartNames, table);\n+\n+    SparkTableUtil.validatePartitionFilter(compatibleSpec, partitionFilter, table.name());\n+\n     // List Partitions via Spark InMemory file search interface\n     List<SparkPartition> partitions =\n-        Spark3Util.getPartitions(spark(), tableLocation, format, partitionFilter, spec);\n+        Spark3Util.getPartitions(spark(), tableLocation, format, partitionFilter, compatibleSpec);\n \n     if (table.spec().isUnpartitioned()) {\n       Preconditions.checkArgument(\n@@ -212,11 +214,12 @@ private void importFileTable(\n       // Build a Global Partition for the source\n       SparkPartition partition =\n           new SparkPartition(Collections.emptyMap(), tableLocation.toString(), format);\n-      importPartitions(table, ImmutableList.of(partition), checkDuplicateFiles, parallelism);\n+      importPartitions(\n+          table, ImmutableList.of(partition), checkDuplicateFiles, compatibleSpec, parallelism);\n     } else {\n       Preconditions.checkArgument(\n           !partitions.isEmpty(), \"Cannot find any matching partitions in table %s\", table.name());\n-      importPartitions(table, partitions, checkDuplicateFiles, parallelism);\n+      importPartitions(table, partitions, checkDuplicateFiles, compatibleSpec, parallelism);\n     }\n   }\n \n@@ -242,16 +245,11 @@ private void importPartitions(\n       Table table,\n       List<SparkTableUtil.SparkPartition> partitions,\n       boolean checkDuplicateFiles,\n+      PartitionSpec spec,\n       int parallelism) {\n     String stagingLocation = getMetadataLocation(table);\n     SparkTableUtil.importSparkPartitions(\n-        spark(),\n-        partitions,\n-        table,\n-        table.spec(),\n-        stagingLocation,\n-        checkDuplicateFiles,\n-        parallelism);\n+        spark(), partitions, table, spec, stagingLocation, checkDuplicateFiles, parallelism);\n   }\n \n   private String getMetadataLocation(Table table) {\n@@ -264,56 +262,4 @@ private String getMetadataLocation(Table table) {\n   public String description() {\n     return \"AddFiles\";\n   }\n-\n-  private void validatePartitionSpec(Table table, Map<String, String> partitionFilter) {\n-    List<PartitionField> partitionFields = table.spec().fields();\n-    Set<String> partitionNames =\n-        table.spec().fields().stream().map(PartitionField::name).collect(Collectors.toSet());\n-\n-    boolean tablePartitioned = !partitionFields.isEmpty();\n-    boolean partitionSpecPassed = !partitionFilter.isEmpty();\n-\n-    // Check for any non-identity partition columns\n-    List<PartitionField> nonIdentityFields =\n-        partitionFields.stream()\n-            .filter(x -> !x.transform().isIdentity())\n-            .collect(Collectors.toList());\n-    Preconditions.checkArgument(\n-        nonIdentityFields.isEmpty(),\n-        \"Cannot add data files to target table %s because that table is partitioned and contains non-identity\"\n-            + \"partition transforms which will not be compatible. Found non-identity fields %s\",\n-        table.name(),\n-        nonIdentityFields);\n-\n-    if (tablePartitioned && partitionSpecPassed) {\n-      // Check to see there are sufficient partition columns to satisfy the filter\n-      Preconditions.checkArgument(\n-          partitionFields.size() >= partitionFilter.size(),\n-          \"Cannot add data files to target table %s because that table is partitioned, \"\n-              + \"but the number of columns in the provided partition filter (%s) \"\n-              + \"is greater than the number of partitioned columns in table (%s)\",\n-          table.name(),\n-          partitionFilter.size(),\n-          partitionFields.size());\n-\n-      // Check for any filters of non existent columns\n-      List<String> unMatchedFilters =\n-          partitionFilter.keySet().stream()\n-              .filter(filterName -> !partitionNames.contains(filterName))\n-              .collect(Collectors.toList());\n-      Preconditions.checkArgument(\n-          unMatchedFilters.isEmpty(),\n-          \"Cannot add files to target table %s. %s is partitioned but the specified partition filter \"\n-              + \"refers to columns that are not partitioned: '%s' . Valid partition columns %s\",\n-          table.name(),\n-          table.name(),\n-          unMatchedFilters,\n-          String.join(\",\", partitionNames));\n-    } else {\n-      Preconditions.checkArgument(\n-          !partitionSpecPassed,\n-          \"Cannot use partition filter with an unpartitioned table %s\",\n-          table.name());\n-    }\n-  }\n }\n",
    "test_patch": "diff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java\nindex 6db8b3d76290..341ce97573b6 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java\n@@ -42,6 +42,7 @@\n import org.apache.iceberg.Parameter;\n import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n+import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.io.FileIO;\n@@ -635,6 +636,83 @@ public void addFilteredPartitionsToPartitionedWithNullValueFilteringOnDept() {\n         sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n   }\n \n+  @TestTemplate\n+  public void addFileTableOldSpecDataAfterPartitionSpecEvolved()\n+      throws NoSuchTableException, ParseException {\n+    createPartitionedFileTable(\"parquet\");\n+    createIcebergTable(\n+        \"id Integer, name String, dept String, subdept String\",\n+        \"PARTITIONED BY (id, dept, subdept)\");\n+    sql(\"ALTER TABLE %s DROP PARTITION FIELD dept\", tableName);\n+    sql(\n+        \"ALTER TABLE %s DROP PARTITION FIELD subdept\",\n+        tableName); // This spec now matches the partitioning of the parquet table\n+    sql(\"ALTER TABLE %s ADD PARTITION FIELD subdept\", tableName);\n+\n+    if (formatVersion == 1) {\n+      // In V1, since we are dropping the partition field, it adds a void transform which will not\n+      // match with the input spec\n+      assertThatThrownBy(\n+              () ->\n+                  scalarSql(\n+                      \"CALL %s.system.add_files('%s', '`parquet`.`%s`')\",\n+                      catalogName, tableName, fileTableDir.getAbsolutePath()))\n+          .isInstanceOf(IllegalArgumentException.class)\n+          .hasMessageContaining(\n+              String.format(\n+                  \"Cannot find a partition spec in Iceberg table %s that matches the partition columns ([id]) in input table\",\n+                  tableName));\n+      return;\n+    }\n+\n+    List<Object[]> result =\n+        sql(\n+            \"CALL %s.system.add_files(table => '%s', source_table => '`parquet`.`%s`')\",\n+            catalogName, tableName, fileTableDir.getAbsolutePath());\n+\n+    assertOutput(result, 8L, 4L);\n+    assertEquals(\n+        \"Iceberg table contains correct data\",\n+        sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", sourceTableName),\n+        sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n+\n+    Table table = Spark3Util.loadIcebergTable(spark, tableName);\n+    // Find the spec that matches the partitioning of the parquet table\n+    PartitionSpec compatibleSpec =\n+        table.specs().values().stream()\n+            .filter(spec -> spec.fields().size() == 1)\n+            .filter(spec -> \"id\".equals(spec.fields().get(0).name()))\n+            .findFirst()\n+            .orElse(null);\n+\n+    assertThat(compatibleSpec).isNotNull();\n+    manifestSpecMatchesGivenSpec(table, compatibleSpec);\n+    verifyUUIDInPath();\n+  }\n+\n+  @TestTemplate\n+  public void addFileTableNoCompatibleSpec() {\n+    createPartitionedFileTable(\"parquet\");\n+    createIcebergTable(\n+        \"id Integer, name String, dept String, subdept String\", \"PARTITIONED BY (dept)\");\n+    sql(\"ALTER TABLE %s ADD PARTITION FIELD subdept\", tableName);\n+\n+    String fullTableName = tableName;\n+    if (implementation.equals(SparkCatalogConfig.SPARK.implementation())) {\n+      fullTableName = String.format(\"%s.%s\", catalogName, tableName);\n+    }\n+    assertThatThrownBy(\n+            () ->\n+                scalarSql(\n+                    \"CALL %s.system.add_files(table => '%s', source_table => '`parquet`.`%s`')\",\n+                    catalogName, tableName, fileTableDir.getAbsolutePath()))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\n+            String.format(\n+                \"Cannot find a partition spec in Iceberg table %s that matches the partition columns (%s) in input table\",\n+                fullTableName, \"[id]\"));\n+  }\n+\n   @TestTemplate\n   public void addWeirdCaseHiveTable() {\n     createWeirdCaseTable();\n@@ -713,7 +791,8 @@ public void invalidDataImport() {\n                     \"CALL %s.system.add_files('%s', '`parquet`.`%s`', map('id', 1))\",\n                     catalogName, tableName, fileTableDir.getAbsolutePath()))\n         .isInstanceOf(IllegalArgumentException.class)\n-        .hasMessageStartingWith(\"Cannot use partition filter with an unpartitioned table\");\n+        .hasMessageStartingWith(\"Cannot find a partition spec in Iceberg table\")\n+        .hasMessageContaining(\"that matches the partition columns\");\n \n     assertThatThrownBy(\n             () ->\n@@ -721,7 +800,8 @@ public void invalidDataImport() {\n                     \"CALL %s.system.add_files('%s', '`parquet`.`%s`')\",\n                     catalogName, tableName, fileTableDir.getAbsolutePath()))\n         .isInstanceOf(IllegalArgumentException.class)\n-        .hasMessageStartingWith(\"Cannot add partitioned files to an unpartitioned table\");\n+        .hasMessageStartingWith(\"Cannot find a partition spec in Iceberg table\")\n+        .hasMessageContaining(\"that matches the partition columns\");\n   }\n \n   @TestTemplate\n@@ -737,8 +817,8 @@ public void invalidDataImportPartitioned() {\n                     \"CALL %s.system.add_files('%s', '`parquet`.`%s`', map('x', '1', 'y', '2'))\",\n                     catalogName, tableName, fileTableDir.getAbsolutePath()))\n         .isInstanceOf(IllegalArgumentException.class)\n-        .hasMessageStartingWith(\"Cannot add data files to target table\")\n-        .hasMessageContaining(\"is greater than the number of partitioned columns\");\n+        .hasMessageStartingWith(\"Cannot find a partition spec in Iceberg table\")\n+        .hasMessageContaining(\"that matches the partition columns\");\n \n     assertThatThrownBy(\n             () ->\n@@ -746,9 +826,8 @@ public void invalidDataImportPartitioned() {\n                     \"CALL %s.system.add_files('%s', '`parquet`.`%s`', map('dept', '2'))\",\n                     catalogName, tableName, fileTableDir.getAbsolutePath()))\n         .isInstanceOf(IllegalArgumentException.class)\n-        .hasMessageStartingWith(\"Cannot add files to target table\")\n-        .hasMessageContaining(\n-            \"specified partition filter refers to columns that are not partitioned\");\n+        .hasMessageStartingWith(\"Cannot find a partition spec in Iceberg table\")\n+        .hasMessageContaining(\"that matches the partition columns\");\n   }\n \n   @TestTemplate\n@@ -1218,15 +1297,18 @@ private void assertOutput(\n \n   private void manifestSpecMatchesTableSpec() throws NoSuchTableException, ParseException {\n     Table table = Spark3Util.loadIcebergTable(spark, tableName);\n-    FileIO io = ((HasTableOperations) table).operations().io();\n+    manifestSpecMatchesGivenSpec(table, table.spec());\n+  }\n \n+  private void manifestSpecMatchesGivenSpec(Table table, PartitionSpec partitionSpec) {\n+    FileIO io = ((HasTableOperations) table).operations().io();\n     // Check that the manifests have the correct partition spec\n     assertThat(\n             table.currentSnapshot().allManifests(io).stream()\n                 .map(mf -> ManifestFiles.read(mf, io, null /* force reading spec from file*/))\n                 .map(ManifestReader::spec)\n                 .collect(Collectors.toList()))\n-        .allSatisfy(spec -> assertThat(spec).isEqualTo(table.spec()));\n+        .allSatisfy(spec -> assertThat(spec).isEqualTo(partitionSpec));\n   }\n \n   private void verifyUUIDInPath() {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12319",
    "pr_id": 12319,
    "issue_id": 12273,
    "repo": "apache/iceberg",
    "problem_statement": "Partition spec mismatch when 'compatibility.snapshot-id-inheritance.enabled' is true\n### Apache Iceberg version\n\n1.7\n\n### Query engine\n\nSpark\n\n### Please describe the bug üêû\n\nThe behavior of `add_files` procedure in Spark is affected by table property `compatibility.snapshot-id-inheritance.enabled`\nSetting this property will lead to the mismatch in source-id in partition-spec: manifest header will take column ids start from 0 but metadata json have id start at 1.\n\nSteps to reproduce\n```\ncreate table snapshot_id_parquet(id int, desc string) using parquet partitioned by (name string)\n    location 's3a://my_bucket_location';\n\ninsert into  snapshot_id_parquet values (1, 'abc', 'a');\n\ncreate table null_snapshot_id_in_manifest(id int, desc string, name string) using iceberg \npartitioned by (name) \nTBLPROPERTIES ('compatibility.snapshot-id-inheritance.enabled'='true');\n\nCALL system.add_files(table => 'default.null_snapshot_id_in_manifest', source_table => 'default.snapshot_id_parquet');\n```\n\nExamine the manifest header and metadata json of the table null_snapshot_id_in_manifest, v3 will be the latest write.\nThis v3.metadata.json will have partition spec written correctly:\n```\n\"partition-specs\" : [ {\n    \"spec-id\" : 0,\n    \"fields\" : [ {\n      \"name\" : \"name\",\n      \"transform\" : \"identity\",\n      \"source-id\" : 3,\n      \"field-id\" : 1000\n    } ]\n  } ],\n```\nHowever, using `avro-tools getmeta` to check the manifest avro file, we found `partition-spec\t[{\"name\":\"name\",\"transform\":\"identity\",\"source-id\":2,\"field-id\":1000}]`, which is a mismatch from json\n\nAlso verified that not setting the flag will write matching `source-id` from manifest and metadata json.\n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 258,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java",
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java"
    ],
    "base_commit": "5e1ce86ecaeeb7bf44c440bfe0000ccb80dc9c85",
    "head_commit": "18dbfd48bf84551777b3fb175b46a837590d80fa",
    "repo_url": "https://github.com/apache/iceberg/pull/12319",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12319",
    "dockerfile": "",
    "pr_merged_at": "2025-02-19T22:19:57.000Z",
    "patch": "diff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java\nindex 160dee9e7058..fcf05f542c1c 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkTableUtil.java\n@@ -27,6 +27,7 @@\n import java.util.Collections;\n import java.util.Iterator;\n import java.util.List;\n+import java.util.Locale;\n import java.util.Map;\n import java.util.UUID;\n import java.util.concurrent.Callable;\n@@ -49,6 +50,7 @@\n import org.apache.iceberg.MetadataTableType;\n import org.apache.iceberg.MetadataTableUtils;\n import org.apache.iceberg.MetricsConfig;\n+import org.apache.iceberg.PartitionField;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableOperations;\n@@ -66,6 +68,7 @@\n import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n import org.apache.iceberg.relocated.com.google.common.base.Objects;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.base.Splitter;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n@@ -540,7 +543,7 @@ public static void importSparkTable(\n \n     try {\n       PartitionSpec spec =\n-          SparkSchemaUtil.specForTable(spark, sourceTableIdentWithDB.unquotedString());\n+          findCompatibleSpec(targetTable, spark, sourceTableIdentWithDB.unquotedString());\n \n       if (Objects.equal(spec, PartitionSpec.unpartitioned())) {\n         importUnpartitionedSparkTable(\n@@ -1085,4 +1088,43 @@ private ExecutorService getService() {\n       return service;\n     }\n   }\n+\n+  /**\n+   * Returns the first partition spec in an IcebergTable that shares the same names and ordering as\n+   * the partition columns in a given Spark Table. Throws an error if not found\n+   */\n+  private static PartitionSpec findCompatibleSpec(\n+      Table icebergTable, SparkSession spark, String sparkTable) throws AnalysisException {\n+    List<String> parts = Lists.newArrayList(Splitter.on('.').limit(2).split(sparkTable));\n+    String db = parts.size() == 1 ? \"default\" : parts.get(0);\n+    String table = parts.get(parts.size() == 1 ? 0 : 1);\n+\n+    List<String> sparkPartNames =\n+        spark.catalog().listColumns(db, table).collectAsList().stream()\n+            .filter(org.apache.spark.sql.catalog.Column::isPartition)\n+            .map(org.apache.spark.sql.catalog.Column::name)\n+            .map(name -> name.toLowerCase(Locale.ROOT))\n+            .collect(Collectors.toList());\n+\n+    for (PartitionSpec icebergSpec : icebergTable.specs().values()) {\n+      boolean allIdentity =\n+          icebergSpec.fields().stream().allMatch(field -> field.transform().isIdentity());\n+      if (allIdentity) {\n+        List<String> icebergPartNames =\n+            icebergSpec.fields().stream()\n+                .map(PartitionField::name)\n+                .map(name -> name.toLowerCase(Locale.ROOT))\n+                .collect(Collectors.toList());\n+        if (icebergPartNames.equals(sparkPartNames)) {\n+          return icebergSpec;\n+        }\n+      }\n+    }\n+\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Cannot find a partition spec in Iceberg table %s that matches the partition\"\n+                + \" columns (%s) in Spark table %s\",\n+            icebergTable, sparkPartNames, sparkTable));\n+  }\n }\n",
    "test_patch": "diff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java\nindex 332669470aea..6db8b3d76290 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestAddFilesProcedure.java\n@@ -36,17 +36,25 @@\n import org.apache.avro.generic.GenericRecord;\n import org.apache.avro.io.DatumWriter;\n import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.ManifestFiles;\n+import org.apache.iceberg.ManifestReader;\n import org.apache.iceberg.Parameter;\n import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n+import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.Spark3Util;\n import org.apache.iceberg.spark.SparkCatalogConfig;\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.RowFactory;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n import org.apache.spark.sql.types.DataTypes;\n import org.apache.spark.sql.types.Metadata;\n import org.apache.spark.sql.types.StructField;\n@@ -454,7 +462,8 @@ public void deleteAndAddBackPartitioned() {\n   }\n \n   @TestTemplate\n-  public void addPartitionToPartitionedSnapshotIdInheritanceEnabledInTwoRuns() {\n+  public void addPartitionToPartitionedSnapshotIdInheritanceEnabledInTwoRuns()\n+      throws NoSuchTableException, ParseException {\n     createPartitionedFileTable(\"parquet\");\n \n     createIcebergTable(\n@@ -476,13 +485,32 @@ public void addPartitionToPartitionedSnapshotIdInheritanceEnabledInTwoRuns() {\n         sql(\"SELECT id, name, dept, subdept FROM %s WHERE id < 3 ORDER BY id\", sourceTableName),\n         sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n \n-    // verify manifest file name has uuid pattern\n-    String manifestPath = (String) sql(\"select path from %s.manifests\", tableName).get(0)[0];\n+    manifestSpecMatchesTableSpec();\n \n-    Pattern uuidPattern = Pattern.compile(\"[a-f0-9]{8}(?:-[a-f0-9]{4}){4}[a-f0-9]{8}\");\n+    verifyUUIDInPath();\n+  }\n \n-    Matcher matcher = uuidPattern.matcher(manifestPath);\n-    assertThat(matcher.find()).as(\"verify manifest path has uuid\").isTrue();\n+  @TestTemplate\n+  public void addPartitionsFromHiveSnapshotInheritanceEnabled()\n+      throws NoSuchTableException, ParseException {\n+    createPartitionedHiveTable();\n+    createIcebergTable(\n+        \"id Integer, name String, dept String, subdept String\", \"PARTITIONED BY (id)\");\n+\n+    sql(\n+        \"ALTER TABLE %s SET TBLPROPERTIES ('%s' 'true')\",\n+        tableName, TableProperties.SNAPSHOT_ID_INHERITANCE_ENABLED);\n+\n+    sql(\"CALL %s.system.add_files('%s', '%s')\", catalogName, tableName, sourceTableName);\n+\n+    assertEquals(\n+        \"Iceberg table contains correct data\",\n+        sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", sourceTableName),\n+        sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n+\n+    manifestSpecMatchesTableSpec();\n+\n+    verifyUUIDInPath();\n   }\n \n   @TestTemplate\n@@ -968,6 +996,28 @@ public void testAddFilesPartitionedWithParallelism() {\n         sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n   }\n \n+  @TestTemplate\n+  public void testAddFilesToTableWithManySpecs() {\n+    createPartitionedHiveTable();\n+    createIcebergTable(\"id Integer, name String, dept String, subdept String\"); // Spec 0\n+\n+    sql(\"ALTER TABLE %s ADD PARTITION FIELD id\", tableName); // Spec 1\n+    sql(\"ALTER TABLE %s ADD PARTITION FIELD name\", tableName); // Spec 2\n+    sql(\"ALTER TABLE %s ADD PARTITION FIELD subdept\", tableName); // Spec 3\n+\n+    List<Object[]> result =\n+        sql(\n+            \"CALL %s.system.add_files('%s', '%s', map('id', 1))\",\n+            catalogName, tableName, sourceTableName);\n+\n+    assertOutput(result, 2L, 1L);\n+\n+    assertEquals(\n+        \"Iceberg table contains correct data\",\n+        sql(\"SELECT id, name, dept, subdept FROM %s WHERE id = 1 ORDER BY id\", sourceTableName),\n+        sql(\"SELECT id, name, dept, subdept FROM %s ORDER BY id\", tableName));\n+  }\n+\n   private static final List<Object[]> EMPTY_QUERY_RESULT = Lists.newArrayList();\n \n   private static final StructField[] STRUCT = {\n@@ -1165,4 +1215,27 @@ private void assertOutput(\n       assertThat(output[1]).isIn(expectedChangedPartitionCount, null);\n     }\n   }\n+\n+  private void manifestSpecMatchesTableSpec() throws NoSuchTableException, ParseException {\n+    Table table = Spark3Util.loadIcebergTable(spark, tableName);\n+    FileIO io = ((HasTableOperations) table).operations().io();\n+\n+    // Check that the manifests have the correct partition spec\n+    assertThat(\n+            table.currentSnapshot().allManifests(io).stream()\n+                .map(mf -> ManifestFiles.read(mf, io, null /* force reading spec from file*/))\n+                .map(ManifestReader::spec)\n+                .collect(Collectors.toList()))\n+        .allSatisfy(spec -> assertThat(spec).isEqualTo(table.spec()));\n+  }\n+\n+  private void verifyUUIDInPath() {\n+    // verify manifest file name has uuid pattern\n+    String manifestPath = (String) sql(\"select path from %s.manifests\", tableName).get(0)[0];\n+\n+    Pattern uuidPattern = Pattern.compile(\"[a-f0-9]{8}(?:-[a-f0-9]{4}){4}[a-f0-9]{8}\");\n+\n+    Matcher matcher = uuidPattern.matcher(manifestPath);\n+    assertThat(matcher.find()).as(\"verify manifest path has uuid\").isTrue();\n+  }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12301",
    "pr_id": 12301,
    "issue_id": 12300,
    "repo": "apache/iceberg",
    "problem_statement": "IndexOutOfBounds in FileFormat#fromFileName\n### Apache Iceberg version\n\n1.8.0 (latest release)\n\n### Query engine\n\nNone\n\n### Please describe the bug üêû\n\n`FileFormat#fromFileName` doesn't account for short file names [here](https://github.com/apache/iceberg/blob/bcbbd0344623ffea5b092e2de5debb0bc12892a1/api/src/main/java/org/apache/iceberg/FileFormat.java#L64). When a file name is shorter than the longest file format extension (`metadata.json`), we get an error like this one:\n```\nbegin -6, end 8, length 8\njava.lang.StringIndexOutOfBoundsException: begin -6, end 8, length 8\n\tat java.base/java.lang.String.checkBoundsBeginEnd(String.java:4604)\n\tat java.base/java.lang.String.substring(String.java:2707)\n\tat java.base/java.lang.String.subSequence(String.java:2745)\n\tat org.apache.iceberg.FileFormat.fromFileName(FileFormat.java:64)\n\tat org.apache.iceberg.FileFormatTest.fromFileName(FileFormatTest.java:40)\n```\n\n### Willingness to contribute\n\n- [x] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 171,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "api/src/main/java/org/apache/iceberg/FileFormat.java",
      "api/src/test/java/org/apache/iceberg/TestFileFormat.java"
    ],
    "pr_changed_test_files": [
      "api/src/test/java/org/apache/iceberg/TestFileFormat.java"
    ],
    "base_commit": "0b47faaada2aafa42c118be78445f6c40fc0ead6",
    "head_commit": "6b0c8ca58212db36e415747f0cafd590fde3b58f",
    "repo_url": "https://github.com/apache/iceberg/pull/12301",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12301",
    "dockerfile": "",
    "pr_merged_at": "2025-02-28T10:29:53.000Z",
    "patch": "diff --git a/api/src/main/java/org/apache/iceberg/FileFormat.java b/api/src/main/java/org/apache/iceberg/FileFormat.java\nindex 6b41aec42c3e..064fc1465fa8 100644\n--- a/api/src/main/java/org/apache/iceberg/FileFormat.java\n+++ b/api/src/main/java/org/apache/iceberg/FileFormat.java\n@@ -58,11 +58,16 @@ public String addExtension(String filename) {\n   }\n \n   public static FileFormat fromFileName(CharSequence filename) {\n+    if (filename == null) {\n+      return null;\n+    }\n+\n     for (FileFormat format : VALUES) {\n       int extStart = filename.length() - format.ext.length();\n-      if (Comparators.charSequences()\n-              .compare(format.ext, filename.subSequence(extStart, filename.length()))\n-          == 0) {\n+      if (extStart > 0\n+          && Comparators.charSequences()\n+                  .compare(format.ext, filename.subSequence(extStart, filename.length()))\n+              == 0) {\n         return format;\n       }\n     }\n",
    "test_patch": "diff --git a/api/src/test/java/org/apache/iceberg/TestFileFormat.java b/api/src/test/java/org/apache/iceberg/TestFileFormat.java\nnew file mode 100644\nindex 000000000000..19aa1d882eee\n--- /dev/null\n+++ b/api/src/test/java/org/apache/iceberg/TestFileFormat.java\n@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.FieldSource;\n+\n+@SuppressWarnings(\"unused\")\n+class TestFileFormat {\n+\n+  private static final Object[][] FILE_NAMES =\n+      new Object[][] {\n+        // Files with format\n+        {\"file.puffin\", FileFormat.PUFFIN},\n+        {\"dir/file.puffin\", FileFormat.PUFFIN},\n+        {\"file.orc\", FileFormat.ORC},\n+        {\"dir/file.orc\", FileFormat.ORC},\n+        {\"file.parquet\", FileFormat.PARQUET},\n+        {\"dir/file.parquet\", FileFormat.PARQUET},\n+        {\"file.avro\", FileFormat.AVRO},\n+        {\"dir/file.avro\", FileFormat.AVRO},\n+        {\"v1.metadata.json\", FileFormat.METADATA},\n+        {\"dir/v1.metadata.json\", FileFormat.METADATA},\n+        // Short file names with format\n+        {\"x.puffin\", FileFormat.PUFFIN},\n+        {\"x.orc\", FileFormat.ORC},\n+        {\"x.parquet\", FileFormat.PARQUET},\n+        {\"x.avro\", FileFormat.AVRO},\n+        {\"x.metadata.json\", FileFormat.METADATA},\n+        // Unsupported formats\n+        {\"file.csv\", null},\n+        {\"dir/file.csv\", null},\n+        // No format\n+        {\"file\", null},\n+        {\"dir\", null},\n+        // Blank strings\n+        {\"\", null},\n+        {\" \", null},\n+        {null, null},\n+      };\n+\n+  @ParameterizedTest\n+  @FieldSource(\"FILE_NAMES\")\n+  void fromFileName(String fileName, FileFormat expected) {\n+    FileFormat actual = FileFormat.fromFileName(fileName);\n+\n+    assertThat(actual).isEqualTo(expected);\n+  }\n+}\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12275",
    "pr_id": 12275,
    "issue_id": 12054,
    "repo": "apache/iceberg",
    "problem_statement": "Add unit tests for ColumnarBatchUtil using mocking\n### Feature Request / Improvement\n\nWe need to add tests for `ColumnarBatchUtil` using mocking to verify correctness of `buildRowIdMapping` and `buildIsDeleted`.\n\n### Query engine\n\nSpark\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [ ] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 63,
    "test_files_count": 8,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/TestColumnarBatchUtil.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryEncodedVectorizedReads.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetVectorizedReads.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/TestColumnarBatchUtil.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryEncodedVectorizedReads.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetVectorizedReads.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/TestColumnarBatchUtil.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryEncodedVectorizedReads.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetVectorizedReads.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/TestColumnarBatchUtil.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryEncodedVectorizedReads.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java",
      "spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetVectorizedReads.java"
    ],
    "base_commit": "40c0a73c6513711b2a58bb159cf8c562fe56c826",
    "head_commit": "0e724eae3620c79f84bf42002eb5625121a3ead4",
    "repo_url": "https://github.com/apache/iceberg/pull/12275",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12275",
    "dockerfile": "",
    "pr_merged_at": "2025-06-24T18:46:23.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/TestColumnarBatchUtil.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/TestColumnarBatchUtil.java\nnew file mode 100644\nindex 000000000000..dadbe3e788b7\n--- /dev/null\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/TestColumnarBatchUtil.java\n@@ -0,0 +1,302 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import static java.util.Collections.nCopies;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+import java.util.Arrays;\n+import java.util.function.Predicate;\n+import java.util.stream.Stream;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.DeleteFilter;\n+import org.apache.iceberg.deletes.PositionDeleteIndex;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+public class TestColumnarBatchUtil {\n+\n+  private ColumnVector[] columnVectors;\n+  private DeleteFilter deleteFilter;\n+\n+  @BeforeEach\n+  public void before() {\n+    columnVectors = mockColumnVector();\n+    deleteFilter = mock(DeleteFilter.class);\n+  }\n+\n+  @Test\n+  public void testBuildRowIdMappingNoDeletes() {\n+    when(deleteFilter.hasPosDeletes()).thenReturn(true);\n+    PositionDeleteIndex deletedRowPos = mock(PositionDeleteIndex.class);\n+\n+    for (long i = 0; i <= 10; i++) {\n+      when(deletedRowPos.isDeleted(i)).thenReturn(false);\n+    }\n+\n+    when(deleteFilter.deletedRowPositions()).thenReturn(deletedRowPos);\n+    var rowIdMapping = ColumnarBatchUtil.buildRowIdMapping(columnVectors, deleteFilter, 0, 10);\n+    assertThat(rowIdMapping).isNull();\n+  }\n+\n+  @Test\n+  public void testBuildRowIdMappingPositionDeletesOnly() {\n+    when(deleteFilter.hasPosDeletes()).thenReturn(true);\n+    PositionDeleteIndex deletedRowPos = mock(PositionDeleteIndex.class);\n+\n+    // 5 position deletes\n+    for (long i = 98; i < 103; i++) {\n+      when(deletedRowPos.isDeleted(i)).thenReturn(true);\n+    }\n+\n+    when(deleteFilter.deletedRowPositions()).thenReturn(deletedRowPos);\n+\n+    var rowIdMapping = ColumnarBatchUtil.buildRowIdMapping(columnVectors, deleteFilter, 0, 200);\n+    assertThat(rowIdMapping).isNotNull();\n+\n+    int[] rowIds = (int[]) rowIdMapping.first();\n+    int liveRows = (int) rowIdMapping.second();\n+\n+    for (int id : rowIds) {\n+      assertThat(id < 98 || id > 102).isTrue();\n+    }\n+\n+    assertThat(rowIds.length).isEqualTo(200);\n+    assertThat(liveRows).isEqualTo(195);\n+  }\n+\n+  @Test\n+  public void testBuildRowIdMappingEqualityDeletesOnly() {\n+    // Define raw equality delete predicate ‚Äî delete rows where value == 42\n+    Predicate<InternalRow> rawEqDelete = row -> row.getInt(0) == 42;\n+\n+    // Mimic real eqDeletedRowFilter(): keep row only if it does NOT match delete condition\n+    Predicate<InternalRow> eqDeletePredicate =\n+        Stream.of(rawEqDelete).map(Predicate::negate).reduce(Predicate::and).orElse(t -> true);\n+\n+    // Mock DeleteFilter\n+    when(deleteFilter.hasPosDeletes()).thenReturn(false);\n+    when(deleteFilter.deletedRowPositions()).thenReturn(null);\n+    when(deleteFilter.eqDeletedRowFilter()).thenReturn(eqDeletePredicate);\n+\n+    var rowIdMapping = ColumnarBatchUtil.buildRowIdMapping(columnVectors, deleteFilter, 0, 5);\n+\n+    assertThat(rowIdMapping).isNotNull();\n+    int[] rowIds = (int[]) rowIdMapping.first();\n+    int liveRows = (Integer) rowIdMapping.second();\n+\n+    // Expect to keep positions 0, 1, 3, 4 ‚Üí values 40, 41, 43, 44\n+    assertThat(liveRows).isEqualTo(4);\n+    assertThat(Arrays.copyOf(rowIds, liveRows)).containsExactly(0, 1, 3, 4);\n+  }\n+\n+  @Test\n+  public void testBuildRowIdMappingPositionAndEqualityDeletes() {\n+\n+    // Define raw equality delete predicate ‚Äî delete rows where value == 42\n+    Predicate<InternalRow> rawEqDelete = row -> row.getInt(0) == 42;\n+\n+    // Mimic real eqDeletedRowFilter(): keep row only if it does NOT match delete condition\n+    Predicate<InternalRow> eqDeletePredicate =\n+        Stream.of(rawEqDelete).map(Predicate::negate).reduce(Predicate::and).orElse(t -> true);\n+    when(deleteFilter.eqDeletedRowFilter()).thenReturn(eqDeletePredicate);\n+\n+    PositionDeleteIndex deletedRowPos = mock(PositionDeleteIndex.class);\n+    when(deletedRowPos.isDeleted(1)).thenReturn(true); // 41\n+    when(deletedRowPos.isDeleted(4)).thenReturn(true); // 44\n+    when(deleteFilter.hasPosDeletes()).thenReturn(true);\n+    when(deleteFilter.deletedRowPositions()).thenReturn(deletedRowPos);\n+\n+    var rowIdMapping = ColumnarBatchUtil.buildRowIdMapping(columnVectors, deleteFilter, 0, 5);\n+\n+    assertThat(rowIdMapping).isNotNull();\n+    int[] rowIds = (int[]) rowIdMapping.first();\n+    int liveRows = (Integer) rowIdMapping.second();\n+\n+    assertThat(liveRows).isEqualTo(2);\n+    assertThat(Arrays.copyOf(rowIds, liveRows)).containsExactly(0, 3);\n+  }\n+\n+  @Test\n+  void testBuildRowIdMappingEmptyColumVectors() {\n+    ColumnVector[] columnVectorsZero = new ColumnVector[0];\n+\n+    PositionDeleteIndex deletedRowPos = mock(PositionDeleteIndex.class);\n+    when(deletedRowPos.isDeleted(1)).thenReturn(true);\n+    when(deletedRowPos.isDeleted(4)).thenReturn(true);\n+    when(deleteFilter.hasPosDeletes()).thenReturn(true);\n+    when(deleteFilter.deletedRowPositions()).thenReturn(deletedRowPos);\n+\n+    var rowIdMapping = ColumnarBatchUtil.buildRowIdMapping(columnVectorsZero, deleteFilter, 0, 0);\n+\n+    // Empty batch size, expect no rows deleted.\n+    assertThat(rowIdMapping).isNull();\n+  }\n+\n+  @Test\n+  void testBuildRowIdMapAllRowsDeleted() {\n+\n+    // Define raw equality delete predicate ‚Äî delete rows where value == 42 or 43\n+    Predicate<InternalRow> rawEqDelete = row -> row.getInt(0) == 42 || row.getInt(0) == 43;\n+\n+    // Mimic real eqDeletedRowFilter(): keep row only if it does NOT match delete condition\n+    Predicate<InternalRow> eqDeletePredicate =\n+        Stream.of(rawEqDelete).map(Predicate::negate).reduce(Predicate::and).orElse(t -> true);\n+    when(deleteFilter.eqDeletedRowFilter()).thenReturn(eqDeletePredicate);\n+\n+    PositionDeleteIndex deletedRowPos = mock(PositionDeleteIndex.class);\n+    when(deletedRowPos.isDeleted(0)).thenReturn(true); // 40\n+    when(deletedRowPos.isDeleted(1)).thenReturn(true); // 41\n+    when(deletedRowPos.isDeleted(4)).thenReturn(true); // 44\n+    when(deleteFilter.hasPosDeletes()).thenReturn(true);\n+    when(deleteFilter.deletedRowPositions()).thenReturn(deletedRowPos);\n+\n+    var rowIdMapping = ColumnarBatchUtil.buildRowIdMapping(columnVectors, deleteFilter, 0, 5);\n+\n+    assertThat(rowIdMapping).isNotNull();\n+    int[] rowIds = (int[]) rowIdMapping.first();\n+    int liveRows = (Integer) rowIdMapping.second();\n+\n+    // Expect all rows to be deleted\n+    assertThat(liveRows).isEqualTo(0);\n+    assertThat(rowIds).containsExactly(0, 0, 0, 0, 0);\n+  }\n+\n+  @Test\n+  void testBuildIsDeletedPositionDeletes() {\n+    PositionDeleteIndex deletedRowPos = mock(PositionDeleteIndex.class);\n+    when(deleteFilter.deletedRowPositions()).thenReturn(deletedRowPos);\n+\n+    for (long i = 98; i < 100; i++) {\n+      when(deletedRowPos.isDeleted(i)).thenReturn(true);\n+    }\n+\n+    var isDeleted = ColumnarBatchUtil.buildIsDeleted(columnVectors, deleteFilter, 0, 100);\n+\n+    assertThat(isDeleted).isNotNull();\n+    assertThat(isDeleted.length).isEqualTo(100);\n+\n+    for (int i = 98; i < 100; i++) {\n+      assertThat(isDeleted[i]).isTrue();\n+    }\n+\n+    for (int i = 0; i < 98; i++) {\n+      assertThat(isDeleted[i]).isFalse();\n+    }\n+  }\n+\n+  @Test\n+  void testBuildIsDeletedEqualityDeletes() {\n+    // Define raw equality delete predicate ‚Äî delete rows where value == 42 or 43\n+    Predicate<InternalRow> rawEqDelete = row -> row.getInt(0) == 42 || row.getInt(0) == 43;\n+\n+    // Mimic real eqDeletedRowFilter(): keep row only if it does NOT match delete condition\n+    Predicate<InternalRow> eqDeletePredicate =\n+        Stream.of(rawEqDelete).map(Predicate::negate).reduce(Predicate::and).orElse(t -> true);\n+    when(deleteFilter.eqDeletedRowFilter()).thenReturn(eqDeletePredicate);\n+\n+    var isDeleted = ColumnarBatchUtil.buildIsDeleted(columnVectors, deleteFilter, 0, 5);\n+\n+    for (int i = 0; i < isDeleted.length; i++) {\n+      if (i == 2 || i == 3) { // 42 and 43\n+        assertThat(isDeleted[i]).isTrue();\n+      } else {\n+        assertThat(isDeleted[i]).isFalse();\n+      }\n+    }\n+  }\n+\n+  @Test\n+  void testBuildIsDeletedPositionAndEqualityDeletes() {\n+    // Define raw equality delete predicate ‚Äî delete rows where value == 42\n+    Predicate<InternalRow> rawEqDelete = row -> row.getInt(0) == 42;\n+\n+    // Mimic real eqDeletedRowFilter(): keep row only if it does NOT match delete condition\n+    Predicate<InternalRow> eqDeletePredicate =\n+        Stream.of(rawEqDelete).map(Predicate::negate).reduce(Predicate::and).orElse(t -> true);\n+    when(deleteFilter.eqDeletedRowFilter()).thenReturn(eqDeletePredicate);\n+\n+    PositionDeleteIndex deletedRowPos = mock(PositionDeleteIndex.class);\n+    when(deletedRowPos.isDeleted(1)).thenReturn(true); // 41\n+    when(deletedRowPos.isDeleted(4)).thenReturn(true); // 44\n+    when(deleteFilter.hasPosDeletes()).thenReturn(true);\n+    when(deleteFilter.deletedRowPositions()).thenReturn(deletedRowPos);\n+\n+    var isDeleted = ColumnarBatchUtil.buildIsDeleted(columnVectors, deleteFilter, 0, 5);\n+\n+    for (int i = 0; i < isDeleted.length; i++) {\n+      if (i == 0 || i == 3) {\n+        assertThat(isDeleted[i]).isFalse();\n+      } else {\n+        assertThat(isDeleted[i]).isTrue(); // 42, 41, 44 are deleted\n+      }\n+    }\n+  }\n+\n+  @Test\n+  void testBuildIsDeletedNoDeletes() {\n+    var result = ColumnarBatchUtil.buildIsDeleted(columnVectors, null, 0, 5);\n+    assertThat(result).isNotNull();\n+    for (int i = 0; i < 5; i++) {\n+      assertThat(result[i]).isFalse();\n+    }\n+  }\n+\n+  @Test\n+  void testRemoveExtraColumns() {\n+    ColumnVector[] vectors = new ColumnVector[5];\n+    for (int i = 0; i < 5; i++) {\n+      vectors[i] = mock(ColumnVector.class);\n+    }\n+    when(deleteFilter.expectedSchema()).thenReturn(mock(Schema.class));\n+    when(deleteFilter.expectedSchema().columns()).thenReturn(nCopies(3, null));\n+\n+    ColumnVector[] result = ColumnarBatchUtil.removeExtraColumns(deleteFilter, vectors);\n+    assertThat(result.length).isEqualTo(3);\n+  }\n+\n+  @Test\n+  void testRemoveExtraColumnsNotNeeded() {\n+    ColumnVector[] vectors = new ColumnVector[3];\n+    for (int i = 0; i < 3; i++) {\n+      vectors[i] = mock(ColumnVector.class);\n+    }\n+    when(deleteFilter.expectedSchema()).thenReturn(mock(Schema.class));\n+    when(deleteFilter.expectedSchema().columns()).thenReturn(nCopies(3, null));\n+\n+    ColumnVector[] result = ColumnarBatchUtil.removeExtraColumns(deleteFilter, vectors);\n+    assertThat(result.length).isEqualTo(3);\n+  }\n+\n+  private ColumnVector[] mockColumnVector() {\n+    // Create a mocked Int column vector with values: 40, 41, 42, 43, 44\n+    ColumnVector intVector = mock(ColumnVector.class);\n+    when(intVector.getInt(0)).thenReturn(40);\n+    when(intVector.getInt(1)).thenReturn(41);\n+    when(intVector.getInt(2)).thenReturn(42);\n+    when(intVector.getInt(3)).thenReturn(43);\n+    when(intVector.getInt(4)).thenReturn(44);\n+\n+    return new ColumnVector[] {intVector};\n+  }\n+}\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryEncodedVectorizedReads.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryEncodedVectorizedReads.java\nsimilarity index 99%\nrename from spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryEncodedVectorizedReads.java\nrename to spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryEncodedVectorizedReads.java\nindex fbf0fa378ea2..95ebf8d278d0 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryEncodedVectorizedReads.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryEncodedVectorizedReads.java\n@@ -16,7 +16,7 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n-package org.apache.iceberg.spark.data.parquet.vectorized;\n+package org.apache.iceberg.spark.data.vectorized.parquet;\n \n import static org.apache.iceberg.TableProperties.PARQUET_DICT_SIZE_BYTES;\n import static org.apache.iceberg.TableProperties.PARQUET_PAGE_ROW_LIMIT;\n\ndiff --git a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java\nsimilarity index 97%\nrename from spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java\nrename to spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java\nindex 070ac95aed65..9f9c2b961d38 100644\n--- a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java\n@@ -16,7 +16,7 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n-package org.apache.iceberg.spark.data.parquet.vectorized;\n+package org.apache.iceberg.spark.data.vectorized.parquet;\n \n import java.io.File;\n import java.io.IOException;\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetVectorizedReads.java\nsimilarity index 99%\nrename from spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java\nrename to spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetVectorizedReads.java\nindex ad8f80caf873..67712546f636 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetVectorizedReads.java\n@@ -16,7 +16,7 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n-package org.apache.iceberg.spark.data.parquet.vectorized;\n+package org.apache.iceberg.spark.data.vectorized.parquet;\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n\ndiff --git a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/TestColumnarBatchUtil.java b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/TestColumnarBatchUtil.java\nnew file mode 100644\nindex 000000000000..dadbe3e788b7\n--- /dev/null\n+++ b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/TestColumnarBatchUtil.java\n@@ -0,0 +1,302 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.spark.data.vectorized;\n+\n+import static java.util.Collections.nCopies;\n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.mockito.Mockito.mock;\n+import static org.mockito.Mockito.when;\n+\n+import java.util.Arrays;\n+import java.util.function.Predicate;\n+import java.util.stream.Stream;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.DeleteFilter;\n+import org.apache.iceberg.deletes.PositionDeleteIndex;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.vectorized.ColumnVector;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Test;\n+\n+public class TestColumnarBatchUtil {\n+\n+  private ColumnVector[] columnVectors;\n+  private DeleteFilter deleteFilter;\n+\n+  @BeforeEach\n+  public void before() {\n+    columnVectors = mockColumnVector();\n+    deleteFilter = mock(DeleteFilter.class);\n+  }\n+\n+  @Test\n+  public void testBuildRowIdMappingNoDeletes() {\n+    when(deleteFilter.hasPosDeletes()).thenReturn(true);\n+    PositionDeleteIndex deletedRowPos = mock(PositionDeleteIndex.class);\n+\n+    for (long i = 0; i <= 10; i++) {\n+      when(deletedRowPos.isDeleted(i)).thenReturn(false);\n+    }\n+\n+    when(deleteFilter.deletedRowPositions()).thenReturn(deletedRowPos);\n+    var rowIdMapping = ColumnarBatchUtil.buildRowIdMapping(columnVectors, deleteFilter, 0, 10);\n+    assertThat(rowIdMapping).isNull();\n+  }\n+\n+  @Test\n+  public void testBuildRowIdMappingPositionDeletesOnly() {\n+    when(deleteFilter.hasPosDeletes()).thenReturn(true);\n+    PositionDeleteIndex deletedRowPos = mock(PositionDeleteIndex.class);\n+\n+    // 5 position deletes\n+    for (long i = 98; i < 103; i++) {\n+      when(deletedRowPos.isDeleted(i)).thenReturn(true);\n+    }\n+\n+    when(deleteFilter.deletedRowPositions()).thenReturn(deletedRowPos);\n+\n+    var rowIdMapping = ColumnarBatchUtil.buildRowIdMapping(columnVectors, deleteFilter, 0, 200);\n+    assertThat(rowIdMapping).isNotNull();\n+\n+    int[] rowIds = (int[]) rowIdMapping.first();\n+    int liveRows = (int) rowIdMapping.second();\n+\n+    for (int id : rowIds) {\n+      assertThat(id < 98 || id > 102).isTrue();\n+    }\n+\n+    assertThat(rowIds.length).isEqualTo(200);\n+    assertThat(liveRows).isEqualTo(195);\n+  }\n+\n+  @Test\n+  public void testBuildRowIdMappingEqualityDeletesOnly() {\n+    // Define raw equality delete predicate ‚Äî delete rows where value == 42\n+    Predicate<InternalRow> rawEqDelete = row -> row.getInt(0) == 42;\n+\n+    // Mimic real eqDeletedRowFilter(): keep row only if it does NOT match delete condition\n+    Predicate<InternalRow> eqDeletePredicate =\n+        Stream.of(rawEqDelete).map(Predicate::negate).reduce(Predicate::and).orElse(t -> true);\n+\n+    // Mock DeleteFilter\n+    when(deleteFilter.hasPosDeletes()).thenReturn(false);\n+    when(deleteFilter.deletedRowPositions()).thenReturn(null);\n+    when(deleteFilter.eqDeletedRowFilter()).thenReturn(eqDeletePredicate);\n+\n+    var rowIdMapping = ColumnarBatchUtil.buildRowIdMapping(columnVectors, deleteFilter, 0, 5);\n+\n+    assertThat(rowIdMapping).isNotNull();\n+    int[] rowIds = (int[]) rowIdMapping.first();\n+    int liveRows = (Integer) rowIdMapping.second();\n+\n+    // Expect to keep positions 0, 1, 3, 4 ‚Üí values 40, 41, 43, 44\n+    assertThat(liveRows).isEqualTo(4);\n+    assertThat(Arrays.copyOf(rowIds, liveRows)).containsExactly(0, 1, 3, 4);\n+  }\n+\n+  @Test\n+  public void testBuildRowIdMappingPositionAndEqualityDeletes() {\n+\n+    // Define raw equality delete predicate ‚Äî delete rows where value == 42\n+    Predicate<InternalRow> rawEqDelete = row -> row.getInt(0) == 42;\n+\n+    // Mimic real eqDeletedRowFilter(): keep row only if it does NOT match delete condition\n+    Predicate<InternalRow> eqDeletePredicate =\n+        Stream.of(rawEqDelete).map(Predicate::negate).reduce(Predicate::and).orElse(t -> true);\n+    when(deleteFilter.eqDeletedRowFilter()).thenReturn(eqDeletePredicate);\n+\n+    PositionDeleteIndex deletedRowPos = mock(PositionDeleteIndex.class);\n+    when(deletedRowPos.isDeleted(1)).thenReturn(true); // 41\n+    when(deletedRowPos.isDeleted(4)).thenReturn(true); // 44\n+    when(deleteFilter.hasPosDeletes()).thenReturn(true);\n+    when(deleteFilter.deletedRowPositions()).thenReturn(deletedRowPos);\n+\n+    var rowIdMapping = ColumnarBatchUtil.buildRowIdMapping(columnVectors, deleteFilter, 0, 5);\n+\n+    assertThat(rowIdMapping).isNotNull();\n+    int[] rowIds = (int[]) rowIdMapping.first();\n+    int liveRows = (Integer) rowIdMapping.second();\n+\n+    assertThat(liveRows).isEqualTo(2);\n+    assertThat(Arrays.copyOf(rowIds, liveRows)).containsExactly(0, 3);\n+  }\n+\n+  @Test\n+  void testBuildRowIdMappingEmptyColumVectors() {\n+    ColumnVector[] columnVectorsZero = new ColumnVector[0];\n+\n+    PositionDeleteIndex deletedRowPos = mock(PositionDeleteIndex.class);\n+    when(deletedRowPos.isDeleted(1)).thenReturn(true);\n+    when(deletedRowPos.isDeleted(4)).thenReturn(true);\n+    when(deleteFilter.hasPosDeletes()).thenReturn(true);\n+    when(deleteFilter.deletedRowPositions()).thenReturn(deletedRowPos);\n+\n+    var rowIdMapping = ColumnarBatchUtil.buildRowIdMapping(columnVectorsZero, deleteFilter, 0, 0);\n+\n+    // Empty batch size, expect no rows deleted.\n+    assertThat(rowIdMapping).isNull();\n+  }\n+\n+  @Test\n+  void testBuildRowIdMapAllRowsDeleted() {\n+\n+    // Define raw equality delete predicate ‚Äî delete rows where value == 42 or 43\n+    Predicate<InternalRow> rawEqDelete = row -> row.getInt(0) == 42 || row.getInt(0) == 43;\n+\n+    // Mimic real eqDeletedRowFilter(): keep row only if it does NOT match delete condition\n+    Predicate<InternalRow> eqDeletePredicate =\n+        Stream.of(rawEqDelete).map(Predicate::negate).reduce(Predicate::and).orElse(t -> true);\n+    when(deleteFilter.eqDeletedRowFilter()).thenReturn(eqDeletePredicate);\n+\n+    PositionDeleteIndex deletedRowPos = mock(PositionDeleteIndex.class);\n+    when(deletedRowPos.isDeleted(0)).thenReturn(true); // 40\n+    when(deletedRowPos.isDeleted(1)).thenReturn(true); // 41\n+    when(deletedRowPos.isDeleted(4)).thenReturn(true); // 44\n+    when(deleteFilter.hasPosDeletes()).thenReturn(true);\n+    when(deleteFilter.deletedRowPositions()).thenReturn(deletedRowPos);\n+\n+    var rowIdMapping = ColumnarBatchUtil.buildRowIdMapping(columnVectors, deleteFilter, 0, 5);\n+\n+    assertThat(rowIdMapping).isNotNull();\n+    int[] rowIds = (int[]) rowIdMapping.first();\n+    int liveRows = (Integer) rowIdMapping.second();\n+\n+    // Expect all rows to be deleted\n+    assertThat(liveRows).isEqualTo(0);\n+    assertThat(rowIds).containsExactly(0, 0, 0, 0, 0);\n+  }\n+\n+  @Test\n+  void testBuildIsDeletedPositionDeletes() {\n+    PositionDeleteIndex deletedRowPos = mock(PositionDeleteIndex.class);\n+    when(deleteFilter.deletedRowPositions()).thenReturn(deletedRowPos);\n+\n+    for (long i = 98; i < 100; i++) {\n+      when(deletedRowPos.isDeleted(i)).thenReturn(true);\n+    }\n+\n+    var isDeleted = ColumnarBatchUtil.buildIsDeleted(columnVectors, deleteFilter, 0, 100);\n+\n+    assertThat(isDeleted).isNotNull();\n+    assertThat(isDeleted.length).isEqualTo(100);\n+\n+    for (int i = 98; i < 100; i++) {\n+      assertThat(isDeleted[i]).isTrue();\n+    }\n+\n+    for (int i = 0; i < 98; i++) {\n+      assertThat(isDeleted[i]).isFalse();\n+    }\n+  }\n+\n+  @Test\n+  void testBuildIsDeletedEqualityDeletes() {\n+    // Define raw equality delete predicate ‚Äî delete rows where value == 42 or 43\n+    Predicate<InternalRow> rawEqDelete = row -> row.getInt(0) == 42 || row.getInt(0) == 43;\n+\n+    // Mimic real eqDeletedRowFilter(): keep row only if it does NOT match delete condition\n+    Predicate<InternalRow> eqDeletePredicate =\n+        Stream.of(rawEqDelete).map(Predicate::negate).reduce(Predicate::and).orElse(t -> true);\n+    when(deleteFilter.eqDeletedRowFilter()).thenReturn(eqDeletePredicate);\n+\n+    var isDeleted = ColumnarBatchUtil.buildIsDeleted(columnVectors, deleteFilter, 0, 5);\n+\n+    for (int i = 0; i < isDeleted.length; i++) {\n+      if (i == 2 || i == 3) { // 42 and 43\n+        assertThat(isDeleted[i]).isTrue();\n+      } else {\n+        assertThat(isDeleted[i]).isFalse();\n+      }\n+    }\n+  }\n+\n+  @Test\n+  void testBuildIsDeletedPositionAndEqualityDeletes() {\n+    // Define raw equality delete predicate ‚Äî delete rows where value == 42\n+    Predicate<InternalRow> rawEqDelete = row -> row.getInt(0) == 42;\n+\n+    // Mimic real eqDeletedRowFilter(): keep row only if it does NOT match delete condition\n+    Predicate<InternalRow> eqDeletePredicate =\n+        Stream.of(rawEqDelete).map(Predicate::negate).reduce(Predicate::and).orElse(t -> true);\n+    when(deleteFilter.eqDeletedRowFilter()).thenReturn(eqDeletePredicate);\n+\n+    PositionDeleteIndex deletedRowPos = mock(PositionDeleteIndex.class);\n+    when(deletedRowPos.isDeleted(1)).thenReturn(true); // 41\n+    when(deletedRowPos.isDeleted(4)).thenReturn(true); // 44\n+    when(deleteFilter.hasPosDeletes()).thenReturn(true);\n+    when(deleteFilter.deletedRowPositions()).thenReturn(deletedRowPos);\n+\n+    var isDeleted = ColumnarBatchUtil.buildIsDeleted(columnVectors, deleteFilter, 0, 5);\n+\n+    for (int i = 0; i < isDeleted.length; i++) {\n+      if (i == 0 || i == 3) {\n+        assertThat(isDeleted[i]).isFalse();\n+      } else {\n+        assertThat(isDeleted[i]).isTrue(); // 42, 41, 44 are deleted\n+      }\n+    }\n+  }\n+\n+  @Test\n+  void testBuildIsDeletedNoDeletes() {\n+    var result = ColumnarBatchUtil.buildIsDeleted(columnVectors, null, 0, 5);\n+    assertThat(result).isNotNull();\n+    for (int i = 0; i < 5; i++) {\n+      assertThat(result[i]).isFalse();\n+    }\n+  }\n+\n+  @Test\n+  void testRemoveExtraColumns() {\n+    ColumnVector[] vectors = new ColumnVector[5];\n+    for (int i = 0; i < 5; i++) {\n+      vectors[i] = mock(ColumnVector.class);\n+    }\n+    when(deleteFilter.expectedSchema()).thenReturn(mock(Schema.class));\n+    when(deleteFilter.expectedSchema().columns()).thenReturn(nCopies(3, null));\n+\n+    ColumnVector[] result = ColumnarBatchUtil.removeExtraColumns(deleteFilter, vectors);\n+    assertThat(result.length).isEqualTo(3);\n+  }\n+\n+  @Test\n+  void testRemoveExtraColumnsNotNeeded() {\n+    ColumnVector[] vectors = new ColumnVector[3];\n+    for (int i = 0; i < 3; i++) {\n+      vectors[i] = mock(ColumnVector.class);\n+    }\n+    when(deleteFilter.expectedSchema()).thenReturn(mock(Schema.class));\n+    when(deleteFilter.expectedSchema().columns()).thenReturn(nCopies(3, null));\n+\n+    ColumnVector[] result = ColumnarBatchUtil.removeExtraColumns(deleteFilter, vectors);\n+    assertThat(result.length).isEqualTo(3);\n+  }\n+\n+  private ColumnVector[] mockColumnVector() {\n+    // Create a mocked Int column vector with values: 40, 41, 42, 43, 44\n+    ColumnVector intVector = mock(ColumnVector.class);\n+    when(intVector.getInt(0)).thenReturn(40);\n+    when(intVector.getInt(1)).thenReturn(41);\n+    when(intVector.getInt(2)).thenReturn(42);\n+    when(intVector.getInt(3)).thenReturn(43);\n+    when(intVector.getInt(4)).thenReturn(44);\n+\n+    return new ColumnVector[] {intVector};\n+  }\n+}\n\ndiff --git a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryEncodedVectorizedReads.java b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryEncodedVectorizedReads.java\nsimilarity index 99%\nrename from spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryEncodedVectorizedReads.java\nrename to spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryEncodedVectorizedReads.java\nindex fbf0fa378ea2..95ebf8d278d0 100644\n--- a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryEncodedVectorizedReads.java\n+++ b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryEncodedVectorizedReads.java\n@@ -16,7 +16,7 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n-package org.apache.iceberg.spark.data.parquet.vectorized;\n+package org.apache.iceberg.spark.data.vectorized.parquet;\n \n import static org.apache.iceberg.TableProperties.PARQUET_DICT_SIZE_BYTES;\n import static org.apache.iceberg.TableProperties.PARQUET_PAGE_ROW_LIMIT;\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java\nsimilarity index 97%\nrename from spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java\nrename to spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java\nindex 070ac95aed65..9f9c2b961d38 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java\n+++ b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetDictionaryFallbackToPlainEncodingVectorizedReads.java\n@@ -16,7 +16,7 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n-package org.apache.iceberg.spark.data.parquet.vectorized;\n+package org.apache.iceberg.spark.data.vectorized.parquet;\n \n import java.io.File;\n import java.io.IOException;\n\ndiff --git a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetVectorizedReads.java\nsimilarity index 99%\nrename from spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java\nrename to spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetVectorizedReads.java\nindex 9d443de74879..ff9d624ae68f 100644\n--- a/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/parquet/vectorized/TestParquetVectorizedReads.java\n+++ b/spark/v4.0/spark/src/test/java/org/apache/iceberg/spark/data/vectorized/parquet/TestParquetVectorizedReads.java\n@@ -16,7 +16,7 @@\n  * specific language governing permissions and limitations\n  * under the License.\n  */\n-package org.apache.iceberg.spark.data.parquet.vectorized;\n+package org.apache.iceberg.spark.data.vectorized.parquet;\n \n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12254",
    "pr_id": 12254,
    "issue_id": 11541,
    "repo": "apache/iceberg",
    "problem_statement": "Remove Dependency on Hadoop's Filesystem Class from Remove Orphan Files\n### Feature Request / Improvement\n\nCurrently RemoveOrphan Files is hardwired to use Hadoop FS classes when listing directories and attempting to locate orphan files. This is a problem for anyone working with a REST catalog since only the Catalog IO (Iceberg's FIleIO) will have credentials and the Hadoop conf will not. \r\n\r\nLuckily, I think we currently have everything we need in SupportPrefixOperations to allow a properly equipped FileIO instance to handle all of RemoveOrphanFiles without using any Hadoop Classes.\r\n\r\nI propose we branch whenever a Hadoop class is about to be used and use FileIO instead if it supports the proper operations. If it does not we should fall back to Hadoop classes.\n\n### Query engine\n\nSpark\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [ ] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 160,
    "test_files_count": 1,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "docs/docs/spark-procedures.md",
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/DeleteOrphanFilesSparkAction.java",
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/RemoveOrphanFilesProcedure.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java"
    ],
    "base_commit": "9fa50f3b82b321a98698c07977096d1638a9b185",
    "head_commit": "4b85d4b34cb3c7067df97d59f398461b5ae1cc4b",
    "repo_url": "https://github.com/apache/iceberg/pull/12254",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12254",
    "dockerfile": "",
    "pr_merged_at": "2025-06-03T15:01:11.000Z",
    "patch": "diff --git a/docs/docs/spark-procedures.md b/docs/docs/spark-procedures.md\nindex c1205d513aee..8d9c4c588f80 100644\n--- a/docs/docs/spark-procedures.md\n+++ b/docs/docs/spark-procedures.md\n@@ -317,6 +317,7 @@ Used to remove files which are not referenced in any metadata files of an Iceber\n | `equal_schemes` |    | map<string, string> | Mapping of file system schemes to be considered equal. Key is a comma-separated list of schemes and value is a scheme (defaults to `map('s3a,s3n','s3')`). |\n | `equal_authorities` |    | map<string, string> | Mapping of file system authorities to be considered equal. Key is a comma-separated list of authorities and value is an authority. |\n | `prefix_mismatch_mode` |    | string | Action behavior when location prefixes (schemes/authorities) mismatch: <ul><li>ERROR - throw an exception. (default) </li><li>IGNORE - no action.</li><li>DELETE - delete files.</li></ul> |  \n+| `prefix_listing` |    | boolean   | When true, use prefix-based file listing via the `SupportsPrefixOperations` interface. The Table FileIO implementation must support `SupportsPrefixOperations` when this flag is enabled (defaults to false) |\n \n #### Output\n \n@@ -370,6 +371,11 @@ CALL catalog_name.system.remove_orphan_files(table => 'db.sample', equal_schemes\n CALL catalog_name.system.remove_orphan_files(table => 'db.sample', equal_authorities => map('ns1', 'ns2'));\n ```\n \n+List all the files that are candidates for removal using prefix listing.\n+```sql\n+CALL catalog_name.system.remove_orphan_files(table => 'db.sample', prefix_listing => true);\n+```\n+\n ### `rewrite_data_files`\n \n Iceberg tracks each data file in a table. More data files leads to more metadata stored in manifest files, and small data files causes an unnecessary amount of metadata and less efficient queries from file open costs.\n\ndiff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/DeleteOrphanFilesSparkAction.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/DeleteOrphanFilesSparkAction.java\nindex 5fbb4117feb8..7fb6c6677396 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/DeleteOrphanFilesSparkAction.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/DeleteOrphanFilesSparkAction.java\n@@ -50,6 +50,7 @@\n import org.apache.iceberg.hadoop.HiddenPathFilter;\n import org.apache.iceberg.io.BulkDeletionFailureException;\n import org.apache.iceberg.io.SupportsBulkOperations;\n+import org.apache.iceberg.io.SupportsPrefixOperations;\n import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.relocated.com.google.common.base.Strings;\n@@ -121,6 +122,7 @@ public class DeleteOrphanFilesSparkAction extends BaseSparkAction<DeleteOrphanFi\n   private Dataset<Row> compareToFileList;\n   private Consumer<String> deleteFunc = null;\n   private ExecutorService deleteExecutorService = null;\n+  private boolean usePrefixListing = false;\n \n   DeleteOrphanFilesSparkAction(SparkSession spark, Table table) {\n     super(spark);\n@@ -206,6 +208,11 @@ public DeleteOrphanFilesSparkAction compareToFileList(Dataset<Row> files) {\n     return this;\n   }\n \n+  public DeleteOrphanFilesSparkAction usePrefixListing(boolean newUsePrefixListing) {\n+    this.usePrefixListing = newUsePrefixListing;\n+    return this;\n+  }\n+\n   private Dataset<String> filteredCompareToFileList() {\n     Dataset<Row> files = compareToFileList;\n     if (location != null) {\n@@ -303,39 +310,90 @@ private Dataset<String> listedFileDS() {\n     List<String> subDirs = Lists.newArrayList();\n     List<String> matchingFiles = Lists.newArrayList();\n \n-    Predicate<FileStatus> predicate = file -> file.getModificationTime() < olderThanTimestamp;\n     PathFilter pathFilter = PartitionAwareHiddenPathFilter.forSpecs(table.specs());\n \n-    // list at most MAX_DRIVER_LISTING_DEPTH levels and only dirs that have\n-    // less than MAX_DRIVER_LISTING_DIRECT_SUB_DIRS direct sub dirs on the driver\n-    listDirRecursively(\n-        location,\n-        predicate,\n-        hadoopConf.value(),\n-        MAX_DRIVER_LISTING_DEPTH,\n-        MAX_DRIVER_LISTING_DIRECT_SUB_DIRS,\n-        subDirs,\n-        pathFilter,\n-        matchingFiles);\n-\n-    JavaRDD<String> matchingFileRDD = sparkContext().parallelize(matchingFiles, 1);\n-\n-    if (subDirs.isEmpty()) {\n+    if (usePrefixListing) {\n+      Preconditions.checkArgument(\n+          table.io() instanceof SupportsPrefixOperations,\n+          \"Cannot use prefix listing with FileIO {} which does not support prefix operations.\",\n+          table.io());\n+\n+      Predicate<org.apache.iceberg.io.FileInfo> predicate =\n+          fileInfo -> fileInfo.createdAtMillis() < olderThanTimestamp;\n+      listDirRecursivelyWithFileIO(\n+          (SupportsPrefixOperations) table.io(), location, predicate, pathFilter, matchingFiles);\n+\n+      JavaRDD<String> matchingFileRDD = sparkContext().parallelize(matchingFiles, 1);\n       return spark().createDataset(matchingFileRDD.rdd(), Encoders.STRING());\n+    } else {\n+      Predicate<FileStatus> predicate = file -> file.getModificationTime() < olderThanTimestamp;\n+      // list at most MAX_DRIVER_LISTING_DEPTH levels and only dirs that have\n+      // less than MAX_DRIVER_LISTING_DIRECT_SUB_DIRS direct sub dirs on the driver\n+      listDirRecursivelyWithHadoop(\n+          location,\n+          predicate,\n+          hadoopConf.value(),\n+          MAX_DRIVER_LISTING_DEPTH,\n+          MAX_DRIVER_LISTING_DIRECT_SUB_DIRS,\n+          subDirs,\n+          pathFilter,\n+          matchingFiles);\n+\n+      JavaRDD<String> matchingFileRDD = sparkContext().parallelize(matchingFiles, 1);\n+\n+      if (subDirs.isEmpty()) {\n+        return spark().createDataset(matchingFileRDD.rdd(), Encoders.STRING());\n+      }\n+\n+      int parallelism = Math.min(subDirs.size(), listingParallelism);\n+      JavaRDD<String> subDirRDD = sparkContext().parallelize(subDirs, parallelism);\n+\n+      Broadcast<SerializableConfiguration> conf = sparkContext().broadcast(hadoopConf);\n+      ListDirsRecursively listDirs = new ListDirsRecursively(conf, olderThanTimestamp, pathFilter);\n+      JavaRDD<String> matchingLeafFileRDD = subDirRDD.mapPartitions(listDirs);\n+\n+      JavaRDD<String> completeMatchingFileRDD = matchingFileRDD.union(matchingLeafFileRDD);\n+      return spark().createDataset(completeMatchingFileRDD.rdd(), Encoders.STRING());\n+    }\n+  }\n+\n+  private static void listDirRecursivelyWithFileIO(\n+      SupportsPrefixOperations io,\n+      String dir,\n+      Predicate<org.apache.iceberg.io.FileInfo> predicate,\n+      PathFilter pathFilter,\n+      List<String> matchingFiles) {\n+    String listPath = dir;\n+    if (!dir.endsWith(\"/\")) {\n+      listPath = dir + \"/\";\n+    }\n+\n+    Iterable<org.apache.iceberg.io.FileInfo> files = io.listPrefix(listPath);\n+    for (org.apache.iceberg.io.FileInfo file : files) {\n+      Path path = new Path(file.location());\n+      if (!isHiddenPath(dir, path, pathFilter) && predicate.test(file)) {\n+        matchingFiles.add(file.location());\n+      }\n     }\n+  }\n \n-    int parallelism = Math.min(subDirs.size(), listingParallelism);\n-    JavaRDD<String> subDirRDD = sparkContext().parallelize(subDirs, parallelism);\n+  private static boolean isHiddenPath(String baseDir, Path path, PathFilter pathFilter) {\n+    boolean isHiddenPath = false;\n+    Path currentPath = path;\n+    while (currentPath.getParent().toString().contains(baseDir)) {\n+      if (!pathFilter.accept(currentPath)) {\n+        isHiddenPath = true;\n+        break;\n+      }\n \n-    Broadcast<SerializableConfiguration> conf = sparkContext().broadcast(hadoopConf);\n-    ListDirsRecursively listDirs = new ListDirsRecursively(conf, olderThanTimestamp, pathFilter);\n-    JavaRDD<String> matchingLeafFileRDD = subDirRDD.mapPartitions(listDirs);\n+      currentPath = currentPath.getParent();\n+    }\n \n-    JavaRDD<String> completeMatchingFileRDD = matchingFileRDD.union(matchingLeafFileRDD);\n-    return spark().createDataset(completeMatchingFileRDD.rdd(), Encoders.STRING());\n+    return isHiddenPath;\n   }\n \n-  private static void listDirRecursively(\n+  @VisibleForTesting\n+  static void listDirRecursivelyWithHadoop(\n       String dir,\n       Predicate<FileStatus> predicate,\n       Configuration conf,\n@@ -372,7 +430,7 @@ private static void listDirRecursively(\n       }\n \n       for (String subDir : subDirs) {\n-        listDirRecursively(\n+        listDirRecursivelyWithHadoop(\n             subDir,\n             predicate,\n             conf,\n@@ -458,7 +516,7 @@ public Iterator<String> call(Iterator<String> dirs) throws Exception {\n       Predicate<FileStatus> predicate = file -> file.getModificationTime() < olderThanTimestamp;\n \n       while (dirs.hasNext()) {\n-        listDirRecursively(\n+        listDirRecursivelyWithHadoop(\n             dirs.next(),\n             predicate,\n             hadoopConf.value().value(),\n\ndiff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/RemoveOrphanFilesProcedure.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/RemoveOrphanFilesProcedure.java\nindex 6609efa95eb1..6e70dc3ccd9c 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/RemoveOrphanFilesProcedure.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/procedures/RemoveOrphanFilesProcedure.java\n@@ -63,6 +63,8 @@ public class RemoveOrphanFilesProcedure extends BaseProcedure {\n         ProcedureParameter.optional(\"equal_schemes\", STRING_MAP),\n         ProcedureParameter.optional(\"equal_authorities\", STRING_MAP),\n         ProcedureParameter.optional(\"prefix_mismatch_mode\", DataTypes.StringType),\n+        // List files with prefix operations. Default is false.\n+        ProcedureParameter.optional(\"prefix_listing\", DataTypes.BooleanType)\n       };\n \n   private static final StructType OUTPUT_TYPE =\n@@ -136,6 +138,8 @@ public InternalRow[] call(InternalRow args) {\n     PrefixMismatchMode prefixMismatchMode =\n         args.isNullAt(8) ? null : PrefixMismatchMode.fromString(args.getString(8));\n \n+    boolean prefixListing = args.isNullAt(9) ? false : args.getBoolean(9);\n+\n     return withIcebergTable(\n         tableIdent,\n         table -> {\n@@ -182,6 +186,8 @@ public InternalRow[] call(InternalRow args) {\n             action.prefixMismatchMode(prefixMismatchMode);\n           }\n \n+          action.usePrefixListing(prefixListing);\n+\n           DeleteOrphanFiles.Result result = action.execute();\n \n           return toOutputRows(result);\n",
    "test_patch": "diff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java\nindex d3508283dd8c..99eaf1a5337e 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java\n@@ -22,6 +22,10 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n import static org.assertj.core.api.Assumptions.assumeThat;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.ArgumentMatchers.anyInt;\n+import static org.mockito.ArgumentMatchers.anyList;\n+import static org.mockito.ArgumentMatchers.anyString;\n \n import java.io.File;\n import java.io.IOException;\n@@ -38,11 +42,14 @@\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.function.Predicate;\n import java.util.stream.Collectors;\n+import java.util.stream.Stream;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.hadoop.fs.FileStatus;\n import org.apache.hadoop.fs.FileSystem;\n import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.PathFilter;\n import org.apache.iceberg.Files;\n import org.apache.iceberg.GenericBlobMetadata;\n import org.apache.iceberg.GenericStatisticsFile;\n@@ -55,6 +62,7 @@\n import org.apache.iceberg.StatisticsFile;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TestHelpers;\n import org.apache.iceberg.Transaction;\n import org.apache.iceberg.actions.DeleteOrphanFiles;\n import org.apache.iceberg.catalog.Namespace;\n@@ -86,6 +94,8 @@\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n import org.junit.jupiter.api.io.TempDir;\n+import org.mockito.MockedStatic;\n+import org.mockito.Mockito;\n \n @ExtendWith(ParameterizedTestExtension.class)\n public abstract class TestRemoveOrphanFilesAction extends TestBase {\n@@ -104,9 +114,15 @@ public abstract class TestRemoveOrphanFilesAction extends TestBase {\n   protected Map<String, String> properties;\n   @Parameter private int formatVersion;\n \n-  @Parameters(name = \"formatVersion = {0}\")\n+  @Parameter(index = 1)\n+  private boolean usePrefixListing;\n+\n+  @Parameters(name = \"formatVersion = {0}, usePrefixListing = {1}\")\n   protected static List<Object> parameters() {\n-    return Arrays.asList(2, 3);\n+    return TestHelpers.ALL_VERSIONS.stream()\n+        .filter(version -> version > 1)\n+        .flatMap(version -> Stream.of(new Object[] {version, true}, new Object[] {version, false}))\n+        .collect(Collectors.toList());\n   }\n \n   @BeforeEach\n@@ -158,7 +174,11 @@ public void testDryRun() throws IOException {\n     SparkActions actions = SparkActions.get();\n \n     DeleteOrphanFiles.Result result1 =\n-        actions.deleteOrphanFiles(table).deleteWith(s -> {}).execute();\n+        actions\n+            .deleteOrphanFiles(table)\n+            .usePrefixListing(usePrefixListing)\n+            .deleteWith(s -> {})\n+            .execute();\n     assertThat(result1.orphanFileLocations())\n         .as(\"Default olderThan interval should be safe\")\n         .isEmpty();\n@@ -166,6 +186,7 @@ public void testDryRun() throws IOException {\n     DeleteOrphanFiles.Result result2 =\n         actions\n             .deleteOrphanFiles(table)\n+            .usePrefixListing(usePrefixListing)\n             .olderThan(System.currentTimeMillis())\n             .deleteWith(s -> {})\n             .execute();\n@@ -177,7 +198,11 @@ public void testDryRun() throws IOException {\n         .isTrue();\n \n     DeleteOrphanFiles.Result result3 =\n-        actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n+        actions\n+            .deleteOrphanFiles(table)\n+            .usePrefixListing(usePrefixListing)\n+            .olderThan(System.currentTimeMillis())\n+            .execute();\n     assertThat(result3.orphanFileLocations())\n         .as(\"Action should delete 1 file\")\n         .isEqualTo(invalidFiles);\n@@ -237,7 +262,11 @@ public void testAllValidFilesAreKept() throws IOException {\n     SparkActions actions = SparkActions.get();\n \n     DeleteOrphanFiles.Result result =\n-        actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n+        actions\n+            .deleteOrphanFiles(table)\n+            .usePrefixListing(usePrefixListing)\n+            .olderThan(System.currentTimeMillis())\n+            .execute();\n \n     assertThat(result.orphanFileLocations()).as(\"Should delete 4 files\").hasSize(4);\n \n@@ -259,6 +288,9 @@ public void testAllValidFilesAreKept() throws IOException {\n \n   @TestTemplate\n   public void orphanedFileRemovedWithParallelTasks() {\n+    assumeThat(usePrefixListing)\n+        .as(\"Should not test both prefix listing and Hadoop file listing (redundant)\")\n+        .isEqualTo(false);\n     Table table = TABLES.create(SCHEMA, SPEC, properties, tableLocation);\n \n     List<ThreeColumnRecord> records1 =\n@@ -321,6 +353,9 @@ public void orphanedFileRemovedWithParallelTasks() {\n \n   @TestTemplate\n   public void testWapFilesAreKept() {\n+    assumeThat(usePrefixListing)\n+        .as(\"Should not test both prefix listing and Hadoop file listing (redundant)\")\n+        .isEqualTo(false);\n     assumeThat(formatVersion).as(\"currently fails with DVs\").isEqualTo(2);\n     Map<String, String> props = Maps.newHashMap();\n     props.put(TableProperties.WRITE_AUDIT_PUBLISH_ENABLED, \"true\");\n@@ -334,19 +369,21 @@ public void testWapFilesAreKept() {\n     // normal write\n     df.select(\"c1\", \"c2\", \"c3\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n \n-    spark.conf().set(SparkSQLProperties.WAP_ID, \"1\");\n+    withSQLConf(\n+        Map.of(SparkSQLProperties.WAP_ID, \"1\"),\n+        () -> {\n+          // wap write\n+          df.select(\"c1\", \"c2\", \"c3\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n \n-    // wap write\n-    df.select(\"c1\", \"c2\", \"c3\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n+          Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n+          List<ThreeColumnRecord> actualRecords =\n+              resultDF.as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n \n-    Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n-    List<ThreeColumnRecord> actualRecords =\n-        resultDF.as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n-\n-    // TODO: currently fails because DVs delete stuff from WAP branch\n-    assertThat(actualRecords)\n-        .as(\"Should not return data from the staged snapshot\")\n-        .isEqualTo(records);\n+          // TODO: currently fails because DVs delete stuff from WAP branch\n+          assertThat(actualRecords)\n+              .as(\"Should not return data from the staged snapshot\")\n+              .isEqualTo(records);\n+        });\n \n     waitUntilAfter(System.currentTimeMillis());\n \n@@ -379,7 +416,11 @@ public void testMetadataFolderIsIntact() {\n     SparkActions actions = SparkActions.get();\n \n     DeleteOrphanFiles.Result result =\n-        actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n+        actions\n+            .deleteOrphanFiles(table)\n+            .usePrefixListing(usePrefixListing)\n+            .olderThan(System.currentTimeMillis())\n+            .execute();\n \n     assertThat(result.orphanFileLocations()).as(\"Should delete 1 file\").hasSize(1);\n \n@@ -413,7 +454,11 @@ public void testOlderThanTimestamp() {\n     SparkActions actions = SparkActions.get();\n \n     DeleteOrphanFiles.Result result =\n-        actions.deleteOrphanFiles(table).olderThan(timestamp).execute();\n+        actions\n+            .deleteOrphanFiles(table)\n+            .usePrefixListing(usePrefixListing)\n+            .olderThan(timestamp)\n+            .execute();\n \n     assertThat(result.orphanFileLocations()).as(\"Should delete only 2 files\").hasSize(2);\n   }\n@@ -439,7 +484,11 @@ public void testRemoveUnreachableMetadataVersionFiles() {\n     SparkActions actions = SparkActions.get();\n \n     DeleteOrphanFiles.Result result =\n-        actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n+        actions\n+            .deleteOrphanFiles(table)\n+            .usePrefixListing(usePrefixListing)\n+            .olderThan(System.currentTimeMillis())\n+            .execute();\n \n     assertThat(result.orphanFileLocations())\n         .containsExactly(tableLocation + \"metadata/v1.metadata.json\");\n@@ -472,7 +521,11 @@ public void testManyTopLevelPartitions() {\n     SparkActions actions = SparkActions.get();\n \n     DeleteOrphanFiles.Result result =\n-        actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n+        actions\n+            .deleteOrphanFiles(table)\n+            .usePrefixListing(usePrefixListing)\n+            .olderThan(System.currentTimeMillis())\n+            .execute();\n \n     assertThat(result.orphanFileLocations()).as(\"Should not delete any files\").isEmpty();\n \n@@ -498,7 +551,11 @@ public void testManyLeafPartitions() {\n     SparkActions actions = SparkActions.get();\n \n     DeleteOrphanFiles.Result result =\n-        actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n+        actions\n+            .deleteOrphanFiles(table)\n+            .usePrefixListing(usePrefixListing)\n+            .olderThan(System.currentTimeMillis())\n+            .execute();\n \n     assertThat(result.orphanFileLocations()).as(\"Should not delete any files\").isEmpty();\n \n@@ -534,7 +591,11 @@ public void testHiddenPartitionPaths() {\n     SparkActions actions = SparkActions.get();\n \n     DeleteOrphanFiles.Result result =\n-        actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n+        actions\n+            .deleteOrphanFiles(table)\n+            .usePrefixListing(usePrefixListing)\n+            .olderThan(System.currentTimeMillis())\n+            .execute();\n \n     assertThat(result.orphanFileLocations()).as(\"Should delete 2 files\").hasSize(2);\n   }\n@@ -570,7 +631,11 @@ public void testHiddenPartitionPathsWithPartitionEvolution() {\n     SparkActions actions = SparkActions.get();\n \n     DeleteOrphanFiles.Result result =\n-        actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n+        actions\n+            .deleteOrphanFiles(table)\n+            .usePrefixListing(usePrefixListing)\n+            .olderThan(System.currentTimeMillis())\n+            .execute();\n \n     assertThat(result.orphanFileLocations()).as(\"Should delete 2 files\").hasSize(2);\n   }\n@@ -605,7 +670,11 @@ public void testHiddenPathsStartingWithPartitionNamesAreIgnored() throws IOExcep\n     SparkActions actions = SparkActions.get();\n \n     DeleteOrphanFiles.Result result =\n-        actions.deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n+        actions\n+            .deleteOrphanFiles(table)\n+            .usePrefixListing(usePrefixListing)\n+            .olderThan(System.currentTimeMillis())\n+            .execute();\n \n     assertThat(result.orphanFileLocations()).as(\"Should delete 0 files\").isEmpty();\n     assertThat(fs.exists(pathToFileInHiddenFolder)).isTrue();\n@@ -624,6 +693,9 @@ private List<String> snapshotFiles(long snapshotId) {\n \n   @TestTemplate\n   public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException {\n+    assumeThat(usePrefixListing)\n+        .as(\"Should not test both prefix listing and Hadoop file listing (redundant)\")\n+        .isEqualTo(false);\n     Table table =\n         TABLES.create(\n             SCHEMA, PartitionSpec.unpartitioned(), properties, tableDir.getAbsolutePath());\n@@ -707,7 +779,11 @@ public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException\n     table.refresh();\n \n     DeleteOrphanFiles.Result result =\n-        SparkActions.get().deleteOrphanFiles(table).olderThan(System.currentTimeMillis()).execute();\n+        SparkActions.get()\n+            .deleteOrphanFiles(table)\n+            .usePrefixListing(usePrefixListing)\n+            .olderThan(System.currentTimeMillis())\n+            .execute();\n \n     assertThat(result.orphanFileLocations()).as(\"Should delete only 1 file\").hasSize(1);\n \n@@ -739,6 +815,7 @@ public void testHiveCatalogTable() throws IOException {\n     DeleteOrphanFiles.Result result =\n         SparkActions.get()\n             .deleteOrphanFiles(table)\n+            .usePrefixListing(usePrefixListing)\n             .olderThan(System.currentTimeMillis() + 1000)\n             .execute();\n     assertThat(result.orphanFileLocations())\n@@ -748,6 +825,9 @@ public void testHiveCatalogTable() throws IOException {\n \n   @TestTemplate\n   public void testGarbageCollectionDisabled() {\n+    assumeThat(usePrefixListing)\n+        .as(\"Should not test both prefix listing and Hadoop file listing (redundant)\")\n+        .isEqualTo(false);\n     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), properties, tableLocation);\n \n     List<ThreeColumnRecord> records =\n@@ -767,6 +847,9 @@ public void testGarbageCollectionDisabled() {\n \n   @TestTemplate\n   public void testCompareToFileList() throws IOException {\n+    assumeThat(usePrefixListing)\n+        .as(\"Should not test both prefix listing and Hadoop file listing (redundant)\")\n+        .isEqualTo(false);\n     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), properties, tableLocation);\n \n     List<ThreeColumnRecord> records =\n@@ -897,6 +980,9 @@ protected long waitUntilAfter(long timestampMillis) {\n \n   @TestTemplate\n   public void testRemoveOrphanFilesWithStatisticFiles() throws Exception {\n+    assumeThat(usePrefixListing)\n+        .as(\"Should not test both prefix listing and Hadoop file listing (redundant)\")\n+        .isEqualTo(false);\n     assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n     Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), properties, tableLocation);\n \n@@ -1057,6 +1143,39 @@ public void testRemoveOrphanFileActionWithDeleteMode() {\n         DeleteOrphanFiles.PrefixMismatchMode.DELETE);\n   }\n \n+  @TestTemplate\n+  public void testDefaultToHadoopListing() {\n+    assumeThat(usePrefixListing)\n+        .as(\n+            \"This test verifies default listing behavior and does not require prefix listing to be enabled.\")\n+        .isEqualTo(false);\n+    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), properties, tableLocation);\n+\n+    List<ThreeColumnRecord> records =\n+        Lists.newArrayList(new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"));\n+    Dataset<Row> df = spark.createDataFrame(records, ThreeColumnRecord.class).coalesce(1);\n+    df.select(\"c1\", \"c2\", \"c3\").write().format(\"iceberg\").mode(\"append\").save(tableLocation);\n+\n+    DeleteOrphanFilesSparkAction deleteOrphanFilesSparkAction =\n+        SparkActions.get().deleteOrphanFiles(table);\n+    DeleteOrphanFilesSparkAction spyAction = Mockito.spy(deleteOrphanFilesSparkAction);\n+    try (MockedStatic<DeleteOrphanFilesSparkAction> mockedStatic =\n+        Mockito.mockStatic(DeleteOrphanFilesSparkAction.class)) {\n+      spyAction.execute();\n+      mockedStatic.verify(\n+          () ->\n+              DeleteOrphanFilesSparkAction.listDirRecursivelyWithHadoop(\n+                  anyString(),\n+                  any(Predicate.class),\n+                  any(Configuration.class),\n+                  anyInt(),\n+                  anyInt(),\n+                  anyList(),\n+                  any(PathFilter.class),\n+                  anyList()));\n+    }\n+  }\n+\n   protected String randomName(String prefix) {\n     return prefix + UUID.randomUUID().toString().replace(\"-\", \"\");\n   }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12238",
    "pr_id": 12238,
    "issue_id": 10392,
    "repo": "apache/iceberg",
    "problem_statement": "Variant Data Type Support\n### Proposed Change\r\n\r\nWe would like to propose to add Variant type to Iceberg data types. \r\n\r\nVariant data types allow for the efficient binary encoding of dynamic semi-structured data such as JSON, Avro,Parquet, etc. By encoding semi-structured data as a variant column, we retain the flexibility of the source data, while allowing query engines to more efficiently operate on the data.\r\n\r\nWith the support of Variant type, such data can be encoded in an efficient binary representation internally for better performance. Without that, we need to parse the data in its format inefficiently.\r\n\r\nThis will allow the following use cases:\r\n\r\n- Create an Iceberg table with a Variant column\r\n`CREATE OR REPLACE TABLE car_sales(record Variant);`\r\n- Insert semi-structured data into the Variant column\r\n`INSERT INTO car_sales SELECT PARSE_JSON(<json_string>)`\r\n- Query against the semi-structured data\r\n`SELECT VARIANT_GET(record, '$.dealer.ship', 'string') FROM car_sales`\r\n\r\n\r\n\r\n\r\n### Proposal document\r\nhttps://docs.google.com/document/d/1sq70XDiWJ2DemWyA5dVB80gKzwi0CWoM0LOWM7VJVd8/edit?tab=t.0\r\n\r\n### Specifications\r\n\r\n- [X] Table\r\n- [ ] View\r\n- [ ] REST\r\n- [ ] Puffin\r\n- [ ] Encryption\r\n- [ ] Other",
    "issue_word_count": 170,
    "test_files_count": 6,
    "non_test_files_count": 14,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/avro/ApplyNameMapping.java",
      "core/src/main/java/org/apache/iceberg/avro/Avro.java",
      "core/src/main/java/org/apache/iceberg/avro/AvroCustomOrderSchemaVisitor.java",
      "core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java",
      "core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java",
      "core/src/main/java/org/apache/iceberg/avro/AvroSchemaVisitor.java",
      "core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java",
      "core/src/main/java/org/apache/iceberg/avro/HasIds.java",
      "core/src/main/java/org/apache/iceberg/avro/MissingIds.java",
      "core/src/main/java/org/apache/iceberg/avro/PruneColumns.java",
      "core/src/main/java/org/apache/iceberg/avro/RemoveIds.java",
      "core/src/main/java/org/apache/iceberg/avro/SchemaToType.java",
      "core/src/main/java/org/apache/iceberg/avro/TypeToSchema.java",
      "core/src/main/java/org/apache/iceberg/avro/VariantLogicalType.java",
      "core/src/test/java/org/apache/iceberg/avro/AvroTestHelpers.java",
      "core/src/test/java/org/apache/iceberg/avro/TestAvroNameMapping.java",
      "core/src/test/java/org/apache/iceberg/avro/TestAvroSchemaProjection.java",
      "core/src/test/java/org/apache/iceberg/avro/TestHasIds.java",
      "core/src/test/java/org/apache/iceberg/avro/TestPruneColumns.java",
      "core/src/test/java/org/apache/iceberg/avro/TestSchemaConversions.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/avro/AvroTestHelpers.java",
      "core/src/test/java/org/apache/iceberg/avro/TestAvroNameMapping.java",
      "core/src/test/java/org/apache/iceberg/avro/TestAvroSchemaProjection.java",
      "core/src/test/java/org/apache/iceberg/avro/TestHasIds.java",
      "core/src/test/java/org/apache/iceberg/avro/TestPruneColumns.java",
      "core/src/test/java/org/apache/iceberg/avro/TestSchemaConversions.java"
    ],
    "base_commit": "a50ec923f3d928f67e2a4a361c0d1162341aa084",
    "head_commit": "dcc7f70b94d1b156f0bdf3d920b4b6b0f4b39ae1",
    "repo_url": "https://github.com/apache/iceberg/pull/12238",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12238",
    "dockerfile": "",
    "pr_merged_at": "2025-03-04T21:03:41.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/avro/ApplyNameMapping.java b/core/src/main/java/org/apache/iceberg/avro/ApplyNameMapping.java\nindex ce619c47fabe..de45a2bbaee1 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/ApplyNameMapping.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/ApplyNameMapping.java\n@@ -128,6 +128,11 @@ public Schema map(Schema map, Schema value) {\n     return map;\n   }\n \n+  @Override\n+  public Schema variant(Schema variant, Schema metadata, Schema value) {\n+    return variant;\n+  }\n+\n   @Override\n   public Schema primitive(Schema primitive) {\n     return primitive;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/Avro.java b/core/src/main/java/org/apache/iceberg/avro/Avro.java\nindex 557a20daf303..2a3ea11590bb 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/Avro.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/Avro.java\n@@ -86,6 +86,7 @@ private enum Codec {\n \n   static {\n     LogicalTypes.register(LogicalMap.NAME, schema -> LogicalMap.get());\n+    LogicalTypes.register(VariantLogicalType.NAME, schema -> VariantLogicalType.get());\n     DEFAULT_MODEL.addLogicalTypeConversion(new Conversions.DecimalConversion());\n     DEFAULT_MODEL.addLogicalTypeConversion(new UUIDConversion());\n   }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/AvroCustomOrderSchemaVisitor.java b/core/src/main/java/org/apache/iceberg/avro/AvroCustomOrderSchemaVisitor.java\nindex 69159b65be3e..f579381ac0c8 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/AvroCustomOrderSchemaVisitor.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/AvroCustomOrderSchemaVisitor.java\n@@ -27,6 +27,9 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n \n abstract class AvroCustomOrderSchemaVisitor<T, F> {\n+  private static final String METADATA = \"metadata\";\n+  private static final String VALUE = \"value\";\n+\n   public static <T, F> T visit(Schema schema, AvroCustomOrderSchemaVisitor<T, F> visitor) {\n     switch (schema.getType()) {\n       case RECORD:\n@@ -35,20 +38,29 @@ public static <T, F> T visit(Schema schema, AvroCustomOrderSchemaVisitor<T, F> v\n         Preconditions.checkState(\n             !visitor.recordLevels.contains(name), \"Cannot process recursive Avro record %s\", name);\n \n-        visitor.recordLevels.push(name);\n-\n-        List<Schema.Field> fields = schema.getFields();\n-        List<String> names = Lists.newArrayListWithExpectedSize(fields.size());\n-        List<Supplier<F>> results = Lists.newArrayListWithExpectedSize(fields.size());\n-        for (Schema.Field field : schema.getFields()) {\n-          names.add(field.name());\n-          results.add(new VisitFieldFuture<>(field, visitor));\n+        if (schema.getLogicalType() instanceof VariantLogicalType) {\n+          Preconditions.checkArgument(\n+              AvroSchemaUtil.isVariantSchema(schema), \"Invalid variant record: %s\", schema);\n+\n+          return visitor.variant(\n+              schema,\n+              new VisitFuture<>(schema.getField(METADATA).schema(), visitor),\n+              new VisitFuture<>(schema.getField(VALUE).schema(), visitor));\n+        } else {\n+          visitor.recordLevels.push(name);\n+\n+          List<Schema.Field> fields = schema.getFields();\n+          List<String> names = Lists.newArrayListWithExpectedSize(fields.size());\n+          List<Supplier<F>> results = Lists.newArrayListWithExpectedSize(fields.size());\n+          for (Schema.Field field : schema.getFields()) {\n+            names.add(field.name());\n+            results.add(new VisitFieldFuture<>(field, visitor));\n+          }\n+\n+          visitor.recordLevels.pop();\n+          return visitor.record(schema, names, Iterables.transform(results, Supplier::get));\n         }\n \n-        visitor.recordLevels.pop();\n-\n-        return visitor.record(schema, names, Iterables.transform(results, Supplier::get));\n-\n       case UNION:\n         List<Schema> types = schema.getTypes();\n         List<Supplier<T>> options = Lists.newArrayListWithExpectedSize(types.size());\n@@ -90,6 +102,10 @@ public T map(Schema map, Supplier<T> value) {\n     return null;\n   }\n \n+  public T variant(Schema variant, Supplier<T> metadataResult, Supplier<T> valueResult) {\n+    throw new UnsupportedOperationException(\"Unsupported type: variant\");\n+  }\n+\n   public T primitive(Schema primitive) {\n     return null;\n   }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java b/core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java\nindex ba3c6fece7f9..0db8d7dd5f9f 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/AvroEncoderUtil.java\n@@ -39,6 +39,7 @@ private AvroEncoderUtil() {}\n \n   static {\n     LogicalTypes.register(LogicalMap.NAME, schema -> LogicalMap.get());\n+    LogicalTypes.register(VariantLogicalType.NAME, schema -> VariantLogicalType.get());\n   }\n \n   private static final byte[] MAGIC_BYTES = new byte[] {(byte) 0xC2, (byte) 0x01};\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java b/core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java\nindex 032d63105dfe..cdc50959ac8a 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/AvroSchemaUtil.java\n@@ -219,6 +219,20 @@ public static boolean isKeyValueSchema(Schema schema) {\n     return schema.getType() == RECORD && schema.getFields().size() == 2;\n   }\n \n+  static boolean isVariantSchema(Schema schema) {\n+    if (schema.getType() != Schema.Type.RECORD || schema.getFields().size() != 2) {\n+      return false;\n+    }\n+\n+    Schema.Field metadataField = schema.getField(\"metadata\");\n+    Schema.Field valueField = schema.getField(\"value\");\n+\n+    return metadataField != null\n+        && metadataField.schema().getType() == Schema.Type.BYTES\n+        && valueField != null\n+        && valueField.schema().getType() == Schema.Type.BYTES;\n+  }\n+\n   static Schema createMap(int keyId, Schema keySchema, int valueId, Schema valueSchema) {\n     String keyValueName = \"k\" + keyId + \"_v\" + valueId;\n \n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/AvroSchemaVisitor.java b/core/src/main/java/org/apache/iceberg/avro/AvroSchemaVisitor.java\nindex 9819924ffa99..df3ca72fa09f 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/AvroSchemaVisitor.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/AvroSchemaVisitor.java\n@@ -25,6 +25,9 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n \n public abstract class AvroSchemaVisitor<T> {\n+  private static final String METADATA = \"metadata\";\n+  private static final String VALUE = \"value\";\n+\n   public static <T> T visit(Schema schema, AvroSchemaVisitor<T> visitor) {\n     switch (schema.getType()) {\n       case RECORD:\n@@ -33,21 +36,30 @@ public static <T> T visit(Schema schema, AvroSchemaVisitor<T> visitor) {\n         Preconditions.checkState(\n             !visitor.recordLevels.contains(name), \"Cannot process recursive Avro record %s\", name);\n \n-        visitor.recordLevels.push(name);\n+        if (schema.getLogicalType() instanceof VariantLogicalType) {\n+          Preconditions.checkArgument(\n+              AvroSchemaUtil.isVariantSchema(schema), \"Invalid variant record: %s\", schema);\n \n-        List<Schema.Field> fields = schema.getFields();\n-        List<String> names = Lists.newArrayListWithExpectedSize(fields.size());\n-        List<T> results = Lists.newArrayListWithExpectedSize(fields.size());\n-        for (Schema.Field field : schema.getFields()) {\n-          names.add(field.name());\n-          T result = visitWithName(field.name(), field.schema(), visitor);\n-          results.add(result);\n+          return visitor.variant(\n+              schema,\n+              visit(schema.getField(METADATA).schema(), visitor),\n+              visit(schema.getField(VALUE).schema(), visitor));\n+        } else {\n+          visitor.recordLevels.push(name);\n+\n+          List<Schema.Field> fields = schema.getFields();\n+          List<String> names = Lists.newArrayListWithExpectedSize(fields.size());\n+          List<T> results = Lists.newArrayListWithExpectedSize(fields.size());\n+          for (Schema.Field field : schema.getFields()) {\n+            names.add(field.name());\n+            T result = visitWithName(field.name(), field.schema(), visitor);\n+            results.add(result);\n+          }\n+\n+          visitor.recordLevels.pop();\n+          return visitor.record(schema, names, results);\n         }\n \n-        visitor.recordLevels.pop();\n-\n-        return visitor.record(schema, names, results);\n-\n       case UNION:\n         List<Schema> types = schema.getTypes();\n         List<T> options = Lists.newArrayListWithExpectedSize(types.size());\n@@ -103,6 +115,10 @@ public T map(Schema map, T value) {\n     return null;\n   }\n \n+  public T variant(Schema variant, T metadataResult, T valueResult) {\n+    throw new UnsupportedOperationException(\"Unsupported type: variant\");\n+  }\n+\n   public T primitive(Schema primitive) {\n     return null;\n   }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java b/core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java\nindex c5c78dd1472a..f4dd2f41302d 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/BuildAvroProjection.java\n@@ -265,6 +265,11 @@ public Schema map(Schema map, Supplier<Schema> value) {\n     }\n   }\n \n+  @Override\n+  public Schema variant(Schema variant, Supplier<Schema> metadata, Supplier<Schema> value) {\n+    return variant;\n+  }\n+\n   @Override\n   public Schema primitive(Schema primitive) {\n     // check for type promotion\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/HasIds.java b/core/src/main/java/org/apache/iceberg/avro/HasIds.java\nindex 52ecfd01eaac..90e2094dfca8 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/HasIds.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/HasIds.java\n@@ -56,6 +56,11 @@ public Boolean union(Schema union, Iterable<Boolean> options) {\n     return Iterables.any(options, Boolean.TRUE::equals);\n   }\n \n+  @Override\n+  public Boolean variant(Schema variant, Supplier<Boolean> metadata, Supplier<Boolean> value) {\n+    return false;\n+  }\n+\n   @Override\n   public Boolean primitive(Schema primitive) {\n     return false;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/MissingIds.java b/core/src/main/java/org/apache/iceberg/avro/MissingIds.java\nindex e47d012a36ee..4bb85f3d382c 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/MissingIds.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/MissingIds.java\n@@ -62,6 +62,11 @@ public Boolean union(Schema union, Iterable<Boolean> options) {\n     return Iterables.any(options, Boolean.TRUE::equals);\n   }\n \n+  @Override\n+  public Boolean variant(Schema variant, Supplier<Boolean> metadata, Supplier<Boolean> value) {\n+    return false;\n+  }\n+\n   @Override\n   public Boolean primitive(Schema primitive) {\n     // primitive node cannot be missing ID as Iceberg do not assign primitive node IDs in the first\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/PruneColumns.java b/core/src/main/java/org/apache/iceberg/avro/PruneColumns.java\nindex 2de2c0fe029d..2a31cd5304e7 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/PruneColumns.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/PruneColumns.java\n@@ -80,7 +80,7 @@ public Schema record(Schema record, List<String> names, List<Schema> fields) {\n       }\n \n       Schema fieldSchema = fields.get(field.pos());\n-      // All primitives are selected by selecting the field, but map and list\n+      // All primitives and variant are selected by selecting the field, but map and list\n       // types can be selected by projecting the keys, values, or elements. Empty\n       // Structs can be selected by selecting the record itself instead of its children.\n       // This creates two conditions where the field should be selected: if the\n@@ -259,6 +259,11 @@ private Schema mapWithIds(Schema map, Integer keyId, Integer valueId) {\n     return map;\n   }\n \n+  @Override\n+  public Schema variant(Schema variant, Schema metadata, Schema value) {\n+    return null;\n+  }\n+\n   @Override\n   public Schema primitive(Schema primitive) {\n     // primitives are not selected directly\n@@ -277,12 +282,11 @@ private static Schema copyRecord(Schema record, List<Schema.Field> newFields) {\n     return copy;\n   }\n \n+  /* Check the schema is a record but not a Variant type */\n   private boolean isRecord(Schema field) {\n-    if (AvroSchemaUtil.isOptionSchema(field)) {\n-      return AvroSchemaUtil.fromOption(field).getType().equals(Schema.Type.RECORD);\n-    } else {\n-      return field.getType().equals(Schema.Type.RECORD);\n-    }\n+    Schema schema = AvroSchemaUtil.isOptionSchema(field) ? AvroSchemaUtil.fromOption(field) : field;\n+    return schema.getType().equals(Schema.Type.RECORD)\n+        && !(schema.getLogicalType() instanceof VariantLogicalType);\n   }\n \n   private static Schema makeEmptyCopy(Schema field) {\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/RemoveIds.java b/core/src/main/java/org/apache/iceberg/avro/RemoveIds.java\nindex dccc8bf57e9d..cc01b107ba9c 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/RemoveIds.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/RemoveIds.java\n@@ -60,6 +60,11 @@ public Schema array(Schema array, Schema element) {\n     return result;\n   }\n \n+  @Override\n+  public Schema variant(Schema variant, Schema metadata, Schema value) {\n+    return variant;\n+  }\n+\n   @Override\n   public Schema primitive(Schema primitive) {\n     return Schema.create(primitive.getType());\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/SchemaToType.java b/core/src/main/java/org/apache/iceberg/avro/SchemaToType.java\nindex 352fe22650ac..444acc670e51 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/SchemaToType.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/SchemaToType.java\n@@ -172,6 +172,11 @@ public Type map(Schema map, Type valueType) {\n     }\n   }\n \n+  @Override\n+  public Type variant(Schema variant, Type metadataType, Type valueType) {\n+    return Types.VariantType.get();\n+  }\n+\n   @Override\n   public Type primitive(Schema primitive) {\n     // first check supported logical types\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/TypeToSchema.java b/core/src/main/java/org/apache/iceberg/avro/TypeToSchema.java\nindex 05ce4e618662..4fcbcef16fd4 100644\n--- a/core/src/main/java/org/apache/iceberg/avro/TypeToSchema.java\n+++ b/core/src/main/java/org/apache/iceberg/avro/TypeToSchema.java\n@@ -187,6 +187,21 @@ public Schema map(Types.MapType map, Schema keySchema, Schema valueSchema) {\n     return mapSchema;\n   }\n \n+  @Override\n+  public Schema variant(Types.VariantType variant) {\n+    String recordName = fieldIds.peek() != null ? \"r\" + fieldIds.peek() : \"variant\";\n+    Schema schema =\n+        Schema.createRecord(\n+            recordName,\n+            null,\n+            null,\n+            false,\n+            List.of(\n+                new Schema.Field(\"metadata\", BINARY_SCHEMA),\n+                new Schema.Field(\"value\", BINARY_SCHEMA)));\n+    return VariantLogicalType.get().addToSchema(schema);\n+  }\n+\n   @Override\n   public Schema primitive(Type.PrimitiveType primitive) {\n     Schema primitiveSchema;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/avro/VariantLogicalType.java b/core/src/main/java/org/apache/iceberg/avro/VariantLogicalType.java\nnew file mode 100644\nindex 000000000000..e8d5db4fdca1\n--- /dev/null\n+++ b/core/src/main/java/org/apache/iceberg/avro/VariantLogicalType.java\n@@ -0,0 +1,43 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.avro;\n+\n+import org.apache.avro.LogicalType;\n+import org.apache.avro.Schema;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+\n+public class VariantLogicalType extends LogicalType {\n+  static final String NAME = \"variant\";\n+  private static final VariantLogicalType INSTANCE = new VariantLogicalType();\n+\n+  static VariantLogicalType get() {\n+    return INSTANCE;\n+  }\n+\n+  private VariantLogicalType() {\n+    super(NAME);\n+  }\n+\n+  @Override\n+  public void validate(Schema schema) {\n+    super.validate(schema);\n+    Preconditions.checkArgument(\n+        AvroSchemaUtil.isVariantSchema(schema), \"Invalid variant record: %s\", schema);\n+  }\n+}\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/avro/AvroTestHelpers.java b/core/src/test/java/org/apache/iceberg/avro/AvroTestHelpers.java\nindex 03108376eb4b..3b96f844b537 100644\n--- a/core/src/test/java/org/apache/iceberg/avro/AvroTestHelpers.java\n+++ b/core/src/test/java/org/apache/iceberg/avro/AvroTestHelpers.java\n@@ -46,6 +46,15 @@ static Schema record(String name, Schema.Field... fields) {\n     return Schema.createRecord(name, null, null, false, Arrays.asList(fields));\n   }\n \n+  static Schema variant(String name) {\n+    Schema schema =\n+        record(\n+            name,\n+            new Schema.Field(\"metadata\", Schema.create(Schema.Type.BYTES), null, null),\n+            new Schema.Field(\"value\", Schema.create(Schema.Type.BYTES), null, null));\n+    return VariantLogicalType.get().addToSchema(schema);\n+  }\n+\n   static Schema.Field addId(int id, Schema.Field field) {\n     field.addProp(AvroSchemaUtil.FIELD_ID_PROP, id);\n     return field;\n\ndiff --git a/core/src/test/java/org/apache/iceberg/avro/TestAvroNameMapping.java b/core/src/test/java/org/apache/iceberg/avro/TestAvroNameMapping.java\nindex cabc9f250c13..7f36e0de8fd3 100644\n--- a/core/src/test/java/org/apache/iceberg/avro/TestAvroNameMapping.java\n+++ b/core/src/test/java/org/apache/iceberg/avro/TestAvroNameMapping.java\n@@ -342,6 +342,23 @@ public void testInferredMapping() throws IOException {\n     assertThat(projected).isEqualTo(record);\n   }\n \n+  @Test\n+  public void testVariantNameMapping() {\n+    Schema icebergSchema =\n+        new Schema(\n+            Types.NestedField.required(0, \"id\", Types.LongType.get()),\n+            Types.NestedField.required(1, \"var\", Types.VariantType.get()));\n+\n+    org.apache.avro.Schema avroSchema = RemoveIds.removeIds(icebergSchema);\n+    assertThat(AvroSchemaUtil.hasIds(avroSchema)).as(\"Expect schema has no ids\").isFalse();\n+\n+    NameMapping nameMapping =\n+        NameMapping.of(\n+            MappedField.of(0, ImmutableList.of(\"id\")), MappedField.of(1, ImmutableList.of(\"var\")));\n+    org.apache.avro.Schema mappedSchema = AvroSchemaUtil.applyNameMapping(avroSchema, nameMapping);\n+    assertThat(mappedSchema).isEqualTo(AvroSchemaUtil.convert(icebergSchema.asStruct(), \"table\"));\n+  }\n+\n   @Test\n   @Override\n   public void testAvroArrayAsLogicalMap() {\n\ndiff --git a/core/src/test/java/org/apache/iceberg/avro/TestAvroSchemaProjection.java b/core/src/test/java/org/apache/iceberg/avro/TestAvroSchemaProjection.java\nindex b71280daf8db..6632c04f3ff0 100644\n--- a/core/src/test/java/org/apache/iceberg/avro/TestAvroSchemaProjection.java\n+++ b/core/src/test/java/org/apache/iceberg/avro/TestAvroSchemaProjection.java\n@@ -23,6 +23,7 @@\n import java.util.Collections;\n import org.apache.avro.SchemaBuilder;\n import org.apache.iceberg.Schema;\n+import org.apache.iceberg.types.Types;\n import org.junit.jupiter.api.Test;\n \n public class TestAvroSchemaProjection {\n@@ -150,4 +151,22 @@ public void projectWithMapSchemaChanged() {\n         .as(\"Result of buildAvroProjection is missing some IDs\")\n         .isFalse();\n   }\n+\n+  @Test\n+  public void projectWithVariantType() {\n+    Schema icebergSchema =\n+        new Schema(\n+            Types.NestedField.required(0, \"id\", Types.LongType.get()),\n+            Types.NestedField.required(1, \"data\", Types.VariantType.get()));\n+\n+    org.apache.avro.Schema projectedSchema =\n+        AvroSchemaUtil.buildAvroProjection(\n+            AvroSchemaUtil.convert(icebergSchema.asStruct()),\n+            icebergSchema.select(\"data\"),\n+            Collections.emptyMap());\n+    assertThat(projectedSchema.getField(\"id\")).isNull();\n+    org.apache.avro.Schema variantSchema = projectedSchema.getField(\"data\").schema();\n+    assertThat(variantSchema.getLogicalType()).isEqualTo(VariantLogicalType.get());\n+    assertThat(AvroSchemaUtil.isVariantSchema(variantSchema)).isTrue();\n+  }\n }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/avro/TestHasIds.java b/core/src/test/java/org/apache/iceberg/avro/TestHasIds.java\nindex 4b69d9c879e9..fab520f109e1 100644\n--- a/core/src/test/java/org/apache/iceberg/avro/TestHasIds.java\n+++ b/core/src/test/java/org/apache/iceberg/avro/TestHasIds.java\n@@ -26,7 +26,7 @@\n \n public class TestHasIds {\n   @Test\n-  public void test() {\n+  public void testRemoveIdsHasIds() {\n     Schema schema =\n         new Schema(\n             Types.NestedField.required(0, \"id\", Types.LongType.get()),\n@@ -39,7 +39,10 @@ public void test() {\n                     Types.StringType.get(),\n                     Types.StructType.of(\n                         Types.NestedField.required(1, \"lat\", Types.FloatType.get()),\n-                        Types.NestedField.optional(2, \"long\", Types.FloatType.get())))));\n+                        Types.NestedField.optional(2, \"long\", Types.FloatType.get())))),\n+            Types.NestedField.required(\n+                8, \"types\", Types.ListType.ofRequired(9, Types.StringType.get())),\n+            Types.NestedField.required(10, \"data\", Types.VariantType.get()));\n \n     org.apache.avro.Schema avroSchema = RemoveIds.removeIds(schema);\n     assertThat(AvroSchemaUtil.hasIds(avroSchema)).as(\"Avro schema should not have IDs\").isFalse();\n\ndiff --git a/core/src/test/java/org/apache/iceberg/avro/TestPruneColumns.java b/core/src/test/java/org/apache/iceberg/avro/TestPruneColumns.java\nnew file mode 100644\nindex 000000000000..ecf92a00b35e\n--- /dev/null\n+++ b/core/src/test/java/org/apache/iceberg/avro/TestPruneColumns.java\n@@ -0,0 +1,114 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.avro;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.types.Types;\n+import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.ValueSource;\n+\n+public class TestPruneColumns {\n+  private static final org.apache.avro.Schema TEST_SCHEMA =\n+      AvroSchemaUtil.convert(\n+          new Schema(\n+                  Types.NestedField.required(0, \"id\", Types.LongType.get()),\n+                  Types.NestedField.optional(\n+                      2,\n+                      \"properties\",\n+                      Types.MapType.ofOptional(\n+                          3, 4, Types.StringType.get(), Types.IntegerType.get())),\n+                  Types.NestedField.required(\n+                      5,\n+                      \"location\",\n+                      Types.StructType.of(\n+                          Types.NestedField.required(6, \"lat\", Types.FloatType.get()),\n+                          Types.NestedField.optional(7, \"long\", Types.FloatType.get()))),\n+                  Types.NestedField.required(\n+                      8, \"tags\", Types.ListType.ofRequired(9, Types.StringType.get())),\n+                  Types.NestedField.optional(10, \"payload\", Types.VariantType.get()))\n+              .asStruct());\n+\n+  @Test\n+  public void testSimple() {\n+    Schema expected = new Schema(Types.NestedField.required(0, \"id\", Types.LongType.get()));\n+    org.apache.avro.Schema prunedSchema =\n+        AvroSchemaUtil.pruneColumns(TEST_SCHEMA, Sets.newHashSet(0));\n+    assertThat(prunedSchema).isEqualTo(AvroSchemaUtil.convert(expected.asStruct()));\n+  }\n+\n+  @ParameterizedTest\n+  @ValueSource(ints = {2, 3, 4})\n+  public void testSelectMap(int selectedId) {\n+    Schema expected =\n+        new Schema(\n+            Types.NestedField.optional(\n+                2,\n+                \"properties\",\n+                Types.MapType.ofOptional(3, 4, Types.StringType.get(), Types.IntegerType.get())));\n+    org.apache.avro.Schema prunedSchema =\n+        AvroSchemaUtil.pruneColumns(TEST_SCHEMA, Sets.newHashSet(selectedId));\n+    assertThat(prunedSchema).isEqualTo(AvroSchemaUtil.convert(expected.asStruct()));\n+  }\n+\n+  @Test\n+  public void testSelectEmptyStruct() {\n+    Schema expected = new Schema(Types.NestedField.required(5, \"location\", Types.StructType.of()));\n+    org.apache.avro.Schema prunedSchema =\n+        AvroSchemaUtil.pruneColumns(TEST_SCHEMA, Sets.newHashSet(5));\n+    assertThat(prunedSchema).isEqualTo(AvroSchemaUtil.convert(expected.asStruct()));\n+  }\n+\n+  @Test\n+  public void testSelectStructField() {\n+    Schema expected =\n+        new Schema(\n+            Types.NestedField.required(\n+                5,\n+                \"location\",\n+                Types.StructType.of(Types.NestedField.optional(7, \"long\", Types.FloatType.get()))));\n+    org.apache.avro.Schema prunedSchema =\n+        AvroSchemaUtil.pruneColumns(TEST_SCHEMA, Sets.newHashSet(7));\n+    assertThat(prunedSchema).isEqualTo(AvroSchemaUtil.convert(expected.asStruct()));\n+  }\n+\n+  @ParameterizedTest\n+  @ValueSource(ints = {8, 9})\n+  public void testSelectList(int selectedId) {\n+    Schema expected =\n+        new Schema(\n+            Types.NestedField.required(\n+                8, \"tags\", Types.ListType.ofRequired(9, Types.StringType.get())));\n+    org.apache.avro.Schema prunedSchema =\n+        AvroSchemaUtil.pruneColumns(TEST_SCHEMA, Sets.newHashSet(selectedId));\n+    assertThat(prunedSchema).isEqualTo(AvroSchemaUtil.convert(expected.asStruct()));\n+  }\n+\n+  @Test\n+  public void testSelectVariant() {\n+    Schema expected =\n+        new Schema(Types.NestedField.optional(10, \"payload\", Types.VariantType.get()));\n+    org.apache.avro.Schema prunedSchema =\n+        AvroSchemaUtil.pruneColumns(TEST_SCHEMA, Sets.newHashSet(10));\n+    assertThat(prunedSchema).isEqualTo(AvroSchemaUtil.convert(expected.asStruct()));\n+  }\n+}\n\ndiff --git a/core/src/test/java/org/apache/iceberg/avro/TestSchemaConversions.java b/core/src/test/java/org/apache/iceberg/avro/TestSchemaConversions.java\nindex d9dc49d17257..2c31fe92834b 100644\n--- a/core/src/test/java/org/apache/iceberg/avro/TestSchemaConversions.java\n+++ b/core/src/test/java/org/apache/iceberg/avro/TestSchemaConversions.java\n@@ -24,6 +24,7 @@\n import static org.apache.iceberg.avro.AvroTestHelpers.optionalField;\n import static org.apache.iceberg.avro.AvroTestHelpers.record;\n import static org.apache.iceberg.avro.AvroTestHelpers.requiredField;\n+import static org.apache.iceberg.avro.AvroTestHelpers.variant;\n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.apache.iceberg.types.Types.NestedField.required;\n import static org.assertj.core.api.Assertions.assertThat;\n@@ -57,7 +58,8 @@ public void testPrimitiveTypes() {\n             Types.UUIDType.get(),\n             Types.FixedType.ofLength(12),\n             Types.BinaryType.get(),\n-            Types.DecimalType.of(9, 4));\n+            Types.DecimalType.of(9, 4),\n+            Types.VariantType.get());\n \n     List<Schema> avroPrimitives =\n         Lists.newArrayList(\n@@ -77,7 +79,8 @@ public void testPrimitiveTypes() {\n             Schema.createFixed(\"fixed_12\", null, null, 12),\n             Schema.create(Schema.Type.BYTES),\n             LogicalTypes.decimal(9, 4)\n-                .addToSchema(Schema.createFixed(\"decimal_9_4\", null, null, 4)));\n+                .addToSchema(Schema.createFixed(\"decimal_9_4\", null, null, 4)),\n+            variant(\"variant\"));\n \n     for (int i = 0; i < primitives.size(); i += 1) {\n       Type type = primitives.get(i);\n@@ -125,7 +128,8 @@ public void testStructAndPrimitiveTypes() {\n             optional(31, \"uuid\", Types.UUIDType.get()),\n             optional(32, \"fixed\", Types.FixedType.ofLength(16)),\n             optional(33, \"binary\", Types.BinaryType.get()),\n-            optional(34, \"decimal\", Types.DecimalType.of(14, 2)));\n+            optional(34, \"decimal\", Types.DecimalType.of(14, 2)),\n+            optional(35, \"variant\", Types.VariantType.get()));\n \n     Schema schema =\n         record(\n@@ -162,7 +166,8 @@ public void testStructAndPrimitiveTypes() {\n                 34,\n                 \"decimal\",\n                 LogicalTypes.decimal(14, 2)\n-                    .addToSchema(Schema.createFixed(\"decimal_14_2\", null, null, 6))));\n+                    .addToSchema(Schema.createFixed(\"decimal_14_2\", null, null, 6))),\n+            optionalField(35, \"variant\", variant(\"r35\")));\n \n     assertThat(AvroSchemaUtil.convert(schema))\n         .as(\"Test conversion from Avro schema\")\n@@ -370,4 +375,23 @@ public void testFieldDocsArePreserved() {\n         Lists.newArrayList(Iterables.transform(origSchema.columns(), Types.NestedField::doc));\n     assertThat(fieldDocs).isEqualTo(origFieldDocs);\n   }\n+\n+  @Test\n+  public void testVariantConversion() {\n+    org.apache.iceberg.Schema schema =\n+        new org.apache.iceberg.Schema(\n+            required(1, \"variantCol1\", Types.VariantType.get()),\n+            required(2, \"variantCol2\", Types.VariantType.get()));\n+    org.apache.avro.Schema avroSchema = AvroSchemaUtil.convert(schema.asStruct());\n+\n+    for (int id : Lists.newArrayList(1, 2)) {\n+      org.apache.avro.Schema variantSchema = avroSchema.getField(\"variantCol\" + id).schema();\n+      assertThat(variantSchema.getName()).isEqualTo(\"r\" + id);\n+      assertThat(variantSchema.getType()).isEqualTo(org.apache.avro.Schema.Type.RECORD);\n+      assertThat(variantSchema.getFields().size()).isEqualTo(2);\n+      assertThat(variantSchema.getField(\"metadata\").schema().getType())\n+          .isEqualTo(Schema.Type.BYTES);\n+      assertThat(variantSchema.getField(\"value\").schema().getType()).isEqualTo(Schema.Type.BYTES);\n+    }\n+  }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12191",
    "pr_id": 12191,
    "issue_id": 10219,
    "repo": "apache/iceberg",
    "problem_statement": "The \"Emitting watermarks\" feature can't be used in flink sql?\n### Query engine\n\nflink 1.18.0\n\n### Question\n\nHi @stevenzwu In the latest version, use flink sql still cannot define watermarks. This is still not possible when our company wants to use flink sql to implement window aggregation to process ODS data. Are there plans to support this?",
    "issue_word_count": 58,
    "test_files_count": 2,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java"
    ],
    "pr_changed_test_files": [
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java"
    ],
    "base_commit": "6e8718113c08aebf76d8e79a9e2534c89c73407a",
    "head_commit": "d4315d686ef3cfef5c015a5ad2a321dd40c77f4c",
    "repo_url": "https://github.com/apache/iceberg/pull/12191",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12191",
    "dockerfile": "",
    "pr_merged_at": "2025-03-19T19:29:16.000Z",
    "patch": "diff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java\nindex 65adce77d9f9..662dc30e27ca 100644\n--- a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/source/IcebergTableSource.java\n@@ -35,12 +35,14 @@\n import org.apache.flink.table.connector.source.abilities.SupportsFilterPushDown;\n import org.apache.flink.table.connector.source.abilities.SupportsLimitPushDown;\n import org.apache.flink.table.connector.source.abilities.SupportsProjectionPushDown;\n+import org.apache.flink.table.connector.source.abilities.SupportsSourceWatermark;\n import org.apache.flink.table.data.RowData;\n import org.apache.flink.table.expressions.ResolvedExpression;\n import org.apache.flink.table.types.DataType;\n import org.apache.iceberg.expressions.Expression;\n import org.apache.iceberg.flink.FlinkConfigOptions;\n import org.apache.iceberg.flink.FlinkFilters;\n+import org.apache.iceberg.flink.FlinkReadOptions;\n import org.apache.iceberg.flink.TableLoader;\n import org.apache.iceberg.flink.source.assigner.SplitAssignerType;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n@@ -53,7 +55,8 @@ public class IcebergTableSource\n     implements ScanTableSource,\n         SupportsProjectionPushDown,\n         SupportsFilterPushDown,\n-        SupportsLimitPushDown {\n+        SupportsLimitPushDown,\n+        SupportsSourceWatermark {\n \n   private int[] projectedFields;\n   private Long limit;\n@@ -175,6 +178,17 @@ public Result applyFilters(List<ResolvedExpression> flinkFilters) {\n     return Result.of(acceptedFilters, flinkFilters);\n   }\n \n+  @Override\n+  public void applySourceWatermark() {\n+    Preconditions.checkArgument(\n+        readableConfig.get(FlinkConfigOptions.TABLE_EXEC_ICEBERG_USE_FLIP27_SOURCE),\n+        \"Source watermarks are supported only in flip-27 iceberg source implementation\");\n+\n+    Preconditions.checkNotNull(\n+        properties.get(FlinkReadOptions.WATERMARK_COLUMN),\n+        \"watermark-column needs to be configured to use source watermark.\");\n+  }\n+\n   @Override\n   public boolean supportsNestedProjection() {\n     // TODO: support nested projection\n",
    "test_patch": "diff --git a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java\nindex c8f0b8172d45..0cdaf8371cbd 100644\n--- a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java\n+++ b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestIcebergSourceSql.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg.flink.source;\n \n import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.io.IOException;\n import java.time.Instant;\n@@ -40,6 +41,7 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.types.Types;\n+import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n \n@@ -53,7 +55,11 @@ public class TestIcebergSourceSql extends TestSqlBase {\n   @BeforeEach\n   @Override\n   public void before() throws IOException {\n-    TableEnvironment tableEnvironment = getTableEnv();\n+    setUpTableEnv(getTableEnv());\n+    setUpTableEnv(getStreamingTableEnv());\n+  }\n+\n+  private static void setUpTableEnv(TableEnvironment tableEnvironment) {\n     Configuration tableConf = tableEnvironment.getConfig().getConfiguration();\n     tableConf.set(FlinkConfigOptions.TABLE_EXEC_ICEBERG_USE_FLIP27_SOURCE, true);\n     // Disable inferring parallelism to avoid interfering watermark tests\n@@ -72,6 +78,11 @@ public void before() throws IOException {\n     tableConf.set(TableConfigOptions.TABLE_DYNAMIC_TABLE_OPTIONS_ENABLED, true);\n   }\n \n+  @AfterEach\n+  public void after() throws IOException {\n+    CATALOG_EXTENSION.catalog().dropTable(TestFixtures.TABLE_IDENTIFIER);\n+  }\n+\n   private Record generateRecord(Instant t1, long t2) {\n     Record record = GenericRecord.create(SCHEMA_TS);\n     record.setField(\"t1\", t1.atZone(ZoneId.systemDefault()).toLocalDateTime());\n@@ -178,4 +189,45 @@ public void testReadFlinkDynamicTable() throws Exception {\n         expected,\n         SCHEMA_TS);\n   }\n+\n+  @Test\n+  public void testWatermarkInvalidConfig() {\n+    CATALOG_EXTENSION.catalog().createTable(TestFixtures.TABLE_IDENTIFIER, SCHEMA_TS);\n+\n+    String flinkTable = \"`default_catalog`.`default_database`.flink_table\";\n+    SqlHelpers.sql(\n+        getStreamingTableEnv(),\n+        \"CREATE TABLE %s \"\n+            + \"(eventTS AS CAST(t1 AS TIMESTAMP(3)), \"\n+            + \"WATERMARK FOR eventTS AS SOURCE_WATERMARK()) LIKE iceberg_catalog.`default`.%s\",\n+        flinkTable,\n+        TestFixtures.TABLE);\n+\n+    assertThatThrownBy(() -> SqlHelpers.sql(getStreamingTableEnv(), \"SELECT * FROM %s\", flinkTable))\n+        .isInstanceOf(NullPointerException.class)\n+        .hasMessage(\"watermark-column needs to be configured to use source watermark.\");\n+  }\n+\n+  @Test\n+  public void testWatermarkValidConfig() throws Exception {\n+    List<Record> expected = generateExpectedRecords(true);\n+\n+    String flinkTable = \"`default_catalog`.`default_database`.flink_table\";\n+\n+    SqlHelpers.sql(\n+        getStreamingTableEnv(),\n+        \"CREATE TABLE %s \"\n+            + \"(eventTS AS CAST(t1 AS TIMESTAMP(3)), \"\n+            + \"WATERMARK FOR eventTS AS SOURCE_WATERMARK()) WITH ('watermark-column'='t1') LIKE iceberg_catalog.`default`.%s\",\n+        flinkTable,\n+        TestFixtures.TABLE);\n+\n+    TestHelpers.assertRecordsWithOrder(\n+        SqlHelpers.sql(\n+            getStreamingTableEnv(),\n+            \"SELECT t1, t2 FROM TABLE(TUMBLE(TABLE %s, DESCRIPTOR(eventTS), INTERVAL '1' SECOND))\",\n+            flinkTable),\n+        expected,\n+        SCHEMA_TS);\n+  }\n }\n\ndiff --git a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java\nindex f9b776397cfc..dd63154fe03b 100644\n--- a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java\n+++ b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestSqlBase.java\n@@ -63,6 +63,8 @@ public abstract class TestSqlBase {\n \n   private volatile TableEnvironment tEnv;\n \n+  private volatile TableEnvironment streamingTEnv;\n+\n   protected TableEnvironment getTableEnv() {\n     if (tEnv == null) {\n       synchronized (this) {\n@@ -75,6 +77,19 @@ protected TableEnvironment getTableEnv() {\n     return tEnv;\n   }\n \n+  protected TableEnvironment getStreamingTableEnv() {\n+    if (streamingTEnv == null) {\n+      synchronized (this) {\n+        if (streamingTEnv == null) {\n+          this.streamingTEnv =\n+              TableEnvironment.create(EnvironmentSettings.newInstance().inStreamingMode().build());\n+        }\n+      }\n+    }\n+\n+    return streamingTEnv;\n+  }\n+\n   @BeforeEach\n   public abstract void before() throws IOException;\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12148",
    "pr_id": 12148,
    "issue_id": 12081,
    "repo": "apache/iceberg",
    "problem_statement": "Make DELETE_RATIO_THRESHOLD configurable in SizeBasedDataRewriter\n### Feature Request / Improvement\n\nThe feature was introduced by https://github.com/apache/iceberg/pull/11825 and it would be good to make the ratio configurable in Spark\n\n### Query engine\n\nSpark\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [ ] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 69,
    "test_files_count": 3,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/actions/SizeBasedDataRewriter.java",
      "docs/docs/spark-procedures.md",
      "spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java"
    ],
    "base_commit": "c5822c40889abe9d00921705b986d2c262595b4b",
    "head_commit": "bfadc2f59b03f69fda5170b3884e059e73f49dac",
    "repo_url": "https://github.com/apache/iceberg/pull/12148",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12148",
    "dockerfile": "",
    "pr_merged_at": "2025-02-03T13:13:31.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/actions/SizeBasedDataRewriter.java b/core/src/main/java/org/apache/iceberg/actions/SizeBasedDataRewriter.java\nindex 61b90d9fc6e3..0c55b2892add 100644\n--- a/core/src/main/java/org/apache/iceberg/actions/SizeBasedDataRewriter.java\n+++ b/core/src/main/java/org/apache/iceberg/actions/SizeBasedDataRewriter.java\n@@ -47,10 +47,26 @@ public abstract class SizeBasedDataRewriter extends SizeBasedFileRewriter<FileSc\n   public static final String DELETE_FILE_THRESHOLD = \"delete-file-threshold\";\n \n   public static final int DELETE_FILE_THRESHOLD_DEFAULT = Integer.MAX_VALUE;\n-  private static final double DELETE_RATIO_THRESHOLD = 0.3;\n+\n+  /**\n+   * The minimum deletion ratio that needs to be associated with a data file for it to be considered\n+   * for rewriting. If the deletion ratio of a data file is greater than or equal to this value, it\n+   * will be rewritten regardless of its file size determined by {@link #MIN_FILE_SIZE_BYTES} and\n+   * {@link #MAX_FILE_SIZE_BYTES}. If a file group contains a file that satisfies this condition,\n+   * the file group will be rewritten regardless of the number of files in the file group determined\n+   * by {@link #MIN_INPUT_FILES}.\n+   *\n+   * <p>Defaults to 0.3, which means that if the deletion ratio of a file reaches or exceeds 30%, it\n+   * may trigger the rewriting operation.\n+   */\n+  public static final String DELETE_RATIO_THRESHOLD = \"delete-ratio-threshold\";\n+\n+  public static final double DELETE_RATIO_THRESHOLD_DEFAULT = 0.3;\n \n   private int deleteFileThreshold;\n \n+  private double deleteRatioThreshold;\n+\n   protected SizeBasedDataRewriter(Table table) {\n     super(table);\n   }\n@@ -60,6 +76,7 @@ public Set<String> validOptions() {\n     return ImmutableSet.<String>builder()\n         .addAll(super.validOptions())\n         .add(DELETE_FILE_THRESHOLD)\n+        .add(DELETE_RATIO_THRESHOLD)\n         .build();\n   }\n \n@@ -67,6 +84,18 @@ public Set<String> validOptions() {\n   public void init(Map<String, String> options) {\n     super.init(options);\n     this.deleteFileThreshold = deleteFileThreshold(options);\n+    this.deleteRatioThreshold = deleteRatioThreshold(options);\n+  }\n+\n+  private double deleteRatioThreshold(Map<String, String> options) {\n+    double value =\n+        PropertyUtil.propertyAsDouble(\n+            options, DELETE_RATIO_THRESHOLD, DELETE_RATIO_THRESHOLD_DEFAULT);\n+    Preconditions.checkArgument(\n+        value > 0, \"'%s' is set to %s but must be > 0\", DELETE_RATIO_THRESHOLD, value);\n+    Preconditions.checkArgument(\n+        value <= 1, \"'%s' is set to %s but must be <= 1\", DELETE_RATIO_THRESHOLD, value);\n+    return value;\n   }\n \n   @Override\n@@ -116,7 +145,7 @@ private boolean tooHighDeleteRatio(FileScanTask task) {\n \n     double deletedRecords = (double) Math.min(knownDeletedRecordCount, task.file().recordCount());\n     double deleteRatio = deletedRecords / task.file().recordCount();\n-    return deleteRatio >= DELETE_RATIO_THRESHOLD;\n+    return deleteRatio >= deleteRatioThreshold;\n   }\n \n   @Override\n\ndiff --git a/docs/docs/spark-procedures.md b/docs/docs/spark-procedures.md\nindex c5f307a54c98..aa8c22186973 100644\n--- a/docs/docs/spark-procedures.md\n+++ b/docs/docs/spark-procedures.md\n@@ -403,6 +403,7 @@ Iceberg can compact data files in parallel using Spark with the `rewriteDataFile\n | `rewrite-all` | false | Force rewriting of all provided files overriding other options |\n | `max-file-group-size-bytes` | 107374182400 (100GB) | Largest amount of data that should be rewritten in a single file group. The entire rewrite operation is broken down into pieces based on partitioning and within partitions based on size into file-groups.  This helps with breaking down the rewriting of very large partitions which may not be rewritable otherwise due to the resource constraints of the cluster. |\n | `delete-file-threshold` | 2147483647 | Minimum number of deletes that needs to be associated with a data file for it to be considered for rewriting |\n+| `delete-ratio-threshold` | 0.3 | Minimum deletion ratio that needs to be associated with a data file for it to be considered for rewriting |\n | `output-spec-id` | current partition spec id | Identifier of the output partition spec. Data will be reorganized during the rewrite to align with the output partitioning. |\n | `remove-dangling-deletes` | false | Remove dangling position and equality deletes after rewriting. A delete file is considered dangling if it does not apply to any live data files. Enabling this will generate an additional commit for the removal. |\n \n",
    "test_patch": "diff --git a/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\nindex eef8fb43468f..1067b4ab7322 100644\n--- a/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\n+++ b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\n@@ -246,7 +246,8 @@ public void testBinPackDataValidOptions() {\n             SparkBinPackDataRewriter.MIN_INPUT_FILES,\n             SparkBinPackDataRewriter.REWRITE_ALL,\n             SparkBinPackDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n-            SparkBinPackDataRewriter.DELETE_FILE_THRESHOLD),\n+            SparkBinPackDataRewriter.DELETE_FILE_THRESHOLD,\n+            SparkBinPackDataRewriter.DELETE_RATIO_THRESHOLD),\n         rewriter.validOptions());\n   }\n \n@@ -265,6 +266,7 @@ public void testSortDataValidOptions() {\n             SparkSortDataRewriter.REWRITE_ALL,\n             SparkSortDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n             SparkSortDataRewriter.DELETE_FILE_THRESHOLD,\n+            SparkSortDataRewriter.DELETE_RATIO_THRESHOLD,\n             SparkSortDataRewriter.COMPRESSION_FACTOR),\n         rewriter.validOptions());\n   }\n@@ -285,6 +287,7 @@ public void testZOrderDataValidOptions() {\n             SparkZOrderDataRewriter.REWRITE_ALL,\n             SparkZOrderDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n             SparkZOrderDataRewriter.DELETE_FILE_THRESHOLD,\n+            SparkZOrderDataRewriter.DELETE_RATIO_THRESHOLD,\n             SparkZOrderDataRewriter.COMPRESSION_FACTOR,\n             SparkZOrderDataRewriter.MAX_OUTPUT_SIZE,\n             SparkZOrderDataRewriter.VAR_LENGTH_CONTRIBUTION),\n@@ -301,7 +304,20 @@ public void testInvalidValuesForBinPackDataOptions() {\n     Map<String, String> invalidDeleteThresholdOptions =\n         ImmutableMap.of(SizeBasedDataRewriter.DELETE_FILE_THRESHOLD, \"-1\");\n     assertThatThrownBy(() -> rewriter.init(invalidDeleteThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'delete-file-threshold' is set to -1 but must be >= 0\");\n+\n+    Map<String, String> negativeDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SizeBasedDataRewriter.DELETE_RATIO_THRESHOLD, \"-1\");\n+    assertThatThrownBy(() -> rewriter.init(negativeDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to -1.0 but must be > 0\");\n+\n+    Map<String, String> invalidDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SizeBasedDataRewriter.DELETE_RATIO_THRESHOLD, \"127\");\n+    assertThatThrownBy(() -> rewriter.init(invalidDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to 127.0 but must be <= 1\");\n   }\n \n   @Test\n@@ -314,12 +330,27 @@ public void testInvalidValuesForSortDataOptions() {\n     Map<String, String> invalidDeleteThresholdOptions =\n         ImmutableMap.of(SizeBasedDataRewriter.DELETE_FILE_THRESHOLD, \"-1\");\n     assertThatThrownBy(() -> rewriter.init(invalidDeleteThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'delete-file-threshold' is set to -1 but must be >= 0\");\n \n     Map<String, String> invalidCompressionFactorOptions =\n         ImmutableMap.of(SparkShufflingDataRewriter.COMPRESSION_FACTOR, \"0\");\n     assertThatThrownBy(() -> rewriter.init(invalidCompressionFactorOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'compression-factor' is set to 0.0 but must be > 0\");\n+\n+    Map<String, String> negativeDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SparkShufflingDataRewriter.DELETE_RATIO_THRESHOLD, \"-1\");\n+    assertThatThrownBy(() -> rewriter.init(negativeDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to -1.0 but must be > 0\");\n+\n+    Map<String, String> invalidDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SparkShufflingDataRewriter.DELETE_RATIO_THRESHOLD, \"127\");\n+\n+    assertThatThrownBy(() -> rewriter.init(invalidDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to 127.0 but must be <= 1\");\n   }\n \n   @Test\n@@ -333,24 +364,40 @@ public void testInvalidValuesForZOrderDataOptions() {\n     Map<String, String> invalidDeleteThresholdOptions =\n         ImmutableMap.of(SizeBasedDataRewriter.DELETE_FILE_THRESHOLD, \"-1\");\n     assertThatThrownBy(() -> rewriter.init(invalidDeleteThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'delete-file-threshold' is set to -1 but must be >= 0\");\n \n     Map<String, String> invalidCompressionFactorOptions =\n         ImmutableMap.of(SparkShufflingDataRewriter.COMPRESSION_FACTOR, \"0\");\n     assertThatThrownBy(() -> rewriter.init(invalidCompressionFactorOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'compression-factor' is set to 0.0 but must be > 0\");\n \n     Map<String, String> invalidMaxOutputOptions =\n         ImmutableMap.of(SparkZOrderDataRewriter.MAX_OUTPUT_SIZE, \"0\");\n     assertThatThrownBy(() -> rewriter.init(invalidMaxOutputOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"Cannot have the interleaved ZOrder value use less than 1 byte\")\n         .hasMessageContaining(\"'max-output-size' was set to 0\");\n \n     Map<String, String> invalidVarLengthContributionOptions =\n         ImmutableMap.of(SparkZOrderDataRewriter.VAR_LENGTH_CONTRIBUTION, \"0\");\n     assertThatThrownBy(() -> rewriter.init(invalidVarLengthContributionOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"Cannot use less than 1 byte for variable length types with ZOrder\")\n         .hasMessageContaining(\"'var-length-contribution' was set to 0\");\n+\n+    Map<String, String> negativeDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SparkZOrderDataRewriter.DELETE_RATIO_THRESHOLD, \"-1\");\n+    assertThatThrownBy(() -> rewriter.init(negativeDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to -1.0 but must be > 0\");\n+\n+    Map<String, String> invalidDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SparkZOrderDataRewriter.DELETE_RATIO_THRESHOLD, \"127\");\n+    assertThatThrownBy(() -> rewriter.init(invalidDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to 127.0 but must be <= 1\");\n   }\n \n   private void validateSizeBasedRewriterOptions(SizeBasedFileRewriter<?, ?> rewriter) {\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\nindex 9722b40f2c45..7d728a912214 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\n@@ -246,7 +246,8 @@ public void testBinPackDataValidOptions() {\n             SparkBinPackDataRewriter.MIN_INPUT_FILES,\n             SparkBinPackDataRewriter.REWRITE_ALL,\n             SparkBinPackDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n-            SparkBinPackDataRewriter.DELETE_FILE_THRESHOLD),\n+            SparkBinPackDataRewriter.DELETE_FILE_THRESHOLD,\n+            SparkBinPackDataRewriter.DELETE_RATIO_THRESHOLD),\n         rewriter.validOptions());\n   }\n \n@@ -266,6 +267,7 @@ public void testSortDataValidOptions() {\n             SparkSortDataRewriter.REWRITE_ALL,\n             SparkSortDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n             SparkSortDataRewriter.DELETE_FILE_THRESHOLD,\n+            SparkSortDataRewriter.DELETE_RATIO_THRESHOLD,\n             SparkSortDataRewriter.COMPRESSION_FACTOR),\n         rewriter.validOptions());\n   }\n@@ -287,6 +289,7 @@ public void testZOrderDataValidOptions() {\n             SparkZOrderDataRewriter.REWRITE_ALL,\n             SparkZOrderDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n             SparkZOrderDataRewriter.DELETE_FILE_THRESHOLD,\n+            SparkZOrderDataRewriter.DELETE_RATIO_THRESHOLD,\n             SparkZOrderDataRewriter.COMPRESSION_FACTOR,\n             SparkZOrderDataRewriter.MAX_OUTPUT_SIZE,\n             SparkZOrderDataRewriter.VAR_LENGTH_CONTRIBUTION),\n@@ -303,7 +306,21 @@ public void testInvalidValuesForBinPackDataOptions() {\n     Map<String, String> invalidDeleteThresholdOptions =\n         ImmutableMap.of(SizeBasedDataRewriter.DELETE_FILE_THRESHOLD, \"-1\");\n     assertThatThrownBy(() -> rewriter.init(invalidDeleteThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'delete-file-threshold' is set to -1 but must be >= 0\");\n+\n+    Map<String, String> negativeDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SizeBasedDataRewriter.DELETE_RATIO_THRESHOLD, \"-1\");\n+    assertThatThrownBy(() -> rewriter.init(negativeDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to -1.0 but must be > 0\");\n+\n+    Map<String, String> invalidDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SizeBasedDataRewriter.DELETE_RATIO_THRESHOLD, \"127\");\n+\n+    assertThatThrownBy(() -> rewriter.init(invalidDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to 127.0 but must be <= 1\");\n   }\n \n   @Test\n@@ -316,12 +333,27 @@ public void testInvalidValuesForSortDataOptions() {\n     Map<String, String> invalidDeleteThresholdOptions =\n         ImmutableMap.of(SizeBasedDataRewriter.DELETE_FILE_THRESHOLD, \"-1\");\n     assertThatThrownBy(() -> rewriter.init(invalidDeleteThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'delete-file-threshold' is set to -1 but must be >= 0\");\n \n     Map<String, String> invalidCompressionFactorOptions =\n         ImmutableMap.of(SparkShufflingDataRewriter.COMPRESSION_FACTOR, \"0\");\n     assertThatThrownBy(() -> rewriter.init(invalidCompressionFactorOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'compression-factor' is set to 0.0 but must be > 0\");\n+\n+    Map<String, String> negativeDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SparkShufflingDataRewriter.DELETE_RATIO_THRESHOLD, \"-1\");\n+    assertThatThrownBy(() -> rewriter.init(negativeDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to -1.0 but must be > 0\");\n+\n+    Map<String, String> invalidDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SparkShufflingDataRewriter.DELETE_RATIO_THRESHOLD, \"127\");\n+\n+    assertThatThrownBy(() -> rewriter.init(invalidDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to 127.0 but must be <= 1\");\n   }\n \n   @Test\n@@ -335,24 +367,41 @@ public void testInvalidValuesForZOrderDataOptions() {\n     Map<String, String> invalidDeleteThresholdOptions =\n         ImmutableMap.of(SizeBasedDataRewriter.DELETE_FILE_THRESHOLD, \"-1\");\n     assertThatThrownBy(() -> rewriter.init(invalidDeleteThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'delete-file-threshold' is set to -1 but must be >= 0\");\n \n     Map<String, String> invalidCompressionFactorOptions =\n         ImmutableMap.of(SparkShufflingDataRewriter.COMPRESSION_FACTOR, \"0\");\n     assertThatThrownBy(() -> rewriter.init(invalidCompressionFactorOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'compression-factor' is set to 0.0 but must be > 0\");\n \n     Map<String, String> invalidMaxOutputOptions =\n         ImmutableMap.of(SparkZOrderDataRewriter.MAX_OUTPUT_SIZE, \"0\");\n     assertThatThrownBy(() -> rewriter.init(invalidMaxOutputOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"Cannot have the interleaved ZOrder value use less than 1 byte\")\n         .hasMessageContaining(\"'max-output-size' was set to 0\");\n \n     Map<String, String> invalidVarLengthContributionOptions =\n         ImmutableMap.of(SparkZOrderDataRewriter.VAR_LENGTH_CONTRIBUTION, \"0\");\n     assertThatThrownBy(() -> rewriter.init(invalidVarLengthContributionOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"Cannot use less than 1 byte for variable length types with ZOrder\")\n         .hasMessageContaining(\"'var-length-contribution' was set to 0\");\n+\n+    Map<String, String> negativeDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SparkZOrderDataRewriter.DELETE_RATIO_THRESHOLD, \"-1\");\n+    assertThatThrownBy(() -> rewriter.init(negativeDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to -1.0 but must be > 0\");\n+\n+    Map<String, String> invalidDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SparkZOrderDataRewriter.DELETE_RATIO_THRESHOLD, \"127\");\n+\n+    assertThatThrownBy(() -> rewriter.init(invalidDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to 127.0 but must be <= 1\");\n   }\n \n   private void validateSizeBasedRewriterOptions(SizeBasedFileRewriter<?, ?> rewriter) {\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\nindex 3ffa53c3e6aa..42e008ef21d3 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestSparkFileRewriter.java\n@@ -269,7 +269,8 @@ public void testBinPackDataValidOptions() {\n                 SparkBinPackDataRewriter.MIN_INPUT_FILES,\n                 SparkBinPackDataRewriter.REWRITE_ALL,\n                 SparkBinPackDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n-                SparkBinPackDataRewriter.DELETE_FILE_THRESHOLD));\n+                SparkBinPackDataRewriter.DELETE_FILE_THRESHOLD,\n+                SparkBinPackDataRewriter.DELETE_RATIO_THRESHOLD));\n   }\n \n   @Test\n@@ -289,6 +290,7 @@ public void testSortDataValidOptions() {\n                 SparkSortDataRewriter.REWRITE_ALL,\n                 SparkSortDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n                 SparkSortDataRewriter.DELETE_FILE_THRESHOLD,\n+                SparkSortDataRewriter.DELETE_RATIO_THRESHOLD,\n                 SparkSortDataRewriter.COMPRESSION_FACTOR));\n   }\n \n@@ -310,6 +312,7 @@ public void testZOrderDataValidOptions() {\n                 SparkZOrderDataRewriter.REWRITE_ALL,\n                 SparkZOrderDataRewriter.MAX_FILE_GROUP_SIZE_BYTES,\n                 SparkZOrderDataRewriter.DELETE_FILE_THRESHOLD,\n+                SparkZOrderDataRewriter.DELETE_RATIO_THRESHOLD,\n                 SparkZOrderDataRewriter.COMPRESSION_FACTOR,\n                 SparkZOrderDataRewriter.MAX_OUTPUT_SIZE,\n                 SparkZOrderDataRewriter.VAR_LENGTH_CONTRIBUTION));\n@@ -325,7 +328,21 @@ public void testInvalidValuesForBinPackDataOptions() {\n     Map<String, String> invalidDeleteThresholdOptions =\n         ImmutableMap.of(SizeBasedDataRewriter.DELETE_FILE_THRESHOLD, \"-1\");\n     assertThatThrownBy(() -> rewriter.init(invalidDeleteThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'delete-file-threshold' is set to -1 but must be >= 0\");\n+\n+    Map<String, String> negativeDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SizeBasedDataRewriter.DELETE_RATIO_THRESHOLD, \"-1\");\n+    assertThatThrownBy(() -> rewriter.init(negativeDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to -1.0 but must be > 0\");\n+\n+    Map<String, String> invalidDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SizeBasedDataRewriter.DELETE_RATIO_THRESHOLD, \"127\");\n+\n+    assertThatThrownBy(() -> rewriter.init(invalidDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to 127.0 but must be <= 1\");\n   }\n \n   @Test\n@@ -338,12 +355,27 @@ public void testInvalidValuesForSortDataOptions() {\n     Map<String, String> invalidDeleteThresholdOptions =\n         ImmutableMap.of(SizeBasedDataRewriter.DELETE_FILE_THRESHOLD, \"-1\");\n     assertThatThrownBy(() -> rewriter.init(invalidDeleteThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'delete-file-threshold' is set to -1 but must be >= 0\");\n \n     Map<String, String> invalidCompressionFactorOptions =\n         ImmutableMap.of(SparkShufflingDataRewriter.COMPRESSION_FACTOR, \"0\");\n     assertThatThrownBy(() -> rewriter.init(invalidCompressionFactorOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'compression-factor' is set to 0.0 but must be > 0\");\n+\n+    Map<String, String> negativeDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SparkShufflingDataRewriter.DELETE_RATIO_THRESHOLD, \"-1\");\n+    assertThatThrownBy(() -> rewriter.init(negativeDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to -1.0 but must be > 0\");\n+\n+    Map<String, String> invalidDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SparkShufflingDataRewriter.DELETE_RATIO_THRESHOLD, \"127\");\n+\n+    assertThatThrownBy(() -> rewriter.init(invalidDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to 127.0 but must be <= 1\");\n   }\n \n   @Test\n@@ -357,24 +389,41 @@ public void testInvalidValuesForZOrderDataOptions() {\n     Map<String, String> invalidDeleteThresholdOptions =\n         ImmutableMap.of(SizeBasedDataRewriter.DELETE_FILE_THRESHOLD, \"-1\");\n     assertThatThrownBy(() -> rewriter.init(invalidDeleteThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'delete-file-threshold' is set to -1 but must be >= 0\");\n \n     Map<String, String> invalidCompressionFactorOptions =\n         ImmutableMap.of(SparkShufflingDataRewriter.COMPRESSION_FACTOR, \"0\");\n     assertThatThrownBy(() -> rewriter.init(invalidCompressionFactorOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"'compression-factor' is set to 0.0 but must be > 0\");\n \n     Map<String, String> invalidMaxOutputOptions =\n         ImmutableMap.of(SparkZOrderDataRewriter.MAX_OUTPUT_SIZE, \"0\");\n     assertThatThrownBy(() -> rewriter.init(invalidMaxOutputOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"Cannot have the interleaved ZOrder value use less than 1 byte\")\n         .hasMessageContaining(\"'max-output-size' was set to 0\");\n \n     Map<String, String> invalidVarLengthContributionOptions =\n         ImmutableMap.of(SparkZOrderDataRewriter.VAR_LENGTH_CONTRIBUTION, \"0\");\n     assertThatThrownBy(() -> rewriter.init(invalidVarLengthContributionOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n         .hasMessageContaining(\"Cannot use less than 1 byte for variable length types with ZOrder\")\n         .hasMessageContaining(\"'var-length-contribution' was set to 0\");\n+\n+    Map<String, String> negativeDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SparkZOrderDataRewriter.DELETE_RATIO_THRESHOLD, \"-1\");\n+    assertThatThrownBy(() -> rewriter.init(negativeDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to -1.0 but must be > 0\");\n+\n+    Map<String, String> invalidDeleteRatioThresholdOptions =\n+        ImmutableMap.of(SparkZOrderDataRewriter.DELETE_RATIO_THRESHOLD, \"127\");\n+\n+    assertThatThrownBy(() -> rewriter.init(invalidDeleteRatioThresholdOptions))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"'delete-ratio-threshold' is set to 127.0 but must be <= 1\");\n   }\n \n   private void validateSizeBasedRewriterOptions(SizeBasedFileRewriter<?, ?> rewriter) {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12140",
    "pr_id": 12140,
    "issue_id": 12131,
    "repo": "apache/iceberg",
    "problem_statement": "Class TestHiveMetastore use getSystemClassLoader instead of getClass.getClassLoader in setupMetastoreDB\n### Apache Iceberg version\n\n1.7.1 (latest release)\n\n### Query engine\n\nSpark\n\n### Please describe the bug üêû\n\n```\nval icebergVersion = \"1.7.1\"\nval sparkVersion = \"3.3.2\"\n\nlazy val icebergDependencies = Seq(\n\"org.apache.iceberg\" %% \"iceberg-spark-runtime-3.3\" % icebergVersion,\n\"org.scala-lang.modules\" % \"scala-collection-compat_2.12\" % \"2.12.0\",\n\"org.apache.iceberg\" % \"iceberg-aws-bundle\" % icebergVersion,\n\"org.apache.iceberg\" % \"iceberg-hive-metastore\" % icebergVersion\n)\n\"org.apache.hive\" % \"hive-metastore\" % \"3.1.3\" % Provided,\n\"org.apache.spark\" %% \"spark-hive\" % sparkVersion % Provided,\n(\"org.apache.hive\" % \"hive-exec\" % \"3.1.3\").classifier(\"core\") % Provided,\n(\"org.apache.iceberg\" % \"iceberg-hive-metastore\" % icebergVersion % IntegrationTest).classifier(\"tests\")\n\n```\nI put hive-schema-3.1.0.derby.sql in my resources folder\nproject/\n‚îú‚îÄ‚îÄ src/\n‚îÇ ‚îú‚îÄ‚îÄ it/\n‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ resources/\n‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ hive-schema-3.1.0.derby.sql\n\nI have some test\n```\nvar metastore: TestHiveMetastore = _\n  var uris: String = _\n  implicit var spark: SparkSession = _\n\n  override def beforeAll(): Unit = {\n    super.beforeAll()\n    metastore = new TestHiveMetastore()\n    metastore.start()\n    uris = metastore.hiveConf().get(\"hive.metastore.uris\")\n    spark = SparkSession.builder\n      .master(\"local[*]\")\n      .config(\"spark.hadoop.hive.metastore.uris\", uris)\n      .config(\"spark.sql.legacy.parquet.nanosAsLong\", \"false\")\n      .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\")\n      .config(s\"spark.sql.catalog.$icebergCatalog\", \"org.apache.iceberg.spark.SparkCatalog\")\n      .config(s\"spark.sql.catalog.$icebergCatalog.type\", \"hive\")\n      .config(\"spark.hadoop.fs.s3a.endpoint\", minioContainer.getHostAddress)\n      .config(\"spark.hadoop.fs.s3a.access.key\", minioContainer.getMinioAccessValue)\n      .config(\"spark.hadoop.fs.s3a.secret.key\", minioContainer.getMinioAccessValue)\n      .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n      .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n      .config(\"spark.sql.iceberg.check-ordering\", \"false\")\n      .config(\"write.metadata.delete-after-commit.enabled\", \"false\")\n      .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\")\n      .enableHiveSupport()\n      .getOrCreate()\n    s3.createBucket(bucket)\n  }\n```\n\nI run my test  > sbt \"IntegrationTest/testOnly\"\n\n```\nCaused by: java.lang.NullPointerException\n[error] at java.base/java.io.Reader.(Reader.java:167)\n[error] at java.base/java.io.InputStreamReader.(InputStreamReader.java:72)\n[error] at org.apache.iceberg.hive.TestHiveMetastore.setupMetastoreDB(TestHiveMetastore.java:281)\n[error] at org.apache.iceberg.hive.TestHiveMetastore.(TestHiveMetastore.java:106)\n[error] at ru.samokat.datalake.loader.BaseLoadSuite.beforeAll(BaseLoadSuite.scala:49)\n[error] at ru.samokat.datalake.loader.BaseLoadSuite.beforeAll$(BaseLoadSuite.scala:46)\n```\nHe can't find hive-schema-3.1.0.derby.sql because file loaded from SystemClassLoader in this place:\n```\nprivate static void setupMetastoreDB(String dbURL) throws SQLException, IOException {\n    Connection connection = DriverManager.getConnection(dbURL);\n    ScriptRunner scriptRunner = new ScriptRunner(connection, true, true);\n\n    ClassLoader classLoader = ClassLoader.getSystemClassLoader();\n    InputStream inputStream = classLoader.getResourceAsStream(\"hive-schema-3.1.0.derby.sql\");\n    try (Reader reader = new InputStreamReader(inputStream)) {\n      scriptRunner.runScript(reader);\n    }\n  }\n```\n\n### Willingness to contribute\n\n- [x] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 473,
    "test_files_count": 1,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java"
    ],
    "pr_changed_test_files": [
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java"
    ],
    "base_commit": "02c8b2d45bef4f74c7fb9e36a9b3148d9901f44c",
    "head_commit": "79964728f522821ccd4dcce2975452974bf0cf28",
    "repo_url": "https://github.com/apache/iceberg/pull/12140",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12140",
    "dockerfile": "",
    "pr_merged_at": "2025-02-06T13:24:09.000Z",
    "patch": "",
    "test_patch": "diff --git a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java\nindex ef8bd7ee0ae3..c141f0cced02 100644\n--- a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java\n+++ b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveMetastore.java\n@@ -47,6 +47,7 @@\n import org.apache.iceberg.common.DynConstructors;\n import org.apache.iceberg.common.DynMethods;\n import org.apache.iceberg.hadoop.Util;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.thrift.TException;\n import org.apache.thrift.protocol.TBinaryProtocol;\n import org.apache.thrift.server.TServer;\n@@ -273,13 +274,17 @@ private void initConf(HiveConf conf, int port) {\n   }\n \n   private static void setupMetastoreDB(String dbURL) throws SQLException, IOException {\n-    Connection connection = DriverManager.getConnection(dbURL);\n-    ScriptRunner scriptRunner = new ScriptRunner(connection, true, true);\n-\n-    ClassLoader classLoader = ClassLoader.getSystemClassLoader();\n-    InputStream inputStream = classLoader.getResourceAsStream(\"hive-schema-3.1.0.derby.sql\");\n-    try (Reader reader = new InputStreamReader(inputStream)) {\n-      scriptRunner.runScript(reader);\n+    try (Connection connection = DriverManager.getConnection(dbURL)) {\n+      ScriptRunner scriptRunner = new ScriptRunner(connection, true, true);\n+      try (InputStream inputStream =\n+              TestHiveMetastore.class\n+                  .getClassLoader()\n+                  .getResourceAsStream(\"hive-schema-3.1.0.derby.sql\");\n+          Reader reader =\n+              new InputStreamReader(\n+                  Preconditions.checkNotNull(inputStream, \"Invalid input stream: null\"))) {\n+        scriptRunner.runScript(reader);\n+      }\n     }\n   }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12137",
    "pr_id": 12137,
    "issue_id": 12168,
    "repo": "apache/iceberg",
    "problem_statement": "PartitionStatsUtil#computeStats returns incomplete stats in case of partition evolution\n### Apache Iceberg version\n\n1.7.0, 1.7.1, main\n\n### Query engine\n\nNone\n\n### Please describe the bug üêû\n\nDescription:\nPartitionMap creates internal wrappers based on table specs partitionType, however, we are passing coercedPartition with unified partition type considering all specs as a KEY, leading to wrong evaluation.\n\nRepro:\nhttps://github.com/apache/hive/blob/master/iceberg/iceberg-handler/src/test/queries/positive/iceberg_bucket_map_join_4.q\n\n### Willingness to contribute\n\n- [x] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 114,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/PartitionStatsUtil.java",
      "core/src/test/java/org/apache/iceberg/TestPartitionStatsUtil.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/TestPartitionStatsUtil.java"
    ],
    "base_commit": "3dc4a5498ebbcf9bb1606752330d1de415db5cb0",
    "head_commit": "986a53f25a4bcda7100d7d00b9656a321f9504bd",
    "repo_url": "https://github.com/apache/iceberg/pull/12137",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12137",
    "dockerfile": "",
    "pr_merged_at": "2025-02-20T11:44:03.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/PartitionStatsUtil.java b/core/src/main/java/org/apache/iceberg/PartitionStatsUtil.java\nindex 1fe4e6767fe6..4cf1902937f3 100644\n--- a/core/src/main/java/org/apache/iceberg/PartitionStatsUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/PartitionStatsUtil.java\n@@ -96,7 +96,10 @@ private static PartitionMap<PartitionStats> collectStats(\n         StructLike key = keyTemplate.copyFor(coercedPartition);\n         Snapshot snapshot = table.snapshot(entry.snapshotId());\n         PartitionStats stats =\n-            statsMap.computeIfAbsent(specId, key, () -> new PartitionStats(key, specId));\n+            statsMap.computeIfAbsent(\n+                specId,\n+                ((PartitionData) file.partition()).copy(),\n+                () -> new PartitionStats(key, specId));\n         if (entry.isLive()) {\n           stats.liveEntry(file, snapshot);\n         } else {\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/TestPartitionStatsUtil.java b/core/src/test/java/org/apache/iceberg/TestPartitionStatsUtil.java\nindex 541fcd2ca22d..bd4b4a2ff698 100644\n--- a/core/src/test/java/org/apache/iceberg/TestPartitionStatsUtil.java\n+++ b/core/src/test/java/org/apache/iceberg/TestPartitionStatsUtil.java\n@@ -26,6 +26,7 @@\n import java.io.IOException;\n import java.util.Collection;\n import java.util.List;\n+import org.apache.iceberg.expressions.Expressions;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.types.Types;\n import org.assertj.core.groups.Tuple;\n@@ -370,16 +371,170 @@ public void testPartitionStatsWithSchemaEvolution() throws Exception {\n             snapshot2.snapshotId()));\n   }\n \n-  private static PartitionData partitionData(Types.StructType partitionType, String c2, String c3) {\n-    PartitionData partitionData = new PartitionData(partitionType);\n-    partitionData.set(0, c2);\n-    partitionData.set(1, c3);\n-    return partitionData;\n+  @Test\n+  @SuppressWarnings(\"MethodLength\")\n+  public void testPartitionStatsWithBucketTransformSchemaEvolution() throws Exception {\n+    PartitionSpec specBefore =\n+        PartitionSpec.builderFor(SCHEMA).identity(\"c2\").bucket(\"c1\", 2).build();\n+\n+    Table testTable =\n+        TestTables.create(\n+            tempDir(\"partition_stats_schema_evolve2\"),\n+            \"partition_stats_schema_evolve2\",\n+            SCHEMA,\n+            specBefore,\n+            SortOrder.unsorted(),\n+            2);\n+\n+    List<DataFile> dataFiles = Lists.newArrayList();\n+    for (int i = 0; i < 2; i++) {\n+      dataFiles.add(FileGenerationUtil.generateDataFile(testTable, TestHelpers.Row.of(\"foo\", i)));\n+    }\n+\n+    AppendFiles appendFiles = testTable.newAppend();\n+    dataFiles.forEach(appendFiles::appendFile);\n+    appendFiles.commit();\n+\n+    Snapshot snapshot1 = testTable.currentSnapshot();\n+    Types.StructType partitionType = Partitioning.partitionType(testTable);\n+\n+    computeAndValidatePartitionStats(\n+        testTable,\n+        Tuple.tuple(\n+            partitionData(partitionType, \"foo\", 0),\n+            0,\n+            dataFiles.get(0).recordCount(),\n+            1,\n+            dataFiles.get(0).fileSizeInBytes(),\n+            0L,\n+            0,\n+            0L,\n+            0,\n+            0L,\n+            snapshot1.timestampMillis(),\n+            snapshot1.snapshotId()),\n+        Tuple.tuple(\n+            partitionData(partitionType, \"foo\", 1),\n+            0,\n+            dataFiles.get(1).recordCount(),\n+            1,\n+            dataFiles.get(1).fileSizeInBytes(),\n+            0L,\n+            0,\n+            0L,\n+            0,\n+            0L,\n+            snapshot1.timestampMillis(),\n+            snapshot1.snapshotId()));\n+\n+    // Evolve the partition spec\n+    testTable\n+        .updateSpec()\n+        .removeField(Expressions.bucket(\"c1\", 2))\n+        .addField(Expressions.bucket(\"c1\", 4))\n+        .commit();\n+\n+    List<DataFile> filesWithNewSpec = Lists.newArrayList();\n+    for (int i = 0; i < 4; i++) {\n+      filesWithNewSpec.add(\n+          FileGenerationUtil.generateDataFile(testTable, TestHelpers.Row.of(\"bar\", i)));\n+    }\n+\n+    appendFiles = testTable.newAppend();\n+    filesWithNewSpec.forEach(appendFiles::appendFile);\n+    appendFiles.commit();\n+\n+    Snapshot snapshot2 = testTable.currentSnapshot();\n+    partitionType = Partitioning.partitionType(testTable);\n+\n+    computeAndValidatePartitionStats(\n+        testTable,\n+        Tuple.tuple(\n+            partitionData(partitionType, \"foo\", 0, null),\n+            0,\n+            dataFiles.get(0).recordCount(),\n+            1,\n+            dataFiles.get(0).fileSizeInBytes(),\n+            0L,\n+            0,\n+            0L,\n+            0,\n+            0L,\n+            snapshot1.timestampMillis(),\n+            snapshot1.snapshotId()),\n+        Tuple.tuple(\n+            partitionData(partitionType, \"foo\", 1, null),\n+            0,\n+            dataFiles.get(1).recordCount(),\n+            1,\n+            dataFiles.get(1).fileSizeInBytes(),\n+            0L,\n+            0,\n+            0L,\n+            0,\n+            0L,\n+            snapshot1.timestampMillis(),\n+            snapshot1.snapshotId()),\n+        Tuple.tuple(\n+            partitionData(partitionType, \"bar\", null, 0),\n+            1,\n+            filesWithNewSpec.get(0).recordCount(),\n+            1,\n+            filesWithNewSpec.get(0).fileSizeInBytes(),\n+            0L,\n+            0,\n+            0L,\n+            0,\n+            0L,\n+            snapshot2.timestampMillis(),\n+            snapshot2.snapshotId()),\n+        Tuple.tuple(\n+            partitionData(partitionType, \"bar\", null, 1),\n+            1,\n+            filesWithNewSpec.get(1).recordCount(),\n+            1,\n+            filesWithNewSpec.get(1).fileSizeInBytes(),\n+            0L,\n+            0,\n+            0L,\n+            0,\n+            0L,\n+            snapshot2.timestampMillis(),\n+            snapshot2.snapshotId()),\n+        Tuple.tuple(\n+            partitionData(partitionType, \"bar\", null, 2),\n+            1,\n+            filesWithNewSpec.get(2).recordCount(),\n+            1,\n+            filesWithNewSpec.get(2).fileSizeInBytes(),\n+            0L,\n+            0,\n+            0L,\n+            0,\n+            0L,\n+            snapshot2.timestampMillis(),\n+            snapshot2.snapshotId()),\n+        Tuple.tuple(\n+            partitionData(partitionType, \"bar\", null, 3),\n+            1,\n+            filesWithNewSpec.get(3).recordCount(),\n+            1,\n+            filesWithNewSpec.get(3).fileSizeInBytes(),\n+            0L,\n+            0,\n+            0L,\n+            0,\n+            0L,\n+            snapshot2.timestampMillis(),\n+            snapshot2.snapshotId()));\n   }\n \n-  private static PartitionData partitionData(Types.StructType partitionType, String c2) {\n+  private static PartitionData partitionData(Types.StructType partitionType, Object... fields) {\n     PartitionData partitionData = new PartitionData(partitionType);\n-    partitionData.set(0, c2);\n+    for (int i = 0; i < fields.length; i++) {\n+      partitionData.set(i, fields[i]);\n+    }\n+\n     return partitionData;\n   }\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12120",
    "pr_id": 12120,
    "issue_id": 12076,
    "repo": "apache/iceberg",
    "problem_statement": "`partial-progress.max-failed-commits` Incorrectly compare the failureCommit value\n### Apache Iceberg version\n\n1.7.1 (latest release)\n\n### Query engine\n\nSpark\n\n### Please describe the bug üêû\n\nDuring the usage of `partial-progress.max-failed-commits`, we've found that the threshold check's false positive rate is too high. After taking a deep look, within this PR: https://github.com/apache/iceberg/pull/9611\n\nIt first get the succeededCommits whenever there is a succeed commit, then calculating `int failedCommits = maxCommits - commitService.succeededCommits();`\n\nHowever, I've found a couple of cases that even though we defined the `partial-progress.max-commits` value, internally iceberg would optimize the group file into a lower number of this max-commits. eg: the actual group file can be smaller than maxCommits definition. In this case, the threshold check above will be wrong.\n\nThe suggested solution would be instead of calculating succeed commit, we should directly collecting failure commit count and do comparison. \n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [x] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 197,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java"
    ],
    "base_commit": "d5971429ea903be873b5884c64a3dd41076179ea",
    "head_commit": "1a144131cb79768c4efd9ecce4441123ef5f8203",
    "repo_url": "https://github.com/apache/iceberg/pull/12120",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12120",
    "dockerfile": "",
    "pr_merged_at": "2025-04-01T14:48:53.000Z",
    "patch": "diff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java\nindex e04a0c88b4bb..664612cfa3dc 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java\n@@ -377,7 +377,8 @@ private Builder doExecuteWithPartialProgress(\n     // stop commit service\n     commitService.close();\n \n-    int failedCommits = maxCommits - commitService.succeededCommits();\n+    int totalCommits = Math.min(ctx.totalGroupCount(), maxCommits);\n+    int failedCommits = totalCommits - commitService.succeededCommits();\n     if (failedCommits > 0 && failedCommits <= maxFailedCommits) {\n       LOG.warn(\n           \"{} is true but {} rewrite commits failed. Check the logs to determine why the individual \"\n",
    "test_patch": "diff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\nindex bdbb8c176812..9b3ab675518c 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\n@@ -1320,6 +1320,34 @@ public void testParallelPartialProgressWithMaxFailedCommits() {\n     shouldHaveACleanCache(table);\n   }\n \n+  @TestTemplate\n+  public void testParallelPartialProgressWithMaxFailedCommitsLargerThanTotalFileGroup() {\n+    Table table = createTable(20);\n+    int fileSize = averageFileSize(table);\n+\n+    List<Object[]> originalData = currentData();\n+\n+    RewriteDataFilesSparkAction rewrite =\n+        basicRewrite(table)\n+            .option(\n+                RewriteDataFiles.MAX_FILE_GROUP_SIZE_BYTES, Integer.toString(fileSize * 2 + 1000))\n+            .option(RewriteDataFiles.MAX_CONCURRENT_FILE_GROUP_REWRITES, \"3\")\n+            .option(RewriteDataFiles.PARTIAL_PROGRESS_ENABLED, \"true\")\n+            // Since we can have at most one commit per file group and there are only 10 file\n+            // groups, actual number of commits is 10\n+            .option(RewriteDataFiles.PARTIAL_PROGRESS_MAX_COMMITS, \"20\")\n+            .option(RewriteDataFiles.PARTIAL_PROGRESS_MAX_FAILED_COMMITS, \"0\");\n+    rewrite.execute();\n+\n+    table.refresh();\n+\n+    List<Object[]> postRewriteData = currentData();\n+    assertEquals(\"We shouldn't have changed the data\", originalData, postRewriteData);\n+    shouldHaveSnapshots(table, 11);\n+    shouldHaveNoOrphans(table);\n+    shouldHaveACleanCache(table);\n+  }\n+\n   @TestTemplate\n   public void testInvalidOptions() {\n     Table table = createTable(20);\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-12012",
    "pr_id": 12012,
    "issue_id": 11732,
    "repo": "apache/iceberg",
    "problem_statement": "Support UnknownType for V3 Schema\n### Feature Request / Improvement\n\nImplement `unknown` type which is added by version 3: https://iceberg.apache.org/spec/#primitive-types\n\n### Query engine\n\nNone\n\n### Willingness to contribute\n\n- [ ] I can contribute this improvement/feature independently\n- [ ] I would be willing to contribute this improvement/feature with guidance from the Iceberg community\n- [ ] I cannot contribute this improvement/feature at this time",
    "issue_word_count": 61,
    "test_files_count": 11,
    "non_test_files_count": 5,
    "pr_changed_files": [
      "api/src/main/java/org/apache/iceberg/Schema.java",
      "api/src/main/java/org/apache/iceberg/expressions/ExpressionUtil.java",
      "api/src/main/java/org/apache/iceberg/types/Type.java",
      "api/src/main/java/org/apache/iceberg/types/TypeUtil.java",
      "api/src/main/java/org/apache/iceberg/types/Types.java",
      "api/src/test/java/org/apache/iceberg/TestPartitionSpecValidation.java",
      "api/src/test/java/org/apache/iceberg/TestSchema.java",
      "api/src/test/java/org/apache/iceberg/transforms/TestBucketing.java",
      "api/src/test/java/org/apache/iceberg/transforms/TestDates.java",
      "api/src/test/java/org/apache/iceberg/transforms/TestIdentity.java",
      "api/src/test/java/org/apache/iceberg/transforms/TestTimestamps.java",
      "api/src/test/java/org/apache/iceberg/transforms/TestTruncate.java",
      "api/src/test/java/org/apache/iceberg/transforms/TestVoid.java",
      "api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java",
      "api/src/test/java/org/apache/iceberg/types/TestTypes.java",
      "core/src/test/java/org/apache/iceberg/TestSortOrder.java"
    ],
    "pr_changed_test_files": [
      "api/src/test/java/org/apache/iceberg/TestPartitionSpecValidation.java",
      "api/src/test/java/org/apache/iceberg/TestSchema.java",
      "api/src/test/java/org/apache/iceberg/transforms/TestBucketing.java",
      "api/src/test/java/org/apache/iceberg/transforms/TestDates.java",
      "api/src/test/java/org/apache/iceberg/transforms/TestIdentity.java",
      "api/src/test/java/org/apache/iceberg/transforms/TestTimestamps.java",
      "api/src/test/java/org/apache/iceberg/transforms/TestTruncate.java",
      "api/src/test/java/org/apache/iceberg/transforms/TestVoid.java",
      "api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java",
      "api/src/test/java/org/apache/iceberg/types/TestTypes.java",
      "core/src/test/java/org/apache/iceberg/TestSortOrder.java"
    ],
    "base_commit": "41b458b7022c7b0cd78eeca9102392db7889d3c9",
    "head_commit": "fda8dd5dd088db7578310739418d15f502707357",
    "repo_url": "https://github.com/apache/iceberg/pull/12012",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/12012",
    "dockerfile": "",
    "pr_merged_at": "2025-01-24T12:35:52.000Z",
    "patch": "diff --git a/api/src/main/java/org/apache/iceberg/Schema.java b/api/src/main/java/org/apache/iceberg/Schema.java\nindex bd07e9798e9b..07ed44b65cf7 100644\n--- a/api/src/main/java/org/apache/iceberg/Schema.java\n+++ b/api/src/main/java/org/apache/iceberg/Schema.java\n@@ -60,7 +60,10 @@ public class Schema implements Serializable {\n \n   @VisibleForTesting\n   static final Map<Type.TypeID, Integer> MIN_FORMAT_VERSIONS =\n-      ImmutableMap.of(Type.TypeID.TIMESTAMP_NANO, 3, Type.TypeID.VARIANT, 3);\n+      ImmutableMap.of(\n+          Type.TypeID.TIMESTAMP_NANO, 3,\n+          Type.TypeID.VARIANT, 3,\n+          Type.TypeID.UNKNOWN, 3);\n \n   private final StructType struct;\n   private final int schemaId;\n\ndiff --git a/api/src/main/java/org/apache/iceberg/expressions/ExpressionUtil.java b/api/src/main/java/org/apache/iceberg/expressions/ExpressionUtil.java\nindex 028b07827b54..02f3880dd96a 100644\n--- a/api/src/main/java/org/apache/iceberg/expressions/ExpressionUtil.java\n+++ b/api/src/main/java/org/apache/iceberg/expressions/ExpressionUtil.java\n@@ -533,13 +533,15 @@ private static String sanitize(Type type, Object value, long now, int today) {\n         return sanitizeTimestamp(DateTimeUtil.nanosToMicros((long) value / 1000), now);\n       case STRING:\n         return sanitizeString((CharSequence) value, now, today);\n+      case UNKNOWN:\n+        return \"(unknown)\";\n       case BOOLEAN:\n       case UUID:\n       case DECIMAL:\n       case FIXED:\n       case BINARY:\n       case VARIANT:\n-        // for boolean, uuid, decimal, fixed, variant, and binary, match the string result\n+        // for boolean, uuid, decimal, fixed, variant, unknown, and binary, match the string result\n         return sanitizeSimpleString(value.toString());\n     }\n     throw new UnsupportedOperationException(\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/Type.java b/api/src/main/java/org/apache/iceberg/types/Type.java\nindex 30870535521f..f4c6f22134a5 100644\n--- a/api/src/main/java/org/apache/iceberg/types/Type.java\n+++ b/api/src/main/java/org/apache/iceberg/types/Type.java\n@@ -46,7 +46,8 @@ enum TypeID {\n     STRUCT(StructLike.class),\n     LIST(List.class),\n     MAP(Map.class),\n-    VARIANT(Object.class);\n+    VARIANT(Object.class),\n+    UNKNOWN(Object.class);\n \n     private final Class<?> javaClass;\n \n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/TypeUtil.java b/api/src/main/java/org/apache/iceberg/types/TypeUtil.java\nindex 7fcf3db3a40d..39f2898757a6 100644\n--- a/api/src/main/java/org/apache/iceberg/types/TypeUtil.java\n+++ b/api/src/main/java/org/apache/iceberg/types/TypeUtil.java\n@@ -536,6 +536,9 @@ private static int estimateSize(Type type) {\n       case BINARY:\n       case VARIANT:\n         return 80;\n+      case UNKNOWN:\n+        // Consider Unknown as null\n+        return 0;\n       case DECIMAL:\n         // 12 (header) + (12 + 12 + 4) (BigInteger) + 4 (scale) = 44 bytes\n         return 44;\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/Types.java b/api/src/main/java/org/apache/iceberg/types/Types.java\nindex 3c03a3defb42..6882f718508b 100644\n--- a/api/src/main/java/org/apache/iceberg/types/Types.java\n+++ b/api/src/main/java/org/apache/iceberg/types/Types.java\n@@ -55,6 +55,7 @@ private Types() {}\n           .put(StringType.get().toString(), StringType.get())\n           .put(UUIDType.get().toString(), UUIDType.get())\n           .put(BinaryType.get().toString(), BinaryType.get())\n+          .put(UnknownType.get().toString(), UnknownType.get())\n           .buildOrThrow();\n \n   private static final Pattern FIXED = Pattern.compile(\"fixed\\\\[\\\\s*(\\\\d+)\\\\s*\\\\]\");\n@@ -447,6 +448,24 @@ public int hashCode() {\n     }\n   }\n \n+  public static class UnknownType extends PrimitiveType {\n+    private static final UnknownType INSTANCE = new UnknownType();\n+\n+    public static UnknownType get() {\n+      return INSTANCE;\n+    }\n+\n+    @Override\n+    public TypeID typeId() {\n+      return TypeID.UNKNOWN;\n+    }\n+\n+    @Override\n+    public String toString() {\n+      return \"unknown\";\n+    }\n+  }\n+\n   public static class DecimalType extends PrimitiveType {\n     public static DecimalType of(int precision, int scale) {\n       return new DecimalType(precision, scale);\n",
    "test_patch": "diff --git a/api/src/test/java/org/apache/iceberg/TestPartitionSpecValidation.java b/api/src/test/java/org/apache/iceberg/TestPartitionSpecValidation.java\nindex 971f5a9e4510..ba7010f196a7 100644\n--- a/api/src/test/java/org/apache/iceberg/TestPartitionSpecValidation.java\n+++ b/api/src/test/java/org/apache/iceberg/TestPartitionSpecValidation.java\n@@ -36,7 +36,8 @@ public class TestPartitionSpecValidation {\n           NestedField.required(4, \"d\", Types.TimestampType.withZone()),\n           NestedField.required(5, \"another_d\", Types.TimestampType.withZone()),\n           NestedField.required(6, \"s\", Types.StringType.get()),\n-          NestedField.required(7, \"v\", Types.VariantType.get()));\n+          NestedField.required(7, \"v\", Types.VariantType.get()),\n+          NestedField.required(8, \"u\", Types.UnknownType.get()));\n \n   @Test\n   public void testMultipleTimestampPartitions() {\n@@ -325,4 +326,15 @@ public void testVariantUnsupported() {\n         .isInstanceOf(ValidationException.class)\n         .hasMessage(\"Cannot partition by non-primitive source field: variant\");\n   }\n+\n+  @Test\n+  public void testUnknownUnsupported() {\n+    assertThatThrownBy(\n+            () ->\n+                PartitionSpec.builderFor(SCHEMA)\n+                    .add(8, 1005, \"unknown_partition1\", Transforms.bucket(5))\n+                    .build())\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessage(\"Invalid source type unknown for transform: bucket[5]\");\n+  }\n }\n\ndiff --git a/api/src/test/java/org/apache/iceberg/TestSchema.java b/api/src/test/java/org/apache/iceberg/TestSchema.java\nindex e9cb387eebb5..46db60852b3f 100644\n--- a/api/src/test/java/org/apache/iceberg/TestSchema.java\n+++ b/api/src/test/java/org/apache/iceberg/TestSchema.java\n@@ -41,7 +41,8 @@ public class TestSchema {\n       ImmutableList.of(\n           Types.TimestampNanoType.withoutZone(),\n           Types.TimestampNanoType.withZone(),\n-          Types.VariantType.get());\n+          Types.VariantType.get(),\n+          Types.UnknownType.get());\n \n   private static final Schema INITIAL_DEFAULT_SCHEMA =\n       new Schema(\n\ndiff --git a/api/src/test/java/org/apache/iceberg/transforms/TestBucketing.java b/api/src/test/java/org/apache/iceberg/transforms/TestBucketing.java\nindex 5f0cac2b5e8c..3c8ff93a85a3 100644\n--- a/api/src/test/java/org/apache/iceberg/transforms/TestBucketing.java\n+++ b/api/src/test/java/org/apache/iceberg/transforms/TestBucketing.java\n@@ -431,6 +431,20 @@ public void testVariantUnsupported() {\n     assertThat(bucket.canTransform(Types.VariantType.get())).isFalse();\n   }\n \n+  @Test\n+  public void testUnknownUnsupported() {\n+    assertThatThrownBy(() -> Transforms.bucket(Types.UnknownType.get(), 3))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\"Cannot bucket by type: unknown\");\n+\n+    Transform<Object, Integer> bucket = Transforms.bucket(3);\n+    assertThatThrownBy(() -> bucket.bind(Types.UnknownType.get()))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\"Cannot bucket by type: unknown\");\n+\n+    assertThat(bucket.canTransform(Types.UnknownType.get())).isFalse();\n+  }\n+\n   private byte[] randomBytes(int length) {\n     byte[] bytes = new byte[length];\n     testRandom.nextBytes(bytes);\n\ndiff --git a/api/src/test/java/org/apache/iceberg/transforms/TestDates.java b/api/src/test/java/org/apache/iceberg/transforms/TestDates.java\nindex c899b4cfa1cb..625220c10925 100644\n--- a/api/src/test/java/org/apache/iceberg/transforms/TestDates.java\n+++ b/api/src/test/java/org/apache/iceberg/transforms/TestDates.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg.transforms;\n \n import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import org.apache.iceberg.expressions.Literal;\n import org.apache.iceberg.types.Type;\n@@ -267,4 +268,46 @@ public void testDatesReturnType() {\n     Type dayResultType = day.getResultType(type);\n     assertThat(dayResultType).isEqualTo(Types.DateType.get());\n   }\n+\n+  @Test\n+  public void testUnknownUnsupportedYear() {\n+    assertThatThrownBy(() -> Transforms.year(Types.UnknownType.get()))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\"Unsupported type: unknown\");\n+\n+    Transform<Object, Integer> year = Transforms.year();\n+    assertThatThrownBy(() -> year.bind(Types.UnknownType.get()))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\"Unsupported type: unknown\");\n+\n+    assertThat(year.canTransform(Types.UnknownType.get())).isFalse();\n+  }\n+\n+  @Test\n+  public void testUnknownUnsupportedMonth() {\n+    assertThatThrownBy(() -> Transforms.month(Types.UnknownType.get()))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\"Unsupported type: unknown\");\n+\n+    Transform<Object, Integer> month = Transforms.month();\n+    assertThatThrownBy(() -> month.bind(Types.UnknownType.get()))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\"Unsupported type: unknown\");\n+\n+    assertThat(month.canTransform(Types.UnknownType.get())).isFalse();\n+  }\n+\n+  @Test\n+  public void testUnknownUnsupportedDay() {\n+    assertThatThrownBy(() -> Transforms.day(Types.UnknownType.get()))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\"Unsupported type: unknown\");\n+\n+    Transform<Object, Integer> day = Transforms.day();\n+    assertThatThrownBy(() -> day.bind(Types.UnknownType.get()))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\"Unsupported type: unknown\");\n+\n+    assertThat(day.canTransform(Types.UnknownType.get())).isFalse();\n+  }\n }\n\ndiff --git a/api/src/test/java/org/apache/iceberg/transforms/TestIdentity.java b/api/src/test/java/org/apache/iceberg/transforms/TestIdentity.java\nindex b5076e08a947..fc24be8d5698 100644\n--- a/api/src/test/java/org/apache/iceberg/transforms/TestIdentity.java\n+++ b/api/src/test/java/org/apache/iceberg/transforms/TestIdentity.java\n@@ -157,6 +157,16 @@ public void testBigDecimalToHumanString() {\n         .isEqualTo(decimalString);\n   }\n \n+  @Test\n+  public void testUnknownToHumanString() {\n+    Types.UnknownType unknownType = Types.UnknownType.get();\n+    Transform<Object, Object> identity = Transforms.identity();\n+\n+    assertThat(identity.toHumanString(unknownType, null))\n+        .as(\"Should produce \\\"null\\\" for null\")\n+        .isEqualTo(\"null\");\n+  }\n+\n   @Test\n   public void testVariantUnsupported() {\n     assertThatThrownBy(() -> Transforms.identity().bind(Types.VariantType.get()))\n\ndiff --git a/api/src/test/java/org/apache/iceberg/transforms/TestTimestamps.java b/api/src/test/java/org/apache/iceberg/transforms/TestTimestamps.java\nindex 78b0e67c686b..565426ffe9cd 100644\n--- a/api/src/test/java/org/apache/iceberg/transforms/TestTimestamps.java\n+++ b/api/src/test/java/org/apache/iceberg/transforms/TestTimestamps.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg.transforms;\n \n import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import org.apache.iceberg.expressions.Literal;\n import org.apache.iceberg.types.Type;\n@@ -633,4 +634,18 @@ public void testTimestampNanosReturnType() {\n     Type hourResultType = hour.getResultType(type);\n     assertThat(hourResultType).isEqualTo(Types.IntegerType.get());\n   }\n+\n+  @Test\n+  public void testUnknownUnsupported() {\n+    assertThatThrownBy(() -> Transforms.hour(Types.UnknownType.get()))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\"Unsupported type: unknown\");\n+\n+    Transform<Object, Integer> hour = Transforms.hour();\n+    assertThatThrownBy(() -> hour.bind(Types.UnknownType.get()))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\"Unsupported type: unknown\");\n+\n+    assertThat(hour.canTransform(Types.UnknownType.get())).isFalse();\n+  }\n }\n\ndiff --git a/api/src/test/java/org/apache/iceberg/transforms/TestTruncate.java b/api/src/test/java/org/apache/iceberg/transforms/TestTruncate.java\nindex 68527b0294d3..a6b7a31ef6cd 100644\n--- a/api/src/test/java/org/apache/iceberg/transforms/TestTruncate.java\n+++ b/api/src/test/java/org/apache/iceberg/transforms/TestTruncate.java\n@@ -116,4 +116,18 @@ public void testVerifiedIllegalWidth() {\n         .isInstanceOf(IllegalArgumentException.class)\n         .hasMessage(\"Invalid truncate width: 0 (must be > 0)\");\n   }\n+\n+  @Test\n+  public void testUnknownUnsupported() {\n+    assertThatThrownBy(() -> Transforms.truncate(Types.UnknownType.get(), 22))\n+        .isInstanceOf(UnsupportedOperationException.class)\n+        .hasMessage(\"Cannot truncate type: unknown\");\n+\n+    Transform<Object, Object> truncate = Transforms.truncate(22);\n+    assertThatThrownBy(() -> truncate.bind(Types.UnknownType.get()))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessage(\"Cannot bind to unsupported type: unknown\");\n+\n+    assertThat(truncate.canTransform(Types.UnknownType.get())).isFalse();\n+  }\n }\n\ndiff --git a/api/src/test/java/org/apache/iceberg/transforms/TestVoid.java b/api/src/test/java/org/apache/iceberg/transforms/TestVoid.java\nnew file mode 100644\nindex 000000000000..95069f5355f9\n--- /dev/null\n+++ b/api/src/test/java/org/apache/iceberg/transforms/TestVoid.java\n@@ -0,0 +1,37 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.transforms;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import org.apache.iceberg.types.Types;\n+import org.junit.jupiter.api.Test;\n+\n+public class TestVoid {\n+\n+  @Test\n+  public void testUnknownToHumanString() {\n+    Types.UnknownType unknownType = Types.UnknownType.get();\n+    Transform<Object, Void> identity = Transforms.alwaysNull();\n+\n+    assertThat(identity.toHumanString(unknownType, null))\n+        .as(\"Should produce \\\"null\\\" for null\")\n+        .isEqualTo(\"null\");\n+  }\n+}\n\ndiff --git a/api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java b/api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java\nindex af2ebae7e1a8..a222e8e66b8e 100644\n--- a/api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java\n+++ b/api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java\n@@ -136,6 +136,15 @@ public void testVariant() throws Exception {\n         .isEqualTo(variant);\n   }\n \n+  @Test\n+  public void testUnknown() throws Exception {\n+    Types.UnknownType unknown = Types.UnknownType.get();\n+    Type copy = TestHelpers.roundTripSerialize(unknown);\n+    assertThat(copy)\n+        .as(\"Unknown serialization should be equal to starting type\")\n+        .isEqualTo(unknown);\n+  }\n+\n   @Test\n   public void testSchema() throws Exception {\n     Schema schema =\n\ndiff --git a/api/src/test/java/org/apache/iceberg/types/TestTypes.java b/api/src/test/java/org/apache/iceberg/types/TestTypes.java\nindex 226c53f1e9ce..cbc37291375f 100644\n--- a/api/src/test/java/org/apache/iceberg/types/TestTypes.java\n+++ b/api/src/test/java/org/apache/iceberg/types/TestTypes.java\n@@ -44,7 +44,7 @@ public void fromPrimitiveString() {\n     assertThat(Types.fromPrimitiveString(\"Decimal(2,3)\")).isEqualTo(Types.DecimalType.of(2, 3));\n \n     assertThatExceptionOfType(IllegalArgumentException.class)\n-        .isThrownBy(() -> Types.fromPrimitiveString(\"Unknown\"))\n-        .withMessageContaining(\"Unknown\");\n+        .isThrownBy(() -> Types.fromPrimitiveString(\"abcdefghij\"))\n+        .withMessage(\"Cannot parse type string to primitive: abcdefghij\");\n   }\n }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestSortOrder.java b/core/src/test/java/org/apache/iceberg/TestSortOrder.java\nindex 3d139543b71c..7d0688e9da96 100644\n--- a/core/src/test/java/org/apache/iceberg/TestSortOrder.java\n+++ b/core/src/test/java/org/apache/iceberg/TestSortOrder.java\n@@ -36,6 +36,7 @@\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.SortOrderUtil;\n import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n import org.junit.jupiter.api.io.TempDir;\n@@ -342,6 +343,18 @@ public void testVariantUnsupported() {\n         .hasMessage(\"Unsupported type for identity: variant\");\n   }\n \n+  @Test\n+  public void testUnknownSupported() {\n+    int fieldId = 22;\n+    Schema v3Schema = new Schema(Types.NestedField.optional(fieldId, \"u\", Types.UnknownType.get()));\n+\n+    SortOrder sortOrder = SortOrder.builderFor(v3Schema).asc(\"u\").build();\n+\n+    assertThat(sortOrder.orderId()).isEqualTo(TableMetadata.INITIAL_SORT_ORDER_ID);\n+    assertThat(sortOrder.fields()).hasSize(1);\n+    assertThat(sortOrder.fields().get(0).sourceId()).isEqualTo(fieldId);\n+  }\n+\n   @TestTemplate\n   public void testPreservingOrderSortedColumnNames() {\n     SortOrder order =\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11986",
    "pr_id": 11986,
    "issue_id": 11968,
    "repo": "apache/iceberg",
    "problem_statement": "Creating Delete Vectors using Java API or Spark\n### Query engine\n\nSpark\n\n### Question\n\nQ1 - Is it possible to create Deletion Vectors in Apache Iceberg? Is a Deletion Vector file generated for a specific Deletion type Positional vs Equality?\n\nQ2 - Looking for pointers on creating Delete Vectors in Apache Iceberg using Spark or the Java API. So far I have tried creating a table using Spark and inserting a few rows and deleting a few. With \"write.delete.vector.enabled\" set to true for the table. The hope is that it will generate Deletion Vector(s). Am I missing any steps here? \n\nHere's the code snippet - https://github.com/piyushdubey/dataformats/blob/main/src/main/java/net/piyushdubey/data/IcebergTableOperations.java\n\nAppreciate any pointers on this!",
    "issue_word_count": 125,
    "test_files_count": 4,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadDelete.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadUpdate.java",
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkWriteConf.java",
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkPositionDeltaWrite.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadDelete.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadUpdate.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java"
    ],
    "base_commit": "246439a966001a1d8c8f5109cb38011acb44d549",
    "head_commit": "b342c7d1845e8ec3e4b72ca9766a04be67eeef27",
    "repo_url": "https://github.com/apache/iceberg/pull/11986",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11986",
    "dockerfile": "",
    "pr_merged_at": "2025-01-17T13:59:22.000Z",
    "patch": "diff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkWriteConf.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkWriteConf.java\nindex f9fb904db394..b3e8af5fe056 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkWriteConf.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkWriteConf.java\n@@ -37,14 +37,14 @@\n \n import java.util.Locale;\n import java.util.Map;\n+import org.apache.iceberg.BaseMetadataTable;\n import org.apache.iceberg.DistributionMode;\n import org.apache.iceberg.FileFormat;\n-import org.apache.iceberg.HasTableOperations;\n import org.apache.iceberg.IsolationLevel;\n import org.apache.iceberg.SnapshotSummary;\n import org.apache.iceberg.Table;\n-import org.apache.iceberg.TableOperations;\n import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TableUtil;\n import org.apache.iceberg.deletes.DeleteGranularity;\n import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n@@ -214,6 +214,10 @@ private boolean fanoutWriterEnabled(boolean defaultValue) {\n   }\n \n   public FileFormat deleteFileFormat() {\n+    if (!(table instanceof BaseMetadataTable) && TableUtil.formatVersion(table) >= 3) {\n+      return FileFormat.PUFFIN;\n+    }\n+\n     String valueAsString =\n         confParser\n             .stringConf()\n@@ -721,9 +725,4 @@ public DeleteGranularity deleteGranularity() {\n         .defaultValue(DeleteGranularity.FILE)\n         .parse();\n   }\n-\n-  public boolean useDVs() {\n-    TableOperations ops = ((HasTableOperations) table).operations();\n-    return ops.current().formatVersion() >= 3;\n-  }\n }\n\ndiff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkPositionDeltaWrite.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkPositionDeltaWrite.java\nindex 56ef3998e853..b1ebaf18711a 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkPositionDeltaWrite.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkPositionDeltaWrite.java\n@@ -786,7 +786,6 @@ private static class Context implements Serializable {\n     private final String queryId;\n     private final boolean useFanoutWriter;\n     private final boolean inputOrdered;\n-    private final boolean useDVs;\n \n     Context(\n         Schema dataSchema,\n@@ -805,7 +804,6 @@ private static class Context implements Serializable {\n       this.queryId = info.queryId();\n       this.useFanoutWriter = writeConf.useFanoutWriter(writeRequirements);\n       this.inputOrdered = writeRequirements.hasOrdering();\n-      this.useDVs = writeConf.useDVs();\n     }\n \n     Schema dataSchema() {\n@@ -853,7 +851,7 @@ boolean inputOrdered() {\n     }\n \n     boolean useDVs() {\n-      return useDVs;\n+      return deleteFileFormat == FileFormat.PUFFIN;\n     }\n \n     int specIdOrdinal() {\n",
    "test_patch": "diff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadDelete.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadDelete.java\nindex 505b88711371..bf9c53f82dbd 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadDelete.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadDelete.java\n@@ -31,6 +31,7 @@\n import java.util.Set;\n import java.util.stream.Collectors;\n import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowDelta;\n import org.apache.iceberg.RowLevelOperationMode;\n@@ -232,6 +233,7 @@ public void testDeleteWithDVAndHistoricalPositionDeletes() {\n         deleteFiles.stream().filter(ContentFileUtil::isDV).collect(Collectors.toList());\n     assertThat(dvs).hasSize(1);\n     assertThat(dvs).allMatch(dv -> dv.recordCount() == 3);\n+    assertThat(dvs).allMatch(dv -> FileFormat.fromFileName(dv.location()) == FileFormat.PUFFIN);\n   }\n \n   private void checkDeleteFileGranularity(DeleteGranularity deleteGranularity)\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java\nindex 7af128bcc171..cb2cf801e0c3 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadMerge.java\n@@ -27,6 +27,7 @@\n import java.util.stream.Collectors;\n import java.util.stream.IntStream;\n import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n@@ -129,6 +130,7 @@ public void testMergeWithDVAndHistoricalPositionDeletes() {\n         deleteFiles.stream().filter(ContentFileUtil::isDV).collect(Collectors.toList());\n     assertThat(dvs).hasSize(1);\n     assertThat(dvs).allMatch(dv -> dv.recordCount() == 3);\n+    assertThat(dvs).allMatch(dv -> FileFormat.fromFileName(dv.location()) == FileFormat.PUFFIN);\n   }\n \n   private void checkMergeDeleteGranularity(DeleteGranularity deleteGranularity) {\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadUpdate.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadUpdate.java\nindex 477a2e73256b..1bec21b9b68d 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadUpdate.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestMergeOnReadUpdate.java\n@@ -26,6 +26,7 @@\n import java.util.Set;\n import java.util.stream.Collectors;\n import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.RowLevelOperationMode;\n import org.apache.iceberg.Snapshot;\n@@ -209,6 +210,7 @@ public void testUpdateWithDVAndHistoricalPositionDeletes() {\n         deleteFiles.stream().filter(ContentFileUtil::isDV).collect(Collectors.toList());\n     assertThat(dvs).hasSize(1);\n     assertThat(dvs.get(0).recordCount()).isEqualTo(3);\n+    assertThat(dvs).allMatch(dv -> FileFormat.fromFileName(dv.location()) == FileFormat.PUFFIN);\n   }\n \n   private void initTable(String partitionedBy, DeleteGranularity deleteGranularity) {\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java\nindex 42d697410377..6041c22e2465 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSparkWriteConf.java\n@@ -51,6 +51,7 @@\n import java.util.List;\n import java.util.Map;\n import org.apache.iceberg.DistributionMode;\n+import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.UpdateProperties;\n@@ -540,6 +541,14 @@ public void testDeleteFileWriteConf() {\n     }\n   }\n \n+  @TestTemplate\n+  public void testDVWriteConf() {\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    table.updateProperties().set(TableProperties.FORMAT_VERSION, \"3\").commit();\n+    SparkWriteConf writeConf = new SparkWriteConf(spark, table, ImmutableMap.of());\n+    assertThat(writeConf.deleteFileFormat()).isEqualTo(FileFormat.PUFFIN);\n+  }\n+\n   private void testWriteProperties(List<Map<String, String>> propertiesSuite) {\n     withSQLConf(\n         propertiesSuite.get(0),\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11967",
    "pr_id": 11967,
    "issue_id": 11922,
    "repo": "apache/iceberg",
    "problem_statement": "create_changelog_view returns no record when end-timestamp is missing\n### Apache Iceberg version\r\n\r\n1.7.1 (latest release)\r\n\r\n### Query engine\r\n\r\nSpark\r\n\r\n### Please describe the bug üêû\r\n**Description**\r\nCreate_changelog_view does not return any records when end-timestamp is missing and parent_id is null.\r\n\r\n**Steps to reproduce**\r\n**1. Add two records in the table one after the other.**\r\n**2. Run create_changelog_view with start-timestamp and end-timestamp**\r\n**3. Run create_changelog_view with start-timestamp** \r\n\r\n**Observations**\r\n**Snapshots**\r\n<img width=\"522\" alt=\"Screenshot 2025-01-07 at 1 46 45‚ÄØPM\" src=\"https://github.com/user-attachments/assets/ce40b8b8-c196-46bb-b5d5-f3219a133ab2\" />\r\n+---------------------------+---------------------------+--------------------------+\r\n | committed_at                      | snapshot_id                          | parent_id                             |\r\n+---------------------------+---------------------------+--------------------------+\r\n|2025-01-07 13:40:34.662    | 3486607060728746628    | NULL                                   |\r\n|2025-01-07 13:40:37.126     | 1370475982752787916      | 3486607060728746628 |\r\n+---------------------------+---------------------------+--------------------------+\r\n\r\n**Run create_changelog_view with start-timestamp and end-timestamp**\r\n`\r\nCall iceberg.system.create_changelog_view(\r\n    compute_updates => true,\r\n    table => 'iceberg.db.table_1',\r\n    options => map('start-timestamp', '1736286033773', 'end-timestamp', '1736286038217'),\r\n    identifier_columns => array('foo')\r\n)\r\n`\r\n**Result**\r\nReturns both the records.\r\n\r\n**Run create_changelog_view with start-timestamp** \r\n`\r\nCall iceberg.system.create_changelog_view(\r\n    compute_updates => true,\r\n    table => 'iceberg.db.table_1',\r\n    options => map('start-timestamp', '1736286033773'),\r\n    identifier_columns => array('foo')\r\n)\r\n`\r\n**Result**\r\nReturns both the records.\r\n\r\n### Willingness to contribute\r\n\r\n- [ ] I can contribute a fix for this bug independently\r\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\r\n- [X] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 220,
    "test_files_count": 3,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java",
      "spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java",
      "spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java",
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java",
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java"
    ],
    "base_commit": "a0a1c002f01d70b428c63b4802fae536e543cda6",
    "head_commit": "22e387b161ca14bfbb88967d91876db59b00540c",
    "repo_url": "https://github.com/apache/iceberg/pull/11967",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11967",
    "dockerfile": "",
    "pr_merged_at": "2025-01-16T17:48:00.000Z",
    "patch": "diff --git a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java\nindex 5634e1436081..181db66e8219 100644\n--- a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java\n+++ b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java\n@@ -561,10 +561,11 @@ public Scan buildChangelogScan() {\n \n     boolean emptyScan = false;\n     if (startTimestamp != null) {\n-      startSnapshotId = getStartSnapshotId(startTimestamp);\n-      if (startSnapshotId == null && endTimestamp == null) {\n+      if (table.currentSnapshot() == null\n+          || startTimestamp > table.currentSnapshot().timestampMillis()) {\n         emptyScan = true;\n       }\n+      startSnapshotId = getStartSnapshotId(startTimestamp);\n     }\n \n     if (endTimestamp != null) {\n\ndiff --git a/spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java b/spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java\nindex d511fefd8ae0..0a06970d0940 100644\n--- a/spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java\n+++ b/spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java\n@@ -561,10 +561,11 @@ public Scan buildChangelogScan() {\n \n     boolean emptyScan = false;\n     if (startTimestamp != null) {\n-      startSnapshotId = getStartSnapshotId(startTimestamp);\n-      if (startSnapshotId == null && endTimestamp == null) {\n+      if (table.currentSnapshot() == null\n+          || startTimestamp > table.currentSnapshot().timestampMillis()) {\n         emptyScan = true;\n       }\n+      startSnapshotId = getStartSnapshotId(startTimestamp);\n     }\n \n     if (endTimestamp != null) {\n\ndiff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java\nindex 33a4e032ccc8..0a06970d0940 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/SparkScanBuilder.java\n@@ -561,14 +561,11 @@ public Scan buildChangelogScan() {\n \n     boolean emptyScan = false;\n     if (startTimestamp != null) {\n-      if (table.currentSnapshot() != null\n-          && table.currentSnapshot().timestampMillis() < startTimestamp) {\n+      if (table.currentSnapshot() == null\n+          || startTimestamp > table.currentSnapshot().timestampMillis()) {\n         emptyScan = true;\n       }\n       startSnapshotId = getStartSnapshotId(startTimestamp);\n-      if (startSnapshotId == null && endTimestamp == null) {\n-        emptyScan = true;\n-      }\n     }\n \n     if (endTimestamp != null) {\n",
    "test_patch": "diff --git a/spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java b/spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java\nindex 9aa4bd3d7c8c..2db40a78fda0 100644\n--- a/spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java\n+++ b/spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java\n@@ -126,6 +126,60 @@ public void testNoSnapshotIdInput() {\n         sql(\"select * from %s order by _change_ordinal, id\", viewName));\n   }\n \n+  @Test\n+  public void testOnlyStartSnapshotIdInput() {\n+    createTableWithTwoColumns();\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap2 = table.currentSnapshot();\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s',\" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.START_SNAPSHOT_ID, snap0.snapshotId());\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(2, \"b\", INSERT, 0, snap1.snapshotId()),\n+            row(-2, \"b\", INSERT, 1, snap2.snapshotId()),\n+            row(2, \"b\", DELETE, 1, snap2.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n+  @Test\n+  public void testOnlyEndTimestampIdInput() {\n+    createTableWithTwoColumns();\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s',\" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.END_SNAPSHOT_ID, snap1.snapshotId());\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(1, \"a\", INSERT, 0, snap0.snapshotId()), row(2, \"b\", INSERT, 1, snap1.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n   @Test\n   public void testTimestampsBasedQuery() {\n     createTableWithTwoColumns();\n@@ -186,6 +240,149 @@ public void testTimestampsBasedQuery() {\n         sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n   }\n \n+  @Test\n+  public void testOnlyStartTimestampInput() {\n+    createTableWithTwoColumns();\n+    long beginning = System.currentTimeMillis();\n+\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+    long afterFirstInsert = waitUntilAfter(snap0.timestampMillis());\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap2 = table.currentSnapshot();\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.START_TIMESTAMP, beginning);\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(1, \"a\", INSERT, 0, snap0.snapshotId()),\n+            row(2, \"b\", INSERT, 1, snap1.snapshotId()),\n+            row(-2, \"b\", INSERT, 2, snap2.snapshotId()),\n+            row(2, \"b\", DELETE, 2, snap2.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+\n+    returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.START_TIMESTAMP, afterFirstInsert);\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(2, \"b\", INSERT, 0, snap1.snapshotId()),\n+            row(-2, \"b\", INSERT, 1, snap2.snapshotId()),\n+            row(2, \"b\", DELETE, 1, snap2.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n+  @Test\n+  public void testOnlyEndTimestampInput() {\n+    createTableWithTwoColumns();\n+\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+    long afterSecondInsert = waitUntilAfter(snap1.timestampMillis());\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.END_TIMESTAMP, afterSecondInsert);\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(1, \"a\", INSERT, 0, snap0.snapshotId()), row(2, \"b\", INSERT, 1, snap1.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n+  @Test\n+  public void testStartTimeStampEndSnapshotId() {\n+    createTableWithTwoColumns();\n+\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+    long afterFirstInsert = waitUntilAfter(snap0.timestampMillis());\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap2 = table.currentSnapshot();\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \"\n+                + \"options => map('%s', '%s', '%s', '%s'))\",\n+            catalogName,\n+            tableName,\n+            SparkReadOptions.START_TIMESTAMP,\n+            afterFirstInsert,\n+            SparkReadOptions.END_SNAPSHOT_ID,\n+            snap2.snapshotId());\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(2, \"b\", INSERT, 0, snap1.snapshotId()),\n+            row(-2, \"b\", INSERT, 1, snap2.snapshotId()),\n+            row(2, \"b\", DELETE, 1, snap2.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n+  @Test\n+  public void testStartSnapshotIdEndTimestamp() {\n+    createTableWithTwoColumns();\n+\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+    long afterSecondInsert = waitUntilAfter(snap1.timestampMillis());\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+    table.refresh();\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \"\n+                + \"options => map('%s', '%s', '%s', '%s'))\",\n+            catalogName,\n+            tableName,\n+            SparkReadOptions.START_SNAPSHOT_ID,\n+            snap0.snapshotId(),\n+            SparkReadOptions.END_TIMESTAMP,\n+            afterSecondInsert);\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(row(2, \"b\", INSERT, 0, snap1.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n   @Test\n   public void testUpdate() {\n     createTableWithTwoColumns();\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java\nindex 9aa4bd3d7c8c..2db40a78fda0 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java\n@@ -126,6 +126,60 @@ public void testNoSnapshotIdInput() {\n         sql(\"select * from %s order by _change_ordinal, id\", viewName));\n   }\n \n+  @Test\n+  public void testOnlyStartSnapshotIdInput() {\n+    createTableWithTwoColumns();\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap2 = table.currentSnapshot();\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s',\" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.START_SNAPSHOT_ID, snap0.snapshotId());\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(2, \"b\", INSERT, 0, snap1.snapshotId()),\n+            row(-2, \"b\", INSERT, 1, snap2.snapshotId()),\n+            row(2, \"b\", DELETE, 1, snap2.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n+  @Test\n+  public void testOnlyEndTimestampIdInput() {\n+    createTableWithTwoColumns();\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s',\" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.END_SNAPSHOT_ID, snap1.snapshotId());\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(1, \"a\", INSERT, 0, snap0.snapshotId()), row(2, \"b\", INSERT, 1, snap1.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n   @Test\n   public void testTimestampsBasedQuery() {\n     createTableWithTwoColumns();\n@@ -186,6 +240,149 @@ public void testTimestampsBasedQuery() {\n         sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n   }\n \n+  @Test\n+  public void testOnlyStartTimestampInput() {\n+    createTableWithTwoColumns();\n+    long beginning = System.currentTimeMillis();\n+\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+    long afterFirstInsert = waitUntilAfter(snap0.timestampMillis());\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap2 = table.currentSnapshot();\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.START_TIMESTAMP, beginning);\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(1, \"a\", INSERT, 0, snap0.snapshotId()),\n+            row(2, \"b\", INSERT, 1, snap1.snapshotId()),\n+            row(-2, \"b\", INSERT, 2, snap2.snapshotId()),\n+            row(2, \"b\", DELETE, 2, snap2.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+\n+    returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.START_TIMESTAMP, afterFirstInsert);\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(2, \"b\", INSERT, 0, snap1.snapshotId()),\n+            row(-2, \"b\", INSERT, 1, snap2.snapshotId()),\n+            row(2, \"b\", DELETE, 1, snap2.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n+  @Test\n+  public void testOnlyEndTimestampInput() {\n+    createTableWithTwoColumns();\n+\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+    long afterSecondInsert = waitUntilAfter(snap1.timestampMillis());\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.END_TIMESTAMP, afterSecondInsert);\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(1, \"a\", INSERT, 0, snap0.snapshotId()), row(2, \"b\", INSERT, 1, snap1.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n+  @Test\n+  public void testStartTimeStampEndSnapshotId() {\n+    createTableWithTwoColumns();\n+\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+    long afterFirstInsert = waitUntilAfter(snap0.timestampMillis());\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap2 = table.currentSnapshot();\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \"\n+                + \"options => map('%s', '%s', '%s', '%s'))\",\n+            catalogName,\n+            tableName,\n+            SparkReadOptions.START_TIMESTAMP,\n+            afterFirstInsert,\n+            SparkReadOptions.END_SNAPSHOT_ID,\n+            snap2.snapshotId());\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(2, \"b\", INSERT, 0, snap1.snapshotId()),\n+            row(-2, \"b\", INSERT, 1, snap2.snapshotId()),\n+            row(2, \"b\", DELETE, 1, snap2.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n+  @Test\n+  public void testStartSnapshotIdEndTimestamp() {\n+    createTableWithTwoColumns();\n+\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+    long afterSecondInsert = waitUntilAfter(snap1.timestampMillis());\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+    table.refresh();\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \"\n+                + \"options => map('%s', '%s', '%s', '%s'))\",\n+            catalogName,\n+            tableName,\n+            SparkReadOptions.START_SNAPSHOT_ID,\n+            snap0.snapshotId(),\n+            SparkReadOptions.END_TIMESTAMP,\n+            afterSecondInsert);\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(row(2, \"b\", INSERT, 0, snap1.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n   @Test\n   public void testUpdate() {\n     createTableWithTwoColumns();\n\ndiff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java\nindex 3fd760c67c4a..f23bebdbf435 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestCreateChangelogViewProcedure.java\n@@ -123,6 +123,60 @@ public void testNoSnapshotIdInput() {\n         sql(\"select * from %s order by _change_ordinal, id\", viewName));\n   }\n \n+  @TestTemplate\n+  public void testOnlyStartSnapshotIdInput() {\n+    createTableWithTwoColumns();\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap2 = table.currentSnapshot();\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s',\" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.START_SNAPSHOT_ID, snap0.snapshotId());\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(2, \"b\", INSERT, 0, snap1.snapshotId()),\n+            row(-2, \"b\", INSERT, 1, snap2.snapshotId()),\n+            row(2, \"b\", DELETE, 1, snap2.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n+  @TestTemplate\n+  public void testOnlyEndTimestampIdInput() {\n+    createTableWithTwoColumns();\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s',\" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.END_SNAPSHOT_ID, snap1.snapshotId());\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(1, \"a\", INSERT, 0, snap0.snapshotId()), row(2, \"b\", INSERT, 1, snap1.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n   @TestTemplate\n   public void testTimestampsBasedQuery() {\n     createTableWithTwoColumns();\n@@ -183,6 +237,149 @@ public void testTimestampsBasedQuery() {\n         sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n   }\n \n+  @TestTemplate\n+  public void testOnlyStartTimestampInput() {\n+    createTableWithTwoColumns();\n+    long beginning = System.currentTimeMillis();\n+\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+    long afterFirstInsert = waitUntilAfter(snap0.timestampMillis());\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap2 = table.currentSnapshot();\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.START_TIMESTAMP, beginning);\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(1, \"a\", INSERT, 0, snap0.snapshotId()),\n+            row(2, \"b\", INSERT, 1, snap1.snapshotId()),\n+            row(-2, \"b\", INSERT, 2, snap2.snapshotId()),\n+            row(2, \"b\", DELETE, 2, snap2.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+\n+    returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.START_TIMESTAMP, afterFirstInsert);\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(2, \"b\", INSERT, 0, snap1.snapshotId()),\n+            row(-2, \"b\", INSERT, 1, snap2.snapshotId()),\n+            row(2, \"b\", DELETE, 1, snap2.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n+  @TestTemplate\n+  public void testOnlyEndTimestampInput() {\n+    createTableWithTwoColumns();\n+\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+    long afterSecondInsert = waitUntilAfter(snap1.timestampMillis());\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \" + \"options => map('%s', '%s'))\",\n+            catalogName, tableName, SparkReadOptions.END_TIMESTAMP, afterSecondInsert);\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(1, \"a\", INSERT, 0, snap0.snapshotId()), row(2, \"b\", INSERT, 1, snap1.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n+  @TestTemplate\n+  public void testStartTimeStampEndSnapshotId() {\n+    createTableWithTwoColumns();\n+\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+    long afterFirstInsert = waitUntilAfter(snap0.timestampMillis());\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap2 = table.currentSnapshot();\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \"\n+                + \"options => map('%s', '%s', '%s', '%s'))\",\n+            catalogName,\n+            tableName,\n+            SparkReadOptions.START_TIMESTAMP,\n+            afterFirstInsert,\n+            SparkReadOptions.END_SNAPSHOT_ID,\n+            snap2.snapshotId());\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(\n+            row(2, \"b\", INSERT, 0, snap1.snapshotId()),\n+            row(-2, \"b\", INSERT, 1, snap2.snapshotId()),\n+            row(2, \"b\", DELETE, 1, snap2.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n+  @TestTemplate\n+  public void testStartSnapshotIdEndTimestamp() {\n+    createTableWithTwoColumns();\n+\n+    sql(\"INSERT INTO %s VALUES (1, 'a')\", tableName);\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    Snapshot snap0 = table.currentSnapshot();\n+\n+    sql(\"INSERT INTO %s VALUES (2, 'b')\", tableName);\n+    table.refresh();\n+    Snapshot snap1 = table.currentSnapshot();\n+    long afterSecondInsert = waitUntilAfter(snap1.timestampMillis());\n+\n+    sql(\"INSERT OVERWRITE %s VALUES (-2, 'b')\", tableName);\n+    table.refresh();\n+\n+    List<Object[]> returns =\n+        sql(\n+            \"CALL %s.system.create_changelog_view(table => '%s', \"\n+                + \"options => map('%s', '%s', '%s', '%s'))\",\n+            catalogName,\n+            tableName,\n+            SparkReadOptions.START_SNAPSHOT_ID,\n+            snap0.snapshotId(),\n+            SparkReadOptions.END_TIMESTAMP,\n+            afterSecondInsert);\n+\n+    assertEquals(\n+        \"Rows should match\",\n+        ImmutableList.of(row(2, \"b\", INSERT, 0, snap1.snapshotId())),\n+        sql(\"select * from %s order by _change_ordinal, id\", returns.get(0)[0]));\n+  }\n+\n   @TestTemplate\n   public void testUpdate() {\n     createTableWithTwoColumns();\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11943",
    "pr_id": 11943,
    "issue_id": 11932,
    "repo": "apache/iceberg",
    "problem_statement": "A casting error occurs when Sanitizing the expression value in a specific case.\n### Apache Iceberg version\n\n1.7.1 (latest release)\n\n### Query engine\n\nFlink\n\n### Please describe the bug üêû\n\nI found a code suspected of being a bug while running rewrite data file flink action.\r\nCasting error occurs When binding the table structure and applying expression filter.\r\nFrom what I saw and understood the code, it seems to be an issue that occurred in the process of sanitize to string value.\r\n\r\nFrom the logic below, all value parameters delivered to the Object are delivered to the literal instance except for value method of the String Sanitizer.\r\n\r\nhttps://github.com/apache/iceberg/blob/apache-iceberg-1.7.1/api/src/main/java/org/apache/iceberg/expressions/ExpressionUtil.java#L514\r\n\r\nTherefore, I think it would be right to modify it as follows.\r\n\r\n``` java\r\n// ExpressionUtils.StringSanitizer\r\nprivate String value(BoundLiteralPredicate<?> pred) {\r\n      return sanitize(pred.term().type(), pred.literal(), nowMicros, today);\r\n    }\r\n\r\n// ExpressionUtils\r\nprivate static String sanitize(Type type, Literal value, long now, int today) {\r\n  // same code\r\n}\r\n```\r\n\r\nor\r\n\r\nChange the pass parameters to literal.value() from where they are called.\n\n### Willingness to contribute\n\n- [X] I can contribute a fix for this bug independently\n- [X] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 226,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "api/src/main/java/org/apache/iceberg/expressions/ExpressionUtil.java",
      "api/src/test/java/org/apache/iceberg/expressions/TestExpressionUtil.java"
    ],
    "pr_changed_test_files": [
      "api/src/test/java/org/apache/iceberg/expressions/TestExpressionUtil.java"
    ],
    "base_commit": "72dcce95e294835f978dc1d6c9a3be5d89123410",
    "head_commit": "2858f82339818a4b5ab67c1ce238d11448cd9097",
    "repo_url": "https://github.com/apache/iceberg/pull/11943",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11943",
    "dockerfile": "",
    "pr_merged_at": "2025-01-16T11:42:38.000Z",
    "patch": "diff --git a/api/src/main/java/org/apache/iceberg/expressions/ExpressionUtil.java b/api/src/main/java/org/apache/iceberg/expressions/ExpressionUtil.java\nindex 68b9e9447926..028b07827b54 100644\n--- a/api/src/main/java/org/apache/iceberg/expressions/ExpressionUtil.java\n+++ b/api/src/main/java/org/apache/iceberg/expressions/ExpressionUtil.java\n@@ -511,6 +511,10 @@ private static List<String> abbreviateValues(List<String> sanitizedValues) {\n     return sanitizedValues;\n   }\n \n+  private static String sanitize(Type type, Literal<?> lit, long now, int today) {\n+    return sanitize(type, lit.value(), now, today);\n+  }\n+\n   private static String sanitize(Type type, Object value, long now, int today) {\n     switch (type.typeId()) {\n       case INTEGER:\n",
    "test_patch": "diff --git a/api/src/test/java/org/apache/iceberg/expressions/TestExpressionUtil.java b/api/src/test/java/org/apache/iceberg/expressions/TestExpressionUtil.java\nindex 10d3b6d0adfa..902c0260d63a 100644\n--- a/api/src/test/java/org/apache/iceberg/expressions/TestExpressionUtil.java\n+++ b/api/src/test/java/org/apache/iceberg/expressions/TestExpressionUtil.java\n@@ -417,6 +417,10 @@ public void testSanitizeDate() {\n         Expressions.equal(\"test\", \"(date)\"),\n         ExpressionUtil.sanitize(Expressions.equal(\"test\", \"2022-04-29\")));\n \n+    assertEquals(\n+        Expressions.equal(\"date\", \"(date)\"),\n+        ExpressionUtil.sanitize(Expressions.equal(\"date\", \"2022-04-29\").bind(STRUCT, true)));\n+\n     assertEquals(\n         Expressions.equal(\"date\", \"(date)\"),\n         ExpressionUtil.sanitize(STRUCT, Expressions.equal(\"date\", \"2022-04-29\"), true));\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11831",
    "pr_id": 11831,
    "issue_id": 10392,
    "repo": "apache/iceberg",
    "problem_statement": "Variant Data Type Support\n### Proposed Change\r\n\r\nWe would like to propose to add Variant type to Iceberg data types. \r\n\r\nVariant data types allow for the efficient binary encoding of dynamic semi-structured data such as JSON, Avro,Parquet, etc. By encoding semi-structured data as a variant column, we retain the flexibility of the source data, while allowing query engines to more efficiently operate on the data.\r\n\r\nWith the support of Variant type, such data can be encoded in an efficient binary representation internally for better performance. Without that, we need to parse the data in its format inefficiently.\r\n\r\nThis will allow the following use cases:\r\n\r\n- Create an Iceberg table with a Variant column\r\n`CREATE OR REPLACE TABLE car_sales(record Variant);`\r\n- Insert semi-structured data into the Variant column\r\n`INSERT INTO car_sales SELECT PARSE_JSON(<json_string>)`\r\n- Query against the semi-structured data\r\n`SELECT VARIANT_GET(record, '$.dealer.ship', 'string') FROM car_sales`\r\n\r\n\r\n\r\n\r\n### Proposal document\r\nhttps://docs.google.com/document/d/1sq70XDiWJ2DemWyA5dVB80gKzwi0CWoM0LOWM7VJVd8/edit?tab=t.0\r\n\r\n### Specifications\r\n\r\n- [X] Table\r\n- [ ] View\r\n- [ ] REST\r\n- [ ] Puffin\r\n- [ ] Encryption\r\n- [ ] Other",
    "issue_word_count": 170,
    "test_files_count": 8,
    "non_test_files_count": 23,
    "pr_changed_files": [
      "api/src/main/java/org/apache/iceberg/Accessors.java",
      "api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java",
      "api/src/main/java/org/apache/iceberg/types/AssignIds.java",
      "api/src/main/java/org/apache/iceberg/types/CheckCompatibility.java",
      "api/src/main/java/org/apache/iceberg/types/FindTypeVisitor.java",
      "api/src/main/java/org/apache/iceberg/types/GetProjectedIds.java",
      "api/src/main/java/org/apache/iceberg/types/IndexById.java",
      "api/src/main/java/org/apache/iceberg/types/IndexByName.java",
      "api/src/main/java/org/apache/iceberg/types/IndexParents.java",
      "api/src/main/java/org/apache/iceberg/types/PrimitiveHolder.java",
      "api/src/main/java/org/apache/iceberg/types/PruneColumns.java",
      "api/src/main/java/org/apache/iceberg/types/ReassignDoc.java",
      "api/src/main/java/org/apache/iceberg/types/ReassignIds.java",
      "api/src/main/java/org/apache/iceberg/types/Type.java",
      "api/src/main/java/org/apache/iceberg/types/TypeUtil.java",
      "api/src/main/java/org/apache/iceberg/types/Types.java",
      "api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java",
      "api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java",
      "api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java",
      "api/src/test/java/org/apache/iceberg/types/TestTypes.java",
      "core/src/main/java/org/apache/iceberg/SchemaParser.java",
      "core/src/main/java/org/apache/iceberg/SchemaUpdate.java",
      "core/src/main/java/org/apache/iceberg/mapping/MappingUtil.java",
      "core/src/main/java/org/apache/iceberg/schema/SchemaWithPartnerVisitor.java",
      "core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java",
      "core/src/main/java/org/apache/iceberg/types/FixupTypes.java",
      "core/src/test/java/org/apache/iceberg/TestSchemaParser.java",
      "core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java",
      "core/src/test/java/org/apache/iceberg/mapping/TestNameMapping.java",
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java"
    ],
    "pr_changed_test_files": [
      "api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java",
      "api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java",
      "api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java",
      "api/src/test/java/org/apache/iceberg/types/TestTypes.java",
      "core/src/test/java/org/apache/iceberg/TestSchemaParser.java",
      "core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java",
      "core/src/test/java/org/apache/iceberg/mapping/TestNameMapping.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java"
    ],
    "base_commit": "bcbbd0344623ffea5b092e2de5debb0bc12892a1",
    "head_commit": "1472417cce3e2671de8197164078f6b056c0bd09",
    "repo_url": "https://github.com/apache/iceberg/pull/11831",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11831",
    "dockerfile": "",
    "pr_merged_at": "2025-02-18T16:27:38.000Z",
    "patch": "diff --git a/api/src/main/java/org/apache/iceberg/Accessors.java b/api/src/main/java/org/apache/iceberg/Accessors.java\nindex 08233624f244..0b36730fbb4b 100644\n--- a/api/src/main/java/org/apache/iceberg/Accessors.java\n+++ b/api/src/main/java/org/apache/iceberg/Accessors.java\n@@ -232,6 +232,11 @@ public Map<Integer, Accessor<StructLike>> struct(\n       return accessors;\n     }\n \n+    @Override\n+    public Map<Integer, Accessor<StructLike>> variant(Types.VariantType variant) {\n+      return null;\n+    }\n+\n     @Override\n     public Map<Integer, Accessor<StructLike>> field(\n         Types.NestedField field, Map<Integer, Accessor<StructLike>> fieldResult) {\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java b/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java\nindex e58f76a8de56..75055cddc197 100644\n--- a/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java\n+++ b/api/src/main/java/org/apache/iceberg/types/AssignFreshIds.java\n@@ -124,6 +124,11 @@ public Type map(Types.MapType map, Supplier<Type> keyFuture, Supplier<Type> valu\n     }\n   }\n \n+  @Override\n+  public Type variant(Types.VariantType variant) {\n+    return variant;\n+  }\n+\n   @Override\n   public Type primitive(Type.PrimitiveType primitive) {\n     return primitive;\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/AssignIds.java b/api/src/main/java/org/apache/iceberg/types/AssignIds.java\nindex 68588f581adc..b2f72751eb89 100644\n--- a/api/src/main/java/org/apache/iceberg/types/AssignIds.java\n+++ b/api/src/main/java/org/apache/iceberg/types/AssignIds.java\n@@ -92,6 +92,11 @@ public Type map(Types.MapType map, Supplier<Type> keyFuture, Supplier<Type> valu\n     }\n   }\n \n+  @Override\n+  public Type variant(Types.VariantType variant) {\n+    return variant;\n+  }\n+\n   @Override\n   public Type primitive(Type.PrimitiveType primitive) {\n     return primitive;\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/CheckCompatibility.java b/api/src/main/java/org/apache/iceberg/types/CheckCompatibility.java\nindex 502e52c345e5..725f7f42562e 100644\n--- a/api/src/main/java/org/apache/iceberg/types/CheckCompatibility.java\n+++ b/api/src/main/java/org/apache/iceberg/types/CheckCompatibility.java\n@@ -250,6 +250,16 @@ public List<String> map(\n     }\n   }\n \n+  @Override\n+  public List<String> variant(Types.VariantType readVariant) {\n+    if (currentType.isVariantType()) {\n+      return NO_ERRORS;\n+    }\n+\n+    // Currently promotion is not allowed to variant type\n+    return ImmutableList.of(String.format(\": %s cannot be read as a %s\", currentType, readVariant));\n+  }\n+\n   @Override\n   public List<String> primitive(Type.PrimitiveType readPrimitive) {\n     if (currentType.equals(readPrimitive)) {\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/FindTypeVisitor.java b/api/src/main/java/org/apache/iceberg/types/FindTypeVisitor.java\nindex f0750f337e2e..64faebb48243 100644\n--- a/api/src/main/java/org/apache/iceberg/types/FindTypeVisitor.java\n+++ b/api/src/main/java/org/apache/iceberg/types/FindTypeVisitor.java\n@@ -77,9 +77,9 @@ public Type map(Types.MapType map, Type keyResult, Type valueResult) {\n   }\n \n   @Override\n-  public Type variant() {\n-    if (predicate.test(Types.VariantType.get())) {\n-      return Types.VariantType.get();\n+  public Type variant(Types.VariantType variant) {\n+    if (predicate.test(variant)) {\n+      return variant;\n     }\n \n     return null;\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/GetProjectedIds.java b/api/src/main/java/org/apache/iceberg/types/GetProjectedIds.java\nindex a8a7de065ece..1ec70b8578bc 100644\n--- a/api/src/main/java/org/apache/iceberg/types/GetProjectedIds.java\n+++ b/api/src/main/java/org/apache/iceberg/types/GetProjectedIds.java\n@@ -47,7 +47,9 @@ public Set<Integer> struct(Types.StructType struct, List<Set<Integer>> fieldResu\n \n   @Override\n   public Set<Integer> field(Types.NestedField field, Set<Integer> fieldResult) {\n-    if ((includeStructIds && field.type().isStructType()) || field.type().isPrimitiveType()) {\n+    if ((includeStructIds && field.type().isStructType())\n+        || field.type().isPrimitiveType()\n+        || field.type().isVariantType()) {\n       fieldIds.add(field.fieldId());\n     }\n     return fieldIds;\n@@ -72,4 +74,9 @@ public Set<Integer> map(Types.MapType map, Set<Integer> keyResult, Set<Integer>\n     }\n     return fieldIds;\n   }\n+\n+  @Override\n+  public Set<Integer> variant(Types.VariantType variant) {\n+    return null;\n+  }\n }\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/IndexById.java b/api/src/main/java/org/apache/iceberg/types/IndexById.java\nindex 40280c5ed9dd..a7b96eb381f7 100644\n--- a/api/src/main/java/org/apache/iceberg/types/IndexById.java\n+++ b/api/src/main/java/org/apache/iceberg/types/IndexById.java\n@@ -64,4 +64,9 @@ public Map<Integer, Types.NestedField> map(\n     }\n     return null;\n   }\n+\n+  @Override\n+  public Map<Integer, Types.NestedField> variant(Types.VariantType variant) {\n+    return null;\n+  }\n }\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/IndexByName.java b/api/src/main/java/org/apache/iceberg/types/IndexByName.java\nindex 131434c9a156..60258f5c5c3e 100644\n--- a/api/src/main/java/org/apache/iceberg/types/IndexByName.java\n+++ b/api/src/main/java/org/apache/iceberg/types/IndexByName.java\n@@ -177,7 +177,7 @@ public Map<String, Integer> map(\n   }\n \n   @Override\n-  public Map<String, Integer> variant() {\n+  public Map<String, Integer> variant(Types.VariantType variant) {\n     return nameToId;\n   }\n \n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/IndexParents.java b/api/src/main/java/org/apache/iceberg/types/IndexParents.java\nindex 952447ed2799..6e611d47e912 100644\n--- a/api/src/main/java/org/apache/iceberg/types/IndexParents.java\n+++ b/api/src/main/java/org/apache/iceberg/types/IndexParents.java\n@@ -77,7 +77,7 @@ public Map<Integer, Integer> map(\n   }\n \n   @Override\n-  public Map<Integer, Integer> variant() {\n+  public Map<Integer, Integer> variant(Types.VariantType variant) {\n     return idToParent;\n   }\n \n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/PrimitiveHolder.java b/api/src/main/java/org/apache/iceberg/types/PrimitiveHolder.java\nindex 42f0da38167d..928a65878d3a 100644\n--- a/api/src/main/java/org/apache/iceberg/types/PrimitiveHolder.java\n+++ b/api/src/main/java/org/apache/iceberg/types/PrimitiveHolder.java\n@@ -33,6 +33,6 @@ class PrimitiveHolder implements Serializable {\n   }\n \n   Object readResolve() throws ObjectStreamException {\n-    return Types.fromPrimitiveString(typeAsString);\n+    return Types.fromTypeName(typeAsString);\n   }\n }\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/PruneColumns.java b/api/src/main/java/org/apache/iceberg/types/PruneColumns.java\nindex daf2e6bbc0ca..56f01cf34bb5 100644\n--- a/api/src/main/java/org/apache/iceberg/types/PruneColumns.java\n+++ b/api/src/main/java/org/apache/iceberg/types/PruneColumns.java\n@@ -159,6 +159,11 @@ public Type map(Types.MapType map, Type ignored, Type valueResult) {\n     return null;\n   }\n \n+  @Override\n+  public Type variant(Types.VariantType variant) {\n+    return null;\n+  }\n+\n   @Override\n   public Type primitive(Type.PrimitiveType primitive) {\n     return null;\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java b/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java\nindex 9ce04a7bd103..328d81c42885 100644\n--- a/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java\n+++ b/api/src/main/java/org/apache/iceberg/types/ReassignDoc.java\n@@ -96,6 +96,11 @@ public Type map(Types.MapType map, Supplier<Type> keyTypeFuture, Supplier<Type>\n     }\n   }\n \n+  @Override\n+  public Type variant(Types.VariantType variant) {\n+    return variant;\n+  }\n+\n   @Override\n   public Type primitive(Type.PrimitiveType primitive) {\n     return primitive;\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/ReassignIds.java b/api/src/main/java/org/apache/iceberg/types/ReassignIds.java\nindex 565ceee2a901..3d114f093f6b 100644\n--- a/api/src/main/java/org/apache/iceberg/types/ReassignIds.java\n+++ b/api/src/main/java/org/apache/iceberg/types/ReassignIds.java\n@@ -157,6 +157,11 @@ public Type map(Types.MapType map, Supplier<Type> keyTypeFuture, Supplier<Type>\n     }\n   }\n \n+  @Override\n+  public Type variant(Types.VariantType variant) {\n+    return variant;\n+  }\n+\n   @Override\n   public Type primitive(Type.PrimitiveType primitive) {\n     return primitive; // nothing to reassign\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/Type.java b/api/src/main/java/org/apache/iceberg/types/Type.java\nindex f4c6f22134a5..67e40df9e939 100644\n--- a/api/src/main/java/org/apache/iceberg/types/Type.java\n+++ b/api/src/main/java/org/apache/iceberg/types/Type.java\n@@ -82,6 +82,10 @@ default Types.MapType asMapType() {\n     throw new IllegalArgumentException(\"Not a map type: \" + this);\n   }\n \n+  default Types.VariantType asVariantType() {\n+    throw new IllegalArgumentException(\"Not a variant type: \" + this);\n+  }\n+\n   default boolean isNestedType() {\n     return false;\n   }\n@@ -98,6 +102,10 @@ default boolean isMapType() {\n     return false;\n   }\n \n+  default boolean isVariantType() {\n+    return false;\n+  }\n+\n   default NestedType asNestedType() {\n     throw new IllegalArgumentException(\"Not a nested type: \" + this);\n   }\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/TypeUtil.java b/api/src/main/java/org/apache/iceberg/types/TypeUtil.java\nindex 39f2898757a6..4892696ab450 100644\n--- a/api/src/main/java/org/apache/iceberg/types/TypeUtil.java\n+++ b/api/src/main/java/org/apache/iceberg/types/TypeUtil.java\n@@ -616,8 +616,16 @@ public T map(Types.MapType map, T keyResult, T valueResult) {\n       return null;\n     }\n \n+    /**\n+     * @deprecated will be removed in 2.0.0; use {@link #variant(Types.VariantType)} instead.\n+     */\n+    @Deprecated\n     public T variant() {\n-      return null;\n+      return variant(Types.VariantType.get());\n+    }\n+\n+    public T variant(Types.VariantType variant) {\n+      throw new UnsupportedOperationException(\"Unsupported type: variant\");\n     }\n \n     public T primitive(Type.PrimitiveType primitive) {\n@@ -684,7 +692,7 @@ public static <T> T visit(Type type, SchemaVisitor<T> visitor) {\n         return visitor.map(map, keyResult, valueResult);\n \n       case VARIANT:\n-        return visitor.variant();\n+        return visitor.variant(type.asVariantType());\n \n       default:\n         return visitor.primitive(type.asPrimitiveType());\n@@ -712,6 +720,10 @@ public T map(Types.MapType map, Supplier<T> keyResult, Supplier<T> valueResult)\n       return null;\n     }\n \n+    public T variant(Types.VariantType variant) {\n+      throw new UnsupportedOperationException(\"Unsupported type: variant\");\n+    }\n+\n     public T primitive(Type.PrimitiveType primitive) {\n       return null;\n     }\n@@ -788,6 +800,9 @@ public static <T> T visit(Type type, CustomOrderSchemaVisitor<T> visitor) {\n             new VisitFuture<>(map.keyType(), visitor),\n             new VisitFuture<>(map.valueType(), visitor));\n \n+      case VARIANT:\n+        return visitor.variant(type.asVariantType());\n+\n       default:\n         return visitor.primitive(type.asPrimitiveType());\n     }\n\ndiff --git a/api/src/main/java/org/apache/iceberg/types/Types.java b/api/src/main/java/org/apache/iceberg/types/Types.java\nindex 6882f718508b..c1935d6980e9 100644\n--- a/api/src/main/java/org/apache/iceberg/types/Types.java\n+++ b/api/src/main/java/org/apache/iceberg/types/Types.java\n@@ -39,8 +39,8 @@ public class Types {\n \n   private Types() {}\n \n-  private static final ImmutableMap<String, PrimitiveType> TYPES =\n-      ImmutableMap.<String, PrimitiveType>builder()\n+  private static final ImmutableMap<String, Type> TYPES =\n+      ImmutableMap.<String, Type>builder()\n           .put(BooleanType.get().toString(), BooleanType.get())\n           .put(IntegerType.get().toString(), IntegerType.get())\n           .put(LongType.get().toString(), LongType.get())\n@@ -56,13 +56,14 @@ private Types() {}\n           .put(UUIDType.get().toString(), UUIDType.get())\n           .put(BinaryType.get().toString(), BinaryType.get())\n           .put(UnknownType.get().toString(), UnknownType.get())\n+          .put(VariantType.get().toString(), VariantType.get())\n           .buildOrThrow();\n \n   private static final Pattern FIXED = Pattern.compile(\"fixed\\\\[\\\\s*(\\\\d+)\\\\s*\\\\]\");\n   private static final Pattern DECIMAL =\n       Pattern.compile(\"decimal\\\\(\\\\s*(\\\\d+)\\\\s*,\\\\s*(\\\\d+)\\\\s*\\\\)\");\n \n-  public static PrimitiveType fromPrimitiveString(String typeString) {\n+  public static Type fromTypeName(String typeString) {\n     String lowerTypeString = typeString.toLowerCase(Locale.ROOT);\n     if (TYPES.containsKey(lowerTypeString)) {\n       return TYPES.get(lowerTypeString);\n@@ -81,6 +82,15 @@ public static PrimitiveType fromPrimitiveString(String typeString) {\n     throw new IllegalArgumentException(\"Cannot parse type string to primitive: \" + typeString);\n   }\n \n+  public static PrimitiveType fromPrimitiveString(String typeString) {\n+    Type type = fromTypeName(typeString);\n+    if (type.isPrimitiveType()) {\n+      return type.asPrimitiveType();\n+    }\n+\n+    throw new IllegalArgumentException(\"Cannot parse type string: variant is not a primitive type\");\n+  }\n+\n   public static class BooleanType extends PrimitiveType {\n     private static final BooleanType INSTANCE = new BooleanType();\n \n@@ -430,6 +440,16 @@ public String toString() {\n       return \"variant\";\n     }\n \n+    @Override\n+    public boolean isVariantType() {\n+      return true;\n+    }\n+\n+    @Override\n+    public VariantType asVariantType() {\n+      return this;\n+    }\n+\n     @Override\n     public boolean equals(Object o) {\n       if (this == o) {\n\ndiff --git a/core/src/main/java/org/apache/iceberg/SchemaParser.java b/core/src/main/java/org/apache/iceberg/SchemaParser.java\nindex 27e6ed048712..04655ce3f7d7 100644\n--- a/core/src/main/java/org/apache/iceberg/SchemaParser.java\n+++ b/core/src/main/java/org/apache/iceberg/SchemaParser.java\n@@ -143,8 +143,8 @@ static void toJson(Type.PrimitiveType primitive, JsonGenerator generator) throws\n   }\n \n   static void toJson(Type type, JsonGenerator generator) throws IOException {\n-    if (type.isPrimitiveType()) {\n-      toJson(type.asPrimitiveType(), generator);\n+    if (type.isPrimitiveType() || type.isVariantType()) {\n+      generator.writeString(type.toString());\n     } else {\n       Type.NestedType nested = type.asNestedType();\n       switch (type.typeId()) {\n@@ -179,7 +179,7 @@ public static String toJson(Schema schema, boolean pretty) {\n \n   private static Type typeFromJson(JsonNode json) {\n     if (json.isTextual()) {\n-      return Types.fromPrimitiveString(json.asText());\n+      return Types.fromTypeName(json.asText());\n     } else if (json.isObject()) {\n       JsonNode typeObj = json.get(TYPE);\n       if (typeObj != null) {\n\ndiff --git a/core/src/main/java/org/apache/iceberg/SchemaUpdate.java b/core/src/main/java/org/apache/iceberg/SchemaUpdate.java\nindex 2b541080ac72..7726c3a785d0 100644\n--- a/core/src/main/java/org/apache/iceberg/SchemaUpdate.java\n+++ b/core/src/main/java/org/apache/iceberg/SchemaUpdate.java\n@@ -722,6 +722,11 @@ public Type map(Types.MapType map, Type kResult, Type valueResult) {\n       }\n     }\n \n+    @Override\n+    public Type variant(Types.VariantType variant) {\n+      return variant;\n+    }\n+\n     @Override\n     public Type primitive(Type.PrimitiveType primitive) {\n       return primitive;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/mapping/MappingUtil.java b/core/src/main/java/org/apache/iceberg/mapping/MappingUtil.java\nindex de6ce2ad0425..fc3d920a4069 100644\n--- a/core/src/main/java/org/apache/iceberg/mapping/MappingUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/mapping/MappingUtil.java\n@@ -302,6 +302,11 @@ public MappedFields map(Types.MapType map, MappedFields keyResult, MappedFields\n           MappedField.of(map.valueId(), \"value\", valueResult));\n     }\n \n+    @Override\n+    public MappedFields variant(Types.VariantType variant) {\n+      return null; // no mapping because variant has no nested fields with IDs\n+    }\n+\n     @Override\n     public MappedFields primitive(Type.PrimitiveType primitive) {\n       return null; // no mapping because primitives have no nested fields\n\ndiff --git a/core/src/main/java/org/apache/iceberg/schema/SchemaWithPartnerVisitor.java b/core/src/main/java/org/apache/iceberg/schema/SchemaWithPartnerVisitor.java\nindex 9b2226f5714d..694bfb2f6242 100644\n--- a/core/src/main/java/org/apache/iceberg/schema/SchemaWithPartnerVisitor.java\n+++ b/core/src/main/java/org/apache/iceberg/schema/SchemaWithPartnerVisitor.java\n@@ -107,6 +107,9 @@ public static <P, T> T visit(\n \n         return visitor.map(map, partner, keyResult, valueResult);\n \n+      case VARIANT:\n+        return visitor.variant(type.asVariantType(), partner);\n+\n       default:\n         return visitor.primitive(type.asPrimitiveType(), partner);\n     }\n@@ -160,6 +163,10 @@ public R map(Types.MapType map, P partner, R keyResult, R valueResult) {\n     return null;\n   }\n \n+  public R variant(Types.VariantType variant, P partner) {\n+    throw new UnsupportedOperationException(\"Unsupported type: variant\");\n+  }\n+\n   public R primitive(Type.PrimitiveType primitive, P partner) {\n     return null;\n   }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java b/core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java\nindex 68172b7062a6..7c4dac9feff1 100644\n--- a/core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java\n+++ b/core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java\n@@ -142,6 +142,11 @@ public Boolean map(\n     return false;\n   }\n \n+  @Override\n+  public Boolean variant(Types.VariantType variant, Integer partnerId) {\n+    return partnerId == null;\n+  }\n+\n   @Override\n   public Boolean primitive(Type.PrimitiveType primitive, Integer partnerId) {\n     return partnerId == null;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/types/FixupTypes.java b/core/src/main/java/org/apache/iceberg/types/FixupTypes.java\nindex 23fccddda3d9..1e4c0b597a6a 100644\n--- a/core/src/main/java/org/apache/iceberg/types/FixupTypes.java\n+++ b/core/src/main/java/org/apache/iceberg/types/FixupTypes.java\n@@ -147,6 +147,12 @@ public Type map(Types.MapType map, Supplier<Type> keyTypeFuture, Supplier<Type>\n     }\n   }\n \n+  @Override\n+  public Type variant(Types.VariantType variant) {\n+    // nothing to fix up\n+    return variant;\n+  }\n+\n   @Override\n   public Type primitive(Type.PrimitiveType primitive) {\n     if (sourceType.equals(primitive)) {\n\ndiff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java\nindex af0fa84f67a1..ad8a4beb55d0 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/Spark3Util.java\n@@ -565,6 +565,11 @@ public String map(Types.MapType map, String keyResult, String valueResult) {\n       return \"map<\" + keyResult + \", \" + valueResult + \">\";\n     }\n \n+    @Override\n+    public String variant(Types.VariantType variant) {\n+      return \"variant\";\n+    }\n+\n     @Override\n     public String primitive(Type.PrimitiveType primitive) {\n       switch (primitive.typeId()) {\n",
    "test_patch": "diff --git a/api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java b/api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java\nindex 2d02da5346a7..debb9c9dc1d6 100644\n--- a/api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java\n+++ b/api/src/test/java/org/apache/iceberg/types/TestReadabilityChecks.java\n@@ -22,10 +22,15 @@\n import static org.apache.iceberg.types.Types.NestedField.required;\n import static org.assertj.core.api.Assertions.assertThat;\n \n+import java.util.Arrays;\n import java.util.List;\n+import java.util.stream.Stream;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.types.Type.PrimitiveType;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.Arguments;\n+import org.junit.jupiter.params.provider.MethodSource;\n \n public class TestReadabilityChecks {\n   private static final Type.PrimitiveType[] PRIMITIVES =\n@@ -112,6 +117,40 @@ private void testDisallowPrimitiveToStruct(PrimitiveType from, Schema fromSchema\n         .contains(\"cannot be read as a struct\");\n   }\n \n+  @Test\n+  public void testVariantToVariant() {\n+    Schema fromSchema = new Schema(required(1, \"from_field\", Types.VariantType.get()));\n+    List<String> errors =\n+        CheckCompatibility.writeCompatibilityErrors(\n+            new Schema(required(1, \"to_field\", Types.VariantType.get())), fromSchema);\n+    assertThat(errors).as(\"Should produce 0 error messages\").isEmpty();\n+  }\n+\n+  private static Stream<Arguments> incompatibleTypesToVariant() {\n+    return Stream.of(\n+            Stream.of(\n+                Arguments.of(Types.StructType.of(required(1, \"from\", Types.IntegerType.get()))),\n+                Arguments.of(\n+                    Types.MapType.ofRequired(\n+                        1, 2, Types.StringType.get(), Types.IntegerType.get())),\n+                Arguments.of(Types.ListType.ofRequired(1, Types.StringType.get()))),\n+            Arrays.stream(PRIMITIVES).map(type -> Arguments.of(type)))\n+        .flatMap(s -> s);\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\"incompatibleTypesToVariant\")\n+  public void testIncompatibleTypesToVariant(Type from) {\n+    Schema fromSchema = new Schema(required(3, \"from_field\", from));\n+    List<String> errors =\n+        CheckCompatibility.writeCompatibilityErrors(\n+            new Schema(required(3, \"to_field\", Types.VariantType.get())), fromSchema);\n+    assertThat(errors).hasSize(1);\n+    assertThat(errors.get(0))\n+        .as(\"Should complain that other type to variant is not allowed\")\n+        .contains(\"cannot be read as a variant\");\n+  }\n+\n   @Test\n   public void testRequiredSchemaField() {\n     Schema write = new Schema(optional(1, \"from_field\", Types.IntegerType.get()));\n\ndiff --git a/api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java b/api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java\nindex a222e8e66b8e..790f59587c59 100644\n--- a/api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java\n+++ b/api/src/test/java/org/apache/iceberg/types/TestSerializableTypes.java\n@@ -46,6 +46,7 @@ public void testIdentityTypes() throws Exception {\n           Types.StringType.get(),\n           Types.UUIDType.get(),\n           Types.BinaryType.get(),\n+          Types.UnknownType.get()\n         };\n \n     for (Type type : identityPrimitives) {\n@@ -136,15 +137,6 @@ public void testVariant() throws Exception {\n         .isEqualTo(variant);\n   }\n \n-  @Test\n-  public void testUnknown() throws Exception {\n-    Types.UnknownType unknown = Types.UnknownType.get();\n-    Type copy = TestHelpers.roundTripSerialize(unknown);\n-    assertThat(copy)\n-        .as(\"Unknown serialization should be equal to starting type\")\n-        .isEqualTo(unknown);\n-  }\n-\n   @Test\n   public void testSchema() throws Exception {\n     Schema schema =\n\ndiff --git a/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java b/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java\nindex 36384d232af3..b6556d70bd85 100644\n--- a/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java\n+++ b/api/src/test/java/org/apache/iceberg/types/TestTypeUtil.java\n@@ -23,12 +23,18 @@\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import java.util.Map;\n import java.util.Set;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import java.util.stream.Stream;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.types.Types.IntegerType;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.Arguments;\n+import org.junit.jupiter.params.provider.MethodSource;\n \n public class TestTypeUtil {\n   @Test\n@@ -645,4 +651,95 @@ public void testReassignOrRefreshIdsCaseInsensitive() {\n                 required(2, \"FIELD2\", Types.IntegerType.get())));\n     assertThat(actualSchema.asStruct()).isEqualTo(expectedSchema.asStruct());\n   }\n+\n+  private static Stream<Arguments> testTypes() {\n+    return Stream.of(\n+        Arguments.of(Types.UnknownType.get()),\n+        Arguments.of(Types.VariantType.get()),\n+        Arguments.of(Types.TimestampNanoType.withoutZone()),\n+        Arguments.of(Types.TimestampNanoType.withZone()));\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\"testTypes\")\n+  public void testAssignIdsWithType(Type testType) {\n+    Types.StructType sourceType =\n+        Types.StructType.of(required(0, \"id\", IntegerType.get()), required(1, \"data\", testType));\n+    Type expectedType =\n+        Types.StructType.of(required(10, \"id\", IntegerType.get()), required(11, \"data\", testType));\n+\n+    Type assignedType = TypeUtil.assignIds(sourceType, oldId -> oldId + 10);\n+    assertThat(assignedType).isEqualTo(expectedType);\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\"testTypes\")\n+  public void testAssignFreshIdsWithType(Type testType) {\n+    Schema schema = new Schema(required(0, \"id\", IntegerType.get()), required(1, \"data\", testType));\n+\n+    Schema assignedSchema = TypeUtil.assignFreshIds(schema, new AtomicInteger(10)::incrementAndGet);\n+    Schema expectedSchema =\n+        new Schema(required(11, \"id\", IntegerType.get()), required(12, \"data\", testType));\n+    assertThat(assignedSchema.asStruct()).isEqualTo(expectedSchema.asStruct());\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\"testTypes\")\n+  public void testReassignIdsWithType(Type testType) {\n+    Schema schema = new Schema(required(0, \"id\", IntegerType.get()), required(1, \"data\", testType));\n+    Schema sourceSchema =\n+        new Schema(required(1, \"id\", IntegerType.get()), required(2, \"data\", testType));\n+\n+    Schema reassignedSchema = TypeUtil.reassignIds(schema, sourceSchema);\n+    assertThat(reassignedSchema.asStruct()).isEqualTo(sourceSchema.asStruct());\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\"testTypes\")\n+  public void testIndexByIdWithType(Type testType) {\n+    Schema schema = new Schema(required(0, \"id\", IntegerType.get()), required(1, \"data\", testType));\n+\n+    Map<Integer, Types.NestedField> indexByIds = TypeUtil.indexById(schema.asStruct());\n+    assertThat(indexByIds.get(1).type()).isEqualTo(testType);\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\"testTypes\")\n+  public void testIndexNameByIdWithType(Type testType) {\n+    Schema schema = new Schema(required(0, \"id\", IntegerType.get()), required(1, \"data\", testType));\n+\n+    Map<Integer, String> indexNameByIds = TypeUtil.indexNameById(schema.asStruct());\n+    assertThat(indexNameByIds.get(1)).isEqualTo(\"data\");\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\"testTypes\")\n+  public void testProjectWithType(Type testType) {\n+    Schema schema = new Schema(required(0, \"id\", IntegerType.get()), required(1, \"data\", testType));\n+\n+    Schema expectedSchema = new Schema(required(1, \"data\", testType));\n+    Schema projectedSchema = TypeUtil.project(schema, Sets.newHashSet(1));\n+    assertThat(projectedSchema.asStruct()).isEqualTo(expectedSchema.asStruct());\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\"testTypes\")\n+  public void testGetProjectedIdsWithType(Type testType) {\n+    Schema schema = new Schema(required(0, \"id\", IntegerType.get()), required(1, \"data\", testType));\n+\n+    Set<Integer> projectedIds = TypeUtil.getProjectedIds(schema);\n+    assertThat(Set.of(0, 1)).isEqualTo(projectedIds);\n+  }\n+\n+  @ParameterizedTest\n+  @MethodSource(\"testTypes\")\n+  public void testReassignDocWithType(Type testType) {\n+    Schema schema = new Schema(required(0, \"id\", IntegerType.get()), required(1, \"data\", testType));\n+    Schema docSourceSchema =\n+        new Schema(\n+            required(0, \"id\", IntegerType.get(), \"id\"), required(1, \"data\", testType, \"data\"));\n+\n+    Schema reassignedSchema = TypeUtil.reassignDoc(schema, docSourceSchema);\n+    assertThat(reassignedSchema.asStruct()).isEqualTo(docSourceSchema.asStruct());\n+  }\n }\n\ndiff --git a/api/src/test/java/org/apache/iceberg/types/TestTypes.java b/api/src/test/java/org/apache/iceberg/types/TestTypes.java\nindex cbc37291375f..b3381d1ff440 100644\n--- a/api/src/test/java/org/apache/iceberg/types/TestTypes.java\n+++ b/api/src/test/java/org/apache/iceberg/types/TestTypes.java\n@@ -25,6 +25,30 @@\n \n public class TestTypes {\n \n+  @Test\n+  public void fromTypeName() {\n+    assertThat(Types.fromTypeName(\"boolean\")).isSameAs(Types.BooleanType.get());\n+    assertThat(Types.fromTypeName(\"BooLean\")).isSameAs(Types.BooleanType.get());\n+\n+    assertThat(Types.fromTypeName(\"timestamp\")).isSameAs(Types.TimestampType.withoutZone());\n+    assertThat(Types.fromTypeName(\"timestamptz\")).isSameAs(Types.TimestampType.withZone());\n+    assertThat(Types.fromTypeName(\"timestamp_ns\")).isSameAs(Types.TimestampNanoType.withoutZone());\n+    assertThat(Types.fromTypeName(\"timestamptz_ns\")).isSameAs(Types.TimestampNanoType.withZone());\n+\n+    assertThat(Types.fromTypeName(\"Fixed[ 3 ]\")).isEqualTo(Types.FixedType.ofLength(3));\n+\n+    assertThat(Types.fromTypeName(\"Decimal( 2 , 3 )\")).isEqualTo(Types.DecimalType.of(2, 3));\n+\n+    assertThat(Types.fromTypeName(\"Decimal(2,3)\")).isEqualTo(Types.DecimalType.of(2, 3));\n+\n+    assertThat(Types.fromTypeName(\"variant\")).isSameAs(Types.VariantType.get());\n+    assertThat(Types.fromTypeName(\"Variant\")).isSameAs(Types.VariantType.get());\n+\n+    assertThatExceptionOfType(IllegalArgumentException.class)\n+        .isThrownBy(() -> Types.fromTypeName(\"abcdefghij\"))\n+        .withMessage(\"Cannot parse type string to primitive: abcdefghij\");\n+  }\n+\n   @Test\n   public void fromPrimitiveString() {\n     assertThat(Types.fromPrimitiveString(\"boolean\")).isSameAs(Types.BooleanType.get());\n@@ -43,6 +67,13 @@ public void fromPrimitiveString() {\n \n     assertThat(Types.fromPrimitiveString(\"Decimal(2,3)\")).isEqualTo(Types.DecimalType.of(2, 3));\n \n+    assertThatExceptionOfType(IllegalArgumentException.class)\n+        .isThrownBy(() -> Types.fromPrimitiveString(\"variant\"))\n+        .withMessage(\"Cannot parse type string: variant is not a primitive type\");\n+    assertThatExceptionOfType(IllegalArgumentException.class)\n+        .isThrownBy(() -> Types.fromPrimitiveString(\"Variant\"))\n+        .withMessage(\"Cannot parse type string: variant is not a primitive type\");\n+\n     assertThatExceptionOfType(IllegalArgumentException.class)\n         .isThrownBy(() -> Types.fromPrimitiveString(\"abcdefghij\"))\n         .withMessage(\"Cannot parse type string to primitive: abcdefghij\");\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestSchemaParser.java b/core/src/test/java/org/apache/iceberg/TestSchemaParser.java\nindex ebd197a68af0..40db5cfee2cb 100644\n--- a/core/src/test/java/org/apache/iceberg/TestSchemaParser.java\n+++ b/core/src/test/java/org/apache/iceberg/TestSchemaParser.java\n@@ -123,4 +123,14 @@ public void testPrimitiveTypeDefaultValues(Type.PrimitiveType type, Object defau\n     assertThat(serialized.findField(\"col_with_default\").initialDefault()).isEqualTo(defaultValue);\n     assertThat(serialized.findField(\"col_with_default\").writeDefault()).isEqualTo(defaultValue);\n   }\n+\n+  @Test\n+  public void testVariantType() throws IOException {\n+    Schema schema =\n+        new Schema(\n+            Types.NestedField.required(1, \"id\", Types.IntegerType.get()),\n+            Types.NestedField.optional(2, \"data\", Types.VariantType.get()));\n+\n+    writeAndValidate(schema);\n+  }\n }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java b/core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java\nindex 656e72a0c19c..8649ada99bef 100644\n--- a/core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java\n+++ b/core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java\n@@ -26,7 +26,7 @@\n import java.util.List;\n import java.util.concurrent.atomic.AtomicInteger;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.types.Type.PrimitiveType;\n+import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.types.Types.BinaryType;\n import org.apache.iceberg.types.Types.BooleanType;\n@@ -42,13 +42,16 @@\n import org.apache.iceberg.types.Types.StringType;\n import org.apache.iceberg.types.Types.StructType;\n import org.apache.iceberg.types.Types.TimeType;\n+import org.apache.iceberg.types.Types.TimestampNanoType;\n import org.apache.iceberg.types.Types.TimestampType;\n import org.apache.iceberg.types.Types.UUIDType;\n+import org.apache.iceberg.types.Types.UnknownType;\n+import org.apache.iceberg.types.Types.VariantType;\n import org.junit.jupiter.api.Test;\n \n public class TestSchemaUnionByFieldName {\n \n-  private static List<? extends PrimitiveType> primitiveTypes() {\n+  private static List<? extends Type> primitiveTypes() {\n     return Lists.newArrayList(\n         StringType.get(),\n         TimeType.get(),\n@@ -63,11 +66,15 @@ private static List<? extends PrimitiveType> primitiveTypes() {\n         FixedType.ofLength(10),\n         DecimalType.of(10, 2),\n         LongType.get(),\n-        FloatType.get());\n+        FloatType.get(),\n+        VariantType.get(),\n+        UnknownType.get(),\n+        TimestampNanoType.withoutZone(),\n+        TimestampNanoType.withZone());\n   }\n \n   private static NestedField[] primitiveFields(\n-      Integer initialValue, List<? extends PrimitiveType> primitiveTypes) {\n+      Integer initialValue, List<? extends Type> primitiveTypes) {\n     AtomicInteger atomicInteger = new AtomicInteger(initialValue);\n     return primitiveTypes.stream()\n         .map(\n@@ -75,7 +82,7 @@ private static NestedField[] primitiveFields(\n                 optional(\n                     atomicInteger.incrementAndGet(),\n                     type.toString(),\n-                    Types.fromPrimitiveString(type.toString())))\n+                    Types.fromTypeName(type.toString())))\n         .toArray(NestedField[]::new);\n   }\n \n@@ -88,7 +95,7 @@ public void testAddTopLevelPrimitives() {\n \n   @Test\n   public void testAddTopLevelListOfPrimitives() {\n-    for (PrimitiveType primitiveType : primitiveTypes()) {\n+    for (Type primitiveType : primitiveTypes()) {\n       Schema newSchema =\n           new Schema(optional(1, \"aList\", Types.ListType.ofOptional(2, primitiveType)));\n       Schema applied = new SchemaUpdate(new Schema(), 0).unionByNameWith(newSchema).apply();\n@@ -98,7 +105,7 @@ public void testAddTopLevelListOfPrimitives() {\n \n   @Test\n   public void testAddTopLevelMapOfPrimitives() {\n-    for (PrimitiveType primitiveType : primitiveTypes()) {\n+    for (Type primitiveType : primitiveTypes()) {\n       Schema newSchema =\n           new Schema(\n               optional(1, \"aMap\", Types.MapType.ofOptional(2, 3, primitiveType, primitiveType)));\n@@ -109,7 +116,7 @@ public void testAddTopLevelMapOfPrimitives() {\n \n   @Test\n   public void testAddTopLevelStructOfPrimitives() {\n-    for (PrimitiveType primitiveType : primitiveTypes()) {\n+    for (Type primitiveType : primitiveTypes()) {\n       Schema currentSchema =\n           new Schema(\n               optional(1, \"aStruct\", Types.StructType.of(optional(2, \"primitive\", primitiveType))));\n@@ -120,7 +127,7 @@ public void testAddTopLevelStructOfPrimitives() {\n \n   @Test\n   public void testAddNestedPrimitive() {\n-    for (PrimitiveType primitiveType : primitiveTypes()) {\n+    for (Type primitiveType : primitiveTypes()) {\n       Schema currentSchema = new Schema(optional(1, \"aStruct\", Types.StructType.of()));\n       Schema newSchema =\n           new Schema(\n\ndiff --git a/core/src/test/java/org/apache/iceberg/mapping/TestNameMapping.java b/core/src/test/java/org/apache/iceberg/mapping/TestNameMapping.java\nindex d30a93d50d49..3ebf4d9242ab 100644\n--- a/core/src/test/java/org/apache/iceberg/mapping/TestNameMapping.java\n+++ b/core/src/test/java/org/apache/iceberg/mapping/TestNameMapping.java\n@@ -289,4 +289,16 @@ public void testMappingFindByName() {\n                 \"location\",\n                 MappedFields.of(MappedField.of(11, \"latitude\"), MappedField.of(12, \"longitude\"))));\n   }\n+\n+  @Test\n+  public void testMappingVariantType() {\n+    Schema schema =\n+        new Schema(\n+            required(1, \"id\", Types.LongType.get()), required(2, \"data\", Types.VariantType.get()));\n+\n+    MappedFields expected = MappedFields.of(MappedField.of(1, \"id\"), MappedField.of(2, \"data\"));\n+\n+    NameMapping mapping = MappingUtil.create(schema);\n+    assertThat(mapping.asMappedFields()).isEqualTo(expected);\n+  }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java\nindex 6f900ffebb10..e4e66abfefa0 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/TestSpark3Util.java\n@@ -105,12 +105,13 @@ public void testDescribeSchema() {\n                 3,\n                 \"pairs\",\n                 Types.MapType.ofOptional(4, 5, Types.StringType.get(), Types.LongType.get())),\n-            required(6, \"time\", Types.TimestampType.withoutZone()));\n+            required(6, \"time\", Types.TimestampType.withoutZone()),\n+            required(7, \"v\", Types.VariantType.get()));\n \n     assertThat(Spark3Util.describe(schema))\n         .as(\"Schema description isn't correct.\")\n         .isEqualTo(\n-            \"struct<data: list<string> not null,pairs: map<string, bigint>,time: timestamp not null>\");\n+            \"struct<data: list<string> not null,pairs: map<string, bigint>,time: timestamp not null,v: variant not null>\");\n   }\n \n   @Test\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11830",
    "pr_id": 11830,
    "issue_id": 10127,
    "repo": "apache/iceberg",
    "problem_statement": "Enable reading WASB and WASBS file paths with ABFS and ABFSS\n### Feature Request / Improvement\n\nWhen you setup a managed Snowflake Iceberg table on an Azure account, they will provide locations that use `wasbs://` and not `abfss://`. `wasb` is currently deprecated by Azure and everyone is encouraged to use ABFS instead. While Snowflake should really change this behavior, in the spirit of allowing people to \"update\" Iceberg tables without rewriting all the metadata files, it would be great if the iceberg library could handle this automatically.\r\n\r\nIts my understanding that you can convert a `wasbs` URI to an `abfss` URI by just making two changes:\r\n- Replacing `wasbs://` with `abfss://`\r\n- Updating `blob.core.windows.net` with `dfs.core.windows.net`.\r\n\r\nIf this change could be replaced when loading the location and file information from metadata files, then every user could effortlessly update to using  `abfs`.\r\n \n\n### Query engine\n\nSnowflake",
    "issue_word_count": 146,
    "test_files_count": 2,
    "non_test_files_count": 4,
    "pr_changed_files": [
      "azure/src/main/java/org/apache/iceberg/azure/AzureProperties.java",
      "azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSFileIO.java",
      "azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSLocation.java",
      "azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSFileIOTest.java",
      "azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSLocationTest.java",
      "core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java"
    ],
    "pr_changed_test_files": [
      "azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSFileIOTest.java",
      "azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSLocationTest.java"
    ],
    "base_commit": "246439a966001a1d8c8f5109cb38011acb44d549",
    "head_commit": "e6d6c7209372e25dd8aa9df86815be7fa1a2f765",
    "repo_url": "https://github.com/apache/iceberg/pull/11830",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11830",
    "dockerfile": "",
    "pr_merged_at": "2025-01-20T18:13:49.000Z",
    "patch": "diff --git a/azure/src/main/java/org/apache/iceberg/azure/AzureProperties.java b/azure/src/main/java/org/apache/iceberg/azure/AzureProperties.java\nindex 2d363cbc5231..8313000ab35e 100644\n--- a/azure/src/main/java/org/apache/iceberg/azure/AzureProperties.java\n+++ b/azure/src/main/java/org/apache/iceberg/azure/AzureProperties.java\n@@ -77,6 +77,15 @@ public Optional<Long> adlsWriteBlockSize() {\n     return Optional.ofNullable(adlsWriteBlockSize);\n   }\n \n+  /**\n+   * Applies configuration to the {@link DataLakeFileSystemClientBuilder} to provide the endpoint\n+   * and credentials required to create an instance of the client.\n+   *\n+   * <p>Default credentials are provided via the {@link com.azure.identity.DefaultAzureCredential}.\n+   *\n+   * @param account the service account key (e.g. a hostname or storage account key to get values)\n+   * @param builder the builder instance\n+   */\n   public void applyClientConfiguration(String account, DataLakeFileSystemClientBuilder builder) {\n     String sasToken = adlsSasTokens.get(account);\n     if (sasToken != null && !sasToken.isEmpty()) {\n\ndiff --git a/azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSFileIO.java b/azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSFileIO.java\nindex 0bfce9d6055b..e1bf21f69dc8 100644\n--- a/azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSFileIO.java\n+++ b/azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSFileIO.java\n@@ -111,7 +111,7 @@ DataLakeFileSystemClient client(ADLSLocation location) {\n         new DataLakeFileSystemClientBuilder().httpClient(HTTP);\n \n     location.container().ifPresent(clientBuilder::fileSystemName);\n-    azureProperties.applyClientConfiguration(location.storageAccount(), clientBuilder);\n+    azureProperties.applyClientConfiguration(location.host(), clientBuilder);\n \n     return clientBuilder.buildClient();\n   }\n\ndiff --git a/azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSLocation.java b/azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSLocation.java\nindex 5af590628fe8..6fe2629774dc 100644\n--- a/azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSLocation.java\n+++ b/azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSLocation.java\n@@ -30,18 +30,26 @@\n  *\n  * <p>Locations follow a URI like structure to identify resources\n  *\n- * <pre>{@code abfs[s]://[<container>@]<storage account host>/<file path>}</pre>\n+ * <pre>{@code abfs[s]://[<container>@]<storageAccount>.dfs.core.windows.net/<path>}</pre>\n+ *\n+ * or\n+ *\n+ * <pre>{@code wasb[s]://<container>@<storageAccount>.blob.core.windows.net/<path>}</pre>\n+ *\n+ * For compatibility, locations using the wasb scheme are also accepted but will use the Azure Data\n+ * Lake Storage Gen2 REST APIs instead of the Blob Storage REST APIs.\n  *\n  * <p>See <a\n  * href=\"https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction-abfs-uri#uri-syntax\">Azure\n  * Data Lake Storage URI</a>\n  */\n class ADLSLocation {\n-  private static final Pattern URI_PATTERN = Pattern.compile(\"^abfss?://([^/?#]+)(.*)?$\");\n+  private static final Pattern URI_PATTERN = Pattern.compile(\"^(abfss?|wasbs?)://([^/?#]+)(.*)?$\");\n \n   private final String storageAccount;\n   private final String container;\n   private final String path;\n+  private final String host;\n \n   /**\n    * Creates a new ADLSLocation from a fully qualified URI.\n@@ -55,17 +63,19 @@ class ADLSLocation {\n \n     ValidationException.check(matcher.matches(), \"Invalid ADLS URI: %s\", location);\n \n-    String authority = matcher.group(1);\n+    String authority = matcher.group(2);\n     String[] parts = authority.split(\"@\", -1);\n     if (parts.length > 1) {\n       this.container = parts[0];\n-      this.storageAccount = parts[1];\n+      this.host = parts[1];\n+      this.storageAccount = host.split(\"\\\\.\", -1)[0];\n     } else {\n       this.container = null;\n-      this.storageAccount = authority;\n+      this.host = authority;\n+      this.storageAccount = authority.split(\"\\\\.\", -1)[0];\n     }\n \n-    String uriPath = matcher.group(2);\n+    String uriPath = matcher.group(3);\n     this.path = uriPath == null ? \"\" : uriPath.startsWith(\"/\") ? uriPath.substring(1) : uriPath;\n   }\n \n@@ -83,4 +93,9 @@ public Optional<String> container() {\n   public String path() {\n     return path;\n   }\n+\n+  /** Returns ADLS host. */\n+  public String host() {\n+    return host;\n+  }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java b/core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java\nindex a858045aab8b..a8adf979f85a 100644\n--- a/core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java\n+++ b/core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java\n@@ -62,7 +62,9 @@ public class ResolvingFileIO implements HadoopConfigurable, DelegateFileIO {\n           \"s3n\", S3_FILE_IO_IMPL,\n           \"gs\", GCS_FILE_IO_IMPL,\n           \"abfs\", ADLS_FILE_IO_IMPL,\n-          \"abfss\", ADLS_FILE_IO_IMPL);\n+          \"abfss\", ADLS_FILE_IO_IMPL,\n+          \"wasb\", ADLS_FILE_IO_IMPL,\n+          \"wasbs\", ADLS_FILE_IO_IMPL);\n \n   private final Map<String, DelegateFileIO> ioInstances = Maps.newConcurrentMap();\n   private final AtomicBoolean isClosed = new AtomicBoolean(false);\n",
    "test_patch": "diff --git a/azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSFileIOTest.java b/azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSFileIOTest.java\nindex 4e3a769dc19a..8f24ca9de368 100644\n--- a/azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSFileIOTest.java\n+++ b/azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSFileIOTest.java\n@@ -18,6 +18,7 @@\n  */\n package org.apache.iceberg.azure.adlsv2;\n \n+import static org.apache.iceberg.azure.AzureProperties.ADLS_SAS_TOKEN_PREFIX;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.mockito.ArgumentMatchers.anyBoolean;\n import static org.mockito.ArgumentMatchers.eq;\n@@ -32,6 +33,7 @@\n import com.azure.core.http.rest.Response;\n import com.azure.storage.file.datalake.DataLakeFileClient;\n import com.azure.storage.file.datalake.DataLakeFileSystemClient;\n+import com.azure.storage.file.datalake.DataLakeFileSystemClientBuilder;\n import com.azure.storage.file.datalake.models.PathItem;\n import java.io.IOException;\n import java.io.InputStream;\n@@ -39,6 +41,7 @@\n import java.time.OffsetDateTime;\n import java.util.Iterator;\n import org.apache.iceberg.TestHelpers;\n+import org.apache.iceberg.azure.AzureProperties;\n import org.apache.iceberg.io.FileIO;\n import org.apache.iceberg.io.FileInfo;\n import org.apache.iceberg.io.InputFile;\n@@ -100,6 +103,21 @@ public void testGetClient() {\n     assertThat(client.exists()).isTrue();\n   }\n \n+  @Test\n+  public void testApplyClientConfigurationWithSas() {\n+    AzureProperties props =\n+        spy(\n+            new AzureProperties(\n+                ImmutableMap.of(\n+                    ADLS_SAS_TOKEN_PREFIX + \"account.dfs.core.windows.net\", \"sasToken\")));\n+    ADLSFileIO io = spy(new ADLSFileIO(props));\n+    String location = AZURITE_CONTAINER.location(\"path/to/file\");\n+    io.client(location);\n+    verify(props)\n+        .applyClientConfiguration(\n+            eq(\"account.dfs.core.windows.net\"), any(DataLakeFileSystemClientBuilder.class));\n+  }\n+\n   /** Azurite does not support ADLSv2 directory operations yet so use mocks here. */\n   @SuppressWarnings(\"unchecked\")\n   @Test\n\ndiff --git a/azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSLocationTest.java b/azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSLocationTest.java\nindex 403886f4b28e..5c0fde805d0e 100644\n--- a/azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSLocationTest.java\n+++ b/azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSLocationTest.java\n@@ -24,6 +24,7 @@\n import org.apache.iceberg.exceptions.ValidationException;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.params.ParameterizedTest;\n+import org.junit.jupiter.params.provider.CsvSource;\n import org.junit.jupiter.params.provider.ValueSource;\n \n public class ADLSLocationTest {\n@@ -33,7 +34,18 @@ public void testLocationParsing(String scheme) {\n     String p1 = scheme + \"://container@account.dfs.core.windows.net/path/to/file\";\n     ADLSLocation location = new ADLSLocation(p1);\n \n-    assertThat(location.storageAccount()).isEqualTo(\"account.dfs.core.windows.net\");\n+    assertThat(location.storageAccount()).isEqualTo(\"account\");\n+    assertThat(location.container().get()).isEqualTo(\"container\");\n+    assertThat(location.path()).isEqualTo(\"path/to/file\");\n+  }\n+\n+  @ParameterizedTest\n+  @ValueSource(strings = {\"wasb\", \"wasbs\"})\n+  public void testWasbLocatonParsing(String scheme) {\n+    String p1 = scheme + \"://container@account.blob.core.windows.net/path/to/file\";\n+    ADLSLocation location = new ADLSLocation(p1);\n+\n+    assertThat(location.storageAccount()).isEqualTo(\"account\");\n     assertThat(location.container().get()).isEqualTo(\"container\");\n     assertThat(location.path()).isEqualTo(\"path/to/file\");\n   }\n@@ -43,7 +55,7 @@ public void testEncodedString() {\n     String p1 = \"abfs://container@account.dfs.core.windows.net/path%20to%20file\";\n     ADLSLocation location = new ADLSLocation(p1);\n \n-    assertThat(location.storageAccount()).isEqualTo(\"account.dfs.core.windows.net\");\n+    assertThat(location.storageAccount()).isEqualTo(\"account\");\n     assertThat(location.container().get()).isEqualTo(\"container\");\n     assertThat(location.path()).isEqualTo(\"path%20to%20file\");\n   }\n@@ -67,7 +79,7 @@ public void testNoContainer() {\n     String p1 = \"abfs://account.dfs.core.windows.net/path/to/file\";\n     ADLSLocation location = new ADLSLocation(p1);\n \n-    assertThat(location.storageAccount()).isEqualTo(\"account.dfs.core.windows.net\");\n+    assertThat(location.storageAccount()).isEqualTo(\"account\");\n     assertThat(location.container().isPresent()).isFalse();\n     assertThat(location.path()).isEqualTo(\"path/to/file\");\n   }\n@@ -77,7 +89,7 @@ public void testNoPath() {\n     String p1 = \"abfs://container@account.dfs.core.windows.net\";\n     ADLSLocation location = new ADLSLocation(p1);\n \n-    assertThat(location.storageAccount()).isEqualTo(\"account.dfs.core.windows.net\");\n+    assertThat(location.storageAccount()).isEqualTo(\"account\");\n     assertThat(location.container().get()).isEqualTo(\"container\");\n     assertThat(location.path()).isEqualTo(\"\");\n   }\n@@ -89,4 +101,19 @@ public void testQuestionMarkInFileName(String path) {\n     ADLSLocation location = new ADLSLocation(fullPath);\n     assertThat(location.path()).contains(path);\n   }\n+\n+  @ParameterizedTest\n+  @CsvSource({\n+    \"abfs://container@account.dfs.core.windows.net/file.txt, account.dfs.core.windows.net\",\n+    \"abfs://container@account.dfs.core.usgovcloudapi.net/file.txt, account.dfs.core.usgovcloudapi.net\",\n+    \"wasb://container@account.blob.core.windows.net/file.txt, account.blob.core.windows.net\",\n+    \"abfs://account.dfs.core.windows.net/path, account.dfs.core.windows.net\",\n+    \"abfss://account.dfs.core.windows.net/path, account.dfs.core.windows.net\",\n+    \"wasb://account.blob.core.windows.net/path, account.blob.core.windows.net\",\n+    \"wasbs://account.blob.core.windows.net/path, account.blob.core.windows.net\"\n+  })\n+  void testHost(String path, String expectedHost) {\n+    ADLSLocation location = new ADLSLocation(path);\n+    assertThat(location.host()).isEqualTo(expectedHost);\n+  }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11808",
    "pr_id": 11808,
    "issue_id": 11122,
    "repo": "apache/iceberg",
    "problem_statement": "Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 118,
    "test_files_count": 3,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/MetadataColumns.java",
      "core/src/main/java/org/apache/iceberg/PositionDeletesTable.java",
      "core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java",
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/DVIterator.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPositionDeletesReader.java"
    ],
    "pr_changed_test_files": [
      "core/src/main/java/org/apache/iceberg/PositionDeletesTable.java",
      "core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPositionDeletesReader.java"
    ],
    "base_commit": "ab9e05e7369c55a95632b856729a05ea876cf788",
    "head_commit": "1a694793db4537f2f3d07d3f04c5c88e024f960f",
    "repo_url": "https://github.com/apache/iceberg/pull/11808",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11808",
    "dockerfile": "",
    "pr_merged_at": "2025-01-24T06:47:34.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/MetadataColumns.java b/core/src/main/java/org/apache/iceberg/MetadataColumns.java\nindex 060d27a018c0..792ed6a062ef 100644\n--- a/core/src/main/java/org/apache/iceberg/MetadataColumns.java\n+++ b/core/src/main/java/org/apache/iceberg/MetadataColumns.java\n@@ -56,6 +56,8 @@ private MetadataColumns() {}\n   public static final int PARTITION_COLUMN_ID = Integer.MAX_VALUE - 5;\n   public static final String PARTITION_COLUMN_NAME = \"_partition\";\n   public static final String PARTITION_COLUMN_DOC = \"Partition to which a row belongs to\";\n+  public static final int CONTENT_OFFSET_COLUMN_ID = Integer.MAX_VALUE - 6;\n+  public static final int CONTENT_SIZE_IN_BYTES_COLUMN_ID = Integer.MAX_VALUE - 7;\n \n   // IDs Integer.MAX_VALUE - (101-200) are used for reserved columns\n   public static final NestedField DELETE_FILE_PATH =\n\ndiff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/DVIterator.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/DVIterator.java\nindex 7b08b86cbfd0..0c319e2bd41a 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/DVIterator.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/DVIterator.java\n@@ -30,6 +30,7 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ScanTaskUtil;\n import org.apache.spark.sql.catalyst.InternalRow;\n import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n import org.apache.spark.unsafe.types.UTF8String;\n@@ -79,6 +80,10 @@ public InternalRow next() {\n           rowValues.add(idToConstant.get(MetadataColumns.SPEC_ID_COLUMN_ID));\n         } else if (fieldId == MetadataColumns.FILE_PATH_COLUMN_ID) {\n           rowValues.add(idToConstant.get(MetadataColumns.FILE_PATH_COLUMN_ID));\n+        } else if (fieldId == MetadataColumns.CONTENT_OFFSET_COLUMN_ID) {\n+          rowValues.add(deleteFile.contentOffset());\n+        } else if (fieldId == MetadataColumns.CONTENT_SIZE_IN_BYTES_COLUMN_ID) {\n+          rowValues.add(ScanTaskUtil.contentSizeInBytes(deleteFile));\n         }\n       }\n \n",
    "test_patch": "diff --git a/core/src/main/java/org/apache/iceberg/PositionDeletesTable.java b/core/src/main/java/org/apache/iceberg/PositionDeletesTable.java\nindex 382ad663a8d1..e205e6a9426e 100644\n--- a/core/src/main/java/org/apache/iceberg/PositionDeletesTable.java\n+++ b/core/src/main/java/org/apache/iceberg/PositionDeletesTable.java\n@@ -52,6 +52,8 @@ public class PositionDeletesTable extends BaseMetadataTable {\n   public static final String PARTITION = \"partition\";\n   public static final String SPEC_ID = \"spec_id\";\n   public static final String DELETE_FILE_PATH = \"delete_file_path\";\n+  public static final String CONTENT_OFFSET = \"content_offset\";\n+  public static final String CONTENT_SIZE_IN_BYTES = \"content_size_in_bytes\";\n \n   private final Schema schema;\n   private final int defaultSpecId;\n@@ -110,31 +112,54 @@ public Map<String, String> properties() {\n   }\n \n   private Schema calculateSchema() {\n+    int formatVersion = TableUtil.formatVersion(table());\n     Types.StructType partitionType = Partitioning.partitionType(table());\n-    List<Types.NestedField> columns =\n-        ImmutableList.of(\n-            MetadataColumns.DELETE_FILE_PATH,\n-            MetadataColumns.DELETE_FILE_POS,\n-            Types.NestedField.optional(\n-                MetadataColumns.DELETE_FILE_ROW_FIELD_ID,\n-                MetadataColumns.DELETE_FILE_ROW_FIELD_NAME,\n-                table().schema().asStruct(),\n-                MetadataColumns.DELETE_FILE_ROW_DOC),\n-            Types.NestedField.required(\n-                MetadataColumns.PARTITION_COLUMN_ID,\n-                PARTITION,\n-                partitionType,\n-                \"Partition that position delete row belongs to\"),\n-            Types.NestedField.required(\n-                MetadataColumns.SPEC_ID_COLUMN_ID,\n-                SPEC_ID,\n-                Types.IntegerType.get(),\n-                MetadataColumns.SPEC_ID_COLUMN_DOC),\n-            Types.NestedField.required(\n-                MetadataColumns.FILE_PATH_COLUMN_ID,\n-                DELETE_FILE_PATH,\n-                Types.StringType.get(),\n-                MetadataColumns.FILE_PATH_COLUMN_DOC));\n+    ImmutableList.Builder<Types.NestedField> builder =\n+        ImmutableList.<Types.NestedField>builder()\n+            .add(MetadataColumns.DELETE_FILE_PATH)\n+            .add(MetadataColumns.DELETE_FILE_POS)\n+            .add(\n+                Types.NestedField.optional(\n+                    MetadataColumns.DELETE_FILE_ROW_FIELD_ID,\n+                    MetadataColumns.DELETE_FILE_ROW_FIELD_NAME,\n+                    table().schema().asStruct(),\n+                    MetadataColumns.DELETE_FILE_ROW_DOC))\n+            .add(\n+                Types.NestedField.required(\n+                    MetadataColumns.PARTITION_COLUMN_ID,\n+                    PARTITION,\n+                    partitionType,\n+                    \"Partition that position delete row belongs to\"))\n+            .add(\n+                Types.NestedField.required(\n+                    MetadataColumns.SPEC_ID_COLUMN_ID,\n+                    SPEC_ID,\n+                    Types.IntegerType.get(),\n+                    MetadataColumns.SPEC_ID_COLUMN_DOC))\n+            .add(\n+                Types.NestedField.required(\n+                    MetadataColumns.FILE_PATH_COLUMN_ID,\n+                    DELETE_FILE_PATH,\n+                    Types.StringType.get(),\n+                    MetadataColumns.FILE_PATH_COLUMN_DOC));\n+\n+    if (formatVersion >= 3) {\n+      builder\n+          .add(\n+              Types.NestedField.optional(\n+                  MetadataColumns.CONTENT_OFFSET_COLUMN_ID,\n+                  CONTENT_OFFSET,\n+                  Types.LongType.get(),\n+                  \"The offset in the DV where the content starts\"))\n+          .add(\n+              Types.NestedField.optional(\n+                  MetadataColumns.CONTENT_SIZE_IN_BYTES_COLUMN_ID,\n+                  CONTENT_SIZE_IN_BYTES,\n+                  Types.LongType.get(),\n+                  \"The length in bytes of the DV blob\"));\n+    }\n+\n+    List<Types.NestedField> columns = builder.build();\n \n     // Calculate used ids (for de-conflict)\n     Set<Integer> currentlyUsedIds =\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java b/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java\nindex 56b11009fc12..0e77e38ca360 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java\n@@ -1704,7 +1704,12 @@ public void testPositionDeletesManyColumns() {\n     table.newRowDelta().addDeletes(delete1).addDeletes(delete2).commit();\n \n     PositionDeletesTable positionDeletesTable = new PositionDeletesTable(table);\n-    assertThat(TypeUtil.indexById(positionDeletesTable.schema().asStruct()).size()).isEqualTo(2010);\n+    int expectedIds =\n+        formatVersion >= 3\n+            ? 2012 // partition col + 8 columns + 2003 ids inside the deleted row column\n+            : 2010; // partition col + 6 columns + 2003 ids inside the deleted row column\n+    assertThat(TypeUtil.indexById(positionDeletesTable.schema().asStruct()).size())\n+        .isEqualTo(expectedIds);\n \n     BatchScan scan = positionDeletesTable.newBatchScan();\n     assertThat(scan).isInstanceOf(PositionDeletesTable.PositionDeletesBatchScan.class);\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPositionDeletesReader.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPositionDeletesReader.java\nindex 5b876dfc57ce..c182413f3938 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPositionDeletesReader.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPositionDeletesReader.java\n@@ -157,13 +157,17 @@ public void readPositionDeletesTableWithMultipleDeleteFiles() throws IOException\n     Table positionDeletesTable =\n         catalog.loadTable(TableIdentifier.of(\"default\", \"test\", \"position_deletes\"));\n \n-    Schema projectedSchema =\n-        positionDeletesTable\n-            .schema()\n-            .select(\n-                MetadataColumns.DELETE_FILE_PATH.name(),\n-                MetadataColumns.DELETE_FILE_POS.name(),\n-                PositionDeletesTable.DELETE_FILE_PATH);\n+    List<String> columns =\n+        Lists.newArrayList(\n+            MetadataColumns.DELETE_FILE_PATH.name(),\n+            MetadataColumns.DELETE_FILE_POS.name(),\n+            PositionDeletesTable.DELETE_FILE_PATH);\n+    if (formatVersion >= 3) {\n+      columns.add(PositionDeletesTable.CONTENT_OFFSET);\n+      columns.add(PositionDeletesTable.CONTENT_SIZE_IN_BYTES);\n+    }\n+\n+    Schema projectedSchema = positionDeletesTable.schema().select(columns);\n \n     List<ScanTask> scanTasks =\n         Lists.newArrayList(\n@@ -187,15 +191,27 @@ public void readPositionDeletesTableWithMultipleDeleteFiles() throws IOException\n \n       String dataFileLocation =\n           formatVersion >= 3 ? deleteFile1.referencedDataFile() : dataFile1.location();\n-      Object[] first = {\n-        UTF8String.fromString(dataFileLocation), 0L, UTF8String.fromString(deleteFile1.location())\n-      };\n-      Object[] second = {\n-        UTF8String.fromString(dataFileLocation), 1L, UTF8String.fromString(deleteFile1.location())\n-      };\n+      List<Object> first =\n+          Lists.newArrayList(\n+              UTF8String.fromString(dataFileLocation),\n+              0L,\n+              UTF8String.fromString(deleteFile1.location()));\n+      List<Object> second =\n+          Lists.newArrayList(\n+              UTF8String.fromString(dataFileLocation),\n+              1L,\n+              UTF8String.fromString(deleteFile1.location()));\n+\n+      if (formatVersion >= 3) {\n+        first.add(deleteFile1.contentOffset());\n+        first.add(deleteFile1.contentSizeInBytes());\n+        second.add(deleteFile1.contentOffset());\n+        second.add(deleteFile1.contentSizeInBytes());\n+      }\n+\n       assertThat(internalRowsToJava(actualRows, projectedSchema))\n           .hasSize(2)\n-          .containsExactly(first, second);\n+          .containsExactly(first.toArray(), second.toArray());\n     }\n \n     assertThat(scanTasks.get(1)).isInstanceOf(PositionDeletesScanTask.class);\n@@ -214,15 +230,27 @@ public void readPositionDeletesTableWithMultipleDeleteFiles() throws IOException\n \n       String dataFileLocation =\n           formatVersion >= 3 ? deleteFile2.referencedDataFile() : dataFile2.location();\n-      Object[] first = {\n-        UTF8String.fromString(dataFileLocation), 2L, UTF8String.fromString(deleteFile2.location())\n-      };\n-      Object[] second = {\n-        UTF8String.fromString(dataFileLocation), 3L, UTF8String.fromString(deleteFile2.location())\n-      };\n+      List<Object> first =\n+          Lists.newArrayList(\n+              UTF8String.fromString(dataFileLocation),\n+              2L,\n+              UTF8String.fromString(deleteFile2.location()));\n+      List<Object> second =\n+          Lists.newArrayList(\n+              UTF8String.fromString(dataFileLocation),\n+              3L,\n+              UTF8String.fromString(deleteFile2.location()));\n+\n+      if (formatVersion >= 3) {\n+        first.add(deleteFile2.contentOffset());\n+        first.add(deleteFile2.contentSizeInBytes());\n+        second.add(deleteFile2.contentOffset());\n+        second.add(deleteFile2.contentSizeInBytes());\n+      }\n+\n       assertThat(internalRowsToJava(actualRows, projectedSchema))\n           .hasSize(2)\n-          .containsExactly(first, second);\n+          .containsExactly(first.toArray(), second.toArray());\n     }\n   }\n \n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11781",
    "pr_id": 11781,
    "issue_id": 11768,
    "repo": "apache/iceberg",
    "problem_statement": "ParallelIterable is deadlocking and is generally really complicated\n### Apache Iceberg version\r\n\r\n1.7.1 (latest release)\r\n\r\n### Query engine\r\n\r\nTrino\r\n\r\n### Please describe the bug üêû\r\n\r\n`ParallelIterable` implementation is really complicated and has subtle concurrency bugs.\r\n\r\n# Context #1\r\nIt was observed that with high concurrency/high workload scenario cluster concurrency is reduced to 0 or 1 due to S3 `Timeout waiting for connection from pool` errors. Once that starts to happening, it will continue to go on effectively making cluster unusable.\r\n\r\n# Context #2\r\n\r\n`ManifestGroup#plan` will create `ManifestReader` per every `ParallelIterable.Task`. These readers will effectively hold onto S3 connection from the pool. When `ParallelIterable` queue is full, `Task` will be tabled for later use. The number of tasks is not bounded by worker pool size, but rather `X = num(ParallelIterable instances) * size(ParallelIterator#taskFutures)`. One can see that `X` can be significant with high number of concurrent queries.\r\n\r\n# Issue #1\r\n\r\n`ParallelIterable` is not batch based. This means it will produce read-ahead results even if downstream consumer doesn't have slots for them. This can lead to subtle concurrency issues. For instance consider two parallel iterables `P1, P2`. Let's assume single threaded reader consumes 500 elements from `P1`, then `P2` then `P1` and so on (this could be splits for instance). If `P1` becomes full then it will no longer fetch more elements while holding of tasks (which in turn hold S3 connections). This will prevent fetching of tasks from `P2` from completion (because there are no \"free\" S3 slots).\r\n\r\nConsider scenario:\r\n`S3 connection pool size=1`\r\n`approximateMaxQueueSize=1`\r\n`workerPoolSize=1`\r\n\r\nP1: starts `TaskP1`\r\nP1: produces result, queue full, `TaskP1` put on hold (holds S3 connection)\r\nP2: starts `TaskP2`, `TaskP2` is scheduled on `workerPool` but is blocked on S3 connection pool\r\nP1: result consumed, `TaskP1` is scheduled again\r\nP1: `TaskP1` waits for `workerPool` to be free, but `TaskP2` is waiting for `TaskP1` to release connection\r\nDEADLOCK\r\n\r\n# Issue #2\r\n\r\nActive waiting. This one is a known one. However, if one looks at `ParallelIterable.ParallelIterator#checkTasks` there is:\r\n```\r\n        if (taskFutures[i] == null || taskFutures[i].isDone()) {\r\n           continuation.ifPresent(yieldedTasks::addLast);\r\n...\r\n          taskFutures[i] = submitNextTask();\r\n      }\r\n```\r\nwhich means active waiting is actually happening though `workerPool` (e.g. task is started on worker pool just to check that queue is full and it should be put on hold).\r\n\r\n# Short term fix?\r\n\r\nOnce `ParallelIterable.Task` is started it should continue until entire task is consumed. This will prevent putting limited resourcs on hold. `if (queue.size() >= approximateMaxQueueSize) {` check should only happen once per task before iterator is created.\r\n\r\n# Long term fix?\r\n\r\nPerhaps the code can be refactored to be more readable and streamlined?\r\n\r\ncc @findepi @raunaqmorarka\r\n\r\n\r\n\r\n\r\n### Willingness to contribute\r\n\r\n- [ ] I can contribute a fix for this bug independently\r\n- [X] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\r\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 478,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/util/ParallelIterable.java",
      "core/src/test/java/org/apache/iceberg/util/TestParallelIterable.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/util/TestParallelIterable.java"
    ],
    "base_commit": "1cbc163a111713798d45e6c4d5eaffa44684f18b",
    "head_commit": "3436e7fd5f1160a82460d26e1ecb3c739015e1d9",
    "repo_url": "https://github.com/apache/iceberg/pull/11781",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11781",
    "dockerfile": "",
    "pr_merged_at": "2025-01-11T16:13:18.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/util/ParallelIterable.java b/core/src/main/java/org/apache/iceberg/util/ParallelIterable.java\nindex d40f64844797..7acab8762fb8 100644\n--- a/core/src/main/java/org/apache/iceberg/util/ParallelIterable.java\n+++ b/core/src/main/java/org/apache/iceberg/util/ParallelIterable.java\n@@ -86,6 +86,7 @@ static class ParallelIterator<T> implements CloseableIterator<T> {\n     private final CompletableFuture<Optional<Task<T>>>[] taskFutures;\n     private final ConcurrentLinkedQueue<T> queue = new ConcurrentLinkedQueue<>();\n     private final AtomicBoolean closed = new AtomicBoolean(false);\n+    private final int maxQueueSize;\n \n     private ParallelIterator(\n         Iterable<? extends Iterable<T>> iterables, ExecutorService workerPool, int maxQueueSize) {\n@@ -97,6 +98,7 @@ private ParallelIterator(\n       this.workerPool = workerPool;\n       // submit 2 tasks per worker at a time\n       this.taskFutures = new CompletableFuture[2 * ThreadPools.WORKER_THREAD_POOL_SIZE];\n+      this.maxQueueSize = maxQueueSize;\n     }\n \n     @Override\n@@ -153,6 +155,7 @@ private synchronized boolean checkTasks() {\n             try {\n               Optional<Task<T>> continuation = taskFutures[i].get();\n               continuation.ifPresent(yieldedTasks::addLast);\n+              taskFutures[i] = null;\n             } catch (ExecutionException e) {\n               if (e.getCause() instanceof RuntimeException) {\n                 // rethrow a runtime exception\n@@ -165,7 +168,10 @@ private synchronized boolean checkTasks() {\n             }\n           }\n \n-          taskFutures[i] = submitNextTask();\n+          // submit a new task if there is space in the queue\n+          if (queue.size() < maxQueueSize) {\n+            taskFutures[i] = submitNextTask();\n+          }\n         }\n \n         if (taskFutures[i] != null) {\n@@ -257,17 +263,24 @@ private static class Task<T> implements Supplier<Optional<Task<T>>>, Closeable {\n     @Override\n     public Optional<Task<T>> get() {\n       try {\n+        if (queue.size() >= approximateMaxQueueSize) {\n+          // Yield when queue is over the size limit. Task will be resubmitted later and continue\n+          // the work.\n+          //\n+          // Tasks might hold references (via iterator) to constrained resources\n+          // (e.g. pooled connections). Hence, tasks should yield only when\n+          // iterator is not instantiated. Otherwise, there could be\n+          // a deadlock when yielded tasks are waiting to be executed while\n+          // currently executed tasks are waiting for the resources that are held\n+          // by the yielded tasks.\n+          return Optional.of(this);\n+        }\n+\n         if (iterator == null) {\n           iterator = input.iterator();\n         }\n \n         while (iterator.hasNext()) {\n-          if (queue.size() >= approximateMaxQueueSize) {\n-            // Yield when queue is over the size limit. Task will be resubmitted later and continue\n-            // the work.\n-            return Optional.of(this);\n-          }\n-\n           T next = iterator.next();\n           if (closed.get()) {\n             break;\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/util/TestParallelIterable.java b/core/src/test/java/org/apache/iceberg/util/TestParallelIterable.java\nindex 410e33058d0c..a1e14a22a74d 100644\n--- a/core/src/test/java/org/apache/iceberg/util/TestParallelIterable.java\n+++ b/core/src/test/java/org/apache/iceberg/util/TestParallelIterable.java\n@@ -25,6 +25,7 @@\n import java.util.List;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n+import java.util.concurrent.Semaphore;\n import java.util.concurrent.TimeUnit;\n import java.util.stream.IntStream;\n import java.util.stream.Stream;\n@@ -39,6 +40,7 @@\n import org.apache.iceberg.util.ParallelIterable.ParallelIterator;\n import org.awaitility.Awaitility;\n import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.Timeout;\n \n public class TestParallelIterable {\n   @Test\n@@ -143,7 +145,7 @@ public CloseableIterator<Integer> iterator() {\n \n   @Test\n   public void limitQueueSize() {\n-    ExecutorService executor = Executors.newCachedThreadPool();\n+    ExecutorService executor = Executors.newSingleThreadExecutor();\n     try {\n       List<Iterable<Integer>> iterables =\n           ImmutableList.of(\n@@ -167,7 +169,7 @@ public void limitQueueSize() {\n       while (iterator.hasNext()) {\n         assertThat(iterator.queueSize())\n             .as(\"iterator internal queue size\")\n-            .isLessThanOrEqualTo(maxQueueSize + iterables.size());\n+            .isLessThanOrEqualTo(100);\n         actualValues.add(iterator.next());\n       }\n \n@@ -182,41 +184,61 @@ public void limitQueueSize() {\n   }\n \n   @Test\n-  public void queueSizeOne() {\n-    ExecutorService executor = Executors.newCachedThreadPool();\n+  @Timeout(10)\n+  public void noDeadlock() {\n+    // This test simulates a scenario where iterators use a constrained resource\n+    // (e.g. an S3 connection pool that has a limit on the number of connections).\n+    // In this case, the constrained resource shouldn't cause a deadlock when queue\n+    // is full and the iterator is waiting for the queue to be drained.\n+    ExecutorService executor = Executors.newFixedThreadPool(1);\n     try {\n-      List<Iterable<Integer>> iterables =\n-          ImmutableList.of(\n-              () -> IntStream.range(0, 100).iterator(),\n-              () -> IntStream.range(0, 100).iterator(),\n-              () -> IntStream.range(0, 100).iterator());\n+      Semaphore semaphore = new Semaphore(1);\n \n-      Multiset<Integer> expectedValues =\n-          IntStream.range(0, 100)\n-              .boxed()\n-              .flatMap(i -> Stream.of(i, i, i))\n-              .collect(ImmutableMultiset.toImmutableMultiset());\n+      List<Iterable<Integer>> iterablesA =\n+          ImmutableList.of(\n+              testIterable(\n+                  semaphore::acquire, semaphore::release, IntStream.range(0, 100).iterator()));\n+      List<Iterable<Integer>> iterablesB =\n+          ImmutableList.of(\n+              testIterable(\n+                  semaphore::acquire, semaphore::release, IntStream.range(200, 300).iterator()));\n \n-      ParallelIterable<Integer> parallelIterable = new ParallelIterable<>(iterables, executor, 1);\n-      ParallelIterator<Integer> iterator = (ParallelIterator<Integer>) parallelIterable.iterator();\n+      ParallelIterable<Integer> parallelIterableA = new ParallelIterable<>(iterablesA, executor, 1);\n+      ParallelIterable<Integer> parallelIterableB = new ParallelIterable<>(iterablesB, executor, 1);\n \n-      Multiset<Integer> actualValues = HashMultiset.create();\n+      parallelIterableA.iterator().next();\n+      parallelIterableB.iterator().next();\n+    } finally {\n+      executor.shutdownNow();\n+    }\n+  }\n \n-      while (iterator.hasNext()) {\n-        assertThat(iterator.queueSize())\n-            .as(\"iterator internal queue size\")\n-            .isLessThanOrEqualTo(1 + iterables.size());\n-        actualValues.add(iterator.next());\n+  private <T> CloseableIterable<T> testIterable(\n+      RunnableWithException open, RunnableWithException close, Iterator<T> iterator) {\n+    return new CloseableIterable<T>() {\n+      @Override\n+      public void close() {\n+        try {\n+          close.run();\n+        } catch (Exception e) {\n+          throw new RuntimeException(e);\n+        }\n       }\n \n-      assertThat(actualValues)\n-          .as(\"multiset of values returned by the iterator\")\n-          .isEqualTo(expectedValues);\n+      @Override\n+      public CloseableIterator<T> iterator() {\n+        try {\n+          open.run();\n+          return CloseableIterator.withClose(iterator);\n+        } catch (Exception e) {\n+          throw new RuntimeException(e);\n+        }\n+      }\n+    };\n+  }\n \n-      iterator.close();\n-    } finally {\n-      executor.shutdown();\n-    }\n+  private interface RunnableWithException {\n+    void run() throws Exception;\n   }\n \n   private void queueHasElements(ParallelIterator<Integer> iterator) {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11779",
    "pr_id": 11779,
    "issue_id": 11777,
    "repo": "apache/iceberg",
    "problem_statement": "REST catalog doesn't return old history if we execute `CREATE OR REPLACE TABLE` statement\n### Apache Iceberg version\r\n\r\n1.7.1 (latest release)\r\n\r\n### Query engine\r\n\r\nTrino\r\n\r\n### Please describe the bug üêû\r\n\r\nhttps://github.com/trinodb/trino/pull/24312 is trying to use `org.apache.iceberg.MetadataTableUtils#createMetadataTableInstance` method for history metadata table. Only REST catalog doesn't return the past history if we execute `CREATE OR REPLACE TABLE` statement. \r\n\r\n```sql\r\ntrino:tpch> TABLE \"region$history\";\r\n          made_current_at           |     snapshot_id     | parent_id | is_current_ancestor\r\n------------------------------------+---------------------+-----------+---------------------\r\n 2024-12-13 16:26:02.387 Asia/Tokyo | 4753270524621689028 |      NULL | true\r\n(1 row)\r\n\r\ntrino:tpch> CREATE OR REPLACE TABLE region AS SELECT * FROM orders;\r\nCREATE TABLE: 15000 rows\r\n\r\ntrino:tpch> TABLE \"region$history\";\r\n          made_current_at           |     snapshot_id     | parent_id | is_current_ancestor\r\n------------------------------------+---------------------+-----------+---------------------\r\n 2024-12-13 16:27:51.756 Asia/Tokyo | 7793069171159305791 |      NULL | true\r\n(1 row)\r\n\r\ntrino:tpch> SHOW CREATE TABLE region;\r\n                                                                          Create Table\r\n-----------------------------------------------------------------------------------------------------------------------------------------------------------------\r\n CREATE TABLE iceberg.tpch.region (\r\n    orderkey bigint,\r\n    custkey bigint,\r\n    orderstatus varchar,\r\n    totalprice double,\r\n    orderdate date,\r\n    orderpriority varchar,\r\n    clerk varchar,\r\n    shippriority integer,\r\n    comment varchar\r\n )\r\n WITH (\r\n    format = 'PARQUET',\r\n    format_version = 2,\r\n    location = '/var/folders/9s/_zwn4r_n2_9bp0krllp1pl3c0000gp/T/a6a26fac-b6be-4c9c-b05b-cbe7ec894b2f/iceberg_data/tpch/region-4f41a42ee7f84d3093d85649bc557e46'\r\n )\r\n(1 row)\r\n```\r\n\r\n[region-4f41a42ee7f84d3093d85649bc557e46.zip](https://github.com/user-attachments/files/18122848/region-4f41a42ee7f84d3093d85649bc557e46.zip)\r\n\r\nThe following request contains `RemoveSnapshotRef` that clears existing snapshot logs.  \r\nhttps://github.com/apache/iceberg/blob/540d6a6251e31b232fe6ed2413680621454d107a/core/src/main/java/org/apache/iceberg/rest/CatalogHandlers.java#L434-L437\r\n\r\nhttps://apache-iceberg.slack.com/archives/C03LG1D563F/p1733195549342209\r\n\r\n### Willingness to contribute\r\n\r\n- [ ] I can contribute a fix for this bug independently\r\n- [X] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\r\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 274,
    "test_files_count": 2,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/TableMetadata.java",
      "core/src/test/java/org/apache/iceberg/TestTableMetadata.java",
      "core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/TestTableMetadata.java",
      "core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java"
    ],
    "base_commit": "f7748f20a0f2524ff16c23b665e87838d2a56cc5",
    "head_commit": "3cf3b66060cdfe3e06fe144dbe25e89c310726ef",
    "repo_url": "https://github.com/apache/iceberg/pull/11779",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11779",
    "dockerfile": "",
    "pr_merged_at": "2024-12-24T15:24:23.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/TableMetadata.java b/core/src/main/java/org/apache/iceberg/TableMetadata.java\nindex 9f6ffbcc8714..19afb7af04aa 100644\n--- a/core/src/main/java/org/apache/iceberg/TableMetadata.java\n+++ b/core/src/main/java/org/apache/iceberg/TableMetadata.java\n@@ -1286,7 +1286,6 @@ public Builder setRef(String name, SnapshotRef ref) {\n     public Builder removeRef(String name) {\n       if (SnapshotRef.MAIN_BRANCH.equals(name)) {\n         this.currentSnapshotId = -1;\n-        snapshotLog.clear();\n       }\n \n       SnapshotRef ref = refs.remove(name);\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/TestTableMetadata.java b/core/src/test/java/org/apache/iceberg/TestTableMetadata.java\nindex 6d066e8a654c..45aa211e5187 100644\n--- a/core/src/test/java/org/apache/iceberg/TestTableMetadata.java\n+++ b/core/src/test/java/org/apache/iceberg/TestTableMetadata.java\n@@ -1693,6 +1693,24 @@ public void buildReplacementKeepsSnapshotLog() throws Exception {\n         .containsExactlyElementsOf(metadata.snapshotLog());\n   }\n \n+  @Test\n+  public void removeRefKeepsSnapshotLog() throws Exception {\n+    TableMetadata metadata =\n+        TableMetadataParser.fromJson(readTableMetadataInputFile(\"TableMetadataV2Valid.json\"));\n+    assertThat(metadata.currentSnapshot()).isNotNull();\n+    assertThat(metadata.snapshots()).hasSize(2);\n+    assertThat(metadata.snapshotLog()).hasSize(2);\n+\n+    TableMetadata removeRef =\n+        TableMetadata.buildFrom(metadata).removeRef(SnapshotRef.MAIN_BRANCH).build();\n+\n+    assertThat(removeRef.currentSnapshot()).isNull();\n+    assertThat(removeRef.snapshots()).hasSize(2).containsExactlyElementsOf(metadata.snapshots());\n+    assertThat(removeRef.snapshotLog())\n+        .hasSize(2)\n+        .containsExactlyElementsOf(metadata.snapshotLog());\n+  }\n+\n   @Test\n   public void testConstructV3Metadata() {\n     TableMetadata.newTableMetadata(\n\ndiff --git a/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java b/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\nindex 4df91a49033d..2be69ce6bed3 100644\n--- a/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\n+++ b/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\n@@ -41,6 +41,7 @@\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.FilesTable;\n import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.HistoryEntry;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.ReachableFileUtil;\n import org.apache.iceberg.ReplaceSortOrder;\n@@ -2152,6 +2153,39 @@ public void testReplaceTransactionRequiresTableExists() {\n         .hasMessageStartingWith(\"Table does not exist: newdb.table\");\n   }\n \n+  @Test\n+  public void testReplaceTableKeepsSnapshotLog() {\n+    C catalog = catalog();\n+\n+    if (requiresNamespaceCreate()) {\n+      catalog.createNamespace(TABLE.namespace());\n+    }\n+\n+    catalog.createTable(TABLE, SCHEMA);\n+\n+    Table table = catalog.loadTable(TABLE);\n+    table.newAppend().appendFile(FILE_A).commit();\n+\n+    List<HistoryEntry> snapshotLogBeforeReplace =\n+        ((BaseTable) table).operations().current().snapshotLog();\n+    assertThat(snapshotLogBeforeReplace).hasSize(1);\n+    HistoryEntry snapshotBeforeReplace = snapshotLogBeforeReplace.get(0);\n+\n+    Transaction replaceTableTransaction = catalog.newReplaceTableTransaction(TABLE, SCHEMA, false);\n+    replaceTableTransaction.newAppend().appendFile(FILE_A).commit();\n+    replaceTableTransaction.commitTransaction();\n+    table.refresh();\n+\n+    List<HistoryEntry> snapshotLogAfterReplace =\n+        ((BaseTable) table).operations().current().snapshotLog();\n+    HistoryEntry snapshotAfterReplace = snapshotLogAfterReplace.get(1);\n+\n+    assertThat(snapshotAfterReplace).isNotEqualTo(snapshotBeforeReplace);\n+    assertThat(snapshotLogAfterReplace)\n+        .hasSize(2)\n+        .containsExactly(snapshotBeforeReplace, snapshotAfterReplace);\n+  }\n+\n   @Test\n   public void testConcurrentReplaceTransactions() {\n     C catalog = catalog();\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11748",
    "pr_id": 11748,
    "issue_id": 11747,
    "repo": "apache/iceberg",
    "problem_statement": "Comment and assertion mismatch in PartitionedWritesTestBase/TestRewritePositionDeleteFilesAction\n### Apache Iceberg version\n\nmain (development)\n\n### Query engine\n\nSpark\n\n### Please describe the bug üêû\n\nThere is a mismatch between the test comment and the assertion in the following unit test, which might lead to confusion or logical errors.\r\n\r\nHere is the relevant code snippet:\r\n\r\n```java\r\n  @TestTemplate\r\n  public void testInsertAppend() {\r\n    assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\r\n        .as(\"Should have 5 rows after insert\")\r\n        .isEqualTo(3L);\r\n```\r\n- The comment \"Should have 5 rows after insert\" indicates the expectation of 5 rows in the table.\r\n- However, the actual assertion isEqualTo(3L) checks for 3 rows instead.\n\n### Willingness to contribute\n\n- [X] I can contribute a fix for this bug independently\n- [X] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 140,
    "test_files_count": 2,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewritePositionDeleteFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewritePositionDeleteFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java"
    ],
    "base_commit": "ff813445916bfd6ec1cc30a02b02f8bade7a26f6",
    "head_commit": "2c797c6a83ab309bfa4d2b8b2931a30af044a988",
    "repo_url": "https://github.com/apache/iceberg/pull/11748",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11748",
    "dockerfile": "",
    "pr_merged_at": "2024-12-15T07:35:05.000Z",
    "patch": "",
    "test_patch": "diff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewritePositionDeleteFilesAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewritePositionDeleteFilesAction.java\nindex 12b104fca27c..5b6983562578 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewritePositionDeleteFilesAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewritePositionDeleteFilesAction.java\n@@ -275,7 +275,7 @@ public void testRewriteFilter() throws Exception {\n             .execute();\n \n     List<DeleteFile> newDeleteFiles = except(deleteFiles(table), deleteFiles);\n-    assertThat(newDeleteFiles).as(\"Should have 4 delete files\").hasSize(2);\n+    assertThat(newDeleteFiles).as(\"Delete files\").hasSize(2);\n \n     List<DeleteFile> expectedRewrittenFiles =\n         filterFiles(table, deleteFiles, ImmutableList.of(1), ImmutableList.of(2));\n@@ -469,7 +469,7 @@ public void testRewriteFilterRemoveDangling() throws Exception {\n             .execute();\n \n     List<DeleteFile> newDeleteFiles = except(deleteFiles(table), deleteFiles);\n-    assertThat(newDeleteFiles).as(\"Should have 2 new delete files\").hasSize(0);\n+    assertThat(newDeleteFiles).as(\"New delete files\").hasSize(0);\n \n     List<DeleteFile> expectedRewrittenFiles =\n         filterFiles(table, deleteFiles, ImmutableList.of(0), ImmutableList.of(1));\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java\nindex 97f8e6142dc5..88d18113f19c 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/PartitionedWritesTestBase.java\n@@ -53,7 +53,7 @@ public void removeTables() {\n   @TestTemplate\n   public void testInsertAppend() {\n     assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n-        .as(\"Should have 5 rows after insert\")\n+        .as(\"Rows before insert\")\n         .isEqualTo(3L);\n \n     sql(\"INSERT INTO %s VALUES (4, 'd'), (5, 'e')\", commitTarget());\n@@ -74,7 +74,7 @@ public void testInsertAppend() {\n   @TestTemplate\n   public void testInsertOverwrite() {\n     assertThat(scalarSql(\"SELECT count(*) FROM %s\", selectTarget()))\n-        .as(\"Should have 5 rows after insert\")\n+        .as(\"Rows before overwrite\")\n         .isEqualTo(3L);\n \n     // 4 and 5 replace 3 in the partition (id - (id % 3)) = 3\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11702",
    "pr_id": 11702,
    "issue_id": 11543,
    "repo": "apache/iceberg",
    "problem_statement": "Flink `write-parallelism` causes distribution-mode: RANGE to not work properly\n### Apache Iceberg version\n\n1.7.0 (latest release)\n\n### Query engine\n\nFlink\n\n### Please describe the bug üêû\n\nWhen we set write-parallelism, if the parallelism of writing is different from the parallelism of tasks, iceberg write operator will be disconnected. In this case, distribution-mode: RANGE will have no effect.\r\n\r\n<img width=\"1440\" alt=\"image\" src=\"https://github.com/user-attachments/assets/d8bb7332-495a-4cc5-ab29-bb2590ac277d\">\r\n\r\n\r\nWe should throw this exception when the task jobgraph is generated\r\n\r\ncc @stevenzwu  @pvary \r\n\r\n\n\n### Willingness to contribute\n\n- [X] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 131,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkDistributionMode.java"
    ],
    "pr_changed_test_files": [
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkDistributionMode.java"
    ],
    "base_commit": "ac865e334e143dfd9e33011d8cf710b46d91f1e5",
    "head_commit": "acef3f9af82fec857cb97692de0980d28cac5a84",
    "repo_url": "https://github.com/apache/iceberg/pull/11702",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11702",
    "dockerfile": "",
    "pr_merged_at": "2024-12-27T01:28:40.000Z",
    "patch": "diff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\nindex e862e88c968c..cd5db97b872e 100644\n--- a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n@@ -33,6 +33,7 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.function.Function;\n+import org.apache.flink.api.common.functions.FlatMapFunction;\n import org.apache.flink.api.common.functions.MapFunction;\n import org.apache.flink.api.common.typeinfo.TypeInformation;\n import org.apache.flink.api.common.typeinfo.Types;\n@@ -666,8 +667,17 @@ private DataStream<RowData> distributeDataStream(\n \n           return shuffleStream\n               .partitionCustom(new RangePartitioner(iSchema, sortOrder), r -> r)\n-              .filter(StatisticsOrRecord::hasRecord)\n-              .map(StatisticsOrRecord::record);\n+              .flatMap(\n+                  (FlatMapFunction<StatisticsOrRecord, RowData>)\n+                      (statisticsOrRecord, out) -> {\n+                        if (statisticsOrRecord.hasRecord()) {\n+                          out.collect(statisticsOrRecord.record());\n+                        }\n+                      })\n+              // Set the parallelism same as writerParallelism to\n+              // promote operator chaining with the downstream writer operator\n+              .setParallelism(writerParallelism)\n+              .returns(RowData.class);\n \n         default:\n           throw new RuntimeException(\"Unrecognized \" + WRITE_DISTRIBUTION_MODE + \": \" + writeMode);\n",
    "test_patch": "diff --git a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkDistributionMode.java b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkDistributionMode.java\nindex df8c3c79d3e3..5831279f4cd6 100644\n--- a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkDistributionMode.java\n+++ b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkDistributionMode.java\n@@ -81,13 +81,18 @@ public class TestFlinkIcebergSinkDistributionMode extends TestFlinkIcebergSinkBa\n   @Parameter(index = 1)\n   private boolean partitioned;\n \n-  @Parameters(name = \"parallelism = {0}, partitioned = {1}\")\n+  @Parameter(index = 2)\n+  private int writeParallelism;\n+\n+  @Parameters(name = \"parallelism = {0}, partitioned = {1}, writeParallelism = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {1, true},\n-      {1, false},\n-      {2, true},\n-      {2, false}\n+      {1, true, 1},\n+      {1, false, 1},\n+      {2, true, 2},\n+      {2, false, 2},\n+      {1, true, 2},\n+      {1, false, 2},\n     };\n   }\n \n@@ -109,7 +114,7 @@ public void before() throws IOException {\n                 MiniFlinkClusterExtension.DISABLE_CLASSLOADER_CHECK_CONFIG)\n             .enableCheckpointing(100)\n             .setParallelism(parallelism)\n-            .setMaxParallelism(parallelism);\n+            .setMaxParallelism(Math.max(parallelism, writeParallelism));\n \n     this.tableLoader = CATALOG_EXTENSION.tableLoader();\n   }\n@@ -179,7 +184,7 @@ public void testOverrideWriteConfigWithUnknownDistributionMode() {\n         FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n             .table(table)\n             .tableLoader(tableLoader)\n-            .writeParallelism(parallelism)\n+            .writeParallelism(writeParallelism)\n             .setAll(newProps);\n \n     assertThatThrownBy(builder::append)\n@@ -205,7 +210,7 @@ public void testRangeDistributionWithoutSortOrderUnpartitioned() throws Exceptio\n         FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n             .table(table)\n             .tableLoader(tableLoader)\n-            .writeParallelism(parallelism);\n+            .writeParallelism(writeParallelism);\n \n     // Range distribution requires either sort order or partition spec defined\n     assertThatThrownBy(builder::append)\n@@ -232,7 +237,7 @@ public void testRangeDistributionWithoutSortOrderPartitioned() throws Exception\n         FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n             .table(table)\n             .tableLoader(tableLoader)\n-            .writeParallelism(parallelism);\n+            .writeParallelism(writeParallelism);\n \n     // sort based on partition columns\n     builder.append();\n@@ -268,7 +273,7 @@ public void testRangeDistributionWithSortOrder() throws Exception {\n     FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n         .table(table)\n         .tableLoader(tableLoader)\n-        .writeParallelism(parallelism)\n+        .writeParallelism(writeParallelism)\n         .rangeDistributionStatisticsType(StatisticsType.Map)\n         .append();\n     env.execute(getClass().getSimpleName());\n@@ -304,9 +309,9 @@ public void testRangeDistributionWithSortOrder() throws Exception {\n         List<DataFile> addedDataFiles =\n             Lists.newArrayList(snapshot.addedDataFiles(table.io()).iterator());\n         // each writer task should only write one file for non-partition sort column\n-        assertThat(addedDataFiles).hasSize(parallelism);\n+        assertThat(addedDataFiles).hasSize(writeParallelism);\n         // verify there is no overlap in min-max stats range\n-        if (parallelism == 2) {\n+        if (parallelism > 1) {\n           assertIdColumnStatsNoRangeOverlap(addedDataFiles.get(0), addedDataFiles.get(1));\n         }\n       }\n@@ -329,7 +334,7 @@ public void testRangeDistributionSketchWithSortOrder() throws Exception {\n     FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n         .table(table)\n         .tableLoader(tableLoader)\n-        .writeParallelism(parallelism)\n+        .writeParallelism(writeParallelism)\n         .rangeDistributionStatisticsType(StatisticsType.Sketch)\n         .append();\n     env.execute(getClass().getSimpleName());\n@@ -360,9 +365,9 @@ public void testRangeDistributionSketchWithSortOrder() throws Exception {\n       List<DataFile> addedDataFiles =\n           Lists.newArrayList(snapshot.addedDataFiles(table.io()).iterator());\n       // each writer task should only write one file for non-partition sort column\n-      assertThat(addedDataFiles).hasSize(parallelism);\n+      assertThat(addedDataFiles).hasSize(writeParallelism);\n       // verify there is no overlap in min-max stats range\n-      if (parallelism == 2) {\n+      if (writeParallelism > 2) {\n         assertIdColumnStatsNoRangeOverlap(addedDataFiles.get(0), addedDataFiles.get(1));\n       }\n     }\n@@ -398,7 +403,7 @@ public void testRangeDistributionStatisticsMigration() throws Exception {\n     FlinkSink.forRow(dataStream, SimpleDataUtil.FLINK_SCHEMA)\n         .table(table)\n         .tableLoader(tableLoader)\n-        .writeParallelism(parallelism)\n+        .writeParallelism(writeParallelism)\n         .rangeDistributionStatisticsType(StatisticsType.Auto)\n         .append();\n     env.execute(getClass().getSimpleName());\n@@ -430,9 +435,9 @@ public void testRangeDistributionStatisticsMigration() throws Exception {\n           Lists.newArrayList(snapshot.addedDataFiles(table.io()).iterator());\n       // each writer task should only write one file for non-partition sort column\n       // sometimes\n-      assertThat(addedDataFiles).hasSize(parallelism);\n+      assertThat(addedDataFiles).hasSize(writeParallelism);\n       // verify there is no overlap in min-max stats range\n-      if (parallelism == 2) {\n+      if (writeParallelism > 1) {\n         assertIdColumnStatsNoRangeOverlap(addedDataFiles.get(0), addedDataFiles.get(1));\n       }\n     }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11696",
    "pr_id": 11696,
    "issue_id": 11438,
    "repo": "apache/iceberg",
    "problem_statement": "the `where` sql in rewriteDataFilesAction is always case sensitive\n### Apache Iceberg version\r\n\r\n1.6.1 (latest release)\r\n\r\n### Query engine\r\n\r\nSpark\r\n\r\n### Please describe the bug üêû\r\n\r\ngiven  an iceberg table created by the following sql:\r\n```sql\r\nCREATE TABLE icbtest.test0(\r\n`pk_id` int,\r\n val string,\r\n start_dt string, \r\nend_dt string\r\n) USING iceberg \r\npartitioned by (truncate(6, end_dt))\r\n;\r\n```\r\nafter that, we run the `rewriteDataFiles` procedure by the sql:\r\n```sql\r\nset spark.sql.caseSensitive=false;\r\ncall my_catalog.system.rewrite_data_files(table=>'icbtest.test0', where => \" END_DT  > '202008' \");\r\n```\r\nthe following exception is raised \r\n![ice-bug](https://github.com/user-attachments/assets/5af39e87-40fb-4090-8fef-9925c581fe79)\r\n\r\nsince the `where` sql is always case sensitive, and `set spark.sql.caseSensitive=false` does not take effects.\r\n\r\n\r\n### Willingness to contribute\r\n\r\n- [X] I can contribute a fix for this bug independently\r\n- [X] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\r\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 157,
    "test_files_count": 2,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java",
      "spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java",
      "spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java",
      "spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java"
    ],
    "base_commit": "6501d29b2d46c8d57f46ad646e2daf7d8865f646",
    "head_commit": "334df0fae013a3f10a67ba1e9769260ff6a0044c",
    "repo_url": "https://github.com/apache/iceberg/pull/11696",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11696",
    "dockerfile": "",
    "pr_merged_at": "2024-12-04T14:39:11.000Z",
    "patch": "diff --git a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java\nindex eed0b2b67b0a..73aa54ffc8a8 100644\n--- a/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java\n+++ b/spark/v3.3/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java\n@@ -59,6 +59,7 @@\n import org.apache.iceberg.relocated.com.google.common.math.IntMath;\n import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.spark.SparkUtil;\n import org.apache.iceberg.types.Types.StructType;\n import org.apache.iceberg.util.PropertyUtil;\n import org.apache.iceberg.util.StructLikeMap;\n@@ -94,11 +95,13 @@ public class RewriteDataFilesSparkAction\n   private boolean useStartingSequenceNumber;\n   private RewriteJobOrder rewriteJobOrder;\n   private FileRewriter<FileScanTask, DataFile> rewriter = null;\n+  private boolean caseSensitive;\n \n   RewriteDataFilesSparkAction(SparkSession spark, Table table) {\n     super(spark.cloneSession());\n     // Disable Adaptive Query Execution as this may change the output partitioning of our write\n     spark().conf().set(SQLConf.ADAPTIVE_EXECUTION_ENABLED().key(), false);\n+    this.caseSensitive = SparkUtil.caseSensitive(spark);\n     this.table = table;\n   }\n \n@@ -183,6 +186,7 @@ StructLikeMap<List<List<FileScanTask>>> planFileGroups(long startingSnapshotId)\n         table\n             .newScan()\n             .useSnapshot(startingSnapshotId)\n+            .caseSensitive(caseSensitive)\n             .filter(filter)\n             .ignoreResiduals()\n             .planFiles();\n\ndiff --git a/spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java b/spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java\nindex 0b2bbb3dfc39..ce0808da50b8 100644\n--- a/spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java\n+++ b/spark/v3.4/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java\n@@ -62,6 +62,7 @@\n import org.apache.iceberg.relocated.com.google.common.math.IntMath;\n import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.spark.SparkUtil;\n import org.apache.iceberg.types.Types.StructType;\n import org.apache.iceberg.util.PropertyUtil;\n import org.apache.iceberg.util.StructLikeMap;\n@@ -100,11 +101,13 @@ public class RewriteDataFilesSparkAction\n   private boolean useStartingSequenceNumber;\n   private RewriteJobOrder rewriteJobOrder;\n   private FileRewriter<FileScanTask, DataFile> rewriter = null;\n+  private boolean caseSensitive;\n \n   RewriteDataFilesSparkAction(SparkSession spark, Table table) {\n     super(spark.cloneSession());\n     // Disable Adaptive Query Execution as this may change the output partitioning of our write\n     spark().conf().set(SQLConf.ADAPTIVE_EXECUTION_ENABLED().key(), false);\n+    this.caseSensitive = SparkUtil.caseSensitive(spark);\n     this.table = table;\n   }\n \n@@ -195,6 +198,7 @@ StructLikeMap<List<List<FileScanTask>>> planFileGroups(long startingSnapshotId)\n         table\n             .newScan()\n             .useSnapshot(startingSnapshotId)\n+            .caseSensitive(caseSensitive)\n             .filter(filter)\n             .ignoreResiduals()\n             .planFiles();\n",
    "test_patch": "diff --git a/spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java b/spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\nindex fd8754b30d5b..f1d4a9e733f8 100644\n--- a/spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\n+++ b/spark/v3.3/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\n@@ -41,6 +41,7 @@\n import org.apache.spark.sql.Dataset;\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.sql.internal.SQLConf;\n import org.junit.After;\n import org.junit.Assert;\n import org.junit.Assume;\n@@ -61,6 +62,29 @@ public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName(QUOTED_SPECIAL_CHARS_TABLE_NAME));\n   }\n \n+  @Test\n+  public void testFilterCaseSensitivity() {\n+    createTable();\n+    insertData(10);\n+    sql(\"set %s = false\", SQLConf.CASE_SENSITIVE().key());\n+    List<Object[]> expectedRecords = currentData();\n+    List<Object[]> output =\n+        sql(\n+            \"CALL %s.system.rewrite_data_files(table=>'%s', where=>'C1 > 0')\",\n+            catalogName, tableIdent);\n+    assertEquals(\n+        \"Action should rewrite 10 data files and add 1 data files\",\n+        row(10, 1),\n+        Arrays.copyOf(output.get(0), 2));\n+    // verify rewritten bytes separately\n+    assertThat(output.get(0)).hasSize(3);\n+    assertThat(output.get(0)[2])\n+        .isInstanceOf(Long.class)\n+        .isEqualTo(Long.valueOf(snapshotSummary().get(SnapshotSummary.REMOVED_FILE_SIZE_PROP)));\n+    List<Object[]> actualRecords = currentData();\n+    assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n+  }\n+\n   @Test\n   public void testZOrderSortExpression() {\n     List<ExtendedParser.RawOrderField> order =\n\ndiff --git a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\nindex e637950ae5d4..7c739fc8f61d 100644\n--- a/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\n+++ b/spark/v3.4/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\n@@ -70,6 +70,29 @@ public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName(QUOTED_SPECIAL_CHARS_TABLE_NAME));\n   }\n \n+  @Test\n+  public void testFilterCaseSensitivity() {\n+    createTable();\n+    insertData(10);\n+    sql(\"set %s = false\", SQLConf.CASE_SENSITIVE().key());\n+    List<Object[]> expectedRecords = currentData();\n+    List<Object[]> output =\n+        sql(\n+            \"CALL %s.system.rewrite_data_files(table=>'%s', where=>'C1 > 0')\",\n+            catalogName, tableIdent);\n+    assertEquals(\n+        \"Action should rewrite 10 data files and add 1 data files\",\n+        row(10, 1),\n+        Arrays.copyOf(output.get(0), 2));\n+    // verify rewritten bytes separately\n+    assertThat(output.get(0)).hasSize(4);\n+    assertThat(output.get(0)[2])\n+        .isInstanceOf(Long.class)\n+        .isEqualTo(Long.valueOf(snapshotSummary().get(SnapshotSummary.REMOVED_FILE_SIZE_PROP)));\n+    List<Object[]> actualRecords = currentData();\n+    assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n+  }\n+\n   @Test\n   public void testZOrderSortExpression() {\n     List<ExtendedParser.RawOrderField> order =\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11671",
    "pr_id": 11671,
    "issue_id": 11664,
    "repo": "apache/iceberg",
    "problem_statement": "CommitReport not reported through custom MetricsReporter\n### Apache Iceberg version\r\n\r\n1.6.0\r\n\r\n### Query engine\r\n\r\nSpark\r\n\r\n### Please describe the bug üêû\r\n\r\nI am running Spark jobs locally on my laptop with a custom MetricsReporter which does no external connections and is pretty much same to in-memory metrics reporter. I am using Spark Config to specify metrics reporter class.\r\n\r\nMy SQL is as simple as:\r\n```\r\n    spark.sql(\"CREATE TABLE source1 USING iceberg AS SELECT * FROM temp\");\r\n    spark.sql(\"CREATE TABLE target USING iceberg AS SELECT * FROM source1\");\r\n```\r\nwith a table having two fields and two rows. \r\n\r\nIn this case, I see my custom metrics reporter got loaded due to the log:\r\n```\r\n2024-11-26 15:11:10 INFO  org.apache.iceberg.CatalogUtil - Loading custom MetricsReporter implementation: io.openlineage.spark.agent.vendor.iceberg.metrics.OpenLineageMetricsReporter\r\n```\r\nI am able to debug that my metrics reporter gets `ScanReport`. However, it is not getting any `CommitReport`. Instead, they're logged with `LoggingMetricsReporter`:\r\n```\r\n2024-11-26 15:11:12 INFO  org.apache.iceberg.metrics.LoggingMetricsReporter - Received metrics report: CommitReport{tableName=default.source1, snapshotId=2507000462464497081, sequenceNumber=1, ...., metadata={engine-version=3.3.4, app-id=local-1732630268734, engine-name=spark, iceberg-version=Apache Iceberg 1.6.0 (commit 229d8f6fcd109e6c8943ea7cbb41dab746c6d0ed)}}\r\n```\r\nThere are no logs of the form `Cannot initialize MetricsReporter`.\r\n\r\nMoreover, when I run the query:\r\n```\r\n spark.sql(\"INSERT INTO target VALUES (4, 5)\");\r\n```\r\nI see commit reports within my custom reporter. That's why I think it's a bug while not being a configuration issue on my side. \r\n\r\nDuring short debugging, I found out that `BaseTransaction` object is initialised through a constructor https://github.com/apache/iceberg/blob/main/core/src/main/java/org/apache/iceberg/BaseTransaction.java#L82 which injects `LoggingMetricsReporter` instead of using the one configured. \r\n\r\n\r\n\r\n### Willingness to contribute\r\n\r\n- [ ] I can contribute a fix for this bug independently\r\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\r\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 331,
    "test_files_count": 8,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java",
      "core/src/main/java/org/apache/iceberg/Transactions.java",
      "core/src/main/java/org/apache/iceberg/inmemory/InMemoryCatalog.java",
      "core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java",
      "core/src/test/java/org/apache/iceberg/inmemory/TestInMemoryCatalog.java",
      "core/src/test/java/org/apache/iceberg/jdbc/TestJdbcCatalog.java",
      "core/src/test/java/org/apache/iceberg/jdbc/TestJdbcCatalogWithV1Schema.java",
      "core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java",
      "nessie/src/test/java/org/apache/iceberg/nessie/TestNessieCatalog.java",
      "open-api/src/test/java/org/apache/iceberg/rest/RESTCompatibilityKitCatalogTests.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java",
      "core/src/test/java/org/apache/iceberg/inmemory/TestInMemoryCatalog.java",
      "core/src/test/java/org/apache/iceberg/jdbc/TestJdbcCatalog.java",
      "core/src/test/java/org/apache/iceberg/jdbc/TestJdbcCatalogWithV1Schema.java",
      "core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java",
      "nessie/src/test/java/org/apache/iceberg/nessie/TestNessieCatalog.java",
      "open-api/src/test/java/org/apache/iceberg/rest/RESTCompatibilityKitCatalogTests.java"
    ],
    "base_commit": "163e2068f96f139632488f36928bf443c9be326f",
    "head_commit": "8219e0c39ed2cd56baab0152e2878ea5e82f4794",
    "repo_url": "https://github.com/apache/iceberg/pull/11671",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11671",
    "dockerfile": "",
    "pr_merged_at": "2024-11-28T14:59:43.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java b/core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java\nindex e794b3121dc3..e960fe2b63e0 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseMetastoreCatalog.java\n@@ -217,7 +217,8 @@ public Transaction createTransaction() {\n       tableProperties.putAll(tableOverrideProperties());\n       TableMetadata metadata =\n           TableMetadata.newTableMetadata(schema, spec, sortOrder, baseLocation, tableProperties);\n-      return Transactions.createTableTransaction(identifier.toString(), ops, metadata);\n+      return Transactions.createTableTransaction(\n+          identifier.toString(), ops, metadata, metricsReporter());\n     }\n \n     @Override\n@@ -249,9 +250,11 @@ private Transaction newReplaceTableTransaction(boolean orCreate) {\n       }\n \n       if (orCreate) {\n-        return Transactions.createOrReplaceTableTransaction(identifier.toString(), ops, metadata);\n+        return Transactions.createOrReplaceTableTransaction(\n+            identifier.toString(), ops, metadata, metricsReporter());\n       } else {\n-        return Transactions.replaceTableTransaction(identifier.toString(), ops, metadata);\n+        return Transactions.replaceTableTransaction(\n+            identifier.toString(), ops, metadata, metricsReporter());\n       }\n     }\n \n\ndiff --git a/core/src/main/java/org/apache/iceberg/Transactions.java b/core/src/main/java/org/apache/iceberg/Transactions.java\nindex 7afed0573a39..a8ea40a6b90b 100644\n--- a/core/src/main/java/org/apache/iceberg/Transactions.java\n+++ b/core/src/main/java/org/apache/iceberg/Transactions.java\n@@ -30,6 +30,12 @@ public static Transaction createOrReplaceTableTransaction(\n     return new BaseTransaction(tableName, ops, TransactionType.CREATE_OR_REPLACE_TABLE, start);\n   }\n \n+  public static Transaction createOrReplaceTableTransaction(\n+      String tableName, TableOperations ops, TableMetadata start, MetricsReporter reporter) {\n+    return new BaseTransaction(\n+        tableName, ops, TransactionType.CREATE_OR_REPLACE_TABLE, start, reporter);\n+  }\n+\n   public static Transaction replaceTableTransaction(\n       String tableName, TableOperations ops, TableMetadata start) {\n     return new BaseTransaction(tableName, ops, TransactionType.REPLACE_TABLE, start);\n\ndiff --git a/core/src/main/java/org/apache/iceberg/inmemory/InMemoryCatalog.java b/core/src/main/java/org/apache/iceberg/inmemory/InMemoryCatalog.java\nindex a880f94f4385..ff71bde71ff5 100644\n--- a/core/src/main/java/org/apache/iceberg/inmemory/InMemoryCatalog.java\n+++ b/core/src/main/java/org/apache/iceberg/inmemory/InMemoryCatalog.java\n@@ -70,6 +70,7 @@ public class InMemoryCatalog extends BaseMetastoreViewCatalog\n   private String catalogName;\n   private String warehouseLocation;\n   private CloseableGroup closeableGroup;\n+  private Map<String, String> catalogProperties;\n \n   public InMemoryCatalog() {\n     this.namespaces = Maps.newConcurrentMap();\n@@ -85,6 +86,7 @@ public String name() {\n   @Override\n   public void initialize(String name, Map<String, String> properties) {\n     this.catalogName = name != null ? name : InMemoryCatalog.class.getSimpleName();\n+    this.catalogProperties = ImmutableMap.copyOf(properties);\n \n     String warehouse = properties.getOrDefault(CatalogProperties.WAREHOUSE_LOCATION, \"\");\n     this.warehouseLocation = warehouse.replaceAll(\"/*$\", \"\");\n@@ -368,6 +370,11 @@ public void renameView(TableIdentifier from, TableIdentifier to) {\n     }\n   }\n \n+  @Override\n+  protected Map<String, String> properties() {\n+    return catalogProperties == null ? ImmutableMap.of() : catalogProperties;\n+  }\n+\n   private class InMemoryTableOperations extends BaseMetastoreTableOperations {\n     private final FileIO fileIO;\n     private final TableIdentifier tableIdentifier;\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java b/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\nindex a011578865b4..4df91a49033d 100644\n--- a/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\n+++ b/core/src/test/java/org/apache/iceberg/catalog/CatalogTests.java\n@@ -30,11 +30,14 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.UUID;\n+import java.util.concurrent.atomic.AtomicInteger;\n import java.util.stream.Collectors;\n import org.apache.iceberg.AppendFiles;\n import org.apache.iceberg.BaseTable;\n+import org.apache.iceberg.CatalogProperties;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataFiles;\n+import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.FilesTable;\n import org.apache.iceberg.HasTableOperations;\n@@ -56,6 +59,10 @@\n import org.apache.iceberg.exceptions.NoSuchTableException;\n import org.apache.iceberg.expressions.Expressions;\n import org.apache.iceberg.io.CloseableIterable;\n+import org.apache.iceberg.metrics.CommitReport;\n+import org.apache.iceberg.metrics.MetricsReport;\n+import org.apache.iceberg.metrics.MetricsReporter;\n+import org.apache.iceberg.metrics.ScanReport;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n@@ -144,6 +151,8 @@ public abstract class CatalogTests<C extends Catalog & SupportsNamespaces> {\n \n   protected abstract C catalog();\n \n+  protected abstract C initCatalog(String catalogName, Map<String, String> additionalProperties);\n+\n   protected boolean supportsNamespaceProperties() {\n     return true;\n   }\n@@ -2695,6 +2704,87 @@ public void testRegisterExistingTable() {\n     assertThat(catalog.dropTable(identifier)).isTrue();\n   }\n \n+  @Test\n+  public void testCatalogWithCustomMetricsReporter() throws IOException {\n+    C catalogWithCustomReporter =\n+        initCatalog(\n+            \"catalog_with_custom_reporter\",\n+            ImmutableMap.of(\n+                CatalogProperties.METRICS_REPORTER_IMPL, CustomMetricsReporter.class.getName()));\n+\n+    if (requiresNamespaceCreate()) {\n+      catalogWithCustomReporter.createNamespace(TABLE.namespace());\n+    }\n+\n+    catalogWithCustomReporter.buildTable(TABLE, SCHEMA).create();\n+\n+    Table table = catalogWithCustomReporter.loadTable(TABLE);\n+    DataFile dataFile =\n+        DataFiles.builder(PartitionSpec.unpartitioned())\n+            .withPath(FileFormat.PARQUET.addExtension(UUID.randomUUID().toString()))\n+            .withFileSizeInBytes(10)\n+            .withRecordCount(2)\n+            .build();\n+\n+    // append file through FastAppend and check and reset counter\n+    table.newFastAppend().appendFile(dataFile).commit();\n+    assertThat(CustomMetricsReporter.COMMIT_COUNTER.get()).isEqualTo(1);\n+    CustomMetricsReporter.COMMIT_COUNTER.set(0);\n+\n+    TableIdentifier identifier = TableIdentifier.of(NS, \"custom_metrics_reporter_table\");\n+    // append file through createTransaction() and check and reset counter\n+    catalogWithCustomReporter\n+        .buildTable(identifier, SCHEMA)\n+        .createTransaction()\n+        .newFastAppend()\n+        .appendFile(dataFile)\n+        .commit();\n+    assertThat(CustomMetricsReporter.COMMIT_COUNTER.get()).isEqualTo(1);\n+    CustomMetricsReporter.COMMIT_COUNTER.set(0);\n+\n+    // append file through createOrReplaceTransaction() and check and reset counter\n+    catalogWithCustomReporter\n+        .buildTable(identifier, SCHEMA)\n+        .createOrReplaceTransaction()\n+        .newFastAppend()\n+        .appendFile(dataFile)\n+        .commit();\n+    assertThat(CustomMetricsReporter.COMMIT_COUNTER.get()).isEqualTo(1);\n+    CustomMetricsReporter.COMMIT_COUNTER.set(0);\n+\n+    // append file through replaceTransaction() and check and reset counter\n+    catalogWithCustomReporter\n+        .buildTable(TABLE, SCHEMA)\n+        .replaceTransaction()\n+        .newFastAppend()\n+        .appendFile(dataFile)\n+        .commit();\n+    assertThat(CustomMetricsReporter.COMMIT_COUNTER.get()).isEqualTo(1);\n+    CustomMetricsReporter.COMMIT_COUNTER.set(0);\n+\n+    try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n+      assertThat(tasks.iterator()).hasNext();\n+    }\n+\n+    assertThat(CustomMetricsReporter.SCAN_COUNTER.get()).isEqualTo(1);\n+    // reset counter in case subclasses run this test multiple times\n+    CustomMetricsReporter.SCAN_COUNTER.set(0);\n+  }\n+\n+  public static class CustomMetricsReporter implements MetricsReporter {\n+    static final AtomicInteger SCAN_COUNTER = new AtomicInteger(0);\n+    static final AtomicInteger COMMIT_COUNTER = new AtomicInteger(0);\n+\n+    @Override\n+    public void report(MetricsReport report) {\n+      if (report instanceof ScanReport) {\n+        SCAN_COUNTER.incrementAndGet();\n+      } else if (report instanceof CommitReport) {\n+        COMMIT_COUNTER.incrementAndGet();\n+      }\n+    }\n+  }\n+\n   private static void assertEmpty(String context, Catalog catalog, Namespace ns) {\n     try {\n       assertThat(catalog.listTables(ns)).as(context).isEmpty();\n\ndiff --git a/core/src/test/java/org/apache/iceberg/inmemory/TestInMemoryCatalog.java b/core/src/test/java/org/apache/iceberg/inmemory/TestInMemoryCatalog.java\nindex 63cd24b4e2c6..2c8650d6358b 100644\n--- a/core/src/test/java/org/apache/iceberg/inmemory/TestInMemoryCatalog.java\n+++ b/core/src/test/java/org/apache/iceberg/inmemory/TestInMemoryCatalog.java\n@@ -18,6 +18,7 @@\n  */\n package org.apache.iceberg.inmemory;\n \n+import java.util.Map;\n import org.apache.iceberg.catalog.CatalogTests;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.junit.jupiter.api.BeforeEach;\n@@ -27,8 +28,7 @@ public class TestInMemoryCatalog extends CatalogTests<InMemoryCatalog> {\n \n   @BeforeEach\n   public void before() {\n-    this.catalog = new InMemoryCatalog();\n-    this.catalog.initialize(\"in-memory-catalog\", ImmutableMap.of());\n+    this.catalog = initCatalog(\"in-memory-catalog\", ImmutableMap.of());\n   }\n \n   @Override\n@@ -36,6 +36,14 @@ protected InMemoryCatalog catalog() {\n     return catalog;\n   }\n \n+  @Override\n+  protected InMemoryCatalog initCatalog(\n+      String catalogName, Map<String, String> additionalProperties) {\n+    InMemoryCatalog cat = new InMemoryCatalog();\n+    cat.initialize(catalogName, additionalProperties);\n+    return cat;\n+  }\n+\n   @Override\n   protected boolean requiresNamespaceCreate() {\n     return true;\n\ndiff --git a/core/src/test/java/org/apache/iceberg/jdbc/TestJdbcCatalog.java b/core/src/test/java/org/apache/iceberg/jdbc/TestJdbcCatalog.java\nindex d21605cace21..2d4eb2f15738 100644\n--- a/core/src/test/java/org/apache/iceberg/jdbc/TestJdbcCatalog.java\n+++ b/core/src/test/java/org/apache/iceberg/jdbc/TestJdbcCatalog.java\n@@ -39,7 +39,6 @@\n import java.util.Map;\n import java.util.Set;\n import java.util.UUID;\n-import java.util.concurrent.atomic.AtomicInteger;\n import java.util.stream.Collectors;\n import java.util.stream.Stream;\n import org.apache.hadoop.conf.Configuration;\n@@ -50,8 +49,6 @@\n import org.apache.iceberg.CatalogUtil;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataFiles;\n-import org.apache.iceberg.FileFormat;\n-import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.SortOrder;\n@@ -68,9 +65,6 @@\n import org.apache.iceberg.exceptions.NoSuchNamespaceException;\n import org.apache.iceberg.exceptions.NoSuchTableException;\n import org.apache.iceberg.hadoop.Util;\n-import org.apache.iceberg.io.CloseableIterable;\n-import org.apache.iceberg.metrics.MetricsReport;\n-import org.apache.iceberg.metrics.MetricsReporter;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n@@ -139,7 +133,8 @@ public void setupTable() throws Exception {\n     catalog = initCatalog(\"test_jdbc_catalog\", Maps.newHashMap());\n   }\n \n-  private JdbcCatalog initCatalog(String catalogName, Map<String, String> props) {\n+  @Override\n+  protected JdbcCatalog initCatalog(String catalogName, Map<String, String> additionalProperties) {\n     Map<String, String> properties = Maps.newHashMap();\n     properties.put(\n         CatalogProperties.URI,\n@@ -150,7 +145,7 @@ private JdbcCatalog initCatalog(String catalogName, Map<String, String> props) {\n     warehouseLocation = this.tableDir.toAbsolutePath().toString();\n     properties.put(CatalogProperties.WAREHOUSE_LOCATION, warehouseLocation);\n     properties.put(\"type\", \"jdbc\");\n-    properties.putAll(props);\n+    properties.putAll(additionalProperties);\n \n     return (JdbcCatalog) CatalogUtil.buildIcebergCatalog(catalogName, properties, conf);\n   }\n@@ -1059,36 +1054,6 @@ public void testConversions() {\n     assertThat(JdbcUtil.stringToNamespace(nsString)).isEqualTo(ns);\n   }\n \n-  @Test\n-  public void testCatalogWithCustomMetricsReporter() throws IOException {\n-    JdbcCatalog catalogWithCustomReporter =\n-        initCatalog(\n-            \"test_jdbc_catalog_with_custom_reporter\",\n-            ImmutableMap.of(\n-                CatalogProperties.METRICS_REPORTER_IMPL, CustomMetricsReporter.class.getName()));\n-    try {\n-      catalogWithCustomReporter.buildTable(TABLE, SCHEMA).create();\n-      Table table = catalogWithCustomReporter.loadTable(TABLE);\n-      table\n-          .newFastAppend()\n-          .appendFile(\n-              DataFiles.builder(PartitionSpec.unpartitioned())\n-                  .withPath(FileFormat.PARQUET.addExtension(UUID.randomUUID().toString()))\n-                  .withFileSizeInBytes(10)\n-                  .withRecordCount(2)\n-                  .build())\n-          .commit();\n-      try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n-        assertThat(tasks.iterator()).hasNext();\n-      }\n-    } finally {\n-      catalogWithCustomReporter.dropTable(TABLE);\n-    }\n-    // counter of custom metrics reporter should have been increased\n-    // 1x for commit metrics / 1x for scan metrics\n-    assertThat(CustomMetricsReporter.COUNTER.get()).isEqualTo(2);\n-  }\n-\n   @Test\n   public void testCommitExceptionWithoutMessage() {\n     TableIdentifier tableIdent = TableIdentifier.of(\"db\", \"tbl\");\n@@ -1129,15 +1094,6 @@ public void testCommitExceptionWithMessage() {\n     }\n   }\n \n-  public static class CustomMetricsReporter implements MetricsReporter {\n-    static final AtomicInteger COUNTER = new AtomicInteger(0);\n-\n-    @Override\n-    public void report(MetricsReport report) {\n-      COUNTER.incrementAndGet();\n-    }\n-  }\n-\n   private String createMetadataLocationViaJdbcCatalog(TableIdentifier identifier)\n       throws SQLException {\n     // temporary connection just to actually create a concrete metadata location\n\ndiff --git a/core/src/test/java/org/apache/iceberg/jdbc/TestJdbcCatalogWithV1Schema.java b/core/src/test/java/org/apache/iceberg/jdbc/TestJdbcCatalogWithV1Schema.java\nindex b47c216ffced..7586d880c188 100644\n--- a/core/src/test/java/org/apache/iceberg/jdbc/TestJdbcCatalogWithV1Schema.java\n+++ b/core/src/test/java/org/apache/iceberg/jdbc/TestJdbcCatalogWithV1Schema.java\n@@ -23,6 +23,7 @@\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.CatalogProperties;\n import org.apache.iceberg.catalog.CatalogTests;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.io.TempDir;\n@@ -38,6 +39,24 @@ protected JdbcCatalog catalog() {\n     return catalog;\n   }\n \n+  @Override\n+  protected JdbcCatalog initCatalog(String catalogName, Map<String, String> additionalProperties) {\n+    Map<String, String> properties = Maps.newHashMap();\n+    properties.put(\n+        CatalogProperties.URI,\n+        \"jdbc:sqlite:file::memory:?ic\" + UUID.randomUUID().toString().replace(\"-\", \"\"));\n+    properties.put(JdbcCatalog.PROPERTY_PREFIX + \"username\", \"user\");\n+    properties.put(JdbcCatalog.PROPERTY_PREFIX + \"password\", \"password\");\n+    properties.put(CatalogProperties.WAREHOUSE_LOCATION, tableDir.toAbsolutePath().toString());\n+    properties.put(JdbcUtil.SCHEMA_VERSION_PROPERTY, JdbcUtil.SchemaVersion.V1.name());\n+    properties.putAll(additionalProperties);\n+\n+    JdbcCatalog cat = new JdbcCatalog();\n+    cat.setConf(new Configuration());\n+    cat.initialize(catalogName, properties);\n+    return cat;\n+  }\n+\n   @Override\n   protected boolean supportsNamespaceProperties() {\n     return true;\n@@ -50,17 +69,6 @@ protected boolean supportsNestedNamespaces() {\n \n   @BeforeEach\n   public void setupCatalog() {\n-    Map<String, String> properties = Maps.newHashMap();\n-    properties.put(\n-        CatalogProperties.URI,\n-        \"jdbc:sqlite:file::memory:?ic\" + UUID.randomUUID().toString().replace(\"-\", \"\"));\n-    properties.put(JdbcCatalog.PROPERTY_PREFIX + \"username\", \"user\");\n-    properties.put(JdbcCatalog.PROPERTY_PREFIX + \"password\", \"password\");\n-    properties.put(CatalogProperties.WAREHOUSE_LOCATION, tableDir.toAbsolutePath().toString());\n-    properties.put(JdbcUtil.SCHEMA_VERSION_PROPERTY, JdbcUtil.SchemaVersion.V1.name());\n-\n-    catalog = new JdbcCatalog();\n-    catalog.setConf(new Configuration());\n-    catalog.initialize(\"testCatalog\", properties);\n+    this.catalog = initCatalog(\"testCatalog\", ImmutableMap.of());\n   }\n }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java b/core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java\nindex 06008761eac1..232cfd31d1a6 100644\n--- a/core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java\n+++ b/core/src/test/java/org/apache/iceberg/rest/TestRESTCatalog.java\n@@ -36,14 +36,12 @@\n import java.util.Optional;\n import java.util.UUID;\n import java.util.concurrent.TimeUnit;\n-import java.util.concurrent.atomic.AtomicInteger;\n import java.util.function.Consumer;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.BaseTransaction;\n import org.apache.iceberg.CatalogProperties;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataFiles;\n-import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.MetadataUpdate;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n@@ -64,9 +62,6 @@\n import org.apache.iceberg.exceptions.ServiceFailureException;\n import org.apache.iceberg.expressions.Expressions;\n import org.apache.iceberg.inmemory.InMemoryCatalog;\n-import org.apache.iceberg.io.CloseableIterable;\n-import org.apache.iceberg.metrics.MetricsReport;\n-import org.apache.iceberg.metrics.MetricsReporter;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n@@ -114,7 +109,6 @@ public class TestRESTCatalog extends CatalogTests<RESTCatalog> {\n   @BeforeEach\n   public void createCatalog() throws Exception {\n     File warehouse = temp.toFile();\n-    Configuration conf = new Configuration();\n \n     this.backendCatalog = new InMemoryCatalog();\n     this.backendCatalog.initialize(\n@@ -164,6 +158,12 @@ public <T extends RESTResponse> T execute(\n     httpServer.setHandler(servletContext);\n     httpServer.start();\n \n+    this.restCatalog = initCatalog(\"prod\", ImmutableMap.of());\n+  }\n+\n+  @Override\n+  protected RESTCatalog initCatalog(String catalogName, Map<String, String> additionalProperties) {\n+    Configuration conf = new Configuration();\n     SessionCatalog.SessionContext context =\n         new SessionCatalog.SessionContext(\n             UUID.randomUUID().toString(),\n@@ -171,20 +171,26 @@ public <T extends RESTResponse> T execute(\n             ImmutableMap.of(\"credential\", \"user:12345\"),\n             ImmutableMap.of());\n \n-    this.restCatalog =\n+    RESTCatalog catalog =\n         new RESTCatalog(\n             context,\n             (config) -> HTTPClient.builder(config).uri(config.get(CatalogProperties.URI)).build());\n-    restCatalog.setConf(conf);\n-    restCatalog.initialize(\n-        \"prod\",\n+    catalog.setConf(conf);\n+    Map<String, String> properties =\n         ImmutableMap.of(\n             CatalogProperties.URI,\n             httpServer.getURI().toString(),\n             CatalogProperties.FILE_IO_IMPL,\n             \"org.apache.iceberg.inmemory.InMemoryFileIO\",\n             \"credential\",\n-            \"catalog:12345\"));\n+            \"catalog:12345\");\n+    catalog.initialize(\n+        catalogName,\n+        ImmutableMap.<String, String>builder()\n+            .putAll(properties)\n+            .putAll(additionalProperties)\n+            .build());\n+    return catalog;\n   }\n \n   @SuppressWarnings(\"unchecked\")\n@@ -1623,61 +1629,6 @@ public void testCatalogRefreshedTokenIsUsed(String oauth2ServerUri) {\n             });\n   }\n \n-  @Test\n-  public void testCatalogWithCustomMetricsReporter() throws IOException {\n-    this.restCatalog =\n-        new RESTCatalog(\n-            new SessionCatalog.SessionContext(\n-                UUID.randomUUID().toString(),\n-                \"user\",\n-                ImmutableMap.of(\"credential\", \"user:12345\"),\n-                ImmutableMap.of()),\n-            (config) -> HTTPClient.builder(config).uri(config.get(CatalogProperties.URI)).build());\n-    restCatalog.setConf(new Configuration());\n-    restCatalog.initialize(\n-        \"prod\",\n-        ImmutableMap.of(\n-            CatalogProperties.URI,\n-            httpServer.getURI().toString(),\n-            \"credential\",\n-            \"catalog:12345\",\n-            CatalogProperties.METRICS_REPORTER_IMPL,\n-            CustomMetricsReporter.class.getName()));\n-\n-    if (requiresNamespaceCreate()) {\n-      restCatalog.createNamespace(TABLE.namespace());\n-    }\n-\n-    restCatalog.buildTable(TABLE, SCHEMA).create();\n-    Table table = restCatalog.loadTable(TABLE);\n-    table\n-        .newFastAppend()\n-        .appendFile(\n-            DataFiles.builder(PartitionSpec.unpartitioned())\n-                .withPath(\"/path/to/data-a.parquet\")\n-                .withFileSizeInBytes(10)\n-                .withRecordCount(2)\n-                .build())\n-        .commit();\n-\n-    try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n-      assertThat(tasks.iterator()).hasNext();\n-    }\n-\n-    // counter of custom metrics reporter should have been increased\n-    // 1x for commit metrics / 1x for scan metrics\n-    assertThat(CustomMetricsReporter.COUNTER.get()).isEqualTo(2);\n-  }\n-\n-  public static class CustomMetricsReporter implements MetricsReporter {\n-    static final AtomicInteger COUNTER = new AtomicInteger(0);\n-\n-    @Override\n-    public void report(MetricsReport report) {\n-      COUNTER.incrementAndGet();\n-    }\n-  }\n-\n   @Test\n   public void testCatalogExpiredBearerTokenRefreshWithoutCredential() {\n     // expires at epoch second = 1\n\ndiff --git a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java\nindex 7d0eb641a385..709bb1caaa62 100644\n--- a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java\n+++ b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCatalog.java\n@@ -110,20 +110,30 @@ public class TestHiveCatalog extends CatalogTests<HiveCatalog> {\n \n   @BeforeEach\n   public void before() throws TException {\n-    catalog =\n-        (HiveCatalog)\n-            CatalogUtil.loadCatalog(\n-                HiveCatalog.class.getName(),\n-                CatalogUtil.ICEBERG_CATALOG_TYPE_HIVE,\n-                ImmutableMap.of(\n-                    CatalogProperties.CLIENT_POOL_CACHE_EVICTION_INTERVAL_MS,\n-                    String.valueOf(TimeUnit.SECONDS.toMillis(10))),\n-                HIVE_METASTORE_EXTENSION.hiveConf());\n+    catalog = initCatalog(\"hive\", ImmutableMap.of());\n     String dbPath = HIVE_METASTORE_EXTENSION.metastore().getDatabasePath(DB_NAME);\n     Database db = new Database(DB_NAME, \"description\", dbPath, Maps.newHashMap());\n     HIVE_METASTORE_EXTENSION.metastoreClient().createDatabase(db);\n   }\n \n+  @Override\n+  protected HiveCatalog initCatalog(String catalogName, Map<String, String> additionalProperties) {\n+    Map<String, String> properties =\n+        ImmutableMap.of(\n+            CatalogProperties.CLIENT_POOL_CACHE_EVICTION_INTERVAL_MS,\n+            String.valueOf(TimeUnit.SECONDS.toMillis(10)));\n+\n+    return (HiveCatalog)\n+        CatalogUtil.loadCatalog(\n+            HiveCatalog.class.getName(),\n+            catalogName,\n+            ImmutableMap.<String, String>builder()\n+                .putAll(properties)\n+                .putAll(additionalProperties)\n+                .build(),\n+            HIVE_METASTORE_EXTENSION.hiveConf());\n+  }\n+\n   @AfterEach\n   public void cleanup() throws Exception {\n     HIVE_METASTORE_EXTENSION.metastore().reset();\n\ndiff --git a/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieCatalog.java b/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieCatalog.java\nindex 55be034221ae..dce8f7ff0f8c 100644\n--- a/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieCatalog.java\n+++ b/nessie/src/test/java/org/apache/iceberg/nessie/TestNessieCatalog.java\n@@ -78,7 +78,7 @@ public void setUp(NessieClientFactory clientFactory, @NessieClientUri URI nessie\n     initialHashOfDefaultBranch = api.getDefaultBranch().getHash();\n     uri = nessieUri.toASCIIString();\n     hadoopConfig = new Configuration();\n-    catalog = initNessieCatalog(\"main\");\n+    catalog = initCatalog(\"nessie\", ImmutableMap.of());\n   }\n \n   @AfterEach\n@@ -112,18 +112,28 @@ private void resetData() throws NessieConflictException, NessieNotFoundException\n         .assign();\n   }\n \n-  private NessieCatalog initNessieCatalog(String ref) {\n+  @Override\n+  protected NessieCatalog initCatalog(\n+      String catalogName, Map<String, String> additionalProperties) {\n     Map<String, String> options =\n         ImmutableMap.of(\n             \"type\",\n             \"nessie\",\n             \"ref\",\n-            ref,\n+            \"main\",\n             CatalogProperties.URI,\n             uri,\n             CatalogProperties.WAREHOUSE_LOCATION,\n             temp.toUri().toString());\n-    return (NessieCatalog) CatalogUtil.buildIcebergCatalog(\"nessie\", options, hadoopConfig);\n+\n+    return (NessieCatalog)\n+        CatalogUtil.buildIcebergCatalog(\n+            catalogName,\n+            ImmutableMap.<String, String>builder()\n+                .putAll(options)\n+                .putAll(additionalProperties)\n+                .build(),\n+            hadoopConfig);\n   }\n \n   @Override\n\ndiff --git a/open-api/src/test/java/org/apache/iceberg/rest/RESTCompatibilityKitCatalogTests.java b/open-api/src/test/java/org/apache/iceberg/rest/RESTCompatibilityKitCatalogTests.java\nindex 4c4860e88a19..a709d814344f 100644\n--- a/open-api/src/test/java/org/apache/iceberg/rest/RESTCompatibilityKitCatalogTests.java\n+++ b/open-api/src/test/java/org/apache/iceberg/rest/RESTCompatibilityKitCatalogTests.java\n@@ -20,6 +20,7 @@\n \n import static org.assertj.core.api.Assertions.assertThat;\n \n+import java.util.Map;\n import org.apache.iceberg.catalog.CatalogTests;\n import org.apache.iceberg.util.PropertyUtil;\n import org.junit.jupiter.api.AfterAll;\n@@ -63,6 +64,11 @@ protected RESTCatalog catalog() {\n     return restCatalog;\n   }\n \n+  @Override\n+  protected RESTCatalog initCatalog(String catalogName, Map<String, String> additionalProperties) {\n+    return RCKUtils.initCatalogClient(additionalProperties);\n+  }\n+\n   @Override\n   protected boolean requiresNamespaceCreate() {\n     return PropertyUtil.propertyAsBoolean(\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11657",
    "pr_id": 11657,
    "issue_id": 11122,
    "repo": "apache/iceberg",
    "problem_statement": "Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 118,
    "test_files_count": 1,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/DVIterator.java",
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/PositionDeletesRowReader.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPositionDeletesReader.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPositionDeletesReader.java"
    ],
    "base_commit": "fd739b32b4713370218cfd8f46ad525bc8c203f1",
    "head_commit": "25eb30a706e191e89c3e0b0069d91e2188b28422",
    "repo_url": "https://github.com/apache/iceberg/pull/11657",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11657",
    "dockerfile": "",
    "pr_merged_at": "2024-12-16T07:50:49.000Z",
    "patch": "diff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/DVIterator.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/DVIterator.java\nnew file mode 100644\nindex 000000000000..7b08b86cbfd0\n--- /dev/null\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/DVIterator.java\n@@ -0,0 +1,101 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.spark.source;\n+\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.data.BaseDeleteLoader;\n+import org.apache.iceberg.io.CloseableIterator;\n+import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.types.Types;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+class DVIterator implements CloseableIterator<InternalRow> {\n+  private final DeleteFile deleteFile;\n+  private final Schema projection;\n+  private final Map<Integer, ?> idToConstant;\n+  private final Iterator<Long> positions;\n+  private Integer deletedPositionIndex;\n+  private GenericInternalRow row;\n+\n+  DVIterator(\n+      InputFile inputFile, DeleteFile deleteFile, Schema projection, Map<Integer, ?> idToConstant) {\n+    this.deleteFile = deleteFile;\n+    this.projection = projection;\n+    this.idToConstant = idToConstant;\n+    List<Long> pos = Lists.newArrayList();\n+    new BaseDeleteLoader(ignored -> inputFile)\n+        .loadPositionDeletes(ImmutableList.of(deleteFile), deleteFile.referencedDataFile())\n+        .forEach(pos::add);\n+    this.positions = pos.iterator();\n+  }\n+\n+  @Override\n+  public boolean hasNext() {\n+    return positions.hasNext();\n+  }\n+\n+  @Override\n+  public InternalRow next() {\n+    long position = positions.next();\n+\n+    if (null == row) {\n+      List<Object> rowValues = Lists.newArrayList();\n+      for (Types.NestedField column : projection.columns()) {\n+        int fieldId = column.fieldId();\n+        if (fieldId == MetadataColumns.DELETE_FILE_PATH.fieldId()) {\n+          rowValues.add(UTF8String.fromString(deleteFile.referencedDataFile()));\n+        } else if (fieldId == MetadataColumns.DELETE_FILE_POS.fieldId()) {\n+          rowValues.add(position);\n+          // remember the index where the deleted position needs to be set\n+          deletedPositionIndex = rowValues.size() - 1;\n+        } else if (fieldId == MetadataColumns.PARTITION_COLUMN_ID) {\n+          rowValues.add(idToConstant.get(MetadataColumns.PARTITION_COLUMN_ID));\n+        } else if (fieldId == MetadataColumns.SPEC_ID_COLUMN_ID) {\n+          rowValues.add(idToConstant.get(MetadataColumns.SPEC_ID_COLUMN_ID));\n+        } else if (fieldId == MetadataColumns.FILE_PATH_COLUMN_ID) {\n+          rowValues.add(idToConstant.get(MetadataColumns.FILE_PATH_COLUMN_ID));\n+        }\n+      }\n+\n+      this.row = new GenericInternalRow(rowValues.toArray());\n+    } else if (null != deletedPositionIndex) {\n+      // only update the deleted position if necessary, everything else stays the same\n+      row.update(deletedPositionIndex, position);\n+    }\n+\n+    return row;\n+  }\n+\n+  @Override\n+  public void remove() {\n+    throw new UnsupportedOperationException(\"Remove is not supported\");\n+  }\n+\n+  @Override\n+  public void close() {}\n+}\n\ndiff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/PositionDeletesRowReader.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/PositionDeletesRowReader.java\nindex 1a894df29166..329bcf085569 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/PositionDeletesRowReader.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/source/PositionDeletesRowReader.java\n@@ -33,6 +33,7 @@\n import org.apache.iceberg.io.InputFile;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.relocated.com.google.common.primitives.Ints;\n+import org.apache.iceberg.util.ContentFileUtil;\n import org.apache.iceberg.util.SnapshotUtil;\n import org.apache.spark.rdd.InputFileBlockHolder;\n import org.apache.spark.sql.catalyst.InternalRow;\n@@ -90,6 +91,10 @@ protected CloseableIterator<InternalRow> open(PositionDeletesScanTask task) {\n         ExpressionUtil.extractByIdInclusive(\n             task.residual(), expectedSchema(), caseSensitive(), Ints.toArray(nonConstantFieldIds));\n \n+    if (ContentFileUtil.isDV(task.file())) {\n+      return new DVIterator(inputFile, task.file(), expectedSchema(), idToConstant);\n+    }\n+\n     return newIterable(\n             inputFile,\n             task.file().format(),\n",
    "test_patch": "diff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPositionDeletesReader.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPositionDeletesReader.java\nnew file mode 100644\nindex 000000000000..5b876dfc57ce\n--- /dev/null\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestPositionDeletesReader.java\n@@ -0,0 +1,301 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.spark.source;\n+\n+import static org.apache.iceberg.types.Types.NestedField.optional;\n+import static org.apache.iceberg.types.Types.NestedField.required;\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.file.Path;\n+import java.util.List;\n+import java.util.stream.Collectors;\n+import org.apache.iceberg.BaseScanTaskGroup;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.Files;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.PositionDeletesScanTask;\n+import org.apache.iceberg.PositionDeletesTable;\n+import org.apache.iceberg.ScanTask;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.TestHelpers;\n+import org.apache.iceberg.catalog.TableIdentifier;\n+import org.apache.iceberg.data.FileHelpers;\n+import org.apache.iceberg.data.GenericRecord;\n+import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.spark.TestBase;\n+import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.CharSequenceSet;\n+import org.apache.iceberg.util.Pair;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import org.junit.jupiter.api.AfterEach;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+import org.junit.jupiter.api.io.TempDir;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public class TestPositionDeletesReader extends TestBase {\n+  private static final Schema SCHEMA =\n+      new Schema(\n+          required(1, \"id\", Types.IntegerType.get()), optional(2, \"data\", Types.StringType.get()));\n+  private static final PartitionSpec SPEC =\n+      PartitionSpec.builderFor(SCHEMA).bucket(\"data\", 16).build();\n+\n+  private Table table;\n+  private DataFile dataFile1;\n+  private DataFile dataFile2;\n+\n+  @TempDir private Path temp;\n+\n+  @Parameter(index = 0)\n+  private int formatVersion;\n+\n+  @Parameters(name = \"formatVersion = {0}\")\n+  protected static List<Object> parameters() {\n+    return ImmutableList.of(2, 3);\n+  }\n+\n+  @BeforeEach\n+  public void before() throws IOException {\n+    table =\n+        catalog.createTable(\n+            TableIdentifier.of(\"default\", \"test\"),\n+            SCHEMA,\n+            SPEC,\n+            ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion)));\n+\n+    GenericRecord record = GenericRecord.create(table.schema());\n+    List<Record> records1 = Lists.newArrayList();\n+    records1.add(record.copy(\"id\", 29, \"data\", \"a\"));\n+    records1.add(record.copy(\"id\", 43, \"data\", \"b\"));\n+    records1.add(record.copy(\"id\", 61, \"data\", \"c\"));\n+    records1.add(record.copy(\"id\", 89, \"data\", \"d\"));\n+\n+    List<Record> records2 = Lists.newArrayList();\n+    records2.add(record.copy(\"id\", 100, \"data\", \"e\"));\n+    records2.add(record.copy(\"id\", 121, \"data\", \"f\"));\n+    records2.add(record.copy(\"id\", 122, \"data\", \"g\"));\n+\n+    dataFile1 = writeDataFile(records1);\n+    dataFile2 = writeDataFile(records2);\n+    table.newAppend().appendFile(dataFile1).appendFile(dataFile2).commit();\n+  }\n+\n+  @AfterEach\n+  public void after() {\n+    catalog.dropTable(TableIdentifier.of(\"default\", \"test\"));\n+  }\n+\n+  @TestTemplate\n+  public void readPositionDeletesTableWithNoDeleteFiles() {\n+    Table positionDeletesTable =\n+        catalog.loadTable(TableIdentifier.of(\"default\", \"test\", \"position_deletes\"));\n+\n+    assertThat(positionDeletesTable.newBatchScan().planFiles()).isEmpty();\n+  }\n+\n+  @TestTemplate\n+  public void readPositionDeletesTableWithMultipleDeleteFiles() throws IOException {\n+    Pair<DeleteFile, CharSequenceSet> posDeletes1 =\n+        FileHelpers.writeDeleteFile(\n+            table,\n+            Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n+            TestHelpers.Row.of(0),\n+            Lists.newArrayList(\n+                Pair.of(dataFile1.location(), 0L), Pair.of(dataFile1.location(), 1L)),\n+            formatVersion);\n+\n+    Pair<DeleteFile, CharSequenceSet> posDeletes2 =\n+        FileHelpers.writeDeleteFile(\n+            table,\n+            Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n+            TestHelpers.Row.of(0),\n+            Lists.newArrayList(\n+                Pair.of(dataFile2.location(), 2L), Pair.of(dataFile2.location(), 3L)),\n+            formatVersion);\n+\n+    DeleteFile deleteFile1 = posDeletes1.first();\n+    DeleteFile deleteFile2 = posDeletes2.first();\n+    table\n+        .newRowDelta()\n+        .addDeletes(deleteFile1)\n+        .addDeletes(deleteFile2)\n+        .validateDataFilesExist(posDeletes1.second())\n+        .validateDataFilesExist(posDeletes2.second())\n+        .commit();\n+\n+    Table positionDeletesTable =\n+        catalog.loadTable(TableIdentifier.of(\"default\", \"test\", \"position_deletes\"));\n+\n+    Schema projectedSchema =\n+        positionDeletesTable\n+            .schema()\n+            .select(\n+                MetadataColumns.DELETE_FILE_PATH.name(),\n+                MetadataColumns.DELETE_FILE_POS.name(),\n+                PositionDeletesTable.DELETE_FILE_PATH);\n+\n+    List<ScanTask> scanTasks =\n+        Lists.newArrayList(\n+            positionDeletesTable.newBatchScan().project(projectedSchema).planFiles());\n+    assertThat(scanTasks).hasSize(2);\n+\n+    assertThat(scanTasks.get(0)).isInstanceOf(PositionDeletesScanTask.class);\n+    PositionDeletesScanTask scanTask1 = (PositionDeletesScanTask) scanTasks.get(0);\n+\n+    try (PositionDeletesRowReader reader =\n+        new PositionDeletesRowReader(\n+            table,\n+            new BaseScanTaskGroup<>(null, ImmutableList.of(scanTask1)),\n+            positionDeletesTable.schema(),\n+            projectedSchema,\n+            false)) {\n+      List<InternalRow> actualRows = Lists.newArrayList();\n+      while (reader.next()) {\n+        actualRows.add(reader.get().copy());\n+      }\n+\n+      String dataFileLocation =\n+          formatVersion >= 3 ? deleteFile1.referencedDataFile() : dataFile1.location();\n+      Object[] first = {\n+        UTF8String.fromString(dataFileLocation), 0L, UTF8String.fromString(deleteFile1.location())\n+      };\n+      Object[] second = {\n+        UTF8String.fromString(dataFileLocation), 1L, UTF8String.fromString(deleteFile1.location())\n+      };\n+      assertThat(internalRowsToJava(actualRows, projectedSchema))\n+          .hasSize(2)\n+          .containsExactly(first, second);\n+    }\n+\n+    assertThat(scanTasks.get(1)).isInstanceOf(PositionDeletesScanTask.class);\n+    PositionDeletesScanTask scanTask2 = (PositionDeletesScanTask) scanTasks.get(1);\n+    try (PositionDeletesRowReader reader =\n+        new PositionDeletesRowReader(\n+            table,\n+            new BaseScanTaskGroup<>(null, ImmutableList.of(scanTask2)),\n+            positionDeletesTable.schema(),\n+            projectedSchema,\n+            false)) {\n+      List<InternalRow> actualRows = Lists.newArrayList();\n+      while (reader.next()) {\n+        actualRows.add(reader.get().copy());\n+      }\n+\n+      String dataFileLocation =\n+          formatVersion >= 3 ? deleteFile2.referencedDataFile() : dataFile2.location();\n+      Object[] first = {\n+        UTF8String.fromString(dataFileLocation), 2L, UTF8String.fromString(deleteFile2.location())\n+      };\n+      Object[] second = {\n+        UTF8String.fromString(dataFileLocation), 3L, UTF8String.fromString(deleteFile2.location())\n+      };\n+      assertThat(internalRowsToJava(actualRows, projectedSchema))\n+          .hasSize(2)\n+          .containsExactly(first, second);\n+    }\n+  }\n+\n+  @TestTemplate\n+  public void readPositionDeletesTableWithDifferentColumnOrdering() throws IOException {\n+    Pair<DeleteFile, CharSequenceSet> posDeletes1 =\n+        FileHelpers.writeDeleteFile(\n+            table,\n+            Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n+            TestHelpers.Row.of(0),\n+            Lists.newArrayList(\n+                Pair.of(dataFile1.location(), 0L), Pair.of(dataFile1.location(), 1L)),\n+            formatVersion);\n+\n+    DeleteFile deleteFile1 = posDeletes1.first();\n+    table\n+        .newRowDelta()\n+        .addDeletes(deleteFile1)\n+        .validateDataFilesExist(posDeletes1.second())\n+        .commit();\n+\n+    Table positionDeletesTable =\n+        catalog.loadTable(TableIdentifier.of(\"default\", \"test\", \"position_deletes\"));\n+\n+    // select a few fields in backwards order\n+    Schema projectedSchema =\n+        new Schema(MetadataColumns.DELETE_FILE_POS, MetadataColumns.DELETE_FILE_PATH);\n+\n+    List<ScanTask> scanTasks =\n+        Lists.newArrayList(\n+            positionDeletesTable.newBatchScan().project(projectedSchema).planFiles());\n+    assertThat(scanTasks).hasSize(1);\n+\n+    assertThat(scanTasks.get(0)).isInstanceOf(PositionDeletesScanTask.class);\n+    PositionDeletesScanTask scanTask1 = (PositionDeletesScanTask) scanTasks.get(0);\n+\n+    try (PositionDeletesRowReader reader =\n+        new PositionDeletesRowReader(\n+            table,\n+            new BaseScanTaskGroup<>(null, ImmutableList.of(scanTask1)),\n+            positionDeletesTable.schema(),\n+            projectedSchema,\n+            false)) {\n+      List<InternalRow> actualRows = Lists.newArrayList();\n+      while (reader.next()) {\n+        actualRows.add(reader.get().copy());\n+      }\n+\n+      assertThat(internalRowsToJava(actualRows, projectedSchema))\n+          .hasSize(2)\n+          .containsExactly(\n+              new Object[] {0L, UTF8String.fromString(dataFile1.location())},\n+              new Object[] {1L, UTF8String.fromString(dataFile1.location())});\n+    }\n+  }\n+\n+  private DataFile writeDataFile(List<Record> records) throws IOException {\n+    return FileHelpers.writeDataFile(\n+        table,\n+        Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n+        TestHelpers.Row.of(0),\n+        records);\n+  }\n+\n+  private List<Object[]> internalRowsToJava(List<InternalRow> rows, Schema projection) {\n+    return rows.stream().map(row -> toJava(row, projection)).collect(Collectors.toList());\n+  }\n+\n+  private Object[] toJava(InternalRow row, Schema projection) {\n+    Object[] values = new Object[row.numFields()];\n+    for (int i = 0; i < projection.columns().size(); i++) {\n+      values[i] = row.get(i, SparkSchemaUtil.convert(projection.columns().get(i).type()));\n+    }\n+    return values;\n+  }\n+}\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11576",
    "pr_id": 11576,
    "issue_id": 11575,
    "repo": "apache/iceberg",
    "problem_statement": "Incorrect Deletion of Snapshot Metadata Due to OutOfMemoryError\n### Apache Iceberg version\n\n1.1.0\n\n### Query engine\n\nFlink\n\n### Please describe the bug üêû\n\nwhen calling checkCommitStatus method, unexpected errors maybe occur,  such as OutOfMemoryError, during the checkCommitStatus method execution.  the code show as below:\r\n```\r\ncommitStatus =\r\n   checkCommitStatus(\r\n       viewName,\r\n       newMetadataLocation,\r\n       metadata.properties(),\r\n       () -> checkCurrentMetadataLocation(newMetadataLocation));\r\n```\r\nDuring the execution of the `org.apache.iceberg.hive.HiveViewOperations.checkCurrentMetadataLocation` method's refresh operation to download and update the current table metadata, memory consumption occurs, potentially leading to an `OutOfMemoryError`. \r\nIt is important to note that `Tasks` may not handle `Error` exceptions, instead throwing them directly. \r\nFinally, in the `finally` block, the cleanupMetadataAndUnlock function may delete the table snapshot metadata file, even if it has been recently committed.\n\n### Willingness to contribute\n\n- [X] I can contribute a fix for this bug independently\n- [X] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 161,
    "test_files_count": 2,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java",
      "hive-metastore/src/main/java/org/apache/iceberg/hive/HiveViewOperations.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveViewCommits.java"
    ],
    "pr_changed_test_files": [
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java",
      "hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveViewCommits.java"
    ],
    "base_commit": "b38951db6a7061a595605229c21c1a1912a3a4c1",
    "head_commit": "03bbae1af50fc8b84aeb95ea236a455b39fffc61",
    "repo_url": "https://github.com/apache/iceberg/pull/11576",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11576",
    "dockerfile": "",
    "pr_merged_at": "2024-11-20T21:26:20.000Z",
    "patch": "diff --git a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java\nindex 518daaf6acd1..619f20ab87a3 100644\n--- a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java\n+++ b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveTableOperations.java\n@@ -292,6 +292,7 @@ protected void doCommit(TableMetadata base, TableMetadata metadata) {\n             database,\n             tableName,\n             e);\n+        commitStatus = BaseMetastoreOperations.CommitStatus.UNKNOWN;\n         commitStatus =\n             BaseMetastoreOperations.CommitStatus.valueOf(\n                 checkCommitStatus(newMetadataLocation, metadata).name());\n\ndiff --git a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveViewOperations.java b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveViewOperations.java\nindex 4fc71299d457..dd0d5b3132d0 100644\n--- a/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveViewOperations.java\n+++ b/hive-metastore/src/main/java/org/apache/iceberg/hive/HiveViewOperations.java\n@@ -33,6 +33,7 @@\n import org.apache.hadoop.hive.metastore.api.InvalidObjectException;\n import org.apache.hadoop.hive.metastore.api.NoSuchObjectException;\n import org.apache.hadoop.hive.metastore.api.Table;\n+import org.apache.iceberg.BaseMetastoreOperations;\n import org.apache.iceberg.BaseMetastoreTableOperations;\n import org.apache.iceberg.CatalogUtil;\n import org.apache.iceberg.ClientPool;\n@@ -226,6 +227,7 @@ public void doCommit(ViewMetadata base, ViewMetadata metadata) {\n             database,\n             viewName,\n             e);\n+        commitStatus = BaseMetastoreOperations.CommitStatus.UNKNOWN;\n         commitStatus =\n             checkCommitStatus(\n                 viewName,\n",
    "test_patch": "diff --git a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java\nindex 136c96934189..754ed55e81e8 100644\n--- a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java\n+++ b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveCommits.java\n@@ -21,6 +21,7 @@\n import static org.apache.iceberg.TableProperties.HIVE_LOCK_ENABLED;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.mockito.ArgumentMatchers.anyString;\n import static org.mockito.Mockito.any;\n import static org.mockito.Mockito.anyBoolean;\n import static org.mockito.Mockito.doAnswer;\n@@ -44,6 +45,7 @@\n import org.apache.iceberg.types.Types;\n import org.apache.thrift.TException;\n import org.junit.jupiter.api.Test;\n+import org.junit.platform.commons.support.ReflectionSupport;\n \n public class TestHiveCommits extends HiveTableBaseTest {\n \n@@ -399,6 +401,59 @@ public void testLockExceptionUnknownSuccessCommit() throws TException, Interrupt\n         .isTrue();\n   }\n \n+  @Test\n+  public void testSuccessCommitWhenCheckCommitStatusOOM() throws TException, InterruptedException {\n+    Table table = catalog.loadTable(TABLE_IDENTIFIER);\n+    HiveTableOperations ops = (HiveTableOperations) ((HasTableOperations) table).operations();\n+\n+    TableMetadata metadataV1 = ops.current();\n+\n+    table.updateSchema().addColumn(\"n\", Types.IntegerType.get()).commit();\n+\n+    ops.refresh();\n+\n+    TableMetadata metadataV2 = ops.current();\n+\n+    assertThat(ops.current().schema().columns()).hasSize(2);\n+\n+    HiveTableOperations spyOps = spy(ops);\n+\n+    // Simulate a communication error after a successful commit\n+    doAnswer(\n+            i -> {\n+              org.apache.hadoop.hive.metastore.api.Table tbl =\n+                  i.getArgument(0, org.apache.hadoop.hive.metastore.api.Table.class);\n+              String location = i.getArgument(2, String.class);\n+              ops.persistTable(tbl, true, location);\n+              throw new UnknownError();\n+            })\n+        .when(spyOps)\n+        .persistTable(any(), anyBoolean(), any());\n+    try {\n+      ReflectionSupport.invokeMethod(\n+          ops.getClass()\n+              .getSuperclass()\n+              .getDeclaredMethod(\"checkCommitStatus\", String.class, TableMetadata.class),\n+          doThrow(new OutOfMemoryError()).when(spyOps),\n+          anyString(),\n+          any());\n+    } catch (Exception e) {\n+      throw new RuntimeException(e);\n+    }\n+\n+    assertThatThrownBy(() -> spyOps.commit(metadataV2, metadataV1))\n+        .isInstanceOf(OutOfMemoryError.class);\n+\n+    ops.refresh();\n+\n+    assertThat(ops.current().location())\n+        .as(\"Current metadata should have changed to metadata V1\")\n+        .isEqualTo(metadataV1.location());\n+    assertThat(metadataFileExists(ops.current()))\n+        .as(\"Current metadata file should still exist\")\n+        .isTrue();\n+  }\n+\n   @Test\n   public void testCommitExceptionWithoutMessage() throws TException, InterruptedException {\n     Table table = catalog.loadTable(TABLE_IDENTIFIER);\n\ndiff --git a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveViewCommits.java b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveViewCommits.java\nindex 47abb51602fa..ae251aacebca 100644\n--- a/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveViewCommits.java\n+++ b/hive-metastore/src/test/java/org/apache/iceberg/hive/TestHiveViewCommits.java\n@@ -21,6 +21,7 @@\n import static org.apache.iceberg.types.Types.NestedField.required;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.mockito.ArgumentMatchers.anyString;\n import static org.mockito.Mockito.any;\n import static org.mockito.Mockito.anyBoolean;\n import static org.mockito.Mockito.doAnswer;\n@@ -31,8 +32,10 @@\n \n import java.io.File;\n import java.io.IOException;\n+import java.util.Map;\n import java.util.concurrent.TimeUnit;\n import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n import org.apache.hadoop.fs.Path;\n import org.apache.iceberg.CatalogProperties;\n import org.apache.iceberg.CatalogUtil;\n@@ -53,6 +56,7 @@\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.extension.RegisterExtension;\n+import org.junit.platform.commons.support.ReflectionSupport;\n \n /** Test Hive locks and Hive errors and retry during commits. */\n public class TestHiveViewCommits {\n@@ -434,6 +438,60 @@ public void testLockExceptionUnknownSuccessCommit() throws TException, Interrupt\n     assertThat(metadataFileCount(metadataV2)).as(\"New metadata file should exist\").isEqualTo(2);\n   }\n \n+  @Test\n+  public void testSuccessCommitWhenCheckCommitStatusOOM() throws TException, InterruptedException {\n+    HiveViewOperations ops = (HiveViewOperations) ((BaseView) view).operations();\n+    ViewMetadata metadataV1 = ops.current();\n+    assertThat(metadataV1.properties()).hasSize(0);\n+\n+    view.updateProperties().set(\"k1\", \"v1\").commit();\n+    ops.refresh();\n+    ViewMetadata metadataV2 = ops.current();\n+    assertThat(metadataV2.properties()).hasSize(1).containsEntry(\"k1\", \"v1\");\n+\n+    HiveViewOperations spyOps = spy(ops);\n+\n+    // Simulate a communication error after a successful commit\n+    doAnswer(\n+            i -> {\n+              org.apache.hadoop.hive.metastore.api.Table tbl =\n+                  i.getArgument(0, org.apache.hadoop.hive.metastore.api.Table.class);\n+              String location = i.getArgument(2, String.class);\n+              ops.persistTable(tbl, true, location);\n+              throw new UnknownError();\n+            })\n+        .when(spyOps)\n+        .persistTable(any(), anyBoolean(), any());\n+    try {\n+      ReflectionSupport.invokeMethod(\n+          ops.getClass()\n+              .getSuperclass()\n+              .getSuperclass()\n+              .getDeclaredMethod(\n+                  \"checkCommitStatus\", String.class, String.class, Map.class, Supplier.class),\n+          doThrow(new OutOfMemoryError()).when(spyOps),\n+          anyString(),\n+          anyString(),\n+          any(),\n+          any());\n+    } catch (Exception e) {\n+      throw new RuntimeException(e);\n+    }\n+\n+    assertThatThrownBy(() -> spyOps.commit(metadataV2, metadataV1))\n+        .isInstanceOf(OutOfMemoryError.class);\n+\n+    ops.refresh();\n+\n+    assertThat(metadataV2.location())\n+        .as(\"Current metadata should have changed to metadata V1\")\n+        .isEqualTo(metadataV1.location());\n+    assertThat(metadataFileExists(metadataV2))\n+        .as(\"Current metadata file should still exist\")\n+        .isTrue();\n+    assertThat(metadataFileCount(metadataV2)).as(\"New metadata file should exist\").isEqualTo(2);\n+  }\n+\n   @Test\n   public void testCommitExceptionWithoutMessage() throws TException, InterruptedException {\n     HiveViewOperations ops = (HiveViewOperations) ((BaseView) view).operations();\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11557",
    "pr_id": 11557,
    "issue_id": 7393,
    "repo": "apache/iceberg",
    "problem_statement": "The serialization problem caused by Flink shuffling  design\n### Feature Request / Improvement\n\nthis issue from #6303ÔºåI open a new issue to discuss this issue\r\n\r\nI am very interested in the project. At present, we have a serious tilt problem in the process of using iceberg to write. I have been paying attention to the progress of this module. Now I want to put forward some of my ideas.\r\n\r\nI took a close look at https://github.com/apache/iceberg/pull/6382 and https://github.com/apache/iceberg/pull/7269\r\n\r\nI think there are some problems. I completed the following simple implementation based on these two PRs on my own branch, but the throughput of the program has dropped significantly, almost reaching the point of being unusable, so I think, should we Stop and think about whether this solution is suitable\r\n\r\nFrom my observation, the problem lies in the DataStatisticsOperator. When output.collect is called here, Flink‚Äôs serialization will be forced to be triggered, but DataStatisticsOrRecord will degenerate into kryo mode during serialization, resulting in a performance drop of more than 4 times\r\n<img width=\"1433\" alt=\"image\" src=\"https://user-images.githubusercontent.com/40817998/233132909-209f9b69-1197-4088-8572-d30e2bbe7ea4.png\">\r\nSpent too many computing resources in serialization, So I think we may need to seriously consider the feasibility of this Proposal\r\n\r\n@stevenzwu @hililiwei\n\n### Query engine\n\nFlink",
    "issue_word_count": 228,
    "test_files_count": 1,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java",
      "flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/shuffle/StatisticsOrRecordTypeInformation.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/shuffle/TestStatisticsOrRecordTypeInformation.java"
    ],
    "pr_changed_test_files": [
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/shuffle/TestStatisticsOrRecordTypeInformation.java"
    ],
    "base_commit": "ac865e334e143dfd9e33011d8cf710b46d91f1e5",
    "head_commit": "41e8a86d170408d6b1f93dcd5f79728c31bd2174",
    "repo_url": "https://github.com/apache/iceberg/pull/11557",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11557",
    "dockerfile": "",
    "pr_merged_at": "2024-12-18T06:08:15.000Z",
    "patch": "diff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\nindex e862e88c968c..b694dcb0e57d 100644\n--- a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/FlinkSink.java\n@@ -65,6 +65,7 @@\n import org.apache.iceberg.flink.sink.shuffle.DataStatisticsOperatorFactory;\n import org.apache.iceberg.flink.sink.shuffle.RangePartitioner;\n import org.apache.iceberg.flink.sink.shuffle.StatisticsOrRecord;\n+import org.apache.iceberg.flink.sink.shuffle.StatisticsOrRecordTypeInformation;\n import org.apache.iceberg.flink.sink.shuffle.StatisticsType;\n import org.apache.iceberg.flink.util.FlinkCompatibilityUtil;\n import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n@@ -646,12 +647,14 @@ private DataStream<RowData> distributeDataStream(\n           }\n \n           LOG.info(\"Range distribute rows by sort order: {}\", sortOrder);\n+          StatisticsOrRecordTypeInformation statisticsOrRecordTypeInformation =\n+              new StatisticsOrRecordTypeInformation(flinkRowType, iSchema, sortOrder);\n           StatisticsType statisticsType = flinkWriteConf.rangeDistributionStatisticsType();\n           SingleOutputStreamOperator<StatisticsOrRecord> shuffleStream =\n               input\n                   .transform(\n                       operatorName(\"range-shuffle\"),\n-                      TypeInformation.of(StatisticsOrRecord.class),\n+                      statisticsOrRecordTypeInformation,\n                       new DataStatisticsOperatorFactory(\n                           iSchema,\n                           sortOrder,\n\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/shuffle/StatisticsOrRecordTypeInformation.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/shuffle/StatisticsOrRecordTypeInformation.java\nnew file mode 100644\nindex 000000000000..921ede9466e0\n--- /dev/null\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/shuffle/StatisticsOrRecordTypeInformation.java\n@@ -0,0 +1,115 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.flink.sink.shuffle;\n+\n+import java.util.Objects;\n+import org.apache.flink.api.common.ExecutionConfig;\n+import org.apache.flink.api.common.serialization.SerializerConfig;\n+import org.apache.flink.api.common.typeinfo.TypeInformation;\n+import org.apache.flink.api.common.typeutils.TypeSerializer;\n+import org.apache.flink.table.data.RowData;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SortOrder;\n+import org.apache.iceberg.flink.util.FlinkCompatibilityUtil;\n+\n+public class StatisticsOrRecordTypeInformation extends TypeInformation<StatisticsOrRecord> {\n+\n+  private final TypeInformation<RowData> rowTypeInformation;\n+  private final SortOrder sortOrder;\n+  private final GlobalStatisticsSerializer globalStatisticsSerializer;\n+\n+  public StatisticsOrRecordTypeInformation(\n+      RowType flinkRowType, Schema schema, SortOrder sortOrder) {\n+    this.sortOrder = sortOrder;\n+    this.rowTypeInformation = FlinkCompatibilityUtil.toTypeInfo(flinkRowType);\n+    this.globalStatisticsSerializer =\n+        new GlobalStatisticsSerializer(new SortKeySerializer(schema, sortOrder));\n+  }\n+\n+  @Override\n+  public boolean isBasicType() {\n+    return false;\n+  }\n+\n+  @Override\n+  public boolean isTupleType() {\n+    return false;\n+  }\n+\n+  @Override\n+  public int getArity() {\n+    return 1;\n+  }\n+\n+  @Override\n+  public int getTotalFields() {\n+    return 1;\n+  }\n+\n+  @Override\n+  public Class<StatisticsOrRecord> getTypeClass() {\n+    return StatisticsOrRecord.class;\n+  }\n+\n+  @Override\n+  public boolean isKeyType() {\n+    return false;\n+  }\n+\n+  @Override\n+  public TypeSerializer<StatisticsOrRecord> createSerializer(SerializerConfig config) {\n+    TypeSerializer<RowData> recordSerializer = rowTypeInformation.createSerializer(config);\n+    return new StatisticsOrRecordSerializer(globalStatisticsSerializer, recordSerializer);\n+  }\n+\n+  @Override\n+  public TypeSerializer<StatisticsOrRecord> createSerializer(ExecutionConfig config) {\n+    return createSerializer(config.getSerializerConfig());\n+  }\n+\n+  @Override\n+  public String toString() {\n+    return \"StatisticsOrRecord\";\n+  }\n+\n+  @Override\n+  public boolean equals(Object o) {\n+    if (this == o) {\n+      return true;\n+    } else if (o != null && this.getClass() == o.getClass()) {\n+      StatisticsOrRecordTypeInformation that = (StatisticsOrRecordTypeInformation) o;\n+      return that.sortOrder.equals(sortOrder)\n+          && that.rowTypeInformation.equals(rowTypeInformation)\n+          && that.globalStatisticsSerializer.equals(globalStatisticsSerializer);\n+    } else {\n+      return false;\n+    }\n+  }\n+\n+  @Override\n+  public int hashCode() {\n+    return Objects.hash(rowTypeInformation, sortOrder, globalStatisticsSerializer);\n+  }\n+\n+  @Override\n+  public boolean canEqual(Object obj) {\n+    return obj instanceof StatisticsOrRecordTypeInformation;\n+  }\n+}\n",
    "test_patch": "diff --git a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/shuffle/TestStatisticsOrRecordTypeInformation.java b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/shuffle/TestStatisticsOrRecordTypeInformation.java\nnew file mode 100644\nindex 000000000000..f54198522e99\n--- /dev/null\n+++ b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/shuffle/TestStatisticsOrRecordTypeInformation.java\n@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.flink.sink.shuffle;\n+\n+import org.apache.flink.api.common.typeutils.TypeInformationTestBase;\n+import org.apache.flink.table.types.logical.RowType;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.SortOrder;\n+import org.apache.iceberg.flink.FlinkSchemaUtil;\n+import org.apache.iceberg.types.Types;\n+\n+public class TestStatisticsOrRecordTypeInformation\n+    extends TypeInformationTestBase<StatisticsOrRecordTypeInformation> {\n+  private static final Schema SCHEMA =\n+      new Schema(\n+          Types.NestedField.optional(1, \"ts\", Types.TimestampType.withoutZone()),\n+          Types.NestedField.optional(2, \"uuid\", Types.UUIDType.get()),\n+          Types.NestedField.optional(3, \"data\", Types.StringType.get()));\n+  private static final RowType ROW_TYPE = FlinkSchemaUtil.convert(SCHEMA);\n+  private static final SortOrder SORT_ORDER1 = SortOrder.builderFor(SCHEMA).asc(\"ts\").build();\n+  private static final SortOrder SORT_ORDER2 = SortOrder.builderFor(SCHEMA).asc(\"data\").build();\n+\n+  @Override\n+  protected StatisticsOrRecordTypeInformation[] getTestData() {\n+    return new StatisticsOrRecordTypeInformation[] {\n+      new StatisticsOrRecordTypeInformation(ROW_TYPE, SCHEMA, SORT_ORDER1),\n+      new StatisticsOrRecordTypeInformation(ROW_TYPE, SCHEMA, SORT_ORDER2),\n+    };\n+  }\n+}\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11538",
    "pr_id": 11538,
    "issue_id": 11122,
    "repo": "apache/iceberg",
    "problem_statement": "Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 118,
    "test_files_count": 10,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "core/src/test/java/org/apache/iceberg/TestMetadataTableFilters.java",
      "data/src/test/java/org/apache/iceberg/data/DeleteReadTests.java",
      "data/src/test/java/org/apache/iceberg/data/TestGenericReaderDeletes.java",
      "flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java",
      "flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java",
      "mr/src/test/java/org/apache/iceberg/mr/TestInputFormatReaderDeletes.java",
      "spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/TestMetadataTableFilters.java",
      "data/src/test/java/org/apache/iceberg/data/DeleteReadTests.java",
      "data/src/test/java/org/apache/iceberg/data/TestGenericReaderDeletes.java",
      "flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java",
      "flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java",
      "mr/src/test/java/org/apache/iceberg/mr/TestInputFormatReaderDeletes.java",
      "spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java"
    ],
    "base_commit": "3659ded18d50206576985339bd55cd82f5e200cc",
    "head_commit": "bccbc14f56c23a981a1eecfcbd75c80f21cde5db",
    "repo_url": "https://github.com/apache/iceberg/pull/11538",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11538",
    "dockerfile": "",
    "pr_merged_at": "2024-11-17T16:20:20.000Z",
    "patch": "",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/TestMetadataTableFilters.java b/core/src/test/java/org/apache/iceberg/TestMetadataTableFilters.java\nindex 7c5a860db15f..0762d3b2dca4 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetadataTableFilters.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetadataTableFilters.java\n@@ -396,8 +396,8 @@ public void testPartitionSpecEvolutionRemovalV2() {\n             .withPartitionPath(\"id=11\")\n             .build();\n \n-    DeleteFile delete10 = posDelete(table, data10);\n-    DeleteFile delete11 = posDelete(table, data11);\n+    DeleteFile delete10 = newDeletes(data10);\n+    DeleteFile delete11 = newDeletes(data11);\n \n     table.newFastAppend().appendFile(data10).commit();\n     table.newFastAppend().appendFile(data11).commit();\n@@ -441,12 +441,6 @@ public void testPartitionSpecEvolutionRemovalV2() {\n     assertThat(tasks).hasSize(expectedScanTaskCount(3));\n   }\n \n-  private DeleteFile posDelete(Table table, DataFile dataFile) {\n-    return formatVersion >= 3\n-        ? FileGenerationUtil.generateDV(table, dataFile)\n-        : FileGenerationUtil.generatePositionDeleteFile(table, dataFile);\n-  }\n-\n   @TestTemplate\n   public void testPartitionSpecEvolutionAdditiveV1() {\n     assumeThat(formatVersion).isEqualTo(1);\n@@ -537,8 +531,8 @@ public void testPartitionSpecEvolutionAdditiveV2AndAbove() {\n             .withPartitionPath(\"data_bucket=1/id=11\")\n             .build();\n \n-    DeleteFile delete10 = posDelete(table, data10);\n-    DeleteFile delete11 = posDelete(table, data11);\n+    DeleteFile delete10 = newDeletes(data10);\n+    DeleteFile delete11 = newDeletes(data11);\n \n     table.newFastAppend().appendFile(data10).commit();\n     table.newFastAppend().appendFile(data11).commit();\n\ndiff --git a/data/src/test/java/org/apache/iceberg/data/DeleteReadTests.java b/data/src/test/java/org/apache/iceberg/data/DeleteReadTests.java\nindex 9d16da124062..ada9e27a2fbe 100644\n--- a/data/src/test/java/org/apache/iceberg/data/DeleteReadTests.java\n+++ b/data/src/test/java/org/apache/iceberg/data/DeleteReadTests.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg.data;\n \n import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.File;\n import java.io.IOException;\n@@ -82,12 +83,16 @@ public abstract class DeleteReadTests {\n \n   @Parameter protected FileFormat format;\n \n+  @Parameter(index = 1)\n+  protected int formatVersion;\n+\n   @Parameters(name = \"fileFormat = {0}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      new Object[] {FileFormat.PARQUET},\n-      new Object[] {FileFormat.AVRO},\n-      new Object[] {FileFormat.ORC}\n+      new Object[] {FileFormat.PARQUET, 2},\n+      new Object[] {FileFormat.AVRO, 2},\n+      new Object[] {FileFormat.ORC, 2},\n+      new Object[] {FileFormat.PARQUET, 3},\n     };\n   }\n \n@@ -384,7 +389,8 @@ public void testPositionDeletes() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -401,6 +407,10 @@ public void testPositionDeletes() throws IOException {\n \n   @TestTemplate\n   public void testMultiplePosDeleteFiles() throws IOException {\n+    assumeThat(formatVersion)\n+        .as(\"Can't write multiple delete files with formatVersion >= 3\")\n+        .isEqualTo(2);\n+\n     List<Pair<CharSequence, Long>> deletes =\n         Lists.newArrayList(\n             Pair.of(dataFile.path(), 0L), // id = 29\n@@ -412,7 +422,8 @@ public void testMultiplePosDeleteFiles() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -420,17 +431,15 @@ public void testMultiplePosDeleteFiles() throws IOException {\n         .validateDataFilesExist(posDeletes.second())\n         .commit();\n \n-    deletes =\n-        Lists.newArrayList(\n-            Pair.of(dataFile.path(), 6L) // id = 122\n-            );\n+    deletes = Lists.newArrayList(Pair.of(dataFile.path(), 6L)); // id = 122\n \n     posDeletes =\n         FileHelpers.writeDeleteFile(\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -475,7 +484,8 @@ public void testMixedPositionAndEqualityDeletes() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n\ndiff --git a/data/src/test/java/org/apache/iceberg/data/TestGenericReaderDeletes.java b/data/src/test/java/org/apache/iceberg/data/TestGenericReaderDeletes.java\nindex d7c70919015d..b15f5b70720b 100644\n--- a/data/src/test/java/org/apache/iceberg/data/TestGenericReaderDeletes.java\n+++ b/data/src/test/java/org/apache/iceberg/data/TestGenericReaderDeletes.java\n@@ -37,7 +37,7 @@ public class TestGenericReaderDeletes extends DeleteReadTests {\n \n   @Override\n   protected Table createTable(String name, Schema schema, PartitionSpec spec) throws IOException {\n-    return TestTables.create(tableDir, name, schema, spec, 2);\n+    return TestTables.create(tableDir, name, schema, spec, formatVersion);\n   }\n \n   @Override\n\ndiff --git a/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java b/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java\nindex 0b5a8011ad3f..188a44d7cdba 100644\n--- a/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java\n+++ b/flink/v1.18/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java\n@@ -73,7 +73,7 @@ protected Table createTable(String name, Schema schema, PartitionSpec spec) {\n     Table table = catalog.createTable(TableIdentifier.of(databaseName, name), schema, spec, props);\n     TableOperations ops = ((BaseTable) table).operations();\n     TableMetadata meta = ops.current();\n-    ops.commit(meta, meta.upgradeToFormatVersion(2));\n+    ops.commit(meta, meta.upgradeToFormatVersion(formatVersion));\n \n     return table;\n   }\n\ndiff --git a/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java b/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java\nindex 0b5a8011ad3f..188a44d7cdba 100644\n--- a/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java\n+++ b/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java\n@@ -73,7 +73,7 @@ protected Table createTable(String name, Schema schema, PartitionSpec spec) {\n     Table table = catalog.createTable(TableIdentifier.of(databaseName, name), schema, spec, props);\n     TableOperations ops = ((BaseTable) table).operations();\n     TableMetadata meta = ops.current();\n-    ops.commit(meta, meta.upgradeToFormatVersion(2));\n+    ops.commit(meta, meta.upgradeToFormatVersion(formatVersion));\n \n     return table;\n   }\n\ndiff --git a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java\nindex 0b5a8011ad3f..188a44d7cdba 100644\n--- a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java\n+++ b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/source/TestFlinkReaderDeletesBase.java\n@@ -73,7 +73,7 @@ protected Table createTable(String name, Schema schema, PartitionSpec spec) {\n     Table table = catalog.createTable(TableIdentifier.of(databaseName, name), schema, spec, props);\n     TableOperations ops = ((BaseTable) table).operations();\n     TableMetadata meta = ops.current();\n-    ops.commit(meta, meta.upgradeToFormatVersion(2));\n+    ops.commit(meta, meta.upgradeToFormatVersion(formatVersion));\n \n     return table;\n   }\n\ndiff --git a/mr/src/test/java/org/apache/iceberg/mr/TestInputFormatReaderDeletes.java b/mr/src/test/java/org/apache/iceberg/mr/TestInputFormatReaderDeletes.java\nindex 2cb41f11295c..ac3efc26d644 100644\n--- a/mr/src/test/java/org/apache/iceberg/mr/TestInputFormatReaderDeletes.java\n+++ b/mr/src/test/java/org/apache/iceberg/mr/TestInputFormatReaderDeletes.java\n@@ -49,18 +49,20 @@ public class TestInputFormatReaderDeletes extends DeleteReadTests {\n   private final HadoopTables tables = new HadoopTables(conf);\n   private TestHelper helper;\n \n-  @Parameter(index = 1)\n+  @Parameter(index = 2)\n   private String inputFormat;\n \n-  @Parameters(name = \"fileFormat = {0}, inputFormat = {1}\")\n+  @Parameters(name = \"fileFormat = {0}, formatVersion = {1}, inputFormat = {2}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      {FileFormat.PARQUET, \"IcebergInputFormat\"},\n-      {FileFormat.AVRO, \"IcebergInputFormat\"},\n-      {FileFormat.ORC, \"IcebergInputFormat\"},\n-      {FileFormat.PARQUET, \"MapredIcebergInputFormat\"},\n-      {FileFormat.AVRO, \"MapredIcebergInputFormat\"},\n-      {FileFormat.ORC, \"MapredIcebergInputFormat\"},\n+      {FileFormat.PARQUET, 2, \"IcebergInputFormat\"},\n+      {FileFormat.AVRO, 2, \"IcebergInputFormat\"},\n+      {FileFormat.ORC, 2, \"IcebergInputFormat\"},\n+      {FileFormat.PARQUET, 2, \"MapredIcebergInputFormat\"},\n+      {FileFormat.AVRO, 2, \"MapredIcebergInputFormat\"},\n+      {FileFormat.ORC, 2, \"MapredIcebergInputFormat\"},\n+      {FileFormat.PARQUET, 3, \"IcebergInputFormat\"},\n+      {FileFormat.PARQUET, 3, \"MapredIcebergInputFormat\"},\n     };\n   }\n \n@@ -82,7 +84,7 @@ protected Table createTable(String name, Schema schema, PartitionSpec spec) thro\n \n     TableOperations ops = ((BaseTable) table).operations();\n     TableMetadata meta = ops.current();\n-    ops.commit(meta, meta.upgradeToFormatVersion(2));\n+    ops.commit(meta, meta.upgradeToFormatVersion(formatVersion));\n \n     return table;\n   }\n\ndiff --git a/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java\nindex bde87778ad62..6c6bbf54ec16 100644\n--- a/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java\n+++ b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java\n@@ -44,6 +44,7 @@\n import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.PlanningMode;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableMetadata;\n@@ -99,16 +100,21 @@ public class TestSparkReaderDeletes extends DeleteReadTests {\n   protected static SparkSession spark = null;\n   protected static HiveCatalog catalog = null;\n \n-  @Parameter(index = 1)\n+  @Parameter(index = 2)\n   private boolean vectorized;\n \n-  @Parameters(name = \"format = {0}, vectorized = {1}\")\n+  @Parameter(index = 3)\n+  private PlanningMode planningMode;\n+\n+  @Parameters(name = \"fileFormat = {0}, formatVersion = {1}, vectorized = {2}, planningMode = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      new Object[] {FileFormat.PARQUET, false},\n-      new Object[] {FileFormat.PARQUET, true},\n-      new Object[] {FileFormat.ORC, false},\n-      new Object[] {FileFormat.AVRO, false}\n+      new Object[] {FileFormat.PARQUET, 2, false, PlanningMode.DISTRIBUTED},\n+      new Object[] {FileFormat.PARQUET, 2, true, PlanningMode.LOCAL},\n+      new Object[] {FileFormat.ORC, 2, false, PlanningMode.DISTRIBUTED},\n+      new Object[] {FileFormat.AVRO, 2, false, PlanningMode.LOCAL},\n+      new Object[] {FileFormat.PARQUET, 3, false, PlanningMode.DISTRIBUTED},\n+      new Object[] {FileFormat.PARQUET, 3, true, PlanningMode.LOCAL},\n     };\n   }\n \n@@ -162,7 +168,13 @@ protected Table createTable(String name, Schema schema, PartitionSpec spec) {\n     TableOperations ops = ((BaseTable) table).operations();\n     TableMetadata meta = ops.current();\n     ops.commit(meta, meta.upgradeToFormatVersion(2));\n-    table.updateProperties().set(TableProperties.DEFAULT_FILE_FORMAT, format.name()).commit();\n+    table\n+        .updateProperties()\n+        .set(TableProperties.DEFAULT_FILE_FORMAT, format.name())\n+        .set(TableProperties.DATA_PLANNING_MODE, planningMode.modeName())\n+        .set(TableProperties.DELETE_PLANNING_MODE, planningMode.modeName())\n+        .set(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion))\n+        .commit();\n     if (format.equals(FileFormat.PARQUET) || format.equals(FileFormat.ORC)) {\n       String vectorizationEnabled =\n           format.equals(FileFormat.PARQUET)\n@@ -342,7 +354,8 @@ public void testPosDeletesAllRowsInBatch() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             TestHelpers.Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -374,7 +387,8 @@ public void testPosDeletesWithDeletedColumn() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             TestHelpers.Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -450,7 +464,8 @@ public void testMixedPosAndEqDeletesWithDeletedColumn() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             TestHelpers.Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -482,7 +497,8 @@ public void testFilterOnDeletedMetadataColumn() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             TestHelpers.Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -604,7 +620,10 @@ public void testPosDeletesOnParquetFileWithMultipleRowGroups() throws IOExceptio\n             Pair.of(dataFile.path(), 109L));\n     Pair<DeleteFile, CharSequenceSet> posDeletes =\n         FileHelpers.writeDeleteFile(\n-            table, Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())), deletes);\n+            table,\n+            Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n+            deletes,\n+            formatVersion);\n     tbl.newRowDelta()\n         .addDeletes(posDeletes.first())\n         .validateDataFilesExist(posDeletes.second())\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java\nindex 29c2d4b39a1e..6c6bbf54ec16 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java\n@@ -100,19 +100,21 @@ public class TestSparkReaderDeletes extends DeleteReadTests {\n   protected static SparkSession spark = null;\n   protected static HiveCatalog catalog = null;\n \n-  @Parameter(index = 1)\n+  @Parameter(index = 2)\n   private boolean vectorized;\n \n-  @Parameter(index = 2)\n+  @Parameter(index = 3)\n   private PlanningMode planningMode;\n \n-  @Parameters(name = \"format = {0}, vectorized = {1}, planningMode = {2}\")\n+  @Parameters(name = \"fileFormat = {0}, formatVersion = {1}, vectorized = {2}, planningMode = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      new Object[] {FileFormat.PARQUET, false, PlanningMode.DISTRIBUTED},\n-      new Object[] {FileFormat.PARQUET, true, PlanningMode.LOCAL},\n-      new Object[] {FileFormat.ORC, false, PlanningMode.DISTRIBUTED},\n-      new Object[] {FileFormat.AVRO, false, PlanningMode.LOCAL}\n+      new Object[] {FileFormat.PARQUET, 2, false, PlanningMode.DISTRIBUTED},\n+      new Object[] {FileFormat.PARQUET, 2, true, PlanningMode.LOCAL},\n+      new Object[] {FileFormat.ORC, 2, false, PlanningMode.DISTRIBUTED},\n+      new Object[] {FileFormat.AVRO, 2, false, PlanningMode.LOCAL},\n+      new Object[] {FileFormat.PARQUET, 3, false, PlanningMode.DISTRIBUTED},\n+      new Object[] {FileFormat.PARQUET, 3, true, PlanningMode.LOCAL},\n     };\n   }\n \n@@ -171,6 +173,7 @@ protected Table createTable(String name, Schema schema, PartitionSpec spec) {\n         .set(TableProperties.DEFAULT_FILE_FORMAT, format.name())\n         .set(TableProperties.DATA_PLANNING_MODE, planningMode.modeName())\n         .set(TableProperties.DELETE_PLANNING_MODE, planningMode.modeName())\n+        .set(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion))\n         .commit();\n     if (format.equals(FileFormat.PARQUET) || format.equals(FileFormat.ORC)) {\n       String vectorizationEnabled =\n@@ -351,7 +354,8 @@ public void testPosDeletesAllRowsInBatch() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             TestHelpers.Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -383,7 +387,8 @@ public void testPosDeletesWithDeletedColumn() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             TestHelpers.Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -459,7 +464,8 @@ public void testMixedPosAndEqDeletesWithDeletedColumn() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             TestHelpers.Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -491,7 +497,8 @@ public void testFilterOnDeletedMetadataColumn() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             TestHelpers.Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -613,7 +620,10 @@ public void testPosDeletesOnParquetFileWithMultipleRowGroups() throws IOExceptio\n             Pair.of(dataFile.path(), 109L));\n     Pair<DeleteFile, CharSequenceSet> posDeletes =\n         FileHelpers.writeDeleteFile(\n-            table, Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())), deletes);\n+            table,\n+            Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n+            deletes,\n+            formatVersion);\n     tbl.newRowDelta()\n         .addDeletes(posDeletes.first())\n         .validateDataFilesExist(posDeletes.second())\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java\nindex 29c2d4b39a1e..35a615aafcce 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkReaderDeletes.java\n@@ -100,19 +100,21 @@ public class TestSparkReaderDeletes extends DeleteReadTests {\n   protected static SparkSession spark = null;\n   protected static HiveCatalog catalog = null;\n \n-  @Parameter(index = 1)\n+  @Parameter(index = 2)\n   private boolean vectorized;\n \n-  @Parameter(index = 2)\n+  @Parameter(index = 3)\n   private PlanningMode planningMode;\n \n-  @Parameters(name = \"format = {0}, vectorized = {1}, planningMode = {2}\")\n+  @Parameters(name = \"fileFormat = {0}, formatVersion = {1}, vectorized = {2}, planningMode = {3}\")\n   public static Object[][] parameters() {\n     return new Object[][] {\n-      new Object[] {FileFormat.PARQUET, false, PlanningMode.DISTRIBUTED},\n-      new Object[] {FileFormat.PARQUET, true, PlanningMode.LOCAL},\n-      new Object[] {FileFormat.ORC, false, PlanningMode.DISTRIBUTED},\n-      new Object[] {FileFormat.AVRO, false, PlanningMode.LOCAL}\n+      new Object[] {FileFormat.PARQUET, 2, false, PlanningMode.DISTRIBUTED},\n+      new Object[] {FileFormat.PARQUET, 2, true, PlanningMode.LOCAL},\n+      new Object[] {FileFormat.ORC, 2, false, PlanningMode.DISTRIBUTED},\n+      new Object[] {FileFormat.AVRO, 2, false, PlanningMode.LOCAL},\n+      new Object[] {FileFormat.PARQUET, 3, false, PlanningMode.DISTRIBUTED},\n+      new Object[] {FileFormat.PARQUET, 3, true, PlanningMode.LOCAL},\n     };\n   }\n \n@@ -165,12 +167,13 @@ protected Table createTable(String name, Schema schema, PartitionSpec spec) {\n     Table table = catalog.createTable(TableIdentifier.of(\"default\", name), schema);\n     TableOperations ops = ((BaseTable) table).operations();\n     TableMetadata meta = ops.current();\n-    ops.commit(meta, meta.upgradeToFormatVersion(2));\n+    ops.commit(meta, meta.upgradeToFormatVersion(formatVersion));\n     table\n         .updateProperties()\n         .set(TableProperties.DEFAULT_FILE_FORMAT, format.name())\n         .set(TableProperties.DATA_PLANNING_MODE, planningMode.modeName())\n         .set(TableProperties.DELETE_PLANNING_MODE, planningMode.modeName())\n+        .set(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion))\n         .commit();\n     if (format.equals(FileFormat.PARQUET) || format.equals(FileFormat.ORC)) {\n       String vectorizationEnabled =\n@@ -351,7 +354,8 @@ public void testPosDeletesAllRowsInBatch() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             TestHelpers.Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -383,7 +387,8 @@ public void testPosDeletesWithDeletedColumn() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             TestHelpers.Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -459,7 +464,8 @@ public void testMixedPosAndEqDeletesWithDeletedColumn() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             TestHelpers.Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -491,7 +497,8 @@ public void testFilterOnDeletedMetadataColumn() throws IOException {\n             table,\n             Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n             TestHelpers.Row.of(0),\n-            deletes);\n+            deletes,\n+            formatVersion);\n \n     table\n         .newRowDelta()\n@@ -613,7 +620,10 @@ public void testPosDeletesOnParquetFileWithMultipleRowGroups() throws IOExceptio\n             Pair.of(dataFile.path(), 109L));\n     Pair<DeleteFile, CharSequenceSet> posDeletes =\n         FileHelpers.writeDeleteFile(\n-            table, Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())), deletes);\n+            table,\n+            Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile())),\n+            deletes,\n+            formatVersion);\n     tbl.newRowDelta()\n         .addDeletes(posDeletes.first())\n         .validateDataFilesExist(posDeletes.second())\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11504",
    "pr_id": 11504,
    "issue_id": 10127,
    "repo": "apache/iceberg",
    "problem_statement": "Enable reading WASB and WASBS file paths with ABFS and ABFSS\n### Feature Request / Improvement\n\nWhen you setup a managed Snowflake Iceberg table on an Azure account, they will provide locations that use `wasbs://` and not `abfss://`. `wasb` is currently deprecated by Azure and everyone is encouraged to use ABFS instead. While Snowflake should really change this behavior, in the spirit of allowing people to \"update\" Iceberg tables without rewriting all the metadata files, it would be great if the iceberg library could handle this automatically.\r\n\r\nIts my understanding that you can convert a `wasbs` URI to an `abfss` URI by just making two changes:\r\n- Replacing `wasbs://` with `abfss://`\r\n- Updating `blob.core.windows.net` with `dfs.core.windows.net`.\r\n\r\nIf this change could be replaced when loading the location and file information from metadata files, then every user could effortlessly update to using  `abfs`.\r\n \n\n### Query engine\n\nSnowflake",
    "issue_word_count": 146,
    "test_files_count": 2,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "azure/src/main/java/org/apache/iceberg/azure/AzureProperties.java",
      "azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSLocation.java",
      "azure/src/test/java/org/apache/iceberg/azure/AzurePropertiesTest.java",
      "azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSLocationTest.java",
      "core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java"
    ],
    "pr_changed_test_files": [
      "azure/src/test/java/org/apache/iceberg/azure/AzurePropertiesTest.java",
      "azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSLocationTest.java"
    ],
    "base_commit": "1c576c5952fbc623591e800408cdba1518e6a410",
    "head_commit": "37bcf16196e1bebd06deebf8bd68c19c8983a8f5",
    "repo_url": "https://github.com/apache/iceberg/pull/11504",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11504",
    "dockerfile": "",
    "pr_merged_at": "2024-11-14T14:38:04.000Z",
    "patch": "diff --git a/azure/src/main/java/org/apache/iceberg/azure/AzureProperties.java b/azure/src/main/java/org/apache/iceberg/azure/AzureProperties.java\nindex 2d363cbc5231..a7f9885a4726 100644\n--- a/azure/src/main/java/org/apache/iceberg/azure/AzureProperties.java\n+++ b/azure/src/main/java/org/apache/iceberg/azure/AzureProperties.java\n@@ -77,6 +77,17 @@ public Optional<Long> adlsWriteBlockSize() {\n     return Optional.ofNullable(adlsWriteBlockSize);\n   }\n \n+  /**\n+   * Applies configuration to the {@link DataLakeFileSystemClientBuilder} to provide the endpoint\n+   * and credentials required to create an instance of the client.\n+   *\n+   * <p>The default endpoint is constructed in the form {@code\n+   * https://{account}.dfs.core.windows.net} and default credentials are provided via the {@link\n+   * com.azure.identity.DefaultAzureCredential}.\n+   *\n+   * @param account the service account name\n+   * @param builder the builder instance\n+   */\n   public void applyClientConfiguration(String account, DataLakeFileSystemClientBuilder builder) {\n     String sasToken = adlsSasTokens.get(account);\n     if (sasToken != null && !sasToken.isEmpty()) {\n@@ -93,7 +104,7 @@ public void applyClientConfiguration(String account, DataLakeFileSystemClientBui\n     if (connectionString != null && !connectionString.isEmpty()) {\n       builder.endpoint(connectionString);\n     } else {\n-      builder.endpoint(\"https://\" + account);\n+      builder.endpoint(\"https://\" + account + \".dfs.core.windows.net\");\n     }\n   }\n }\n\ndiff --git a/azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSLocation.java b/azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSLocation.java\nindex 5af590628fe8..fb91c4cb3233 100644\n--- a/azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSLocation.java\n+++ b/azure/src/main/java/org/apache/iceberg/azure/adlsv2/ADLSLocation.java\n@@ -30,14 +30,21 @@\n  *\n  * <p>Locations follow a URI like structure to identify resources\n  *\n- * <pre>{@code abfs[s]://[<container>@]<storage account host>/<file path>}</pre>\n+ * <pre>{@code abfs[s]://[<container>@]<storageAccount>.dfs.core.windows.net/<path>}</pre>\n+ *\n+ * or\n+ *\n+ * <pre>{@code wasb[s]://<container>@<storageAccount>.blob.core.windows.net/<path>}</pre>\n+ *\n+ * For compatibility, locations using the wasb scheme are also accepted but will use the Azure Data\n+ * Lake Storage Gen2 REST APIs instead of the Blob Storage REST APIs.\n  *\n  * <p>See <a\n  * href=\"https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction-abfs-uri#uri-syntax\">Azure\n  * Data Lake Storage URI</a>\n  */\n class ADLSLocation {\n-  private static final Pattern URI_PATTERN = Pattern.compile(\"^abfss?://([^/?#]+)(.*)?$\");\n+  private static final Pattern URI_PATTERN = Pattern.compile(\"^(abfss?|wasbs?)://([^/?#]+)(.*)?$\");\n \n   private final String storageAccount;\n   private final String container;\n@@ -55,17 +62,18 @@ class ADLSLocation {\n \n     ValidationException.check(matcher.matches(), \"Invalid ADLS URI: %s\", location);\n \n-    String authority = matcher.group(1);\n+    String authority = matcher.group(2);\n     String[] parts = authority.split(\"@\", -1);\n     if (parts.length > 1) {\n       this.container = parts[0];\n-      this.storageAccount = parts[1];\n+      String host = parts[1];\n+      this.storageAccount = host.split(\"\\\\.\", -1)[0];\n     } else {\n       this.container = null;\n-      this.storageAccount = authority;\n+      this.storageAccount = authority.split(\"\\\\.\", -1)[0];\n     }\n \n-    String uriPath = matcher.group(2);\n+    String uriPath = matcher.group(3);\n     this.path = uriPath == null ? \"\" : uriPath.startsWith(\"/\") ? uriPath.substring(1) : uriPath;\n   }\n \n\ndiff --git a/core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java b/core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java\nindex a858045aab8b..a8adf979f85a 100644\n--- a/core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java\n+++ b/core/src/main/java/org/apache/iceberg/io/ResolvingFileIO.java\n@@ -62,7 +62,9 @@ public class ResolvingFileIO implements HadoopConfigurable, DelegateFileIO {\n           \"s3n\", S3_FILE_IO_IMPL,\n           \"gs\", GCS_FILE_IO_IMPL,\n           \"abfs\", ADLS_FILE_IO_IMPL,\n-          \"abfss\", ADLS_FILE_IO_IMPL);\n+          \"abfss\", ADLS_FILE_IO_IMPL,\n+          \"wasb\", ADLS_FILE_IO_IMPL,\n+          \"wasbs\", ADLS_FILE_IO_IMPL);\n \n   private final Map<String, DelegateFileIO> ioInstances = Maps.newConcurrentMap();\n   private final AtomicBoolean isClosed = new AtomicBoolean(false);\n",
    "test_patch": "diff --git a/azure/src/test/java/org/apache/iceberg/azure/AzurePropertiesTest.java b/azure/src/test/java/org/apache/iceberg/azure/AzurePropertiesTest.java\nindex 6b8287c44e58..4f032d7ab125 100644\n--- a/azure/src/test/java/org/apache/iceberg/azure/AzurePropertiesTest.java\n+++ b/azure/src/test/java/org/apache/iceberg/azure/AzurePropertiesTest.java\n@@ -97,11 +97,13 @@ public void testNoSasToken() {\n   @Test\n   public void testWithConnectionString() {\n     AzureProperties props =\n-        new AzureProperties(ImmutableMap.of(\"adls.connection-string.account1\", \"http://endpoint\"));\n+        new AzureProperties(\n+            ImmutableMap.of(\n+                \"adls.connection-string.account1\", \"https://account1.dfs.core.usgovcloudapi.net\"));\n \n     DataLakeFileSystemClientBuilder clientBuilder = mock(DataLakeFileSystemClientBuilder.class);\n     props.applyClientConfiguration(\"account1\", clientBuilder);\n-    verify(clientBuilder).endpoint(\"http://endpoint\");\n+    verify(clientBuilder).endpoint(\"https://account1.dfs.core.usgovcloudapi.net\");\n   }\n \n   @Test\n@@ -111,7 +113,7 @@ public void testNoMatchingConnectionString() {\n \n     DataLakeFileSystemClientBuilder clientBuilder = mock(DataLakeFileSystemClientBuilder.class);\n     props.applyClientConfiguration(\"account1\", clientBuilder);\n-    verify(clientBuilder).endpoint(\"https://account1\");\n+    verify(clientBuilder).endpoint(\"https://account1.dfs.core.windows.net\");\n   }\n \n   @Test\n@@ -120,7 +122,7 @@ public void testNoConnectionString() {\n \n     DataLakeFileSystemClientBuilder clientBuilder = mock(DataLakeFileSystemClientBuilder.class);\n     props.applyClientConfiguration(\"account\", clientBuilder);\n-    verify(clientBuilder).endpoint(\"https://account\");\n+    verify(clientBuilder).endpoint(\"https://account.dfs.core.windows.net\");\n   }\n \n   @Test\n\ndiff --git a/azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSLocationTest.java b/azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSLocationTest.java\nindex 403886f4b28e..10b5e1877cca 100644\n--- a/azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSLocationTest.java\n+++ b/azure/src/test/java/org/apache/iceberg/azure/adlsv2/ADLSLocationTest.java\n@@ -33,7 +33,18 @@ public void testLocationParsing(String scheme) {\n     String p1 = scheme + \"://container@account.dfs.core.windows.net/path/to/file\";\n     ADLSLocation location = new ADLSLocation(p1);\n \n-    assertThat(location.storageAccount()).isEqualTo(\"account.dfs.core.windows.net\");\n+    assertThat(location.storageAccount()).isEqualTo(\"account\");\n+    assertThat(location.container().get()).isEqualTo(\"container\");\n+    assertThat(location.path()).isEqualTo(\"path/to/file\");\n+  }\n+\n+  @ParameterizedTest\n+  @ValueSource(strings = {\"wasb\", \"wasbs\"})\n+  public void testWasbLocatonParsing(String scheme) {\n+    String p1 = scheme + \"://container@account.blob.core.windows.net/path/to/file\";\n+    ADLSLocation location = new ADLSLocation(p1);\n+\n+    assertThat(location.storageAccount()).isEqualTo(\"account\");\n     assertThat(location.container().get()).isEqualTo(\"container\");\n     assertThat(location.path()).isEqualTo(\"path/to/file\");\n   }\n@@ -43,7 +54,7 @@ public void testEncodedString() {\n     String p1 = \"abfs://container@account.dfs.core.windows.net/path%20to%20file\";\n     ADLSLocation location = new ADLSLocation(p1);\n \n-    assertThat(location.storageAccount()).isEqualTo(\"account.dfs.core.windows.net\");\n+    assertThat(location.storageAccount()).isEqualTo(\"account\");\n     assertThat(location.container().get()).isEqualTo(\"container\");\n     assertThat(location.path()).isEqualTo(\"path%20to%20file\");\n   }\n@@ -67,7 +78,7 @@ public void testNoContainer() {\n     String p1 = \"abfs://account.dfs.core.windows.net/path/to/file\";\n     ADLSLocation location = new ADLSLocation(p1);\n \n-    assertThat(location.storageAccount()).isEqualTo(\"account.dfs.core.windows.net\");\n+    assertThat(location.storageAccount()).isEqualTo(\"account\");\n     assertThat(location.container().isPresent()).isFalse();\n     assertThat(location.path()).isEqualTo(\"path/to/file\");\n   }\n@@ -77,7 +88,7 @@ public void testNoPath() {\n     String p1 = \"abfs://container@account.dfs.core.windows.net\";\n     ADLSLocation location = new ADLSLocation(p1);\n \n-    assertThat(location.storageAccount()).isEqualTo(\"account.dfs.core.windows.net\");\n+    assertThat(location.storageAccount()).isEqualTo(\"account\");\n     assertThat(location.container().get()).isEqualTo(\"container\");\n     assertThat(location.path()).isEqualTo(\"\");\n   }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11495",
    "pr_id": 11495,
    "issue_id": 11122,
    "repo": "apache/iceberg",
    "problem_statement": "Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 118,
    "test_files_count": 17,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/BaseRowDelta.java",
      "core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java",
      "core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java",
      "core/src/test/java/org/apache/iceberg/ScanPlanningAndReportingTestBase.java",
      "core/src/test/java/org/apache/iceberg/TestBase.java",
      "core/src/test/java/org/apache/iceberg/TestBatchScans.java",
      "core/src/test/java/org/apache/iceberg/TestCommitReporting.java",
      "core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java",
      "core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java",
      "core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java",
      "core/src/test/java/org/apache/iceberg/TestRewriteFiles.java",
      "core/src/test/java/org/apache/iceberg/TestRewriteManifests.java",
      "core/src/test/java/org/apache/iceberg/TestRowDelta.java",
      "core/src/test/java/org/apache/iceberg/TestSnapshot.java",
      "core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java",
      "data/src/test/java/org/apache/iceberg/io/TestDVWriters.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanDeletes.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanReporting.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java",
      "core/src/test/java/org/apache/iceberg/ScanPlanningAndReportingTestBase.java",
      "core/src/test/java/org/apache/iceberg/TestBase.java",
      "core/src/test/java/org/apache/iceberg/TestBatchScans.java",
      "core/src/test/java/org/apache/iceberg/TestCommitReporting.java",
      "core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java",
      "core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java",
      "core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java",
      "core/src/test/java/org/apache/iceberg/TestRewriteFiles.java",
      "core/src/test/java/org/apache/iceberg/TestRewriteManifests.java",
      "core/src/test/java/org/apache/iceberg/TestRowDelta.java",
      "core/src/test/java/org/apache/iceberg/TestSnapshot.java",
      "core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java",
      "data/src/test/java/org/apache/iceberg/io/TestDVWriters.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanDeletes.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanReporting.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java"
    ],
    "base_commit": "166edc7298825321f677e1e70cf88d7249e8035c",
    "head_commit": "a57f47c28ce8b7515596d9ece4aefc8d1fd8ee16",
    "repo_url": "https://github.com/apache/iceberg/pull/11495",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11495",
    "dockerfile": "",
    "pr_merged_at": "2024-11-11T21:08:23.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/BaseRowDelta.java b/core/src/main/java/org/apache/iceberg/BaseRowDelta.java\nindex 85c2269ee526..372fc5367f08 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseRowDelta.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseRowDelta.java\n@@ -139,6 +139,8 @@ protected void validate(TableMetadata base, Snapshot parent) {\n       if (validateNewDeleteFiles) {\n         validateNoNewDeleteFiles(base, startingSnapshotId, conflictDetectionFilter, parent);\n       }\n+\n+      validateAddedDVs(base, startingSnapshotId, conflictDetectionFilter, parent);\n     }\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java b/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java\nindex 50885dbb06c7..6198ad00f680 100644\n--- a/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java\n+++ b/core/src/main/java/org/apache/iceberg/MergingSnapshotProducer.java\n@@ -48,11 +48,13 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.util.CharSequenceSet;\n+import org.apache.iceberg.util.ContentFileUtil;\n import org.apache.iceberg.util.DataFileSet;\n import org.apache.iceberg.util.DeleteFileSet;\n import org.apache.iceberg.util.Pair;\n import org.apache.iceberg.util.PartitionSet;\n import org.apache.iceberg.util.SnapshotUtil;\n+import org.apache.iceberg.util.Tasks;\n import org.slf4j.Logger;\n import org.slf4j.LoggerFactory;\n \n@@ -70,6 +72,9 @@ abstract class MergingSnapshotProducer<ThisT> extends SnapshotProducer<ThisT> {\n   // delete files can be added in \"overwrite\" or \"delete\" operations\n   private static final Set<String> VALIDATE_ADDED_DELETE_FILES_OPERATIONS =\n       ImmutableSet.of(DataOperations.OVERWRITE, DataOperations.DELETE);\n+  // DVs can be added in \"overwrite\", \"delete\", and \"replace\" operations\n+  private static final Set<String> VALIDATE_ADDED_DVS_OPERATIONS =\n+      ImmutableSet.of(DataOperations.OVERWRITE, DataOperations.DELETE, DataOperations.REPLACE);\n \n   private final String tableName;\n   private final TableOperations ops;\n@@ -83,6 +88,7 @@ abstract class MergingSnapshotProducer<ThisT> extends SnapshotProducer<ThisT> {\n   private final Map<Integer, DataFileSet> newDataFilesBySpec = Maps.newHashMap();\n   private Long newDataFilesDataSequenceNumber;\n   private final Map<Integer, DeleteFileSet> newDeleteFilesBySpec = Maps.newHashMap();\n+  private final Set<String> newDVRefs = Sets.newHashSet();\n   private final List<ManifestFile> appendManifests = Lists.newArrayList();\n   private final List<ManifestFile> rewrittenAppendManifests = Lists.newArrayList();\n   private final SnapshotSummary.Builder addedFilesSummary = SnapshotSummary.builder();\n@@ -245,13 +251,13 @@ private PartitionSpec spec(int specId) {\n \n   /** Add a delete file to the new snapshot. */\n   protected void add(DeleteFile file) {\n-    Preconditions.checkNotNull(file, \"Invalid delete file: null\");\n+    validateNewDeleteFile(file);\n     add(new PendingDeleteFile(file));\n   }\n \n   /** Add a delete file to the new snapshot. */\n   protected void add(DeleteFile file, long dataSequenceNumber) {\n-    Preconditions.checkNotNull(file, \"Invalid delete file: null\");\n+    validateNewDeleteFile(file);\n     add(new PendingDeleteFile(file, dataSequenceNumber));\n   }\n \n@@ -268,9 +274,39 @@ private void add(PendingDeleteFile file) {\n     if (deleteFiles.add(file)) {\n       addedFilesSummary.addedFile(spec, file);\n       hasNewDeleteFiles = true;\n+      if (ContentFileUtil.isDV(file)) {\n+        newDVRefs.add(file.referencedDataFile());\n+      }\n+    }\n+  }\n+\n+  protected void validateNewDeleteFile(DeleteFile file) {\n+    Preconditions.checkNotNull(file, \"Invalid delete file: null\");\n+    switch (formatVersion()) {\n+      case 1:\n+        throw new IllegalArgumentException(\"Deletes are supported in V2 and above\");\n+      case 2:\n+        Preconditions.checkArgument(\n+            file.content() == FileContent.EQUALITY_DELETES || !ContentFileUtil.isDV(file),\n+            \"Must not use DVs for position deletes in V2: %s\",\n+            ContentFileUtil.dvDesc(file));\n+        break;\n+      case 3:\n+        Preconditions.checkArgument(\n+            file.content() == FileContent.EQUALITY_DELETES || ContentFileUtil.isDV(file),\n+            \"Must use DVs for position deletes in V%s: %s\",\n+            formatVersion(),\n+            file.location());\n+        break;\n+      default:\n+        throw new IllegalArgumentException(\"Unsupported format version: \" + formatVersion());\n     }\n   }\n \n+  private int formatVersion() {\n+    return ops.current().formatVersion();\n+  }\n+\n   /** Add all files in a manifest to the new snapshot. */\n   protected void add(ManifestFile manifest) {\n     Preconditions.checkArgument(\n@@ -769,6 +805,58 @@ protected void validateDataFilesExist(\n     }\n   }\n \n+  // validates there are no concurrently added DVs for referenced data files\n+  protected void validateAddedDVs(\n+      TableMetadata base,\n+      Long startingSnapshotId,\n+      Expression conflictDetectionFilter,\n+      Snapshot parent) {\n+    // skip if there is no current table state or this operation doesn't add new DVs\n+    if (parent == null || newDVRefs.isEmpty()) {\n+      return;\n+    }\n+\n+    Pair<List<ManifestFile>, Set<Long>> history =\n+        validationHistory(\n+            base,\n+            startingSnapshotId,\n+            VALIDATE_ADDED_DVS_OPERATIONS,\n+            ManifestContent.DELETES,\n+            parent);\n+    List<ManifestFile> newDeleteManifests = history.first();\n+    Set<Long> newSnapshotIds = history.second();\n+\n+    Tasks.foreach(newDeleteManifests)\n+        .stopOnFailure()\n+        .throwFailureWhenFinished()\n+        .executeWith(workerPool())\n+        .run(manifest -> validateAddedDVs(manifest, conflictDetectionFilter, newSnapshotIds));\n+  }\n+\n+  private void validateAddedDVs(\n+      ManifestFile manifest, Expression conflictDetectionFilter, Set<Long> newSnapshotIds) {\n+    try (CloseableIterable<ManifestEntry<DeleteFile>> entries =\n+        ManifestFiles.readDeleteManifest(manifest, ops.io(), ops.current().specsById())\n+            .filterRows(conflictDetectionFilter)\n+            .caseSensitive(caseSensitive)\n+            .liveEntries()) {\n+\n+      for (ManifestEntry<DeleteFile> entry : entries) {\n+        DeleteFile file = entry.file();\n+        if (newSnapshotIds.contains(entry.snapshotId()) && ContentFileUtil.isDV(file)) {\n+          ValidationException.check(\n+              !newDVRefs.contains(file.referencedDataFile()),\n+              \"Found concurrently added DV for %s: %s\",\n+              file.referencedDataFile(),\n+              ContentFileUtil.dvDesc(file));\n+        }\n+      }\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n+\n+  // returns newly added manifests and snapshot IDs between the starting and parent snapshots\n   private Pair<List<ManifestFile>, Set<Long>> validationHistory(\n       TableMetadata base,\n       Long startingSnapshotId,\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java b/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java\nindex 6ef28191e78e..481422457b73 100644\n--- a/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java\n@@ -53,15 +53,6 @@ public static List<Object> parameters() {\n     return Arrays.asList(2, 3);\n   }\n \n-  static final DeleteFile FILE_A_POS_1 =\n-      FileMetadata.deleteFileBuilder(SPEC)\n-          .ofPositionDeletes()\n-          .withPath(\"/path/to/data-a-pos-deletes.parquet\")\n-          .withFileSizeInBytes(10)\n-          .withPartition(FILE_A.partition())\n-          .withRecordCount(1)\n-          .build();\n-\n   static final DeleteFile FILE_A_EQ_1 =\n       FileMetadata.deleteFileBuilder(SPEC)\n           .ofEqualityDeletes()\n@@ -311,7 +302,7 @@ public void testUnpartitionedTableScan() throws IOException {\n   public void testPartitionedTableWithPartitionPosDeletes() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n-    table.newRowDelta().addDeletes(FILE_A_POS_1).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).commit();\n \n     List<T> tasks = Lists.newArrayList(newScan(table).planFiles().iterator());\n     assertThat(tasks).as(\"Should have one task\").hasSize(1);\n@@ -323,7 +314,7 @@ public void testPartitionedTableWithPartitionPosDeletes() {\n     assertThat(task.deletes()).as(\"Should have one associated delete file\").hasSize(1);\n     assertThat(task.deletes().get(0).path())\n         .as(\"Should have only pos delete file\")\n-        .isEqualTo(FILE_A_POS_1.path());\n+        .isEqualTo(fileADeletes().path());\n   }\n \n   @TestTemplate\n@@ -349,7 +340,7 @@ public void testPartitionedTableWithPartitionEqDeletes() {\n   public void testPartitionedTableWithUnrelatedPartitionDeletes() {\n     table.newAppend().appendFile(FILE_B).commit();\n \n-    table.newRowDelta().addDeletes(FILE_A_POS_1).addDeletes(FILE_A_EQ_1).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A_EQ_1).commit();\n \n     List<T> tasks = Lists.newArrayList(newScan(table).planFiles().iterator());\n     assertThat(tasks).as(\"Should have one task\").hasSize(1);\n@@ -363,7 +354,9 @@ public void testPartitionedTableWithUnrelatedPartitionDeletes() {\n \n   @TestTemplate\n   public void testPartitionedTableWithOlderPartitionDeletes() {\n-    table.newRowDelta().addDeletes(FILE_A_POS_1).addDeletes(FILE_A_EQ_1).commit();\n+    assumeThat(formatVersion).as(\"DVs are not filtered using sequence numbers\").isEqualTo(2);\n+\n+    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A_EQ_1).commit();\n \n     table.newAppend().appendFile(FILE_A).commit();\n \n@@ -379,6 +372,8 @@ public void testPartitionedTableWithOlderPartitionDeletes() {\n \n   @TestTemplate\n   public void testPartitionedTableScanWithGlobalDeletes() {\n+    assumeThat(formatVersion).as(\"Requires V2 position deletes\").isEqualTo(2);\n+\n     table.newAppend().appendFile(FILE_A).commit();\n \n     TableMetadata base = table.ops().current();\n@@ -407,6 +402,8 @@ public void testPartitionedTableScanWithGlobalDeletes() {\n \n   @TestTemplate\n   public void testPartitionedTableScanWithGlobalAndPartitionDeletes() {\n+    assumeThat(formatVersion).as(\"Requires V2 position deletes\").isEqualTo(2);\n+\n     table.newAppend().appendFile(FILE_A).commit();\n \n     table.newRowDelta().addDeletes(FILE_A_EQ_1).commit();\n@@ -437,7 +434,7 @@ public void testPartitionedTableScanWithGlobalAndPartitionDeletes() {\n \n   @TestTemplate\n   public void testPartitionedTableSequenceNumbers() {\n-    table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_EQ_1).addDeletes(FILE_A_POS_1).commit();\n+    table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_EQ_1).addDeletes(fileADeletes()).commit();\n \n     List<T> tasks = Lists.newArrayList(newScan(table).planFiles().iterator());\n     assertThat(tasks).as(\"Should have one task\").hasSize(1);\n@@ -449,7 +446,7 @@ public void testPartitionedTableSequenceNumbers() {\n     assertThat(task.deletes()).as(\"Should have one associated delete file\").hasSize(1);\n     assertThat(task.deletes().get(0).path())\n         .as(\"Should have only pos delete file\")\n-        .isEqualTo(FILE_A_POS_1.path());\n+        .isEqualTo(fileADeletes().path());\n   }\n \n   @TestTemplate\n@@ -501,7 +498,7 @@ public void testPartitionedTableWithExistingDeleteFile() {\n \n     table.newRowDelta().addDeletes(FILE_A_EQ_1).commit();\n \n-    table.newRowDelta().addDeletes(FILE_A_POS_1).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).commit();\n \n     table\n         .updateProperties()\n@@ -557,7 +554,7 @@ public void testPartitionedTableWithExistingDeleteFile() {\n     assertThat(task.deletes()).as(\"Should have two associated delete files\").hasSize(2);\n     assertThat(Sets.newHashSet(Iterables.transform(task.deletes(), ContentFile::path)))\n         .as(\"Should have expected delete files\")\n-        .isEqualTo(Sets.newHashSet(FILE_A_EQ_1.path(), FILE_A_POS_1.path()));\n+        .isEqualTo(Sets.newHashSet(FILE_A_EQ_1.path(), fileADeletes().path()));\n   }\n \n   @TestTemplate\n\ndiff --git a/core/src/test/java/org/apache/iceberg/ScanPlanningAndReportingTestBase.java b/core/src/test/java/org/apache/iceberg/ScanPlanningAndReportingTestBase.java\nindex 13e96869b454..80551f0a2247 100644\n--- a/core/src/test/java/org/apache/iceberg/ScanPlanningAndReportingTestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/ScanPlanningAndReportingTestBase.java\n@@ -186,7 +186,7 @@ public void scanningWithDeletes() throws IOException {\n             reporter);\n \n     table.newAppend().appendFile(FILE_A).appendFile(FILE_B).appendFile(FILE_C).commit();\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_B_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(fileBDeletes()).commit();\n     ScanT tableScan = newScan(table);\n \n     try (CloseableIterable<T> fileScanTasks = tableScan.planFiles()) {\n@@ -208,12 +208,19 @@ public void scanningWithDeletes() throws IOException {\n     assertThat(result.totalDataManifests().value()).isEqualTo(1);\n     assertThat(result.totalDeleteManifests().value()).isEqualTo(1);\n     assertThat(result.totalFileSizeInBytes().value()).isEqualTo(30L);\n-    assertThat(result.totalDeleteFileSizeInBytes().value()).isEqualTo(20L);\n+    assertThat(result.totalDeleteFileSizeInBytes().value())\n+        .isEqualTo(contentSize(fileADeletes(), fileBDeletes()));\n     assertThat(result.skippedDataFiles().value()).isEqualTo(0);\n     assertThat(result.skippedDeleteFiles().value()).isEqualTo(0);\n     assertThat(result.indexedDeleteFiles().value()).isEqualTo(2);\n     assertThat(result.equalityDeleteFiles().value()).isEqualTo(0);\n-    assertThat(result.positionalDeleteFiles().value()).isEqualTo(2);\n+    if (formatVersion == 2) {\n+      assertThat(result.positionalDeleteFiles().value()).isEqualTo(2);\n+      assertThat(result.dvs().value()).isEqualTo(0);\n+    } else {\n+      assertThat(result.positionalDeleteFiles().value()).isEqualTo(0);\n+      assertThat(result.dvs().value()).isEqualTo(2);\n+    }\n   }\n \n   @TestTemplate\n@@ -264,8 +271,8 @@ public void scanningWithSkippedDeleteFiles() throws IOException {\n             tableDir, tableName, SCHEMA, SPEC, SortOrder.unsorted(), formatVersion, reporter);\n     table.newAppend().appendFile(FILE_A).appendFile(FILE_B).appendFile(FILE_D).commit();\n     table.newOverwrite().deleteFile(FILE_A).addFile(FILE_A2).commit();\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_D2_DELETES).commit();\n-    table.newRowDelta().addDeletes(FILE_B_DELETES).addDeletes(FILE_C2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_D2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileBDeletes()).addDeletes(FILE_C2_DELETES).commit();\n     ScanT tableScan = newScan(table);\n \n     List<FileScanTask> fileTasks = Lists.newArrayList();\n@@ -308,7 +315,7 @@ public void scanningWithEqualityAndPositionalDeleteFiles() throws IOException {\n             tableDir, tableName, SCHEMA, SPEC, SortOrder.unsorted(), formatVersion, reporter);\n     table.newAppend().appendFile(FILE_A).commit();\n     // FILE_A_DELETES = positionalDelete / FILE_A2_DELETES = equalityDelete\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n     ScanT tableScan = newScan(table);\n \n     try (CloseableIterable<T> fileScanTasks =\n@@ -321,7 +328,13 @@ public void scanningWithEqualityAndPositionalDeleteFiles() throws IOException {\n     ScanMetricsResult result = scanReport.scanMetrics();\n     assertThat(result.indexedDeleteFiles().value()).isEqualTo(2);\n     assertThat(result.equalityDeleteFiles().value()).isEqualTo(1);\n-    assertThat(result.positionalDeleteFiles().value()).isEqualTo(1);\n+    if (formatVersion == 2) {\n+      assertThat(result.positionalDeleteFiles().value()).isEqualTo(1);\n+      assertThat(result.dvs().value()).isEqualTo(0);\n+    } else {\n+      assertThat(result.positionalDeleteFiles().value()).isEqualTo(0);\n+      assertThat(result.dvs().value()).isEqualTo(1);\n+    }\n   }\n \n   static class TestMetricsReporter implements MetricsReporter {\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestBase.java b/core/src/test/java/org/apache/iceberg/TestBase.java\nindex 9813d02910a6..46a1518e877f 100644\n--- a/core/src/test/java/org/apache/iceberg/TestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/TestBase.java\n@@ -45,6 +45,7 @@\n import org.apache.iceberg.relocated.com.google.common.io.Files;\n import org.apache.iceberg.types.Conversions;\n import org.apache.iceberg.types.Types;\n+import org.apache.iceberg.util.ScanTaskUtil;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.extension.ExtendWith;\n@@ -85,6 +86,17 @@ public class TestBase {\n           .withPartitionPath(\"data_bucket=0\") // easy way to set partition data for now\n           .withRecordCount(1)\n           .build();\n+  static final DeleteFile FILE_A_DV =\n+      FileMetadata.deleteFileBuilder(SPEC)\n+          .ofPositionDeletes()\n+          .withPath(\"/path/to/data-a-deletes.puffin\")\n+          .withFileSizeInBytes(10)\n+          .withPartitionPath(\"data_bucket=0\")\n+          .withRecordCount(1)\n+          .withReferencedDataFile(FILE_A.location())\n+          .withContentOffset(4)\n+          .withContentSizeInBytes(6)\n+          .build();\n   // Equality delete files.\n   static final DeleteFile FILE_A2_DELETES =\n       FileMetadata.deleteFileBuilder(SPEC)\n@@ -110,6 +122,17 @@ public class TestBase {\n           .withPartitionPath(\"data_bucket=1\") // easy way to set partition data for now\n           .withRecordCount(1)\n           .build();\n+  static final DeleteFile FILE_B_DV =\n+      FileMetadata.deleteFileBuilder(SPEC)\n+          .ofPositionDeletes()\n+          .withPath(\"/path/to/data-b-deletes.puffin\")\n+          .withFileSizeInBytes(10)\n+          .withPartitionPath(\"data_bucket=1\")\n+          .withRecordCount(1)\n+          .withReferencedDataFile(FILE_B.location())\n+          .withContentOffset(4)\n+          .withContentSizeInBytes(6)\n+          .build();\n   static final DataFile FILE_C =\n       DataFiles.builder(SPEC)\n           .withPath(\"/path/to/data-c.parquet\")\n@@ -643,6 +666,22 @@ protected DataFile newDataFile(String partitionPath) {\n         .build();\n   }\n \n+  protected DeleteFile fileADeletes() {\n+    return formatVersion >= 3 ? FILE_A_DV : FILE_A_DELETES;\n+  }\n+\n+  protected DeleteFile fileBDeletes() {\n+    return formatVersion >= 3 ? FILE_B_DV : FILE_B_DELETES;\n+  }\n+\n+  protected DeleteFile newDeletes(DataFile dataFile) {\n+    if (formatVersion >= 3) {\n+      return FileGenerationUtil.generateDV(table, dataFile);\n+    } else {\n+      return FileGenerationUtil.generatePositionDeleteFile(table, dataFile);\n+    }\n+  }\n+\n   protected DeleteFile newDeleteFile(int specId, String partitionPath) {\n     PartitionSpec spec = table.specs().get(specId);\n     return FileMetadata.deleteFileBuilder(spec)\n@@ -764,6 +803,14 @@ static Iterator<DataFile> files(ManifestFile manifest) {\n     return ManifestFiles.read(manifest, FILE_IO).iterator();\n   }\n \n+  static long recordCount(ContentFile<?>... files) {\n+    return Arrays.stream(files).mapToLong(ContentFile::recordCount).sum();\n+  }\n+\n+  static long contentSize(ContentFile<?>... files) {\n+    return ScanTaskUtil.contentSizeInBytes(Arrays.asList(files));\n+  }\n+\n   /** Used for assertions that only apply if the table version is v2. */\n   protected static class TableAssertions {\n     private boolean enabled;\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestBatchScans.java b/core/src/test/java/org/apache/iceberg/TestBatchScans.java\nindex 1597f44f6338..72cd00e0573d 100644\n--- a/core/src/test/java/org/apache/iceberg/TestBatchScans.java\n+++ b/core/src/test/java/org/apache/iceberg/TestBatchScans.java\n@@ -42,7 +42,7 @@ public void testDataTableScan() {\n     table.newFastAppend().appendFile(FILE_A).appendFile(FILE_B).commit();\n \n     if (formatVersion > 1) {\n-      table.newRowDelta().addDeletes(FILE_A_DELETES).commit();\n+      table.newRowDelta().addDeletes(fileADeletes()).commit();\n     }\n \n     BatchScan scan = table.newBatchScan();\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestCommitReporting.java b/core/src/test/java/org/apache/iceberg/TestCommitReporting.java\nindex 41b301668722..d333af98d623 100644\n--- a/core/src/test/java/org/apache/iceberg/TestCommitReporting.java\n+++ b/core/src/test/java/org/apache/iceberg/TestCommitReporting.java\n@@ -95,11 +95,13 @@ public void addAndDeleteDeleteFiles() {\n     // 2 positional + 1 equality\n     table\n         .newRowDelta()\n-        .addDeletes(FILE_A_DELETES)\n-        .addDeletes(FILE_B_DELETES)\n+        .addDeletes(fileADeletes())\n+        .addDeletes(fileBDeletes())\n         .addDeletes(FILE_C2_DELETES)\n         .commit();\n \n+    long totalDeleteContentSize = contentSize(fileADeletes(), fileBDeletes(), FILE_C2_DELETES);\n+\n     CommitReport report = reporter.lastCommitReport();\n     assertThat(report).isNotNull();\n     assertThat(report.operation()).isEqualTo(\"delete\");\n@@ -110,7 +112,13 @@ public void addAndDeleteDeleteFiles() {\n     CommitMetricsResult metrics = report.commitMetrics();\n     assertThat(metrics.addedDeleteFiles().value()).isEqualTo(3L);\n     assertThat(metrics.totalDeleteFiles().value()).isEqualTo(3L);\n-    assertThat(metrics.addedPositionalDeleteFiles().value()).isEqualTo(2L);\n+    if (formatVersion == 2) {\n+      assertThat(metrics.addedPositionalDeleteFiles().value()).isEqualTo(2L);\n+      assertThat(metrics.addedDVs()).isNull();\n+    } else {\n+      assertThat(metrics.addedPositionalDeleteFiles()).isNull();\n+      assertThat(metrics.addedDVs().value()).isEqualTo(2L);\n+    }\n     assertThat(metrics.addedEqualityDeleteFiles().value()).isEqualTo(1L);\n \n     assertThat(metrics.addedPositionalDeletes().value()).isEqualTo(2L);\n@@ -119,15 +127,15 @@ public void addAndDeleteDeleteFiles() {\n     assertThat(metrics.addedEqualityDeletes().value()).isEqualTo(1L);\n     assertThat(metrics.totalEqualityDeletes().value()).isEqualTo(1L);\n \n-    assertThat(metrics.addedFilesSizeInBytes().value()).isEqualTo(30L);\n-    assertThat(metrics.totalFilesSizeInBytes().value()).isEqualTo(30L);\n+    assertThat(metrics.addedFilesSizeInBytes().value()).isEqualTo(totalDeleteContentSize);\n+    assertThat(metrics.totalFilesSizeInBytes().value()).isEqualTo(totalDeleteContentSize);\n \n     // now remove those 2 positional + 1 equality delete files\n     table\n         .newRewrite()\n         .rewriteFiles(\n             ImmutableSet.of(),\n-            ImmutableSet.of(FILE_A_DELETES, FILE_B_DELETES, FILE_C2_DELETES),\n+            ImmutableSet.of(fileADeletes(), fileBDeletes(), FILE_C2_DELETES),\n             ImmutableSet.of(),\n             ImmutableSet.of())\n         .commit();\n@@ -142,7 +150,13 @@ public void addAndDeleteDeleteFiles() {\n     metrics = report.commitMetrics();\n     assertThat(metrics.removedDeleteFiles().value()).isEqualTo(3L);\n     assertThat(metrics.totalDeleteFiles().value()).isEqualTo(0L);\n-    assertThat(metrics.removedPositionalDeleteFiles().value()).isEqualTo(2L);\n+    if (formatVersion == 2) {\n+      assertThat(metrics.removedPositionalDeleteFiles().value()).isEqualTo(2L);\n+      assertThat(metrics.removedDVs()).isNull();\n+    } else {\n+      assertThat(metrics.removedPositionalDeleteFiles()).isNull();\n+      assertThat(metrics.removedDVs().value()).isEqualTo(2L);\n+    }\n     assertThat(metrics.removedEqualityDeleteFiles().value()).isEqualTo(1L);\n \n     assertThat(metrics.removedPositionalDeletes().value()).isEqualTo(2L);\n@@ -151,7 +165,7 @@ public void addAndDeleteDeleteFiles() {\n     assertThat(metrics.removedEqualityDeletes().value()).isEqualTo(1L);\n     assertThat(metrics.totalEqualityDeletes().value()).isEqualTo(0L);\n \n-    assertThat(metrics.removedFilesSizeInBytes().value()).isEqualTo(30L);\n+    assertThat(metrics.removedFilesSizeInBytes().value()).isEqualTo(totalDeleteContentSize);\n     assertThat(metrics.totalFilesSizeInBytes().value()).isEqualTo(0L);\n   }\n \n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java b/core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java\nindex 9bce4e60a4f3..e061567e72a8 100644\n--- a/core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java\n+++ b/core/src/test/java/org/apache/iceberg/TestEntriesMetadataTable.java\n@@ -131,7 +131,7 @@ public void testEntriesTableWithDeleteManifests() {\n     assumeThat(formatVersion).as(\"Only V2 Tables Support Deletes\").isGreaterThanOrEqualTo(2);\n     table.newAppend().appendFile(FILE_A).appendFile(FILE_B).commit();\n \n-    table.newRowDelta().addDeletes(FILE_A_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).commit();\n \n     Table entriesTable = new ManifestEntriesTable(table);\n     TableScan scan = entriesTable.newScan();\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java b/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java\nindex 30fdae01cd94..f811dac02043 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java\n@@ -58,14 +58,14 @@ private void preparePartitionedTable(boolean transactional) {\n       if (transactional) {\n         table\n             .newRowDelta()\n-            .addDeletes(FILE_A_DELETES)\n-            .addDeletes(FILE_B_DELETES)\n+            .addDeletes(fileADeletes())\n+            .addDeletes(fileBDeletes())\n             .addDeletes(FILE_C2_DELETES)\n             .addDeletes(FILE_D2_DELETES)\n             .commit();\n       } else {\n-        table.newRowDelta().addDeletes(FILE_A_DELETES).commit();\n-        table.newRowDelta().addDeletes(FILE_B_DELETES).commit();\n+        table.newRowDelta().addDeletes(fileADeletes()).commit();\n+        table.newRowDelta().addDeletes(fileBDeletes()).commit();\n         table.newRowDelta().addDeletes(FILE_C2_DELETES).commit();\n         table.newRowDelta().addDeletes(FILE_D2_DELETES).commit();\n       }\n@@ -721,7 +721,7 @@ public void testDeleteFilesTableSelection() throws IOException {\n     assumeThat(formatVersion).as(\"Position deletes are not supported by V1 Tables\").isNotEqualTo(1);\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n \n     Table deleteFilesTable = new DeleteFilesTable(table);\n \n@@ -1409,10 +1409,10 @@ public void testPositionDeletesWithFilter() {\n         .containsEntry(MetadataColumns.SPEC_ID.fieldId(), 0);\n     assertThat(posDeleteTask.file().path())\n         .as(\"Expected correct delete file on task\")\n-        .isEqualTo(FILE_B_DELETES.path());\n+        .isEqualTo(fileBDeletes().path());\n     assertThat((Map<Integer, String>) constantsMap(posDeleteTask, partitionType))\n         .as(\"Expected correct delete file on constant column\")\n-        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), FILE_B_DELETES.path().toString());\n+        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), fileBDeletes().path().toString());\n   }\n \n   @TestTemplate\n@@ -1479,17 +1479,16 @@ private void testPositionDeletesBaseTableFilter(boolean transactional) {\n         .containsEntry(MetadataColumns.SPEC_ID.fieldId(), 0);\n     assertThat(posDeleteTask.file().path())\n         .as(\"Expected correct delete file on task\")\n-        .isEqualTo(FILE_A_DELETES.path());\n+        .isEqualTo(fileADeletes().path());\n     assertThat((Map<Integer, String>) constantsMap(posDeleteTask, partitionType))\n         .as(\"Expected correct delete file on constant column\")\n-        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), FILE_A_DELETES.path().toString());\n+        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), fileADeletes().path().toString());\n   }\n \n   @TestTemplate\n   public void testPositionDeletesWithBaseTableFilterNot() {\n-    assumeThat(formatVersion)\n-        .as(\"Position deletes are not supported by V1 Tables\")\n-        .isNotEqualTo(1); // use identity rather than bucket partition spec,\n+    assumeThat(formatVersion).as(\"Position deletes are not supported by V1 Tables\").isEqualTo(2);\n+    // use identity rather than bucket partition spec,\n     // as bucket.project does not support projecting notEq\n     table.updateSpec().removeField(\"data_bucket\").addField(\"id\").commit();\n     PartitionSpec spec = table.spec();\n@@ -1619,20 +1618,8 @@ public void testPositionDeletesUnpartitioned() {\n             .build();\n     table.newAppend().appendFile(dataFile1).appendFile(dataFile2).commit();\n \n-    DeleteFile delete1 =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\"/path/to/delete1.parquet\")\n-            .withFileSizeInBytes(10)\n-            .withRecordCount(1)\n-            .build();\n-    DeleteFile delete2 =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\"/path/to/delete2.parquet\")\n-            .withFileSizeInBytes(10)\n-            .withRecordCount(1)\n-            .build();\n+    DeleteFile delete1 = newDeletes(dataFile1);\n+    DeleteFile delete2 = newDeletes(dataFile2);\n     table.newRowDelta().addDeletes(delete1).addDeletes(delete2).commit();\n \n     PositionDeletesTable positionDeletesTable = new PositionDeletesTable(table);\n@@ -1655,16 +1642,16 @@ public void testPositionDeletesUnpartitioned() {\n         .isEqualTo(1);\n \n     assertThat(scanTasks).hasSize(2);\n-    scanTasks.sort(Comparator.comparing(f -> f.file().path().toString()));\n-    assertThat(scanTasks.get(0).file().path().toString()).isEqualTo(\"/path/to/delete1.parquet\");\n-    assertThat(scanTasks.get(1).file().path().toString()).isEqualTo(\"/path/to/delete2.parquet\");\n+    scanTasks.sort(Comparator.comparing(f -> f.file().pos()));\n+    assertThat(scanTasks.get(0).file().location()).isEqualTo(delete1.location());\n+    assertThat(scanTasks.get(1).file().location()).isEqualTo(delete2.location());\n \n     Types.StructType partitionType = Partitioning.partitionType(table);\n \n     assertThat((Map<Integer, String>) constantsMap(scanTasks.get(0), partitionType))\n-        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), \"/path/to/delete1.parquet\");\n+        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), delete1.location());\n     assertThat((Map<Integer, String>) constantsMap(scanTasks.get(1), partitionType))\n-        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), \"/path/to/delete2.parquet\");\n+        .containsEntry(MetadataColumns.FILE_PATH.fieldId(), delete2.location());\n     assertThat((Map<Integer, Integer>) constantsMap(scanTasks.get(0), partitionType))\n         .containsEntry(MetadataColumns.SPEC_ID.fieldId(), 1);\n     assertThat((Map<Integer, Integer>) constantsMap(scanTasks.get(1), partitionType))\n@@ -1712,20 +1699,8 @@ public void testPositionDeletesManyColumns() {\n             .build();\n     table.newAppend().appendFile(dataFile1).appendFile(dataFile2).commit();\n \n-    DeleteFile delete1 =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\"/path/to/delete1.parquet\")\n-            .withFileSizeInBytes(10)\n-            .withRecordCount(1)\n-            .build();\n-    DeleteFile delete2 =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\"/path/to/delete2.parquet\")\n-            .withFileSizeInBytes(10)\n-            .withRecordCount(1)\n-            .build();\n+    DeleteFile delete1 = newDeletes(dataFile1);\n+    DeleteFile delete2 = newDeletes(dataFile2);\n     table.newRowDelta().addDeletes(delete1).addDeletes(delete2).commit();\n \n     PositionDeletesTable positionDeletesTable = new PositionDeletesTable(table);\n@@ -1745,9 +1720,9 @@ public void testPositionDeletesManyColumns() {\n                   return (PositionDeletesScanTask) task;\n                 }));\n     assertThat(scanTasks).hasSize(2);\n-    scanTasks.sort(Comparator.comparing(f -> f.file().path().toString()));\n-    assertThat(scanTasks.get(0).file().path().toString()).isEqualTo(\"/path/to/delete1.parquet\");\n-    assertThat(scanTasks.get(1).file().path().toString()).isEqualTo(\"/path/to/delete2.parquet\");\n+    scanTasks.sort(Comparator.comparing(f -> f.file().pos()));\n+    assertThat(scanTasks.get(0).file().location()).isEqualTo(delete1.location());\n+    assertThat(scanTasks.get(1).file().location()).isEqualTo(delete2.location());\n   }\n \n   @TestTemplate\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java b/core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java\nindex 2de38541777b..84860d34bb31 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetadataTableScansWithPartitionEvolution.java\n@@ -159,7 +159,7 @@ public void testPartitionsTableScanWithAddPartitionOnNestedField() {\n \n   @TestTemplate\n   public void testPositionDeletesPartitionSpecRemoval() {\n-    assumeThat(formatVersion).as(\"Position deletes are not supported by V1 Tables\").isNotEqualTo(1);\n+    assumeThat(formatVersion).as(\"Position deletes are not supported by V1 Tables\").isEqualTo(2);\n     table.updateSpec().removeField(\"id\").commit();\n \n     DeleteFile deleteFile = newDeleteFile(table.ops().current().spec().specId(), \"nested.id=1\");\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestRewriteFiles.java b/core/src/test/java/org/apache/iceberg/TestRewriteFiles.java\nindex 124cc2f28dd5..5b108e9ee565 100644\n--- a/core/src/test/java/org/apache/iceberg/TestRewriteFiles.java\n+++ b/core/src/test/java/org/apache/iceberg/TestRewriteFiles.java\n@@ -55,6 +55,7 @@ protected static List<Object> parameters() {\n \n   @TestTemplate\n   public void testEmptyTable() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n     assertThat(listManifestFiles()).isEmpty();\n \n     TableMetadata base = readMetadata();\n@@ -87,6 +88,7 @@ public void testEmptyTable() {\n \n   @TestTemplate\n   public void testAddOnly() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n     assertThat(listManifestFiles()).isEmpty();\n \n     assertThatThrownBy(\n@@ -130,6 +132,7 @@ public void testAddOnly() {\n \n   @TestTemplate\n   public void testDeleteOnly() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n     assertThat(listManifestFiles()).isEmpty();\n \n     assertThatThrownBy(\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java b/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java\nindex f1d23de32a42..72bb85c0446e 100644\n--- a/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java\n+++ b/core/src/test/java/org/apache/iceberg/TestRewriteManifests.java\n@@ -1096,7 +1096,7 @@ public void testRewriteDataManifestsPreservesDeletes() {\n     long appendSnapshotSeq = appendSnapshot.sequenceNumber();\n \n     // commit delete files\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n \n     // save the delete snapshot info\n     Snapshot deleteSnapshot = table.currentSnapshot();\n@@ -1139,7 +1139,7 @@ public void testRewriteDataManifestsPreservesDeletes() {\n         dataSeqs(deleteSnapshotSeq, deleteSnapshotSeq),\n         fileSeqs(deleteSnapshotSeq, deleteSnapshotSeq),\n         ids(deleteSnapshotId, deleteSnapshotId),\n-        files(FILE_A_DELETES, FILE_A2_DELETES),\n+        files(fileADeletes(), FILE_A2_DELETES),\n         statuses(ManifestEntry.Status.ADDED, ManifestEntry.Status.ADDED));\n   }\n \n@@ -1158,7 +1158,7 @@ public void testReplaceDeleteManifestsOnly() throws IOException {\n     long appendSnapshotSeq = appendSnapshot.sequenceNumber();\n \n     // commit delete files\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n \n     // save the delete snapshot info\n     Snapshot deleteSnapshot = table.currentSnapshot();\n@@ -1179,7 +1179,7 @@ public void testReplaceDeleteManifestsOnly() throws IOException {\n                 deleteSnapshotId,\n                 deleteSnapshotSeq,\n                 deleteSnapshotSeq,\n-                FILE_A_DELETES));\n+                fileADeletes()));\n     ManifestFile newDeleteManifest2 =\n         writeManifest(\n             \"delete-manifest-file-2.avro\",\n@@ -1218,7 +1218,7 @@ public void testReplaceDeleteManifestsOnly() throws IOException {\n         dataSeqs(deleteSnapshotSeq),\n         fileSeqs(deleteSnapshotSeq),\n         ids(deleteSnapshotId),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(ManifestEntry.Status.EXISTING));\n     validateDeleteManifest(\n         deleteManifests.get(1),\n@@ -1244,7 +1244,7 @@ public void testReplaceDataAndDeleteManifests() throws IOException {\n     long appendSnapshotSeq = appendSnapshot.sequenceNumber();\n \n     // commit delete files\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n \n     // save the delete snapshot info\n     Snapshot deleteSnapshot = table.currentSnapshot();\n@@ -1287,7 +1287,7 @@ public void testReplaceDataAndDeleteManifests() throws IOException {\n                 deleteSnapshotId,\n                 deleteSnapshotSeq,\n                 deleteSnapshotSeq,\n-                FILE_A_DELETES));\n+                fileADeletes()));\n     ManifestFile newDeleteManifest2 =\n         writeManifest(\n             \"delete-manifest-file-2.avro\",\n@@ -1337,7 +1337,7 @@ public void testReplaceDataAndDeleteManifests() throws IOException {\n         dataSeqs(deleteSnapshotSeq),\n         fileSeqs(deleteSnapshotSeq),\n         ids(deleteSnapshotId),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(ManifestEntry.Status.EXISTING));\n     validateDeleteManifest(\n         deleteManifests.get(1),\n@@ -1361,7 +1361,7 @@ public void testDeleteManifestReplacementConcurrentAppend() throws IOException {\n     long appendSnapshotSeq = appendSnapshot.sequenceNumber();\n \n     // commit delete files\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n \n     // save the delete snapshot info\n     Snapshot deleteSnapshot = table.currentSnapshot();\n@@ -1379,7 +1379,7 @@ public void testDeleteManifestReplacementConcurrentAppend() throws IOException {\n                 deleteSnapshotId,\n                 deleteSnapshotSeq,\n                 deleteSnapshotSeq,\n-                FILE_A_DELETES));\n+                fileADeletes()));\n     ManifestFile newDeleteManifest2 =\n         writeManifest(\n             \"delete-manifest-file-2.avro\",\n@@ -1440,7 +1440,7 @@ public void testDeleteManifestReplacementConcurrentAppend() throws IOException {\n         dataSeqs(deleteSnapshotSeq),\n         fileSeqs(deleteSnapshotSeq),\n         ids(deleteSnapshotId),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(ManifestEntry.Status.EXISTING));\n     validateDeleteManifest(\n         deleteManifests.get(1),\n@@ -1464,7 +1464,7 @@ public void testDeleteManifestReplacementConcurrentDeleteFileRemoval() throws IO\n     long appendSnapshotSeq = appendSnapshot.sequenceNumber();\n \n     // commit the first set of delete files\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n \n     // save the first delete snapshot info\n     Snapshot deleteSnapshot1 = table.currentSnapshot();\n@@ -1472,7 +1472,7 @@ public void testDeleteManifestReplacementConcurrentDeleteFileRemoval() throws IO\n     long deleteSnapshotSeq1 = deleteSnapshot1.sequenceNumber();\n \n     // commit the second set of delete files\n-    table.newRowDelta().addDeletes(FILE_B_DELETES).addDeletes(FILE_C2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileBDeletes()).addDeletes(FILE_C2_DELETES).commit();\n \n     // save the second delete snapshot info\n     Snapshot deleteSnapshot2 = table.currentSnapshot();\n@@ -1489,7 +1489,7 @@ public void testDeleteManifestReplacementConcurrentDeleteFileRemoval() throws IO\n                 deleteSnapshotId1,\n                 deleteSnapshotSeq1,\n                 deleteSnapshotSeq1,\n-                FILE_A_DELETES));\n+                fileADeletes()));\n     ManifestFile newDeleteManifest2 =\n         writeManifest(\n             \"delete-manifest-file-2.avro\",\n@@ -1507,7 +1507,7 @@ public void testDeleteManifestReplacementConcurrentDeleteFileRemoval() throws IO\n     rewriteManifests.addManifest(newDeleteManifest2);\n \n     // commit the third set of delete files concurrently\n-    table.newRewrite().deleteFile(FILE_B_DELETES).commit();\n+    table.newRewrite().deleteFile(fileBDeletes()).commit();\n \n     Snapshot concurrentSnapshot = table.currentSnapshot();\n     long concurrentSnapshotId = concurrentSnapshot.snapshotId();\n@@ -1541,7 +1541,7 @@ public void testDeleteManifestReplacementConcurrentDeleteFileRemoval() throws IO\n         dataSeqs(deleteSnapshotSeq1),\n         fileSeqs(deleteSnapshotSeq1),\n         ids(deleteSnapshotId1),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(ManifestEntry.Status.EXISTING));\n     validateDeleteManifest(\n         deleteManifests.get(1),\n@@ -1555,7 +1555,7 @@ public void testDeleteManifestReplacementConcurrentDeleteFileRemoval() throws IO\n         dataSeqs(deleteSnapshotSeq2, deleteSnapshotSeq2),\n         fileSeqs(deleteSnapshotSeq2, deleteSnapshotSeq2),\n         ids(concurrentSnapshotId, deleteSnapshotId2),\n-        files(FILE_B_DELETES, FILE_C2_DELETES),\n+        files(fileBDeletes(), FILE_C2_DELETES),\n         statuses(ManifestEntry.Status.DELETED, ManifestEntry.Status.EXISTING));\n   }\n \n@@ -1567,7 +1567,7 @@ public void testDeleteManifestReplacementConflictingDeleteFileRemoval() throws I\n     table.newFastAppend().appendFile(FILE_A).appendFile(FILE_B).appendFile(FILE_C).commit();\n \n     // commit delete files\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).addDeletes(FILE_A2_DELETES).commit();\n \n     // save the delete snapshot info\n     Snapshot deleteSnapshot = table.currentSnapshot();\n@@ -1584,7 +1584,7 @@ public void testDeleteManifestReplacementConflictingDeleteFileRemoval() throws I\n                 deleteSnapshotId,\n                 deleteSnapshotSeq,\n                 deleteSnapshotSeq,\n-                FILE_A_DELETES));\n+                fileADeletes()));\n     ManifestFile newDeleteManifest2 =\n         writeManifest(\n             \"delete-manifest-file-2.avro\",\n@@ -1602,7 +1602,7 @@ public void testDeleteManifestReplacementConflictingDeleteFileRemoval() throws I\n     rewriteManifests.addManifest(newDeleteManifest2);\n \n     // modify the original delete manifest concurrently\n-    table.newRewrite().deleteFile(FILE_A_DELETES).commit();\n+    table.newRewrite().deleteFile(fileADeletes()).commit();\n \n     // the rewrite must fail as the original delete manifest was replaced concurrently\n     assertThatThrownBy(rewriteManifests::commit)\n@@ -1621,7 +1621,7 @@ public void testDeleteManifestReplacementFailure() throws IOException {\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n     // commit the first delete file\n-    table.newRowDelta().addDeletes(FILE_A_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).commit();\n \n     // save the first delete snapshot info\n     Snapshot deleteSnapshot1 = table.currentSnapshot();\n@@ -1648,7 +1648,7 @@ public void testDeleteManifestReplacementFailure() throws IOException {\n                 deleteSnapshotId1,\n                 deleteSnapshotSeq1,\n                 deleteSnapshotSeq1,\n-                FILE_A_DELETES),\n+                fileADeletes()),\n             manifestEntry(\n                 ManifestEntry.Status.EXISTING,\n                 deleteSnapshotId2,\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestRowDelta.java b/core/src/test/java/org/apache/iceberg/TestRowDelta.java\nindex 1d67e48a2ce2..b41be0c7a636 100644\n--- a/core/src/test/java/org/apache/iceberg/TestRowDelta.java\n+++ b/core/src/test/java/org/apache/iceberg/TestRowDelta.java\n@@ -29,7 +29,9 @@\n import static org.apache.iceberg.util.SnapshotUtil.latestSnapshot;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n+import java.io.IOException;\n import java.util.Arrays;\n import java.util.List;\n import java.util.Map;\n@@ -38,8 +40,11 @@\n import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.expressions.Expression;\n import org.apache.iceberg.expressions.Expressions;\n+import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n@@ -52,13 +57,17 @@ public class TestRowDelta extends V2TableTestBase {\n \n   @Parameters(name = \"formatVersion = {0}, branch = {1}\")\n   protected static List<Object> parameters() {\n-    return Arrays.asList(new Object[] {2, \"main\"}, new Object[] {2, \"testBranch\"});\n+    return Arrays.asList(\n+        new Object[] {2, \"main\"},\n+        new Object[] {2, \"testBranch\"},\n+        new Object[] {3, \"main\"},\n+        new Object[] {3, \"testBranch\"});\n   }\n \n   @TestTemplate\n   public void addOnlyDeleteFilesProducesDeleteOperation() {\n     SnapshotUpdate<?> rowDelta =\n-        table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_B_DELETES);\n+        table.newRowDelta().addDeletes(fileADeletes()).addDeletes(fileBDeletes());\n \n     commit(table, rowDelta, branch);\n     Snapshot snap = latestSnapshot(table, branch);\n@@ -70,7 +79,7 @@ public void addOnlyDeleteFilesProducesDeleteOperation() {\n   @TestTemplate\n   public void testAddDeleteFile() {\n     SnapshotUpdate<?> rowDelta =\n-        table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_DELETES).addDeletes(FILE_B_DELETES);\n+        table.newRowDelta().addRows(FILE_A).addDeletes(fileADeletes()).addDeletes(fileBDeletes());\n \n     commit(table, rowDelta, branch);\n     Snapshot snap = latestSnapshot(table, branch);\n@@ -95,7 +104,7 @@ public void testAddDeleteFile() {\n         dataSeqs(1L, 1L),\n         fileSeqs(1L, 1L),\n         ids(snap.snapshotId(), snap.snapshotId()),\n-        files(FILE_A_DELETES, FILE_B_DELETES),\n+        files(fileADeletes(), fileBDeletes()),\n         statuses(Status.ADDED, Status.ADDED));\n   }\n \n@@ -126,7 +135,7 @@ public void testValidateDataFilesExistDefaults() {\n                     table,\n                     table\n                         .newRowDelta()\n-                        .addDeletes(FILE_A_DELETES)\n+                        .addDeletes(fileADeletes())\n                         .validateFromSnapshot(validateFromSnapshotId)\n                         .validateDataFilesExist(ImmutableList.of(FILE_A.path())),\n                     branch))\n@@ -143,7 +152,7 @@ public void testValidateDataFilesExistDefaults() {\n         table,\n         table\n             .newRowDelta()\n-            .addDeletes(FILE_B_DELETES)\n+            .addDeletes(fileBDeletes())\n             .validateDataFilesExist(ImmutableList.of(FILE_B.path()))\n             .validateFromSnapshot(validateFromSnapshotId),\n         branch);\n@@ -155,7 +164,7 @@ public void testValidateDataFilesExistDefaults() {\n         dataSeqs(4L),\n         fileSeqs(4L),\n         ids(latestSnapshot(table, branch).snapshotId()),\n-        files(FILE_B_DELETES),\n+        files(fileBDeletes()),\n         statuses(Status.ADDED));\n   }\n \n@@ -177,7 +186,7 @@ public void testValidateDataFilesExistOverwrite() {\n                     table,\n                     table\n                         .newRowDelta()\n-                        .addDeletes(FILE_A_DELETES)\n+                        .addDeletes(fileADeletes())\n                         .validateFromSnapshot(validateFromSnapshotId)\n                         .validateDataFilesExist(ImmutableList.of(FILE_A.path())),\n                     branch))\n@@ -209,7 +218,7 @@ public void testValidateDataFilesExistReplacePartitions() {\n                     table,\n                     table\n                         .newRowDelta()\n-                        .addDeletes(FILE_A_DELETES)\n+                        .addDeletes(fileADeletes())\n                         .validateFromSnapshot(validateFromSnapshotId)\n                         .validateDataFilesExist(ImmutableList.of(FILE_A.path())),\n                     branch))\n@@ -242,7 +251,7 @@ public void testValidateDataFilesExistFromSnapshot() {\n         table,\n         table\n             .newRowDelta()\n-            .addDeletes(FILE_A_DELETES)\n+            .addDeletes(fileADeletes())\n             .validateFromSnapshot(validateFromSnapshotId)\n             .validateDataFilesExist(ImmutableList.of(FILE_A.path())),\n         branch);\n@@ -276,7 +285,7 @@ public void testValidateDataFilesExistFromSnapshot() {\n         dataSeqs(3L),\n         fileSeqs(3L),\n         ids(snap.snapshotId()),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(Status.ADDED));\n   }\n \n@@ -301,7 +310,7 @@ public void testValidateDataFilesExistRewrite() {\n                     table,\n                     table\n                         .newRowDelta()\n-                        .addDeletes(FILE_A_DELETES)\n+                        .addDeletes(fileADeletes())\n                         .validateFromSnapshot(validateFromSnapshotId)\n                         .validateDataFilesExist(ImmutableList.of(FILE_A.path())),\n                     branch))\n@@ -333,7 +342,7 @@ public void testValidateDataFilesExistValidateDeletes() {\n                     table,\n                     table\n                         .newRowDelta()\n-                        .addDeletes(FILE_A_DELETES)\n+                        .addDeletes(fileADeletes())\n                         .validateDeletedFiles()\n                         .validateFromSnapshot(validateFromSnapshotId)\n                         .validateDataFilesExist(ImmutableList.of(FILE_A.path())),\n@@ -366,7 +375,7 @@ public void testValidateNoConflicts() {\n                     table,\n                     table\n                         .newRowDelta()\n-                        .addDeletes(FILE_A_DELETES)\n+                        .addDeletes(fileADeletes())\n                         .validateFromSnapshot(validateFromSnapshotId)\n                         .conflictDetectionFilter(\n                             Expressions.equal(\"data\", \"u\")) // bucket16(\"u\") -> 0\n@@ -399,7 +408,7 @@ public void testValidateNoConflictsFromSnapshot() {\n         table,\n         table\n             .newRowDelta()\n-            .addDeletes(FILE_A_DELETES)\n+            .addDeletes(fileADeletes())\n             .validateDeletedFiles()\n             .validateFromSnapshot(validateFromSnapshotId)\n             .validateDataFilesExist(ImmutableList.of(FILE_A.path()))\n@@ -436,7 +445,7 @@ public void testValidateNoConflictsFromSnapshot() {\n         dataSeqs(3L),\n         fileSeqs(3L),\n         ids(snap.snapshotId()),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(Status.ADDED));\n   }\n \n@@ -444,7 +453,7 @@ public void testValidateNoConflictsFromSnapshot() {\n   public void testOverwriteWithDeleteFile() {\n     commit(\n         table,\n-        table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_DELETES).addDeletes(FILE_B_DELETES),\n+        table.newRowDelta().addRows(FILE_A).addDeletes(fileADeletes()).addDeletes(fileBDeletes()),\n         branch);\n \n     long deltaSnapshotId = latestSnapshot(table, branch).snapshotId();\n@@ -479,7 +488,7 @@ public void testOverwriteWithDeleteFile() {\n         dataSeqs(1L, 1L),\n         fileSeqs(1L, 1L),\n         ids(snap.snapshotId(), deltaSnapshotId),\n-        files(FILE_A_DELETES, FILE_B_DELETES),\n+        files(fileADeletes(), fileBDeletes()),\n         statuses(Status.DELETED, Status.EXISTING));\n   }\n \n@@ -487,7 +496,7 @@ public void testOverwriteWithDeleteFile() {\n   public void testReplacePartitionsWithDeleteFile() {\n     commit(\n         table,\n-        table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_DELETES).addDeletes(FILE_B_DELETES),\n+        table.newRowDelta().addRows(FILE_A).addDeletes(fileADeletes()).addDeletes(fileBDeletes()),\n         branch);\n \n     long deltaSnapshotId = latestSnapshot(table, branch).snapshotId();\n@@ -526,7 +535,7 @@ public void testReplacePartitionsWithDeleteFile() {\n         dataSeqs(1L, 1L),\n         fileSeqs(1L, 1L),\n         ids(snap.snapshotId(), deltaSnapshotId),\n-        files(FILE_A_DELETES, FILE_B_DELETES),\n+        files(fileADeletes(), fileBDeletes()),\n         statuses(Status.DELETED, Status.EXISTING));\n   }\n \n@@ -534,7 +543,7 @@ public void testReplacePartitionsWithDeleteFile() {\n   public void testDeleteByExpressionWithDeleteFile() {\n     commit(\n         table,\n-        table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_DELETES).addDeletes(FILE_B_DELETES),\n+        table.newRowDelta().addRows(FILE_A).addDeletes(fileADeletes()).addDeletes(fileBDeletes()),\n         branch);\n \n     long deltaSnapshotId = latestSnapshot(table, branch).snapshotId();\n@@ -564,13 +573,13 @@ public void testDeleteByExpressionWithDeleteFile() {\n         dataSeqs(1L, 1L),\n         fileSeqs(1L, 1L),\n         ids(snap.snapshotId(), snap.snapshotId()),\n-        files(FILE_A_DELETES, FILE_B_DELETES),\n+        files(fileADeletes(), fileBDeletes()),\n         statuses(Status.DELETED, Status.DELETED));\n   }\n \n   @TestTemplate\n   public void testDeleteDataFileWithDeleteFile() {\n-    commit(table, table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_DELETES), branch);\n+    commit(table, table.newRowDelta().addRows(FILE_A).addDeletes(fileADeletes()), branch);\n \n     long deltaSnapshotId = latestSnapshot(table, branch).snapshotId();\n     assertThat(latestSnapshot(table, branch).sequenceNumber()).isEqualTo(1);\n@@ -598,7 +607,7 @@ public void testDeleteDataFileWithDeleteFile() {\n         dataSeqs(1L),\n         fileSeqs(1L),\n         ids(deltaSnapshotId),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(Status.ADDED));\n \n     // the manifest that removed FILE_A will be dropped next commit, causing the min sequence number\n@@ -619,13 +628,13 @@ public void testDeleteDataFileWithDeleteFile() {\n         dataSeqs(1L),\n         fileSeqs(1L),\n         ids(nextSnap.snapshotId()),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(Status.DELETED));\n   }\n \n   @TestTemplate\n   public void testFastAppendDoesNotRemoveStaleDeleteFiles() {\n-    commit(table, table.newRowDelta().addRows(FILE_A).addDeletes(FILE_A_DELETES), branch);\n+    commit(table, table.newRowDelta().addRows(FILE_A).addDeletes(fileADeletes()), branch);\n \n     long deltaSnapshotId = latestSnapshot(table, branch).snapshotId();\n     assertThat(latestSnapshot(table, branch).sequenceNumber()).isEqualTo(1);\n@@ -653,7 +662,7 @@ public void testFastAppendDoesNotRemoveStaleDeleteFiles() {\n         dataSeqs(1L),\n         fileSeqs(1L),\n         ids(deltaSnapshotId),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(Status.ADDED));\n \n     // the manifest that removed FILE_A will be dropped next merging commit, but FastAppend will not\n@@ -689,7 +698,7 @@ public void testFastAppendDoesNotRemoveStaleDeleteFiles() {\n         dataSeqs(1L),\n         fileSeqs(1L),\n         ids(deltaSnapshotId),\n-        files(FILE_A_DELETES),\n+        files(fileADeletes()),\n         statuses(Status.ADDED));\n   }\n \n@@ -728,14 +737,7 @@ public void testValidateDataFilesExistWithConflictDetectionFilter() {\n     Snapshot baseSnapshot = latestSnapshot(table, branch);\n \n     // add a delete file for partition A\n-    DeleteFile deleteFile =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\"/path/to/data-a-deletes.parquet\")\n-            .withFileSizeInBytes(10)\n-            .withPartitionPath(\"data=a\")\n-            .withRecordCount(1)\n-            .build();\n+    DeleteFile deleteFile = newDeletes(dataFile1);\n \n     Expression conflictDetectionFilter = Expressions.equal(\"data\", \"a\");\n     RowDelta rowDelta =\n@@ -789,14 +791,7 @@ public void testValidateDataFilesDoNotExistWithConflictDetectionFilter() {\n     Snapshot baseSnapshot = latestSnapshot(table, branch);\n \n     // add a delete file for partition A\n-    DeleteFile deleteFile =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\"/path/to/data-a-deletes.parquet\")\n-            .withFileSizeInBytes(10)\n-            .withPartitionPath(\"data=a\")\n-            .withRecordCount(1)\n-            .build();\n+    DeleteFile deleteFile = newDeletes(dataFile1);\n \n     Expression conflictDetectionFilter = Expressions.equal(\"data\", \"a\");\n     RowDelta rowDelta =\n@@ -847,9 +842,9 @@ public void testAddDeleteFilesMultipleSpecs() {\n     // commit a row delta with 1 data file and 3 delete files where delete files have different\n     // specs\n     DataFile dataFile = newDataFile(\"data=xyz\");\n-    DeleteFile firstDeleteFile = newDeleteFile(firstSnapshotDataFile.specId(), \"data_bucket=0\");\n-    DeleteFile secondDeleteFile = newDeleteFile(secondSnapshotDataFile.specId(), \"\");\n-    DeleteFile thirdDeleteFile = newDeleteFile(thirdSnapshotDataFile.specId(), \"data=abc\");\n+    DeleteFile firstDeleteFile = newDeletes(firstSnapshotDataFile);\n+    DeleteFile secondDeleteFile = newDeletes(secondSnapshotDataFile);\n+    DeleteFile thirdDeleteFile = newDeletes(thirdSnapshotDataFile);\n \n     commit(\n         table,\n@@ -867,6 +862,7 @@ public void testAddDeleteFilesMultipleSpecs() {\n     assertThat(snapshot.operation()).isEqualTo(DataOperations.OVERWRITE);\n \n     Map<String, String> summary = snapshot.summary();\n+    long posDeletesCount = recordCount(firstDeleteFile, secondDeleteFile, thirdDeleteFile);\n \n     assertThat(summary)\n         .containsEntry(CHANGED_PARTITION_COUNT_PROP, \"4\")\n@@ -874,8 +870,8 @@ public void testAddDeleteFilesMultipleSpecs() {\n         .containsEntry(TOTAL_DATA_FILES_PROP, \"4\")\n         .containsEntry(ADDED_DELETE_FILES_PROP, \"3\")\n         .containsEntry(TOTAL_DELETE_FILES_PROP, \"3\")\n-        .containsEntry(ADDED_POS_DELETES_PROP, \"3\")\n-        .containsEntry(TOTAL_POS_DELETES_PROP, \"3\")\n+        .containsEntry(ADDED_POS_DELETES_PROP, String.valueOf(posDeletesCount))\n+        .containsEntry(TOTAL_POS_DELETES_PROP, String.valueOf(posDeletesCount))\n         .hasEntrySatisfying(\n             CHANGED_PARTITION_PREFIX + \"data_bucket=0\",\n             v -> assertThat(v).contains(ADDED_DELETE_FILES_PROP + \"=1\"))\n@@ -953,8 +949,8 @@ public void testManifestMergingMultipleSpecs() {\n     commit(table, table.newAppend().appendFile(secondSnapshotDataFile), branch);\n \n     // commit two delete files to two specs in a single operation\n-    DeleteFile firstDeleteFile = newDeleteFile(firstSnapshotDataFile.specId(), \"data_bucket=0\");\n-    DeleteFile secondDeleteFile = newDeleteFile(secondSnapshotDataFile.specId(), \"\");\n+    DeleteFile firstDeleteFile = newDeletes(firstSnapshotDataFile);\n+    DeleteFile secondDeleteFile = newDeletes(secondSnapshotDataFile);\n \n     commit(\n         table,\n@@ -968,12 +964,18 @@ public void testManifestMergingMultipleSpecs() {\n     assertThat(thirdSnapshot.deleteManifests(table.io())).hasSize(2);\n \n     // commit two more delete files to the same specs to trigger merging\n-    DeleteFile thirdDeleteFile = newDeleteFile(firstSnapshotDataFile.specId(), \"data_bucket=0\");\n-    DeleteFile fourthDeleteFile = newDeleteFile(secondSnapshotDataFile.specId(), \"\");\n+    DeleteFile thirdDeleteFile = newDeletes(firstSnapshotDataFile);\n+    DeleteFile fourthDeleteFile = newDeletes(secondSnapshotDataFile);\n \n     commit(\n         table,\n-        table.newRowDelta().addDeletes(thirdDeleteFile).addDeletes(fourthDeleteFile),\n+        table\n+            .newRowDelta()\n+            .removeDeletes(firstDeleteFile)\n+            .addDeletes(thirdDeleteFile)\n+            .removeDeletes(secondDeleteFile)\n+            .addDeletes(fourthDeleteFile)\n+            .validateFromSnapshot(thirdSnapshot.snapshotId()),\n         branch);\n \n     Snapshot fourthSnapshot = latestSnapshot(table, branch);\n@@ -988,9 +990,9 @@ public void testManifestMergingMultipleSpecs() {\n         firstDeleteManifest,\n         dataSeqs(4L, 3L),\n         fileSeqs(4L, 3L),\n-        ids(fourthSnapshot.snapshotId(), thirdSnapshot.snapshotId()),\n+        ids(fourthSnapshot.snapshotId(), fourthSnapshot.snapshotId()),\n         files(thirdDeleteFile, firstDeleteFile),\n-        statuses(Status.ADDED, Status.EXISTING));\n+        statuses(Status.ADDED, Status.DELETED));\n \n     ManifestFile secondDeleteManifest = fourthSnapshot.deleteManifests(table.io()).get(0);\n     assertThat(secondDeleteManifest.partitionSpecId()).isEqualTo(secondSnapshotDataFile.specId());\n@@ -998,9 +1000,9 @@ public void testManifestMergingMultipleSpecs() {\n         secondDeleteManifest,\n         dataSeqs(4L, 3L),\n         fileSeqs(4L, 3L),\n-        ids(fourthSnapshot.snapshotId(), thirdSnapshot.snapshotId()),\n+        ids(fourthSnapshot.snapshotId(), fourthSnapshot.snapshotId()),\n         files(fourthDeleteFile, secondDeleteFile),\n-        statuses(Status.ADDED, Status.EXISTING));\n+        statuses(Status.ADDED, Status.DELETED));\n   }\n \n   @TestTemplate\n@@ -1019,8 +1021,8 @@ public void testAbortMultipleSpecs() {\n     commit(table, table.newAppend().appendFile(secondSnapshotDataFile), branch);\n \n     // prepare two delete files that belong to different specs\n-    DeleteFile firstDeleteFile = newDeleteFile(firstSnapshotDataFile.specId(), \"data_bucket=0\");\n-    DeleteFile secondDeleteFile = newDeleteFile(secondSnapshotDataFile.specId(), \"\");\n+    DeleteFile firstDeleteFile = newDeletes(firstSnapshotDataFile);\n+    DeleteFile secondDeleteFile = newDeletes(secondSnapshotDataFile);\n \n     // capture all deletes\n     Set<String> deletedFiles = Sets.newHashSet();\n@@ -1062,7 +1064,7 @@ public void testConcurrentConflictingRowDelta() {\n             .newRowDelta()\n             .toBranch(branch)\n             .addRows(FILE_B)\n-            .addDeletes(FILE_A_DELETES)\n+            .addDeletes(fileADeletes())\n             .validateFromSnapshot(firstSnapshot.snapshotId())\n             .conflictDetectionFilter(conflictDetectionFilter)\n             .validateNoConflictingDataFiles()\n@@ -1071,7 +1073,7 @@ public void testConcurrentConflictingRowDelta() {\n     table\n         .newRowDelta()\n         .toBranch(branch)\n-        .addDeletes(FILE_A_DELETES)\n+        .addDeletes(fileADeletes())\n         .validateFromSnapshot(firstSnapshot.snapshotId())\n         .conflictDetectionFilter(conflictDetectionFilter)\n         .validateNoConflictingDataFiles()\n@@ -1094,7 +1096,7 @@ public void testConcurrentConflictingRowDeltaWithoutAppendValidation() {\n     RowDelta rowDelta =\n         table\n             .newRowDelta()\n-            .addDeletes(FILE_A_DELETES)\n+            .addDeletes(fileADeletes())\n             .validateFromSnapshot(firstSnapshot.snapshotId())\n             .conflictDetectionFilter(conflictDetectionFilter)\n             .validateNoConflictingDeleteFiles();\n@@ -1102,7 +1104,7 @@ public void testConcurrentConflictingRowDeltaWithoutAppendValidation() {\n     table\n         .newRowDelta()\n         .toBranch(branch)\n-        .addDeletes(FILE_A_DELETES)\n+        .addDeletes(fileADeletes())\n         .validateFromSnapshot(firstSnapshot.snapshotId())\n         .conflictDetectionFilter(conflictDetectionFilter)\n         .validateNoConflictingDataFiles()\n@@ -1149,14 +1151,7 @@ public void testConcurrentNonConflictingRowDelta() {\n     Expression conflictDetectionFilter = Expressions.equal(\"data\", \"a\");\n \n     // add a delete file for partition A\n-    DeleteFile deleteFile1 =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\"/path/to/data-a-deletes.parquet\")\n-            .withFileSizeInBytes(10)\n-            .withPartitionPath(\"data=a\")\n-            .withRecordCount(1)\n-            .build();\n+    DeleteFile deleteFile1 = newDeletes(dataFile1);\n \n     // mock a DELETE operation with serializable isolation\n     RowDelta rowDelta =\n@@ -1170,14 +1165,7 @@ public void testConcurrentNonConflictingRowDelta() {\n             .validateNoConflictingDeleteFiles();\n \n     // add a delete file for partition B\n-    DeleteFile deleteFile2 =\n-        FileMetadata.deleteFileBuilder(table.spec())\n-            .ofPositionDeletes()\n-            .withPath(\"/path/to/data-b-deletes.parquet\")\n-            .withFileSizeInBytes(10)\n-            .withPartitionPath(\"data=b\")\n-            .withRecordCount(1)\n-            .build();\n+    DeleteFile deleteFile2 = newDeletes(dataFile2);\n \n     table\n         .newRowDelta()\n@@ -1320,8 +1308,8 @@ public void testConcurrentConflictingRowDeltaAndRewriteFilesWithSequenceNumber()\n \n     Snapshot baseSnapshot = latestSnapshot(table, branch);\n \n-    // add an position delete file\n-    DeleteFile deleteFile1 = newDeleteFile(table.spec().specId(), \"data=a\");\n+    // add position deletes\n+    DeleteFile deleteFile1 = newDeletes(dataFile1);\n \n     // mock a DELETE operation with serializable isolation\n     RowDelta rowDelta =\n@@ -1357,7 +1345,7 @@ public void testRowDeltaCaseSensitivity() {\n \n     Snapshot firstSnapshot = latestSnapshot(table, branch);\n \n-    commit(table, table.newRowDelta().addDeletes(FILE_A_DELETES), branch);\n+    commit(table, table.newRowDelta().addDeletes(fileADeletes()), branch);\n \n     Expression conflictDetectionFilter = Expressions.equal(Expressions.bucket(\"dAtA\", 16), 0);\n \n@@ -1413,12 +1401,12 @@ public void testRowDeltaCaseSensitivity() {\n   @TestTemplate\n   public void testRewrittenDeleteFiles() {\n     DataFile dataFile = newDataFile(\"data_bucket=0\");\n-    DeleteFile deleteFile = newDeleteFile(dataFile.specId(), \"data_bucket=0\");\n+    DeleteFile deleteFile = newDeletes(dataFile);\n     RowDelta baseRowDelta = table.newRowDelta().addRows(dataFile).addDeletes(deleteFile);\n     Snapshot baseSnapshot = commit(table, baseRowDelta, branch);\n     assertThat(baseSnapshot.operation()).isEqualTo(DataOperations.OVERWRITE);\n \n-    DeleteFile newDeleteFile = newDeleteFile(dataFile.specId(), \"data_bucket=0\");\n+    DeleteFile newDeleteFile = newDeletes(dataFile);\n     RowDelta rowDelta =\n         table\n             .newRowDelta()\n@@ -1458,14 +1446,16 @@ public void testRewrittenDeleteFiles() {\n \n   @TestTemplate\n   public void testConcurrentDeletesRewriteSameDeleteFile() {\n+    assumeThat(formatVersion).isEqualTo(2);\n+\n     DataFile dataFile = newDataFile(\"data_bucket=0\");\n-    DeleteFile deleteFile = newDeleteFile(dataFile.specId(), \"data_bucket=0\");\n+    DeleteFile deleteFile = newDeletes(dataFile);\n     RowDelta baseRowDelta = table.newRowDelta().addRows(dataFile).addDeletes(deleteFile);\n     Snapshot baseSnapshot = commit(table, baseRowDelta, branch);\n     assertThat(baseSnapshot.operation()).isEqualTo(DataOperations.OVERWRITE);\n \n     // commit the first DELETE operation that replaces `deleteFile`\n-    DeleteFile newDeleteFile1 = newDeleteFile(dataFile.specId(), \"data_bucket=0\");\n+    DeleteFile newDeleteFile1 = newDeletes(dataFile);\n     RowDelta delete1 =\n         table\n             .newRowDelta()\n@@ -1478,7 +1468,7 @@ public void testConcurrentDeletesRewriteSameDeleteFile() {\n     assertThat(snapshot1.sequenceNumber()).isEqualTo(2L);\n \n     // commit the second DELETE operation that replaces `deleteFile`\n-    DeleteFile newDeleteFile2 = newDeleteFile(dataFile.specId(), \"data_bucket=0\");\n+    DeleteFile newDeleteFile2 = newDeletes(dataFile);\n     RowDelta delete2 =\n         table\n             .newRowDelta()\n@@ -1522,13 +1512,13 @@ public void testConcurrentDeletesRewriteSameDeleteFile() {\n   @TestTemplate\n   public void testConcurrentMergeRewriteSameDeleteFile() {\n     DataFile dataFile = newDataFile(\"data_bucket=0\");\n-    DeleteFile deleteFile = newDeleteFile(dataFile.specId(), \"data_bucket=0\");\n+    DeleteFile deleteFile = newDeletes(dataFile);\n     RowDelta baseRowDelta = table.newRowDelta().addRows(dataFile).addDeletes(deleteFile);\n     Snapshot baseSnapshot = commit(table, baseRowDelta, branch);\n     assertThat(baseSnapshot.operation()).isEqualTo(DataOperations.OVERWRITE);\n \n     // commit a DELETE operation that replaces `deleteFile`\n-    DeleteFile newDeleteFile1 = newDeleteFile(dataFile.specId(), \"data_bucket=0\");\n+    DeleteFile newDeleteFile1 = newDeletes(dataFile);\n     RowDelta delete =\n         table\n             .newRowDelta()\n@@ -1540,7 +1530,7 @@ public void testConcurrentMergeRewriteSameDeleteFile() {\n \n     // attempt to commit a MERGE operation that replaces `deleteFile`\n     DataFile newDataFile2 = newDataFile(\"data_bucket=0\");\n-    DeleteFile newDeleteFile2 = newDeleteFile(dataFile.specId(), \"data_bucket=0\");\n+    DeleteFile newDeleteFile2 = newDeletes(dataFile);\n     RowDelta merge =\n         table\n             .newRowDelta()\n@@ -1556,4 +1546,102 @@ public void testConcurrentMergeRewriteSameDeleteFile() {\n         .isInstanceOf(ValidationException.class)\n         .hasMessageStartingWith(\"Found new conflicting delete files that can apply\");\n   }\n+\n+  @TestTemplate\n+  public void testConcurrentDVsForSameDataFile() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(3);\n+\n+    DataFile dataFile = newDataFile(\"data_bucket=0\");\n+    commit(table, table.newRowDelta().addRows(dataFile), branch);\n+\n+    DeleteFile deleteFile1 = newDeletes(dataFile);\n+    RowDelta rowDelta1 = table.newRowDelta().addDeletes(deleteFile1);\n+\n+    DeleteFile deleteFile2 = newDeletes(dataFile);\n+    RowDelta rowDelta2 = table.newRowDelta().addDeletes(deleteFile2);\n+\n+    commit(table, rowDelta1, branch);\n+\n+    assertThatThrownBy(() -> commit(table, rowDelta2, branch))\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessageContaining(\"Found concurrently added DV for %s\", dataFile.location());\n+  }\n+\n+  @TestTemplate\n+  public void testManifestMergingAfterUpgradeToV3() {\n+    assumeThat(formatVersion).isEqualTo(2);\n+\n+    // enable manifest merging\n+    table\n+        .updateProperties()\n+        .set(TableProperties.MANIFEST_MERGE_ENABLED, \"true\")\n+        .set(TableProperties.MANIFEST_MIN_MERGE_COUNT, \"2\")\n+        .commit();\n+\n+    // add a data file\n+    DataFile dataFile = newDataFile(\"data_bucket=0\");\n+    commit(table, table.newAppend().appendFile(dataFile), branch);\n+\n+    // commit a delete operation using a positional delete file\n+    DeleteFile deleteFile = newDeleteFileWithRef(dataFile);\n+    assertThat(deleteFile.format()).isEqualTo(FileFormat.PARQUET);\n+    RowDelta rowDelta1 = table.newRowDelta().addDeletes(deleteFile);\n+    Snapshot deleteFileSnapshot = commit(table, rowDelta1, branch);\n+\n+    // upgrade the table\n+    table.updateProperties().set(TableProperties.FORMAT_VERSION, \"3\").commit();\n+\n+    // commit a DV\n+    DeleteFile dv = newDV(dataFile);\n+    assertThat(dv.format()).isEqualTo(FileFormat.PUFFIN);\n+    RowDelta rowDelta2 = table.newRowDelta().addDeletes(dv);\n+    Snapshot dvSnapshot = commit(table, rowDelta2, branch);\n+\n+    // both must be part of the table and merged into one manifest\n+    ManifestFile deleteManifest = Iterables.getOnlyElement(dvSnapshot.deleteManifests(table.io()));\n+    validateDeleteManifest(\n+        deleteManifest,\n+        dataSeqs(3L, 2L),\n+        fileSeqs(3L, 2L),\n+        ids(dvSnapshot.snapshotId(), deleteFileSnapshot.snapshotId()),\n+        files(dv, deleteFile),\n+        statuses(Status.ADDED, Status.EXISTING));\n+\n+    // only the DV must be assigned during planning\n+    List<ScanTask> tasks = planFiles();\n+    FileScanTask task = Iterables.getOnlyElement(tasks).asFileScanTask();\n+    assertThat(task.deletes()).hasSize(1);\n+    DeleteFile taskDV = Iterables.getOnlyElement(task.deletes());\n+    assertThat(taskDV.location()).isEqualTo(dv.location());\n+    assertThat(taskDV.referencedDataFile()).isEqualTo(dv.referencedDataFile());\n+    assertThat(taskDV.contentOffset()).isEqualTo(dv.contentOffset());\n+    assertThat(taskDV.contentSizeInBytes()).isEqualTo(dv.contentSizeInBytes());\n+  }\n+\n+  @TestTemplate\n+  public void testInabilityToAddPositionDeleteFilesInTablesWithDVs() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(3);\n+    DeleteFile deleteFile = newDeleteFile(table.spec().specId(), \"data_bucket=0\");\n+    assertThatThrownBy(() -> table.newRowDelta().addDeletes(deleteFile))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"Must use DVs for position deletes in V%s\", formatVersion);\n+  }\n+\n+  @TestTemplate\n+  public void testInabilityToAddDVToV2Tables() {\n+    assumeThat(formatVersion).isEqualTo(2);\n+    DataFile dataFile = newDataFile(\"data_bucket=0\");\n+    DeleteFile dv = newDV(dataFile);\n+    assertThatThrownBy(() -> table.newRowDelta().addDeletes(dv))\n+        .isInstanceOf(IllegalArgumentException.class)\n+        .hasMessageContaining(\"Must not use DVs for position deletes in V2\");\n+  }\n+\n+  private List<ScanTask> planFiles() {\n+    try (CloseableIterable<ScanTask> tasks = table.newBatchScan().useRef(branch).planFiles()) {\n+      return Lists.newArrayList(tasks);\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+  }\n }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestSnapshot.java b/core/src/test/java/org/apache/iceberg/TestSnapshot.java\nindex 8a30036f3242..bbe5e8f6cdd8 100644\n--- a/core/src/test/java/org/apache/iceberg/TestSnapshot.java\n+++ b/core/src/test/java/org/apache/iceberg/TestSnapshot.java\n@@ -123,7 +123,7 @@ public void testCachedDeleteFiles() {\n     int specId = table.spec().specId();\n \n     DataFile secondSnapshotDataFile = newDataFile(\"data_bucket=8/data_trunc_2=aa\");\n-    DeleteFile secondSnapshotDeleteFile = newDeleteFile(specId, \"data_bucket=8/data_trunc_2=aa\");\n+    DeleteFile secondSnapshotDeleteFile = newDeletes(secondSnapshotDataFile);\n \n     table\n         .newRowDelta()\n@@ -131,7 +131,7 @@ public void testCachedDeleteFiles() {\n         .addDeletes(secondSnapshotDeleteFile)\n         .commit();\n \n-    DeleteFile thirdSnapshotDeleteFile = newDeleteFile(specId, \"data_bucket=8/data_trunc_2=aa\");\n+    DeleteFile thirdSnapshotDeleteFile = newDeletes(secondSnapshotDataFile);\n \n     ImmutableSet<DeleteFile> replacedDeleteFiles = ImmutableSet.of(secondSnapshotDeleteFile);\n     ImmutableSet<DeleteFile> newDeleteFiles = ImmutableSet.of(thirdSnapshotDeleteFile);\n@@ -248,11 +248,9 @@ public void testSequenceNumbersInAddedDeleteFiles() {\n \n     table.newFastAppend().appendFile(FILE_A).appendFile(FILE_B).commit();\n \n-    int specId = table.spec().specId();\n-\n-    runAddedDeleteFileSequenceNumberTest(newDeleteFile(specId, \"data_bucket=8\"), 2);\n+    runAddedDeleteFileSequenceNumberTest(newDeletes(FILE_A), 2);\n \n-    runAddedDeleteFileSequenceNumberTest(newDeleteFile(specId, \"data_bucket=28\"), 3);\n+    runAddedDeleteFileSequenceNumberTest(newDeletes(FILE_B), 3);\n   }\n \n   private void runAddedDeleteFileSequenceNumberTest(\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java b/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java\nindex b0b9d003e35b..9c67e766a993 100644\n--- a/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java\n+++ b/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java\n@@ -78,7 +78,7 @@ public void testFileSizeSummary() {\n \n   @TestTemplate\n   public void testFileSizeSummaryWithDeletes() {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n \n     table.newRowDelta().addDeletes(FILE_A_DELETES).addDeletes(FILE_A2_DELETES).commit();\n \n@@ -260,7 +260,7 @@ public void rowDeltaWithDuplicates() {\n \n   @TestTemplate\n   public void rowDeltaWithDeletesAndDuplicates() {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n     assertThat(listManifestFiles()).isEmpty();\n \n     table\n@@ -325,7 +325,7 @@ public void rewriteWithDuplicateFiles() {\n \n   @TestTemplate\n   public void rewriteWithDeletesAndDuplicates() {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n     assertThat(listManifestFiles()).isEmpty();\n \n     table.newRowDelta().addRows(FILE_A2).addDeletes(FILE_A_DELETES).commit();\n\ndiff --git a/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java b/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java\nindex 23e0090ca49f..4e50ee57db41 100644\n--- a/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java\n+++ b/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java\n@@ -34,6 +34,7 @@\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.RowDelta;\n+import org.apache.iceberg.Snapshot;\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.data.BaseDeleteLoader;\n@@ -295,9 +296,13 @@ public void testApplyPartitionScopedPositionDeletes() throws IOException {\n   }\n \n   private void commit(DeleteWriteResult result) {\n+    Snapshot startSnapshot = table.currentSnapshot();\n     RowDelta rowDelta = table.newRowDelta();\n     result.rewrittenDeleteFiles().forEach(rowDelta::removeDeletes);\n     result.deleteFiles().forEach(rowDelta::addDeletes);\n+    if (startSnapshot != null) {\n+      rowDelta.validateFromSnapshot(startSnapshot.snapshotId());\n+    }\n     rowDelta.commit();\n   }\n \n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanDeletes.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanDeletes.java\nindex 9361c63176e0..659507e4c5e3 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanDeletes.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanDeletes.java\n@@ -42,7 +42,11 @@ public static List<Object> parameters() {\n         new Object[] {2, LOCAL, LOCAL},\n         new Object[] {2, LOCAL, DISTRIBUTED},\n         new Object[] {2, DISTRIBUTED, LOCAL},\n-        new Object[] {2, LOCAL, DISTRIBUTED});\n+        new Object[] {2, LOCAL, DISTRIBUTED},\n+        new Object[] {3, LOCAL, LOCAL},\n+        new Object[] {3, LOCAL, DISTRIBUTED},\n+        new Object[] {3, DISTRIBUTED, LOCAL},\n+        new Object[] {3, DISTRIBUTED, DISTRIBUTED});\n   }\n \n   private static SparkSession spark = null;\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanReporting.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanReporting.java\nindex acd4688440d1..2665d7ba8d3b 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanReporting.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/TestSparkDistributedDataScanReporting.java\n@@ -41,7 +41,11 @@ public static List<Object> parameters() {\n         new Object[] {2, LOCAL, LOCAL},\n         new Object[] {2, LOCAL, DISTRIBUTED},\n         new Object[] {2, DISTRIBUTED, LOCAL},\n-        new Object[] {2, DISTRIBUTED, DISTRIBUTED});\n+        new Object[] {2, DISTRIBUTED, DISTRIBUTED},\n+        new Object[] {3, LOCAL, LOCAL},\n+        new Object[] {3, LOCAL, DISTRIBUTED},\n+        new Object[] {3, DISTRIBUTED, LOCAL},\n+        new Object[] {3, DISTRIBUTED, DISTRIBUTED});\n   }\n \n   private static SparkSession spark = null;\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\nindex 79e48f47f241..11d61e599eba 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\n@@ -719,7 +719,7 @@ public void testRewriteLargeManifestsEvolvedUnpartitionedV1Table() throws IOExce\n \n   @TestTemplate\n   public void testRewriteSmallDeleteManifestsNonPartitionedTable() throws IOException {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n \n     PartitionSpec spec = PartitionSpec.unpartitioned();\n     Map<String, String> options = Maps.newHashMap();\n@@ -792,7 +792,7 @@ public void testRewriteSmallDeleteManifestsNonPartitionedTable() throws IOExcept\n \n   @TestTemplate\n   public void testRewriteSmallDeleteManifestsPartitionedTable() throws IOException {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n \n     PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"c3\").build();\n     Map<String, String> options = Maps.newHashMap();\n@@ -895,7 +895,7 @@ public void testRewriteSmallDeleteManifestsPartitionedTable() throws IOException\n \n   @TestTemplate\n   public void testRewriteLargeDeleteManifestsPartitionedTable() throws IOException {\n-    assumeThat(formatVersion).isGreaterThan(1);\n+    assumeThat(formatVersion).isEqualTo(2);\n \n     PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"c3\").build();\n     Map<String, String> options = Maps.newHashMap();\n@@ -956,6 +956,62 @@ public void testRewriteLargeDeleteManifestsPartitionedTable() throws IOException\n     assertThat(deleteManifests).hasSizeGreaterThanOrEqualTo(2);\n   }\n \n+  @TestTemplate\n+  public void testRewriteManifestsAfterUpgradeToV3() throws IOException {\n+    assumeThat(formatVersion).isEqualTo(2);\n+\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+    Map<String, String> options = ImmutableMap.of(TableProperties.FORMAT_VERSION, \"2\");\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    DataFile dataFile1 = newDataFile(table, \"c1=1\");\n+    DeleteFile deleteFile1 = newDeletes(table, dataFile1);\n+    table.newRowDelta().addRows(dataFile1).addDeletes(deleteFile1).commit();\n+\n+    DataFile dataFile2 = newDataFile(table, \"c1=1\");\n+    DeleteFile deleteFile2 = newDeletes(table, dataFile2);\n+    table.newRowDelta().addRows(dataFile2).addDeletes(deleteFile2).commit();\n+\n+    // upgrade the table to enable DVs\n+    table.updateProperties().set(TableProperties.FORMAT_VERSION, \"3\").commit();\n+\n+    DataFile dataFile3 = newDataFile(table, \"c1=1\");\n+    DeleteFile dv3 = newDV(table, dataFile3);\n+    table.newRowDelta().addRows(dataFile3).addDeletes(dv3).commit();\n+\n+    SparkActions actions = SparkActions.get();\n+\n+    RewriteManifests.Result result =\n+        actions\n+            .rewriteManifests(table)\n+            .rewriteIf(manifest -> true)\n+            .option(RewriteManifestsSparkAction.USE_CACHING, useCaching)\n+            .execute();\n+\n+    assertThat(result.rewrittenManifests()).as(\"Action should rewrite 6 manifests\").hasSize(6);\n+    assertThat(result.addedManifests()).as(\"Action should add 2 manifests\").hasSize(2);\n+    assertManifestsLocation(result.addedManifests());\n+\n+    table.refresh();\n+\n+    try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n+      for (FileScanTask fileTask : tasks) {\n+        DataFile dataFile = fileTask.file();\n+        DeleteFile deleteFile = Iterables.getOnlyElement(fileTask.deletes());\n+        if (dataFile.location().equals(dataFile1.location())) {\n+          assertThat(deleteFile.referencedDataFile()).isEqualTo(deleteFile1.referencedDataFile());\n+          assertEqual(deleteFile, deleteFile1);\n+        } else if (dataFile.location().equals(dataFile2.location())) {\n+          assertThat(deleteFile.referencedDataFile()).isEqualTo(deleteFile2.referencedDataFile());\n+          assertEqual(deleteFile, deleteFile2);\n+        } else {\n+          assertThat(deleteFile.referencedDataFile()).isEqualTo(dv3.referencedDataFile());\n+          assertEqual(deleteFile, dv3);\n+        }\n+      }\n+    }\n+  }\n+\n   private List<ThreeColumnRecord> actualRecords() {\n     return spark\n         .read()\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11485",
    "pr_id": 11485,
    "issue_id": 11122,
    "repo": "apache/iceberg",
    "problem_statement": "Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 118,
    "test_files_count": 11,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/BaseContentScanTask.java",
      "core/src/main/java/org/apache/iceberg/BaseFileScanTask.java",
      "core/src/test/java/org/apache/iceberg/TestMetadataTableFilters.java",
      "core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java",
      "data/src/test/java/org/apache/iceberg/data/FileHelpers.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveDanglingDeleteAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/TestMetadataTableFilters.java",
      "core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java",
      "data/src/test/java/org/apache/iceberg/data/FileHelpers.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveDanglingDeleteAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java"
    ],
    "base_commit": "e3f39972863f891481ad9f5a559ffef093976bd7",
    "head_commit": "a2a2634753a2a70bea8946861f99a8de760507fa",
    "repo_url": "https://github.com/apache/iceberg/pull/11485",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11485",
    "dockerfile": "",
    "pr_merged_at": "2024-11-13T15:42:07.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/BaseContentScanTask.java b/core/src/main/java/org/apache/iceberg/BaseContentScanTask.java\nindex 8d38a48309c6..960c04cc0f37 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseContentScanTask.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseContentScanTask.java\n@@ -23,6 +23,7 @@\n import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.util.ArrayUtil;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n abstract class BaseContentScanTask<ThisT extends ContentScanTask<F>, F extends ContentFile<F>>\n     implements ContentScanTask<F>, SplittableScanTask<ThisT> {\n@@ -82,7 +83,7 @@ public long start() {\n \n   @Override\n   public long length() {\n-    return file.fileSizeInBytes();\n+    return ScanTaskUtil.contentSizeInBytes(file);\n   }\n \n   @Override\n\ndiff --git a/core/src/main/java/org/apache/iceberg/BaseFileScanTask.java b/core/src/main/java/org/apache/iceberg/BaseFileScanTask.java\nindex aa37f40be7c0..2cc406444552 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseFileScanTask.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseFileScanTask.java\n@@ -176,6 +176,8 @@ public boolean canMerge(ScanTask other) {\n     @Override\n     public SplitScanTask merge(ScanTask other) {\n       SplitScanTask that = (SplitScanTask) other;\n+      // don't use deletesSizeBytes() here so that deletesSizeBytes is only calculated once after\n+      // merging rather than for each task before merging\n       return new SplitScanTask(offset, len + that.length(), fileScanTask, deletesSizeBytes);\n     }\n \n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/TestMetadataTableFilters.java b/core/src/test/java/org/apache/iceberg/TestMetadataTableFilters.java\nindex f8c34019875f..7c5a860db15f 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetadataTableFilters.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetadataTableFilters.java\n@@ -52,18 +52,26 @@ protected static List<Object> parameters() {\n     return Arrays.asList(\n         new Object[] {1, MetadataTableType.DATA_FILES},\n         new Object[] {2, MetadataTableType.DATA_FILES},\n+        new Object[] {3, MetadataTableType.DATA_FILES},\n         new Object[] {2, MetadataTableType.DELETE_FILES},\n+        new Object[] {3, MetadataTableType.DELETE_FILES},\n         new Object[] {1, MetadataTableType.FILES},\n         new Object[] {2, MetadataTableType.FILES},\n+        new Object[] {3, MetadataTableType.FILES},\n         new Object[] {1, MetadataTableType.ALL_DATA_FILES},\n         new Object[] {2, MetadataTableType.ALL_DATA_FILES},\n+        new Object[] {3, MetadataTableType.ALL_DATA_FILES},\n         new Object[] {2, MetadataTableType.ALL_DELETE_FILES},\n+        new Object[] {3, MetadataTableType.ALL_DELETE_FILES},\n         new Object[] {1, MetadataTableType.ALL_FILES},\n         new Object[] {2, MetadataTableType.ALL_FILES},\n+        new Object[] {3, MetadataTableType.ALL_FILES},\n         new Object[] {1, MetadataTableType.ENTRIES},\n         new Object[] {2, MetadataTableType.ENTRIES},\n+        new Object[] {3, MetadataTableType.ENTRIES},\n         new Object[] {1, MetadataTableType.ALL_ENTRIES},\n-        new Object[] {2, MetadataTableType.ALL_ENTRIES});\n+        new Object[] {2, MetadataTableType.ALL_ENTRIES},\n+        new Object[] {3, MetadataTableType.ALL_ENTRIES});\n   }\n \n   @BeforeEach\n@@ -76,9 +84,9 @@ public void setupTable() throws Exception {\n     table.newFastAppend().appendFile(FILE_D).commit();\n     table.newFastAppend().appendFile(FILE_B).commit();\n \n-    if (formatVersion == 2) {\n-      table.newRowDelta().addDeletes(FILE_A_DELETES).commit();\n-      table.newRowDelta().addDeletes(FILE_B_DELETES).commit();\n+    if (formatVersion >= 2) {\n+      table.newRowDelta().addDeletes(fileADeletes()).commit();\n+      table.newRowDelta().addDeletes(fileBDeletes()).commit();\n       table.newRowDelta().addDeletes(FILE_C2_DELETES).commit();\n       table.newRowDelta().addDeletes(FILE_D2_DELETES).commit();\n     }\n@@ -366,7 +374,7 @@ public void testPartitionSpecEvolutionRemovalV1() {\n \n   @TestTemplate\n   public void testPartitionSpecEvolutionRemovalV2() {\n-    assumeThat(formatVersion).isEqualTo(2);\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n \n     // Change spec and add two data and delete files each\n     table.updateSpec().removeField(Expressions.bucket(\"data\", 16)).addField(\"id\").commit();\n@@ -388,27 +396,13 @@ public void testPartitionSpecEvolutionRemovalV2() {\n             .withPartitionPath(\"id=11\")\n             .build();\n \n-    DeleteFile delete10 =\n-        FileMetadata.deleteFileBuilder(newSpec)\n-            .ofPositionDeletes()\n-            .withPath(\"/path/to/data-10-deletes.parquet\")\n-            .withFileSizeInBytes(10)\n-            .withPartitionPath(\"id=10\")\n-            .withRecordCount(1)\n-            .build();\n-    DeleteFile delete11 =\n-        FileMetadata.deleteFileBuilder(newSpec)\n-            .ofPositionDeletes()\n-            .withPath(\"/path/to/data-11-deletes.parquet\")\n-            .withFileSizeInBytes(10)\n-            .withPartitionPath(\"id=11\")\n-            .withRecordCount(1)\n-            .build();\n+    DeleteFile delete10 = posDelete(table, data10);\n+    DeleteFile delete11 = posDelete(table, data11);\n \n     table.newFastAppend().appendFile(data10).commit();\n     table.newFastAppend().appendFile(data11).commit();\n \n-    if (formatVersion == 2) {\n+    if (formatVersion >= 2) {\n       table.newRowDelta().addDeletes(delete10).commit();\n       table.newRowDelta().addDeletes(delete11).commit();\n     }\n@@ -447,6 +441,12 @@ public void testPartitionSpecEvolutionRemovalV2() {\n     assertThat(tasks).hasSize(expectedScanTaskCount(3));\n   }\n \n+  private DeleteFile posDelete(Table table, DataFile dataFile) {\n+    return formatVersion >= 3\n+        ? FileGenerationUtil.generateDV(table, dataFile)\n+        : FileGenerationUtil.generatePositionDeleteFile(table, dataFile);\n+  }\n+\n   @TestTemplate\n   public void testPartitionSpecEvolutionAdditiveV1() {\n     assumeThat(formatVersion).isEqualTo(1);\n@@ -514,8 +514,8 @@ public void testPartitionSpecEvolutionAdditiveV1() {\n   }\n \n   @TestTemplate\n-  public void testPartitionSpecEvolutionAdditiveV2() {\n-    assumeThat(formatVersion).isEqualTo(2);\n+  public void testPartitionSpecEvolutionAdditiveV2AndAbove() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n \n     // Change spec and add two data and delete files each\n     table.updateSpec().addField(\"id\").commit();\n@@ -537,27 +537,13 @@ public void testPartitionSpecEvolutionAdditiveV2() {\n             .withPartitionPath(\"data_bucket=1/id=11\")\n             .build();\n \n-    DeleteFile delete10 =\n-        FileMetadata.deleteFileBuilder(newSpec)\n-            .ofPositionDeletes()\n-            .withPath(\"/path/to/data-10-deletes.parquet\")\n-            .withFileSizeInBytes(10)\n-            .withPartitionPath(\"data_bucket=0/id=10\")\n-            .withRecordCount(1)\n-            .build();\n-    DeleteFile delete11 =\n-        FileMetadata.deleteFileBuilder(newSpec)\n-            .ofPositionDeletes()\n-            .withPath(\"/path/to/data-11-deletes.parquet\")\n-            .withFileSizeInBytes(10)\n-            .withPartitionPath(\"data_bucket=1/id=11\")\n-            .withRecordCount(1)\n-            .build();\n+    DeleteFile delete10 = posDelete(table, data10);\n+    DeleteFile delete11 = posDelete(table, data11);\n \n     table.newFastAppend().appendFile(data10).commit();\n     table.newFastAppend().appendFile(data11).commit();\n \n-    if (formatVersion == 2) {\n+    if (formatVersion >= 2) {\n       table.newRowDelta().addDeletes(delete10).commit();\n       table.newRowDelta().addDeletes(delete11).commit();\n     }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java b/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java\nindex f811dac02043..a31e02144167 100644\n--- a/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java\n+++ b/core/src/test/java/org/apache/iceberg/TestMetadataTableScans.java\n@@ -1733,7 +1733,7 @@ public void testFilesTableEstimateSize() throws Exception {\n     assertEstimatedRowCount(new AllDataFilesTable(table), 4);\n     assertEstimatedRowCount(new AllFilesTable(table), 4);\n \n-    if (formatVersion == 2) {\n+    if (formatVersion >= 2) {\n       assertEstimatedRowCount(new DeleteFilesTable(table), 4);\n       assertEstimatedRowCount(new AllDeleteFilesTable(table), 4);\n     }\n\ndiff --git a/data/src/test/java/org/apache/iceberg/data/FileHelpers.java b/data/src/test/java/org/apache/iceberg/data/FileHelpers.java\nindex 62df1634aa3f..181ca18138a1 100644\n--- a/data/src/test/java/org/apache/iceberg/data/FileHelpers.java\n+++ b/data/src/test/java/org/apache/iceberg/data/FileHelpers.java\n@@ -23,9 +23,12 @@\n import java.util.List;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.StructLike;\n import org.apache.iceberg.Table;\n+import org.apache.iceberg.deletes.BaseDVFileWriter;\n+import org.apache.iceberg.deletes.DVFileWriter;\n import org.apache.iceberg.deletes.EqualityDeleteWriter;\n import org.apache.iceberg.deletes.PositionDelete;\n import org.apache.iceberg.deletes.PositionDeleteWriter;\n@@ -35,6 +38,8 @@\n import org.apache.iceberg.io.DataWriter;\n import org.apache.iceberg.io.FileWriterFactory;\n import org.apache.iceberg.io.OutputFile;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.CharSequenceSet;\n import org.apache.iceberg.util.Pair;\n@@ -47,21 +52,53 @@ public static Pair<DeleteFile, CharSequenceSet> writeDeleteFile(\n     return writeDeleteFile(table, out, null, deletes);\n   }\n \n+  public static Pair<DeleteFile, CharSequenceSet> writeDeleteFile(\n+      Table table, OutputFile out, List<Pair<CharSequence, Long>> deletes, int formatVersion)\n+      throws IOException {\n+    return writeDeleteFile(table, out, null, deletes, formatVersion);\n+  }\n+\n   public static Pair<DeleteFile, CharSequenceSet> writeDeleteFile(\n       Table table, OutputFile out, StructLike partition, List<Pair<CharSequence, Long>> deletes)\n       throws IOException {\n-    FileWriterFactory<Record> factory = GenericFileWriterFactory.builderFor(table).build();\n+    return writeDeleteFile(table, out, partition, deletes, 2);\n+  }\n \n-    PositionDeleteWriter<Record> writer =\n-        factory.newPositionDeleteWriter(encrypt(out), table.spec(), partition);\n-    PositionDelete<Record> posDelete = PositionDelete.create();\n-    try (Closeable toClose = writer) {\n-      for (Pair<CharSequence, Long> delete : deletes) {\n-        writer.write(posDelete.set(delete.first(), delete.second(), null));\n+  public static Pair<DeleteFile, CharSequenceSet> writeDeleteFile(\n+      Table table,\n+      OutputFile out,\n+      StructLike partition,\n+      List<Pair<CharSequence, Long>> deletes,\n+      int formatVersion)\n+      throws IOException {\n+    if (formatVersion >= 3) {\n+      OutputFileFactory fileFactory =\n+          OutputFileFactory.builderFor(table, 1, 1).format(FileFormat.PUFFIN).build();\n+      DVFileWriter writer = new BaseDVFileWriter(fileFactory, p -> null);\n+      try (DVFileWriter closeableWriter = writer) {\n+        for (Pair<CharSequence, Long> delete : deletes) {\n+          closeableWriter.delete(\n+              delete.first().toString(), delete.second(), table.spec(), partition);\n+        }\n+      }\n+\n+      return Pair.of(\n+          Iterables.getOnlyElement(writer.result().deleteFiles()),\n+          writer.result().referencedDataFiles());\n+    } else {\n+      FileWriterFactory<Record> factory = GenericFileWriterFactory.builderFor(table).build();\n+\n+      PositionDeleteWriter<Record> writer =\n+          factory.newPositionDeleteWriter(encrypt(out), table.spec(), partition);\n+      PositionDelete<Record> posDelete = PositionDelete.create();\n+      try (Closeable toClose = writer) {\n+        for (Pair<CharSequence, Long> delete : deletes) {\n+          writer.write(posDelete.set(delete.first(), delete.second(), null));\n+        }\n       }\n-    }\n \n-    return Pair.of(writer.toDeleteFile(), writer.referencedDataFiles());\n+      return Pair.of(writer.toDeleteFile(), writer.referencedDataFiles());\n+    }\n   }\n \n   public static DeleteFile writeDeleteFile(\n@@ -121,18 +158,43 @@ public static DataFile writeDataFile(\n   public static DeleteFile writePosDeleteFile(\n       Table table, OutputFile out, StructLike partition, List<PositionDelete<?>> deletes)\n       throws IOException {\n-    FileWriterFactory<Record> factory =\n-        GenericFileWriterFactory.builderFor(table).positionDeleteRowSchema(table.schema()).build();\n+    return writePosDeleteFile(table, out, partition, deletes, 2);\n+  }\n \n-    PositionDeleteWriter<?> writer =\n-        factory.newPositionDeleteWriter(encrypt(out), table.spec(), partition);\n-    try (Closeable toClose = writer) {\n-      for (PositionDelete delete : deletes) {\n-        writer.write(delete);\n+  public static DeleteFile writePosDeleteFile(\n+      Table table,\n+      OutputFile out,\n+      StructLike partition,\n+      List<PositionDelete<?>> deletes,\n+      int formatVersion)\n+      throws IOException {\n+    if (formatVersion >= 3) {\n+      OutputFileFactory fileFactory =\n+          OutputFileFactory.builderFor(table, 1, 1).format(FileFormat.PUFFIN).build();\n+      DVFileWriter writer = new BaseDVFileWriter(fileFactory, p -> null);\n+      try (DVFileWriter closeableWriter = writer) {\n+        for (PositionDelete<?> delete : deletes) {\n+          closeableWriter.delete(delete.path().toString(), delete.pos(), table.spec(), partition);\n+        }\n       }\n-    }\n \n-    return writer.toDeleteFile();\n+      return Iterables.getOnlyElement(writer.result().deleteFiles());\n+    } else {\n+      FileWriterFactory<Record> factory =\n+          GenericFileWriterFactory.builderFor(table)\n+              .positionDeleteRowSchema(table.schema())\n+              .build();\n+\n+      PositionDeleteWriter<?> writer =\n+          factory.newPositionDeleteWriter(encrypt(out), table.spec(), partition);\n+      try (Closeable toClose = writer) {\n+        for (PositionDelete delete : deletes) {\n+          writer.write(delete);\n+        }\n+      }\n+\n+      return writer.toDeleteFile();\n+    }\n   }\n \n   private static EncryptedOutputFile encrypt(OutputFile out) {\n\ndiff --git a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java\nindex 6b8399f666d4..085e7e48204c 100644\n--- a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java\n+++ b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/actions/TestRewriteDataFilesAction.java\n@@ -25,6 +25,7 @@\n import java.io.File;\n import java.io.IOException;\n import java.nio.file.Path;\n+import java.util.Arrays;\n import java.util.Collection;\n import java.util.List;\n import java.util.Set;\n@@ -46,6 +47,7 @@\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.actions.RewriteDataFilesActionResult;\n import org.apache.iceberg.catalog.Namespace;\n import org.apache.iceberg.catalog.TableIdentifier;\n@@ -77,6 +79,9 @@ public class TestRewriteDataFilesAction extends CatalogTestBase {\n   @Parameter(index = 2)\n   private FileFormat format;\n \n+  @Parameter(index = 3)\n+  private int formatVersion;\n+\n   private Table icebergTableUnPartitioned;\n   private Table icebergTablePartitioned;\n   private Table icebergTableWithPk;\n@@ -87,15 +92,17 @@ protected TableEnvironment getTableEnv() {\n     return super.getTableEnv();\n   }\n \n-  @Parameters(name = \"catalogName={0}, baseNamespace={1}, format={2}\")\n+  @Parameters(name = \"catalogName={0}, baseNamespace={1}, format={2}, formatVersion={3}\")\n   public static List<Object[]> parameters() {\n     List<Object[]> parameters = Lists.newArrayList();\n     for (FileFormat format :\n         new FileFormat[] {FileFormat.AVRO, FileFormat.ORC, FileFormat.PARQUET}) {\n       for (Object[] catalogParams : CatalogTestBase.parameters()) {\n-        String catalogName = (String) catalogParams[0];\n-        Namespace baseNamespace = (Namespace) catalogParams[1];\n-        parameters.add(new Object[] {catalogName, baseNamespace, format});\n+        for (int version : Arrays.asList(2, 3)) {\n+          String catalogName = (String) catalogParams[0];\n+          Namespace baseNamespace = (Namespace) catalogParams[1];\n+          parameters.add(new Object[] {catalogName, baseNamespace, format, version});\n+        }\n       }\n     }\n     return parameters;\n@@ -111,21 +118,21 @@ public void before() {\n     sql(\"USE CATALOG %s\", catalogName);\n     sql(\"USE %s\", DATABASE);\n     sql(\n-        \"CREATE TABLE %s (id int, data varchar) with ('write.format.default'='%s')\",\n-        TABLE_NAME_UNPARTITIONED, format.name());\n+        \"CREATE TABLE %s (id int, data varchar) with ('write.format.default'='%s', '%s'='%s')\",\n+        TABLE_NAME_UNPARTITIONED, format.name(), TableProperties.FORMAT_VERSION, formatVersion);\n     icebergTableUnPartitioned =\n         validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, TABLE_NAME_UNPARTITIONED));\n \n     sql(\n         \"CREATE TABLE %s (id int, data varchar,spec varchar) \"\n-            + \" PARTITIONED BY (data,spec) with ('write.format.default'='%s')\",\n-        TABLE_NAME_PARTITIONED, format.name());\n+            + \" PARTITIONED BY (data,spec) with ('write.format.default'='%s', '%s'='%s')\",\n+        TABLE_NAME_PARTITIONED, format.name(), TableProperties.FORMAT_VERSION, formatVersion);\n     icebergTablePartitioned =\n         validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, TABLE_NAME_PARTITIONED));\n \n     sql(\n-        \"CREATE TABLE %s (id int, data varchar, PRIMARY KEY(`id`) NOT ENFORCED) with ('write.format.default'='%s', 'format-version'='2')\",\n-        TABLE_NAME_WITH_PK, format.name());\n+        \"CREATE TABLE %s (id int, data varchar, PRIMARY KEY(`id`) NOT ENFORCED) with ('write.format.default'='%s', '%s'='%s')\",\n+        TABLE_NAME_WITH_PK, format.name(), TableProperties.FORMAT_VERSION, formatVersion);\n     icebergTableWithPk =\n         validationCatalog.loadTable(TableIdentifier.of(icebergNamespace, TABLE_NAME_WITH_PK));\n   }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java\nindex bfa09552396a..ad93b80baf81 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestDeleteReachableFilesAction.java\n@@ -23,6 +23,8 @@\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n import java.io.File;\n+import java.util.Arrays;\n+import java.util.List;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.Executors;\n@@ -32,8 +34,12 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataFiles;\n import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileGenerationUtil;\n import org.apache.iceberg.FileMetadata;\n import org.apache.iceberg.HasTableOperations;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -47,14 +53,15 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.types.Types;\n import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n import org.junit.jupiter.api.io.TempDir;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestDeleteReachableFilesAction extends TestBase {\n   private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n   private static final Schema SCHEMA =\n@@ -112,13 +119,24 @@ public class TestDeleteReachableFilesAction extends TestBase {\n           .build();\n \n   @TempDir private File tableDir;\n+  @Parameter private int formatVersion;\n+\n+  @Parameters(name = \"formatVersion = {0}\")\n+  protected static List<Object> parameters() {\n+    return Arrays.asList(2, 3);\n+  }\n \n   private Table table;\n \n   @BeforeEach\n   public void setupTableLocation() throws Exception {\n     String tableLocation = tableDir.toURI().toString();\n-    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+    this.table =\n+        TABLES.create(\n+            SCHEMA,\n+            SPEC,\n+            ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion)),\n+            tableLocation);\n     spark.conf().set(\"spark.sql.shuffle.partitions\", SHUFFLE_PARTITIONS);\n   }\n \n@@ -155,7 +173,7 @@ private void checkRemoveFilesResults(\n         .isEqualTo(expectedOtherFilesDeleted);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void dataFilesCleanupWithParallelTasks() {\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n@@ -206,7 +224,7 @@ public void dataFilesCleanupWithParallelTasks() {\n     checkRemoveFilesResults(4L, 0, 0, 6L, 4L, 6, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testWithExpiringDanglingStageCommit() {\n     table.location();\n     // `A` commit\n@@ -224,7 +242,7 @@ public void testWithExpiringDanglingStageCommit() {\n     checkRemoveFilesResults(3L, 0, 0, 3L, 3L, 5, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveFileActionOnEmptyTable() {\n     DeleteReachableFiles.Result result =\n         sparkActions().deleteReachableFiles(metadataLocation(table)).io(table.io()).execute();\n@@ -232,7 +250,7 @@ public void testRemoveFileActionOnEmptyTable() {\n     checkRemoveFilesResults(0, 0, 0, 0, 0, 2, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveFilesActionWithReducedVersionsTable() {\n     table.updateProperties().set(TableProperties.METADATA_PREVIOUS_VERSIONS_MAX, \"2\").commit();\n     table.newAppend().appendFile(FILE_A).commit();\n@@ -252,7 +270,7 @@ public void testRemoveFilesActionWithReducedVersionsTable() {\n     checkRemoveFilesResults(4, 0, 0, 5, 5, 8, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveFilesAction() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n@@ -263,20 +281,20 @@ public void testRemoveFilesAction() {\n     checkRemoveFilesResults(2, 0, 0, 2, 2, 4, baseRemoveFilesSparkAction.execute());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPositionDeleteFiles() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n     table.newAppend().appendFile(FILE_B).commit();\n \n-    table.newRowDelta().addDeletes(FILE_A_POS_DELETES).commit();\n+    table.newRowDelta().addDeletes(fileADeletes()).commit();\n \n     DeleteReachableFiles baseRemoveFilesSparkAction =\n         sparkActions().deleteReachableFiles(metadataLocation(table)).io(table.io());\n     checkRemoveFilesResults(2, 1, 0, 3, 3, 5, baseRemoveFilesSparkAction.execute());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testEqualityDeleteFiles() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n@@ -289,7 +307,7 @@ public void testEqualityDeleteFiles() {\n     checkRemoveFilesResults(2, 0, 1, 3, 3, 5, baseRemoveFilesSparkAction.execute());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveFilesActionWithDefaultIO() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n@@ -302,7 +320,7 @@ public void testRemoveFilesActionWithDefaultIO() {\n     checkRemoveFilesResults(2, 0, 0, 2, 2, 4, baseRemoveFilesSparkAction.execute());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUseLocalIterator() {\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n@@ -333,7 +351,7 @@ public void testUseLocalIterator() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testIgnoreMetadataFilesNotFound() {\n     table.updateProperties().set(TableProperties.METADATA_PREVIOUS_VERSIONS_MAX, \"1\").commit();\n \n@@ -354,7 +372,7 @@ public void testIgnoreMetadataFilesNotFound() {\n     checkRemoveFilesResults(1, 0, 0, 1, 1, 4, res);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testEmptyIOThrowsException() {\n     DeleteReachableFiles baseRemoveFilesSparkAction =\n         sparkActions().deleteReachableFiles(metadataLocation(table)).io(null);\n@@ -364,7 +382,7 @@ public void testEmptyIOThrowsException() {\n         .hasMessage(\"File IO cannot be null\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveFilesActionWhenGarbageCollectionDisabled() {\n     table.updateProperties().set(TableProperties.GC_ENABLED, \"false\").commit();\n \n@@ -381,4 +399,8 @@ private String metadataLocation(Table tbl) {\n   private ActionsProvider sparkActions() {\n     return SparkActions.get();\n   }\n+\n+  private DeleteFile fileADeletes() {\n+    return formatVersion >= 3 ? FileGenerationUtil.generateDV(table, FILE_A) : FILE_A_POS_DELETES;\n+  }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java\nindex 5909dec51c05..661df99ef3e6 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestExpireSnapshotsAction.java\n@@ -21,10 +21,12 @@\n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.File;\n import java.io.IOException;\n import java.nio.file.Path;\n+import java.util.Arrays;\n import java.util.List;\n import java.util.Set;\n import java.util.concurrent.ConcurrentHashMap;\n@@ -36,8 +38,12 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataFiles;\n import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileGenerationUtil;\n import org.apache.iceberg.FileMetadata;\n import org.apache.iceberg.ManifestFile;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.ReachableFileUtil;\n import org.apache.iceberg.Schema;\n@@ -53,16 +59,17 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.spark.TestBase;\n import org.apache.iceberg.spark.data.TestHelpers;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.Dataset;\n import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n import org.junit.jupiter.api.io.TempDir;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestExpireSnapshotsAction extends TestBase {\n   private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n   private static final Schema SCHEMA =\n@@ -120,6 +127,12 @@ public class TestExpireSnapshotsAction extends TestBase {\n           .build();\n \n   @TempDir private Path temp;\n+  @Parameter private int formatVersion;\n+\n+  @Parameters(name = \"formatVersion = {0}\")\n+  protected static List<Object> parameters() {\n+    return Arrays.asList(2, 3);\n+  }\n \n   @TempDir private File tableDir;\n   private String tableLocation;\n@@ -128,7 +141,12 @@ public class TestExpireSnapshotsAction extends TestBase {\n   @BeforeEach\n   public void setupTableLocation() throws Exception {\n     this.tableLocation = tableDir.toURI().toString();\n-    this.table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+    this.table =\n+        TABLES.create(\n+            SCHEMA,\n+            SPEC,\n+            ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion)),\n+            tableLocation);\n     spark.conf().set(\"spark.sql.shuffle.partitions\", SHUFFLE_PARTITIONS);\n   }\n \n@@ -144,6 +162,10 @@ private Long rightAfterSnapshot(long snapshotId) {\n     return end;\n   }\n \n+  private DeleteFile fileADeletes() {\n+    return formatVersion >= 3 ? FileGenerationUtil.generateDV(table, FILE_A) : FILE_A_POS_DELETES;\n+  }\n+\n   private void checkExpirationResults(\n       long expectedDatafiles,\n       long expectedPosDeleteFiles,\n@@ -173,7 +195,7 @@ private void checkExpirationResults(\n         .isEqualTo(expectedManifestListsDeleted);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testFilesCleaned() throws Exception {\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n@@ -191,7 +213,7 @@ public void testFilesCleaned() throws Exception {\n     checkExpirationResults(1L, 0L, 0L, 1L, 2L, results);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void dataFilesCleanupWithParallelTasks() throws IOException {\n \n     table.newFastAppend().appendFile(FILE_A).commit();\n@@ -245,7 +267,7 @@ public void dataFilesCleanupWithParallelTasks() throws IOException {\n     checkExpirationResults(2L, 0L, 0L, 3L, 3L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n@@ -253,7 +275,7 @@ public void testNoFilesDeletedWhenNoSnapshotsExpired() throws Exception {\n     checkExpirationResults(0L, 0L, 0L, 0L, 0L, results);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCleanupRepeatedOverwrites() throws Exception {\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n@@ -269,7 +291,7 @@ public void testCleanupRepeatedOverwrites() throws Exception {\n     checkExpirationResults(1L, 0L, 0L, 39L, 20L, results);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRetainLastWithExpireOlderThan() {\n     table\n         .newAppend()\n@@ -300,7 +322,7 @@ public void testRetainLastWithExpireOlderThan() {\n     assertThat(table.snapshot(firstSnapshotId)).as(\"First snapshot should not present.\").isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireTwoSnapshotsById() throws Exception {\n     table\n         .newAppend()\n@@ -335,7 +357,7 @@ public void testExpireTwoSnapshotsById() throws Exception {\n     checkExpirationResults(0L, 0L, 0L, 0L, 2L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRetainLastWithExpireById() {\n     table\n         .newAppend()\n@@ -366,7 +388,7 @@ public void testRetainLastWithExpireById() {\n     checkExpirationResults(0L, 0L, 0L, 0L, 1L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRetainLastWithTooFewSnapshots() {\n     table\n         .newAppend()\n@@ -393,7 +415,7 @@ public void testRetainLastWithTooFewSnapshots() {\n     checkExpirationResults(0L, 0L, 0L, 0L, 0L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRetainLastKeepsExpiringSnapshot() {\n     table\n         .newAppend()\n@@ -432,7 +454,7 @@ public void testRetainLastKeepsExpiringSnapshot() {\n     checkExpirationResults(0L, 0L, 0L, 0L, 1L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireSnapshotsWithDisabledGarbageCollection() {\n     table.updateProperties().set(TableProperties.GC_ENABLED, \"false\").commit();\n \n@@ -444,7 +466,7 @@ public void testExpireSnapshotsWithDisabledGarbageCollection() {\n             \"Cannot expire snapshots: GC is disabled (deleting files may corrupt other tables)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOlderThanMultipleCalls() {\n     table\n         .newAppend()\n@@ -480,7 +502,7 @@ public void testExpireOlderThanMultipleCalls() {\n     checkExpirationResults(0L, 0L, 0L, 0L, 2L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRetainLastMultipleCalls() {\n     table\n         .newAppend()\n@@ -517,14 +539,14 @@ public void testRetainLastMultipleCalls() {\n     checkExpirationResults(0L, 0L, 0L, 0L, 2L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRetainZeroSnapshots() {\n     assertThatThrownBy(() -> SparkActions.get().expireSnapshots(table).retainLast(0).execute())\n         .isInstanceOf(IllegalArgumentException.class)\n         .hasMessage(\"Number of snapshots to retain must be at least 1, cannot be: 0\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testScanExpiredManifestInValidSnapshotAppend() {\n     table.newAppend().appendFile(FILE_A).appendFile(FILE_B).commit();\n \n@@ -547,7 +569,7 @@ public void testScanExpiredManifestInValidSnapshotAppend() {\n     checkExpirationResults(1L, 0L, 0L, 1L, 2L, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testScanExpiredManifestInValidSnapshotFastAppend() {\n     table\n         .updateProperties()\n@@ -580,7 +602,7 @@ public void testScanExpiredManifestInValidSnapshotFastAppend() {\n    * Test on table below, and expiring the staged commit `B` using `expireOlderThan` API. Table: A -\n    * C ` B (staged)\n    */\n-  @Test\n+  @TestTemplate\n   public void testWithExpiringDanglingStageCommit() {\n     // `A` commit\n     table.newAppend().appendFile(FILE_A).commit();\n@@ -641,7 +663,7 @@ public void testWithExpiringDanglingStageCommit() {\n    * Expire cherry-pick the commit as shown below, when `B` is in table's current state Table: A - B\n    * - C <--current snapshot `- D (source=B)\n    */\n-  @Test\n+  @TestTemplate\n   public void testWithCherryPickTableSnapshot() {\n     // `A` commit\n     table.newAppend().appendFile(FILE_A).commit();\n@@ -696,7 +718,7 @@ public void testWithCherryPickTableSnapshot() {\n    * Test on table below, and expiring `B` which is not in current table state. 1) Expire `B` 2) All\n    * commit Table: A - C - D (B) ` B (staged)\n    */\n-  @Test\n+  @TestTemplate\n   public void testWithExpiringStagedThenCherrypick() {\n     // `A` commit\n     table.newAppend().appendFile(FILE_A).commit();\n@@ -760,7 +782,7 @@ public void testWithExpiringStagedThenCherrypick() {\n     checkExpirationResults(0L, 0L, 0L, 0L, 2L, secondResult);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOlderThan() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n@@ -796,7 +818,7 @@ public void testExpireOlderThan() {\n     checkExpirationResults(0, 0, 0, 0, 1, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOlderThanWithDelete() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n@@ -858,7 +880,7 @@ public void testExpireOlderThanWithDelete() {\n     checkExpirationResults(1, 0, 0, 2, 2, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOlderThanWithDeleteInMergedManifests() {\n     // merge every commit\n     table.updateProperties().set(TableProperties.MANIFEST_MIN_MERGE_COUNT, \"0\").commit();\n@@ -924,7 +946,7 @@ public void testExpireOlderThanWithDeleteInMergedManifests() {\n     checkExpirationResults(1, 0, 0, 1, 2, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOlderThanWithRollback() {\n     // merge every commit\n     table.updateProperties().set(TableProperties.MANIFEST_MIN_MERGE_COUNT, \"0\").commit();\n@@ -982,7 +1004,7 @@ public void testExpireOlderThanWithRollback() {\n     checkExpirationResults(0, 0, 0, 1, 1, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOlderThanWithRollbackAndMergedManifests() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n@@ -1037,20 +1059,18 @@ public void testExpireOlderThanWithRollbackAndMergedManifests() {\n     checkExpirationResults(1, 0, 0, 1, 1, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOlderThanWithDeleteFile() {\n-    table\n-        .updateProperties()\n-        .set(TableProperties.FORMAT_VERSION, \"2\")\n-        .set(TableProperties.MANIFEST_MERGE_ENABLED, \"false\")\n-        .commit();\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n+    table.updateProperties().set(TableProperties.MANIFEST_MERGE_ENABLED, \"false\").commit();\n \n     // Add Data File\n     table.newAppend().appendFile(FILE_A).commit();\n     Snapshot firstSnapshot = table.currentSnapshot();\n \n     // Add POS Delete\n-    table.newRowDelta().addDeletes(FILE_A_POS_DELETES).commit();\n+    DeleteFile fileADeletes = fileADeletes();\n+    table.newRowDelta().addDeletes(fileADeletes).commit();\n     Snapshot secondSnapshot = table.currentSnapshot();\n \n     // Add EQ Delete\n@@ -1081,7 +1101,7 @@ public void testExpireOlderThanWithDeleteFile() {\n             thirdSnapshot.manifestListLocation(),\n             fourthSnapshot.manifestListLocation(),\n             FILE_A.path().toString(),\n-            FILE_A_POS_DELETES.path().toString(),\n+            fileADeletes.path().toString(),\n             FILE_A_EQ_DELETES.path().toString());\n \n     expectedDeletes.addAll(\n@@ -1103,7 +1123,7 @@ public void testExpireOlderThanWithDeleteFile() {\n     checkExpirationResults(1, 1, 1, 6, 4, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireOnEmptyTable() {\n     Set<String> deletedFiles = Sets.newHashSet();\n \n@@ -1118,7 +1138,7 @@ public void testExpireOnEmptyTable() {\n     checkExpirationResults(0, 0, 0, 0, 0, result);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireAction() {\n     table.newAppend().appendFile(FILE_A).commit();\n \n@@ -1167,7 +1187,7 @@ public void testExpireAction() {\n         .isEqualTo(pendingDeletes.count());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUseLocalIterator() {\n     table.newFastAppend().appendFile(FILE_A).commit();\n \n@@ -1201,7 +1221,7 @@ public void testUseLocalIterator() {\n         });\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireAfterExecute() {\n     table\n         .newAppend()\n@@ -1236,12 +1256,12 @@ public void testExpireAfterExecute() {\n     assertThat(untypedExpiredFiles).as(\"Expired results must match\").hasSize(1);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireFileDeletionMostExpired() {\n     textExpireAllCheckFilesDeleted(5, 2);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireFileDeletionMostRetained() {\n     textExpireAllCheckFilesDeleted(2, 5);\n   }\n@@ -1303,7 +1323,7 @@ public void textExpireAllCheckFilesDeleted(int dataFilesExpired, int dataFilesRe\n         .isEqualTo(expectedDeletes);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testExpireSomeCheckFilesDeleted() {\n \n     table.newAppend().appendFile(FILE_A).commit();\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveDanglingDeleteAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveDanglingDeleteAction.java\nindex 3b4dce73fee5..e58966cfea3f 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveDanglingDeleteAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveDanglingDeleteAction.java\n@@ -22,6 +22,7 @@\n import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.File;\n+import java.util.Arrays;\n import java.util.List;\n import java.util.Set;\n import java.util.stream.Collectors;\n@@ -30,7 +31,11 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataFiles;\n import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileGenerationUtil;\n import org.apache.iceberg.FileMetadata;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Table;\n@@ -44,10 +49,12 @@\n import org.apache.spark.sql.Encoders;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n import org.junit.jupiter.api.io.TempDir;\n import scala.Tuple2;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestRemoveDanglingDeleteAction extends TestBase {\n \n   private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n@@ -202,6 +209,12 @@ public class TestRemoveDanglingDeleteAction extends TestBase {\n           .build();\n \n   @TempDir private File tableDir;\n+  @Parameter private int formatVersion;\n+\n+  @Parameters(name = \"formatVersion = {0}\")\n+  protected static List<Object> parameters() {\n+    return Arrays.asList(2, 3);\n+  }\n \n   private String tableLocation = null;\n   private Table table;\n@@ -219,7 +232,10 @@ public void after() {\n   private void setupPartitionedTable() {\n     this.table =\n         TABLES.create(\n-            SCHEMA, SPEC, ImmutableMap.of(TableProperties.FORMAT_VERSION, \"2\"), tableLocation);\n+            SCHEMA,\n+            SPEC,\n+            ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion)),\n+            tableLocation);\n   }\n \n   private void setupUnpartitionedTable() {\n@@ -227,11 +243,33 @@ private void setupUnpartitionedTable() {\n         TABLES.create(\n             SCHEMA,\n             PartitionSpec.unpartitioned(),\n-            ImmutableMap.of(TableProperties.FORMAT_VERSION, \"2\"),\n+            ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion)),\n             tableLocation);\n   }\n \n-  @Test\n+  private DeleteFile fileADeletes() {\n+    return formatVersion >= 3 ? FileGenerationUtil.generateDV(table, FILE_A) : FILE_A_POS_DELETES;\n+  }\n+\n+  private DeleteFile fileA2Deletes() {\n+    return formatVersion >= 3 ? FileGenerationUtil.generateDV(table, FILE_A2) : FILE_A2_POS_DELETES;\n+  }\n+\n+  private DeleteFile fileBDeletes() {\n+    return formatVersion >= 3 ? FileGenerationUtil.generateDV(table, FILE_B) : FILE_B_POS_DELETES;\n+  }\n+\n+  private DeleteFile fileB2Deletes() {\n+    return formatVersion >= 3 ? FileGenerationUtil.generateDV(table, FILE_B2) : FILE_B2_POS_DELETES;\n+  }\n+\n+  private DeleteFile fileUnpartitionedDeletes() {\n+    return formatVersion >= 3\n+        ? FileGenerationUtil.generateDV(table, FILE_UNPARTITIONED)\n+        : FILE_UNPARTITIONED_POS_DELETE;\n+  }\n+\n+  @TestTemplate\n   public void testPartitionedDeletesWithLesserSeqNo() {\n     setupPartitionedTable();\n \n@@ -239,12 +277,16 @@ public void testPartitionedDeletesWithLesserSeqNo() {\n     table.newAppend().appendFile(FILE_B).appendFile(FILE_C).appendFile(FILE_D).commit();\n \n     // Add Delete Files\n+    DeleteFile fileADeletes = fileADeletes();\n+    DeleteFile fileA2Deletes = fileA2Deletes();\n+    DeleteFile fileBDeletes = fileBDeletes();\n+    DeleteFile fileB2Deletes = fileB2Deletes();\n     table\n         .newRowDelta()\n-        .addDeletes(FILE_A_POS_DELETES)\n-        .addDeletes(FILE_A2_POS_DELETES)\n-        .addDeletes(FILE_B_POS_DELETES)\n-        .addDeletes(FILE_B2_POS_DELETES)\n+        .addDeletes(fileADeletes)\n+        .addDeletes(fileA2Deletes)\n+        .addDeletes(fileBDeletes)\n+        .addDeletes(fileB2Deletes)\n         .addDeletes(FILE_A_EQ_DELETES)\n         .addDeletes(FILE_A2_EQ_DELETES)\n         .addDeletes(FILE_B_EQ_DELETES)\n@@ -275,18 +317,18 @@ public void testPartitionedDeletesWithLesserSeqNo() {\n             Tuple2.apply(1L, FILE_C.path().toString()),\n             Tuple2.apply(1L, FILE_D.path().toString()),\n             Tuple2.apply(2L, FILE_A_EQ_DELETES.path().toString()),\n-            Tuple2.apply(2L, FILE_A_POS_DELETES.path().toString()),\n+            Tuple2.apply(2L, fileADeletes.path().toString()),\n             Tuple2.apply(2L, FILE_A2_EQ_DELETES.path().toString()),\n-            Tuple2.apply(2L, FILE_A2_POS_DELETES.path().toString()),\n+            Tuple2.apply(2L, fileA2Deletes.path().toString()),\n             Tuple2.apply(2L, FILE_B_EQ_DELETES.path().toString()),\n-            Tuple2.apply(2L, FILE_B_POS_DELETES.path().toString()),\n+            Tuple2.apply(2L, fileBDeletes.path().toString()),\n             Tuple2.apply(2L, FILE_B2_EQ_DELETES.path().toString()),\n-            Tuple2.apply(2L, FILE_B2_POS_DELETES.path().toString()),\n+            Tuple2.apply(2L, fileB2Deletes.path().toString()),\n             Tuple2.apply(3L, FILE_A2.path().toString()),\n             Tuple2.apply(3L, FILE_B2.path().toString()),\n             Tuple2.apply(3L, FILE_C2.path().toString()),\n             Tuple2.apply(3L, FILE_D2.path().toString()));\n-    assertThat(actual).isEqualTo(expected);\n+    assertThat(actual).containsExactlyInAnyOrderElementsOf(expected);\n \n     RemoveDanglingDeleteFiles.Result result =\n         SparkActions.get().removeDanglingDeleteFiles(table).execute();\n@@ -302,8 +344,8 @@ public void testPartitionedDeletesWithLesserSeqNo() {\n         .as(\"Expected 4 delete files removed\")\n         .hasSize(4)\n         .containsExactlyInAnyOrder(\n-            FILE_A_POS_DELETES.path(),\n-            FILE_A2_POS_DELETES.path(),\n+            fileADeletes.path(),\n+            fileA2Deletes.path(),\n             FILE_A_EQ_DELETES.path(),\n             FILE_A2_EQ_DELETES.path());\n \n@@ -323,17 +365,17 @@ public void testPartitionedDeletesWithLesserSeqNo() {\n             Tuple2.apply(1L, FILE_C.path().toString()),\n             Tuple2.apply(1L, FILE_D.path().toString()),\n             Tuple2.apply(2L, FILE_B_EQ_DELETES.path().toString()),\n-            Tuple2.apply(2L, FILE_B_POS_DELETES.path().toString()),\n+            Tuple2.apply(2L, fileBDeletes.path().toString()),\n             Tuple2.apply(2L, FILE_B2_EQ_DELETES.path().toString()),\n-            Tuple2.apply(2L, FILE_B2_POS_DELETES.path().toString()),\n+            Tuple2.apply(2L, fileB2Deletes.path().toString()),\n             Tuple2.apply(3L, FILE_A2.path().toString()),\n             Tuple2.apply(3L, FILE_B2.path().toString()),\n             Tuple2.apply(3L, FILE_C2.path().toString()),\n             Tuple2.apply(3L, FILE_D2.path().toString()));\n-    assertThat(actualAfter).isEqualTo(expectedAfter);\n+    assertThat(actualAfter).containsExactlyInAnyOrderElementsOf(expectedAfter);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartitionedDeletesWithEqSeqNo() {\n     setupPartitionedTable();\n \n@@ -341,18 +383,22 @@ public void testPartitionedDeletesWithEqSeqNo() {\n     table.newAppend().appendFile(FILE_A).appendFile(FILE_C).appendFile(FILE_D).commit();\n \n     // Add Data Files with EQ and POS deletes\n+    DeleteFile fileADeletes = fileADeletes();\n+    DeleteFile fileA2Deletes = fileA2Deletes();\n+    DeleteFile fileBDeletes = fileBDeletes();\n+    DeleteFile fileB2Deletes = fileB2Deletes();\n     table\n         .newRowDelta()\n         .addRows(FILE_A2)\n         .addRows(FILE_B2)\n         .addRows(FILE_C2)\n         .addRows(FILE_D2)\n-        .addDeletes(FILE_A_POS_DELETES)\n-        .addDeletes(FILE_A2_POS_DELETES)\n+        .addDeletes(fileADeletes)\n+        .addDeletes(fileA2Deletes)\n         .addDeletes(FILE_A_EQ_DELETES)\n         .addDeletes(FILE_A2_EQ_DELETES)\n-        .addDeletes(FILE_B_POS_DELETES)\n-        .addDeletes(FILE_B2_POS_DELETES)\n+        .addDeletes(fileBDeletes)\n+        .addDeletes(fileB2Deletes)\n         .addDeletes(FILE_B_EQ_DELETES)\n         .addDeletes(FILE_B2_EQ_DELETES)\n         .commit();\n@@ -372,18 +418,18 @@ public void testPartitionedDeletesWithEqSeqNo() {\n             Tuple2.apply(1L, FILE_C.path().toString()),\n             Tuple2.apply(1L, FILE_D.path().toString()),\n             Tuple2.apply(2L, FILE_A_EQ_DELETES.path().toString()),\n-            Tuple2.apply(2L, FILE_A_POS_DELETES.path().toString()),\n+            Tuple2.apply(2L, fileADeletes.path().toString()),\n             Tuple2.apply(2L, FILE_A2.path().toString()),\n             Tuple2.apply(2L, FILE_A2_EQ_DELETES.path().toString()),\n-            Tuple2.apply(2L, FILE_A2_POS_DELETES.path().toString()),\n+            Tuple2.apply(2L, fileA2Deletes.path().toString()),\n             Tuple2.apply(2L, FILE_B_EQ_DELETES.path().toString()),\n-            Tuple2.apply(2L, FILE_B_POS_DELETES.path().toString()),\n+            Tuple2.apply(2L, fileBDeletes.path().toString()),\n             Tuple2.apply(2L, FILE_B2.path().toString()),\n             Tuple2.apply(2L, FILE_B2_EQ_DELETES.path().toString()),\n-            Tuple2.apply(2L, FILE_B2_POS_DELETES.path().toString()),\n+            Tuple2.apply(2L, fileB2Deletes.path().toString()),\n             Tuple2.apply(2L, FILE_C2.path().toString()),\n             Tuple2.apply(2L, FILE_D2.path().toString()));\n-    assertThat(actual).isEqualTo(expected);\n+    assertThat(actual).containsExactlyInAnyOrderElementsOf(expected);\n \n     RemoveDanglingDeleteFiles.Result result =\n         SparkActions.get().removeDanglingDeleteFiles(table).execute();\n@@ -415,25 +461,25 @@ public void testPartitionedDeletesWithEqSeqNo() {\n             Tuple2.apply(1L, FILE_C.path().toString()),\n             Tuple2.apply(1L, FILE_D.path().toString()),\n             Tuple2.apply(2L, FILE_A_EQ_DELETES.path().toString()),\n-            Tuple2.apply(2L, FILE_A_POS_DELETES.path().toString()),\n+            Tuple2.apply(2L, fileADeletes.path().toString()),\n             Tuple2.apply(2L, FILE_A2.path().toString()),\n             Tuple2.apply(2L, FILE_A2_EQ_DELETES.path().toString()),\n-            Tuple2.apply(2L, FILE_A2_POS_DELETES.path().toString()),\n-            Tuple2.apply(2L, FILE_B_POS_DELETES.path().toString()),\n+            Tuple2.apply(2L, fileA2Deletes.path().toString()),\n+            Tuple2.apply(2L, fileBDeletes.path().toString()),\n             Tuple2.apply(2L, FILE_B2.path().toString()),\n-            Tuple2.apply(2L, FILE_B2_POS_DELETES.path().toString()),\n+            Tuple2.apply(2L, fileB2Deletes.path().toString()),\n             Tuple2.apply(2L, FILE_C2.path().toString()),\n             Tuple2.apply(2L, FILE_D2.path().toString()));\n-    assertThat(actualAfter).isEqualTo(expectedAfter);\n+    assertThat(actualAfter).containsExactlyInAnyOrderElementsOf(expectedAfter);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testUnpartitionedTable() {\n     setupUnpartitionedTable();\n \n     table\n         .newRowDelta()\n-        .addDeletes(FILE_UNPARTITIONED_POS_DELETE)\n+        .addDeletes(fileUnpartitionedDeletes())\n         .addDeletes(FILE_UNPARTITIONED_EQ_DELETE)\n         .commit();\n     table.newAppend().appendFile(FILE_UNPARTITIONED).commit();\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java\nindex 12defafff06d..d36898d4c464 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction.java\n@@ -21,6 +21,7 @@\n import static org.apache.iceberg.types.Types.NestedField.optional;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.File;\n import java.io.IOException;\n@@ -35,6 +36,7 @@\n import java.util.concurrent.ConcurrentHashMap;\n import java.util.concurrent.ExecutorService;\n import java.util.concurrent.Executors;\n+import java.util.concurrent.ThreadLocalRandom;\n import java.util.concurrent.atomic.AtomicInteger;\n import java.util.stream.Collectors;\n import java.util.stream.StreamSupport;\n@@ -45,6 +47,9 @@\n import org.apache.iceberg.Files;\n import org.apache.iceberg.GenericBlobMetadata;\n import org.apache.iceberg.GenericStatisticsFile;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n import org.apache.iceberg.Schema;\n import org.apache.iceberg.Snapshot;\n@@ -81,9 +86,11 @@\n import org.apache.spark.sql.types.DataTypes;\n import org.apache.spark.sql.types.StructType;\n import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n import org.junit.jupiter.api.io.TempDir;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public abstract class TestRemoveOrphanFilesAction extends TestBase {\n \n   private static final HadoopTables TABLES = new HadoopTables(new Configuration());\n@@ -97,16 +104,23 @@ public abstract class TestRemoveOrphanFilesAction extends TestBase {\n \n   @TempDir private File tableDir = null;\n   protected String tableLocation = null;\n+  protected Map<String, String> properties;\n+  @Parameter private int formatVersion;\n+\n+  @Parameters(name = \"formatVersion = {0}\")\n+  protected static List<Object> parameters() {\n+    return Arrays.asList(2, 3);\n+  }\n \n   @BeforeEach\n   public void setupTableLocation() throws Exception {\n     this.tableLocation = tableDir.toURI().toString();\n+    properties = ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion));\n   }\n \n-  @Test\n-  public void testDryRun() throws IOException, InterruptedException {\n-    Table table =\n-        TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap(), tableLocation);\n+  @TestTemplate\n+  public void testDryRun() throws IOException {\n+    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), properties, tableLocation);\n \n     List<ThreeColumnRecord> records =\n         Lists.newArrayList(new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"));\n@@ -184,9 +198,9 @@ public void testDryRun() throws IOException, InterruptedException {\n     assertThat(actualRecords).isEqualTo(expectedRecords);\n   }\n \n-  @Test\n-  public void testAllValidFilesAreKept() throws IOException, InterruptedException {\n-    Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  @TestTemplate\n+  public void testAllValidFilesAreKept() throws IOException {\n+    Table table = TABLES.create(SCHEMA, SPEC, properties, tableLocation);\n \n     List<ThreeColumnRecord> records1 =\n         Lists.newArrayList(new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"));\n@@ -246,9 +260,9 @@ public void testAllValidFilesAreKept() throws IOException, InterruptedException\n     }\n   }\n \n-  @Test\n-  public void orphanedFileRemovedWithParallelTasks() throws InterruptedException, IOException {\n-    Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  @TestTemplate\n+  public void orphanedFileRemovedWithParallelTasks() {\n+    Table table = TABLES.create(SCHEMA, SPEC, properties, tableLocation);\n \n     List<ThreeColumnRecord> records1 =\n         Lists.newArrayList(new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"));\n@@ -310,10 +324,12 @@ public void orphanedFileRemovedWithParallelTasks() throws InterruptedException,\n     assertThat(deletedFiles).hasSize(4);\n   }\n \n-  @Test\n-  public void testWapFilesAreKept() throws InterruptedException {\n+  @TestTemplate\n+  public void testWapFilesAreKept() {\n+    assumeThat(formatVersion).as(\"currently fails with DVs\").isEqualTo(2);\n     Map<String, String> props = Maps.newHashMap();\n     props.put(TableProperties.WRITE_AUDIT_PUBLISH_ENABLED, \"true\");\n+    props.putAll(properties);\n     Table table = TABLES.create(SCHEMA, SPEC, props, tableLocation);\n \n     List<ThreeColumnRecord> records =\n@@ -331,6 +347,8 @@ public void testWapFilesAreKept() throws InterruptedException {\n     Dataset<Row> resultDF = spark.read().format(\"iceberg\").load(tableLocation);\n     List<ThreeColumnRecord> actualRecords =\n         resultDF.as(Encoders.bean(ThreeColumnRecord.class)).collectAsList();\n+\n+    // TODO: currently fails because DVs delete stuff from WAP branch\n     assertThat(actualRecords)\n         .as(\"Should not return data from the staged snapshot\")\n         .isEqualTo(records);\n@@ -345,11 +363,12 @@ public void testWapFilesAreKept() throws InterruptedException {\n     assertThat(result.orphanFileLocations()).as(\"Should not delete any files\").isEmpty();\n   }\n \n-  @Test\n-  public void testMetadataFolderIsIntact() throws InterruptedException {\n+  @TestTemplate\n+  public void testMetadataFolderIsIntact() {\n     // write data directly to the table location\n     Map<String, String> props = Maps.newHashMap();\n     props.put(TableProperties.WRITE_DATA_LOCATION, tableLocation);\n+    props.putAll(properties);\n     Table table = TABLES.create(SCHEMA, SPEC, props, tableLocation);\n \n     List<ThreeColumnRecord> records =\n@@ -375,9 +394,9 @@ public void testMetadataFolderIsIntact() throws InterruptedException {\n     assertThat(actualRecords).as(\"Rows must match\").isEqualTo(records);\n   }\n \n-  @Test\n-  public void testOlderThanTimestamp() throws InterruptedException {\n-    Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  @TestTemplate\n+  public void testOlderThanTimestamp() {\n+    Table table = TABLES.create(SCHEMA, SPEC, properties, tableLocation);\n \n     List<ThreeColumnRecord> records =\n         Lists.newArrayList(new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"));\n@@ -404,11 +423,12 @@ public void testOlderThanTimestamp() throws InterruptedException {\n     assertThat(result.orphanFileLocations()).as(\"Should delete only 2 files\").hasSize(2);\n   }\n \n-  @Test\n-  public void testRemoveUnreachableMetadataVersionFiles() throws InterruptedException {\n+  @TestTemplate\n+  public void testRemoveUnreachableMetadataVersionFiles() {\n     Map<String, String> props = Maps.newHashMap();\n     props.put(TableProperties.WRITE_DATA_LOCATION, tableLocation);\n     props.put(TableProperties.METADATA_PREVIOUS_VERSIONS_MAX, \"1\");\n+    props.putAll(properties);\n     Table table = TABLES.create(SCHEMA, SPEC, props, tableLocation);\n \n     List<ThreeColumnRecord> records =\n@@ -441,9 +461,9 @@ public void testRemoveUnreachableMetadataVersionFiles() throws InterruptedExcept\n     assertThat(actualRecords).as(\"Rows must match\").isEqualTo(expectedRecords);\n   }\n \n-  @Test\n-  public void testManyTopLevelPartitions() throws InterruptedException {\n-    Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  @TestTemplate\n+  public void testManyTopLevelPartitions() {\n+    Table table = TABLES.create(SCHEMA, SPEC, properties, tableLocation);\n \n     List<ThreeColumnRecord> records = Lists.newArrayList();\n     for (int i = 0; i < 100; i++) {\n@@ -467,9 +487,9 @@ public void testManyTopLevelPartitions() throws InterruptedException {\n     assertThat(resultDF.count()).as(\"Rows count must match\").isEqualTo(records.size());\n   }\n \n-  @Test\n-  public void testManyLeafPartitions() throws InterruptedException {\n-    Table table = TABLES.create(SCHEMA, SPEC, Maps.newHashMap(), tableLocation);\n+  @TestTemplate\n+  public void testManyLeafPartitions() {\n+    Table table = TABLES.create(SCHEMA, SPEC, properties, tableLocation);\n \n     List<ThreeColumnRecord> records = Lists.newArrayList();\n     for (int i = 0; i < 100; i++) {\n@@ -493,15 +513,15 @@ public void testManyLeafPartitions() throws InterruptedException {\n     assertThat(resultDF.count()).as(\"Row count must match\").isEqualTo(records.size());\n   }\n \n-  @Test\n-  public void testHiddenPartitionPaths() throws InterruptedException {\n+  @TestTemplate\n+  public void testHiddenPartitionPaths() {\n     Schema schema =\n         new Schema(\n             optional(1, \"c1\", Types.IntegerType.get()),\n             optional(2, \"_c2\", Types.StringType.get()),\n             optional(3, \"c3\", Types.StringType.get()));\n     PartitionSpec spec = PartitionSpec.builderFor(schema).truncate(\"_c2\", 2).identity(\"c3\").build();\n-    Table table = TABLES.create(schema, spec, Maps.newHashMap(), tableLocation);\n+    Table table = TABLES.create(schema, spec, properties, tableLocation);\n \n     StructType structType =\n         new StructType()\n@@ -526,15 +546,15 @@ public void testHiddenPartitionPaths() throws InterruptedException {\n     assertThat(result.orphanFileLocations()).as(\"Should delete 2 files\").hasSize(2);\n   }\n \n-  @Test\n-  public void testHiddenPartitionPathsWithPartitionEvolution() throws InterruptedException {\n+  @TestTemplate\n+  public void testHiddenPartitionPathsWithPartitionEvolution() {\n     Schema schema =\n         new Schema(\n             optional(1, \"_c1\", Types.IntegerType.get()),\n             optional(2, \"_c2\", Types.StringType.get()),\n             optional(3, \"c3\", Types.StringType.get()));\n     PartitionSpec spec = PartitionSpec.builderFor(schema).truncate(\"_c2\", 2).build();\n-    Table table = TABLES.create(schema, spec, Maps.newHashMap(), tableLocation);\n+    Table table = TABLES.create(schema, spec, properties, tableLocation);\n \n     StructType structType =\n         new StructType()\n@@ -562,16 +582,15 @@ public void testHiddenPartitionPathsWithPartitionEvolution() throws InterruptedE\n     assertThat(result.orphanFileLocations()).as(\"Should delete 2 files\").hasSize(2);\n   }\n \n-  @Test\n-  public void testHiddenPathsStartingWithPartitionNamesAreIgnored()\n-      throws InterruptedException, IOException {\n+  @TestTemplate\n+  public void testHiddenPathsStartingWithPartitionNamesAreIgnored() throws IOException {\n     Schema schema =\n         new Schema(\n             optional(1, \"c1\", Types.IntegerType.get()),\n             optional(2, \"_c2\", Types.StringType.get()),\n             optional(3, \"c3\", Types.StringType.get()));\n     PartitionSpec spec = PartitionSpec.builderFor(schema).truncate(\"_c2\", 2).identity(\"c3\").build();\n-    Table table = TABLES.create(schema, spec, Maps.newHashMap(), tableLocation);\n+    Table table = TABLES.create(schema, spec, properties, tableLocation);\n \n     StructType structType =\n         new StructType()\n@@ -610,11 +629,11 @@ private List<String> snapshotFiles(long snapshotId) {\n         .collectAsList();\n   }\n \n-  @Test\n-  public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, InterruptedException {\n+  @TestTemplate\n+  public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException {\n     Table table =\n         TABLES.create(\n-            SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap(), tableDir.getAbsolutePath());\n+            SCHEMA, PartitionSpec.unpartitioned(), properties, tableDir.getAbsolutePath());\n \n     List<ThreeColumnRecord> records =\n         Lists.newArrayList(new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"));\n@@ -670,7 +689,7 @@ public void testRemoveOrphanFilesWithRelativeFilePath() throws IOException, Inte\n         .isTrue();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException {\n     HadoopCatalog catalog = new HadoopCatalog(new Configuration(), tableLocation);\n     String namespaceName = \"testDb\";\n@@ -705,15 +724,11 @@ public void testRemoveOrphanFilesWithHadoopCatalog() throws InterruptedException\n     assertThat(actualRecords).as(\"Rows must match\").isEqualTo(records);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testHiveCatalogTable() throws IOException {\n-    Table table =\n-        catalog.createTable(\n-            TableIdentifier.of(\"default\", \"hivetestorphan\"),\n-            SCHEMA,\n-            SPEC,\n-            tableLocation,\n-            Maps.newHashMap());\n+    TableIdentifier identifier =\n+        TableIdentifier.of(\"default\", \"hivetestorphan\" + ThreadLocalRandom.current().nextInt(1000));\n+    Table table = catalog.createTable(identifier, SCHEMA, SPEC, tableLocation, properties);\n \n     List<ThreeColumnRecord> records =\n         Lists.newArrayList(new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"));\n@@ -724,7 +739,7 @@ public void testHiveCatalogTable() throws IOException {\n         .write()\n         .format(\"iceberg\")\n         .mode(\"append\")\n-        .save(\"default.hivetestorphan\");\n+        .save(identifier.toString());\n \n     String location = table.location().replaceFirst(\"file:\", \"\");\n     new File(location + \"/data/trashfile\").createNewFile();\n@@ -739,10 +754,9 @@ public void testHiveCatalogTable() throws IOException {\n         .anyMatch(file -> file.contains(\"file:\" + location + \"/data/trashfile\"));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testGarbageCollectionDisabled() {\n-    Table table =\n-        TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap(), tableLocation);\n+    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), properties, tableLocation);\n \n     List<ThreeColumnRecord> records =\n         Lists.newArrayList(new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"));\n@@ -759,10 +773,9 @@ public void testGarbageCollectionDisabled() {\n             \"Cannot delete orphan files: GC is disabled (deleting files may corrupt other tables)\");\n   }\n \n-  @Test\n-  public void testCompareToFileList() throws IOException, InterruptedException {\n-    Table table =\n-        TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), Maps.newHashMap(), tableLocation);\n+  @TestTemplate\n+  public void testCompareToFileList() throws IOException {\n+    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), properties, tableLocation);\n \n     List<ThreeColumnRecord> records =\n         Lists.newArrayList(new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"));\n@@ -890,14 +903,10 @@ protected long waitUntilAfter(long timestampMillis) {\n     return current;\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFilesWithStatisticFiles() throws Exception {\n-    Table table =\n-        TABLES.create(\n-            SCHEMA,\n-            PartitionSpec.unpartitioned(),\n-            ImmutableMap.of(TableProperties.FORMAT_VERSION, \"2\"),\n-            tableLocation);\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n+    Table table = TABLES.create(SCHEMA, PartitionSpec.unpartitioned(), properties, tableLocation);\n \n     List<ThreeColumnRecord> records =\n         Lists.newArrayList(new ThreeColumnRecord(1, \"AAAAAAAAAA\", \"AAAA\"));\n@@ -966,28 +975,28 @@ public void testRemoveOrphanFilesWithStatisticFiles() throws Exception {\n     assertThat(statsLocation.exists()).as(\"stats file should be deleted\").isFalse();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPathsWithExtraSlashes() {\n     List<String> validFiles = Lists.newArrayList(\"file:///dir1/dir2/file1\");\n     List<String> actualFiles = Lists.newArrayList(\"file:///dir1/////dir2///file1\");\n     executeTest(validFiles, actualFiles, Lists.newArrayList());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPathsWithValidFileHavingNoAuthority() {\n     List<String> validFiles = Lists.newArrayList(\"hdfs:///dir1/dir2/file1\");\n     List<String> actualFiles = Lists.newArrayList(\"hdfs://servicename/dir1/dir2/file1\");\n     executeTest(validFiles, actualFiles, Lists.newArrayList());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPathsWithActualFileHavingNoAuthority() {\n     List<String> validFiles = Lists.newArrayList(\"hdfs://servicename/dir1/dir2/file1\");\n     List<String> actualFiles = Lists.newArrayList(\"hdfs:///dir1/dir2/file1\");\n     executeTest(validFiles, actualFiles, Lists.newArrayList());\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPathsWithEqualSchemes() {\n     List<String> validFiles = Lists.newArrayList(\"scheme1://bucket1/dir1/dir2/file1\");\n     List<String> actualFiles = Lists.newArrayList(\"scheme2://bucket1/dir1/dir2/file1\");\n@@ -1016,7 +1025,7 @@ public void testPathsWithEqualSchemes() {\n         DeleteOrphanFiles.PrefixMismatchMode.ERROR);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPathsWithEqualAuthorities() {\n     List<String> validFiles = Lists.newArrayList(\"hdfs://servicename1/dir1/dir2/file1\");\n     List<String> actualFiles = Lists.newArrayList(\"hdfs://servicename2/dir1/dir2/file1\");\n@@ -1045,7 +1054,7 @@ public void testPathsWithEqualAuthorities() {\n         DeleteOrphanFiles.PrefixMismatchMode.ERROR);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveOrphanFileActionWithDeleteMode() {\n     List<String> validFiles = Lists.newArrayList(\"hdfs://servicename1/dir1/dir2/file1\");\n     List<String> actualFiles = Lists.newArrayList(\"hdfs://servicename2/dir1/dir2/file1\");\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java\nindex 2476d1bb7078..14784da4f74f 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRemoveOrphanFilesAction3.java\n@@ -21,10 +21,9 @@\n import static org.assertj.core.api.Assertions.assertThat;\n \n import java.io.File;\n-import java.util.Map;\n+import java.util.concurrent.ThreadLocalRandom;\n import java.util.stream.StreamSupport;\n import org.apache.iceberg.actions.DeleteOrphanFiles;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.spark.SparkCatalog;\n import org.apache.iceberg.spark.SparkSchemaUtil;\n import org.apache.iceberg.spark.SparkSessionCatalog;\n@@ -32,10 +31,10 @@\n import org.apache.spark.sql.connector.catalog.Identifier;\n import org.apache.spark.sql.connector.expressions.Transform;\n import org.junit.jupiter.api.AfterEach;\n-import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.TestTemplate;\n \n public class TestRemoveOrphanFilesAction3 extends TestRemoveOrphanFilesAction {\n-  @Test\n+  @TestTemplate\n   public void testSparkCatalogTable() throws Exception {\n     spark.conf().set(\"spark.sql.catalog.mycat\", \"org.apache.iceberg.spark.SparkCatalog\");\n     spark.conf().set(\"spark.sql.catalog.mycat.type\", \"hadoop\");\n@@ -43,16 +42,16 @@ public void testSparkCatalogTable() throws Exception {\n     SparkCatalog cat = (SparkCatalog) spark.sessionState().catalogManager().catalog(\"mycat\");\n \n     String[] database = {\"default\"};\n-    Identifier id = Identifier.of(database, \"table\");\n-    Map<String, String> options = Maps.newHashMap();\n+    Identifier id = Identifier.of(database, \"table\" + ThreadLocalRandom.current().nextInt(1000));\n     Transform[] transforms = {};\n-    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, properties);\n     SparkTable table = (SparkTable) cat.loadTable(id);\n \n-    spark.sql(\"INSERT INTO mycat.default.table VALUES (1,1,1)\");\n+    sql(\"INSERT INTO mycat.default.%s VALUES (1,1,1)\", id.name());\n \n     String location = table.table().location().replaceFirst(\"file:\", \"\");\n-    new File(location + \"/data/trashfile\").createNewFile();\n+    String trashFile = \"/data/trashfile\" + ThreadLocalRandom.current().nextInt(1000);\n+    new File(location + trashFile).createNewFile();\n \n     DeleteOrphanFiles.Result results =\n         SparkActions.get()\n@@ -61,10 +60,10 @@ public void testSparkCatalogTable() throws Exception {\n             .execute();\n     assertThat(StreamSupport.stream(results.orphanFileLocations().spliterator(), false))\n         .as(\"trash file should be removed\")\n-        .anyMatch(file -> file.contains(\"file:\" + location + \"/data/trashfile\"));\n+        .anyMatch(file -> file.contains(\"file:\" + location + trashFile));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkCatalogNamedHadoopTable() throws Exception {\n     spark.conf().set(\"spark.sql.catalog.hadoop\", \"org.apache.iceberg.spark.SparkCatalog\");\n     spark.conf().set(\"spark.sql.catalog.hadoop.type\", \"hadoop\");\n@@ -72,16 +71,16 @@ public void testSparkCatalogNamedHadoopTable() throws Exception {\n     SparkCatalog cat = (SparkCatalog) spark.sessionState().catalogManager().catalog(\"hadoop\");\n \n     String[] database = {\"default\"};\n-    Identifier id = Identifier.of(database, \"table\");\n-    Map<String, String> options = Maps.newHashMap();\n+    Identifier id = Identifier.of(database, \"table\" + ThreadLocalRandom.current().nextInt(1000));\n     Transform[] transforms = {};\n-    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, properties);\n     SparkTable table = (SparkTable) cat.loadTable(id);\n \n-    spark.sql(\"INSERT INTO hadoop.default.table VALUES (1,1,1)\");\n+    sql(\"INSERT INTO hadoop.default.%s VALUES (1,1,1)\", id.name());\n \n     String location = table.table().location().replaceFirst(\"file:\", \"\");\n-    new File(location + \"/data/trashfile\").createNewFile();\n+    String trashFile = \"/data/trashfile\" + ThreadLocalRandom.current().nextInt(1000);\n+    new File(location + trashFile).createNewFile();\n \n     DeleteOrphanFiles.Result results =\n         SparkActions.get()\n@@ -90,10 +89,10 @@ public void testSparkCatalogNamedHadoopTable() throws Exception {\n             .execute();\n     assertThat(StreamSupport.stream(results.orphanFileLocations().spliterator(), false))\n         .as(\"trash file should be removed\")\n-        .anyMatch(file -> file.contains(\"file:\" + location + \"/data/trashfile\"));\n+        .anyMatch(file -> file.contains(\"file:\" + location + trashFile));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkCatalogNamedHiveTable() throws Exception {\n     spark.conf().set(\"spark.sql.catalog.hive\", \"org.apache.iceberg.spark.SparkCatalog\");\n     spark.conf().set(\"spark.sql.catalog.hive.type\", \"hadoop\");\n@@ -101,16 +100,16 @@ public void testSparkCatalogNamedHiveTable() throws Exception {\n     SparkCatalog cat = (SparkCatalog) spark.sessionState().catalogManager().catalog(\"hive\");\n \n     String[] database = {\"default\"};\n-    Identifier id = Identifier.of(database, \"table\");\n-    Map<String, String> options = Maps.newHashMap();\n+    Identifier id = Identifier.of(database, \"table\" + ThreadLocalRandom.current().nextInt(1000));\n     Transform[] transforms = {};\n-    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, properties);\n     SparkTable table = (SparkTable) cat.loadTable(id);\n \n-    spark.sql(\"INSERT INTO hive.default.table VALUES (1,1,1)\");\n+    sql(\"INSERT INTO hive.default.%s VALUES (1,1,1)\", id.name());\n \n     String location = table.table().location().replaceFirst(\"file:\", \"\");\n-    new File(location + \"/data/trashfile\").createNewFile();\n+    String trashFile = \"/data/trashfile\" + ThreadLocalRandom.current().nextInt(1000);\n+    new File(location + trashFile).createNewFile();\n \n     DeleteOrphanFiles.Result results =\n         SparkActions.get()\n@@ -120,10 +119,10 @@ public void testSparkCatalogNamedHiveTable() throws Exception {\n \n     assertThat(StreamSupport.stream(results.orphanFileLocations().spliterator(), false))\n         .as(\"trash file should be removed\")\n-        .anyMatch(file -> file.contains(\"file:\" + location + \"/data/trashfile\"));\n+        .anyMatch(file -> file.contains(\"file:\" + location + trashFile));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkSessionCatalogHadoopTable() throws Exception {\n     spark\n         .conf()\n@@ -134,16 +133,16 @@ public void testSparkSessionCatalogHadoopTable() throws Exception {\n         (SparkSessionCatalog) spark.sessionState().catalogManager().v2SessionCatalog();\n \n     String[] database = {\"default\"};\n-    Identifier id = Identifier.of(database, \"table\");\n-    Map<String, String> options = Maps.newHashMap();\n+    Identifier id = Identifier.of(database, \"table\" + ThreadLocalRandom.current().nextInt(1000));\n     Transform[] transforms = {};\n-    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, properties);\n     SparkTable table = (SparkTable) cat.loadTable(id);\n \n-    spark.sql(\"INSERT INTO default.table VALUES (1,1,1)\");\n+    sql(\"INSERT INTO default.%s VALUES (1,1,1)\", id.name());\n \n     String location = table.table().location().replaceFirst(\"file:\", \"\");\n-    new File(location + \"/data/trashfile\").createNewFile();\n+    String trashFile = \"/data/trashfile\" + ThreadLocalRandom.current().nextInt(1000);\n+    new File(location + trashFile).createNewFile();\n \n     DeleteOrphanFiles.Result results =\n         SparkActions.get()\n@@ -152,10 +151,10 @@ public void testSparkSessionCatalogHadoopTable() throws Exception {\n             .execute();\n     assertThat(StreamSupport.stream(results.orphanFileLocations().spliterator(), false))\n         .as(\"trash file should be removed\")\n-        .anyMatch(file -> file.contains(\"file:\" + location + \"/data/trashfile\"));\n+        .anyMatch(file -> file.contains(\"file:\" + location + trashFile));\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSparkSessionCatalogHiveTable() throws Exception {\n     spark\n         .conf()\n@@ -166,16 +165,16 @@ public void testSparkSessionCatalogHiveTable() throws Exception {\n \n     String[] database = {\"default\"};\n     Identifier id = Identifier.of(database, \"sessioncattest\");\n-    Map<String, String> options = Maps.newHashMap();\n     Transform[] transforms = {};\n     cat.dropTable(id);\n-    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, options);\n+    cat.createTable(id, SparkSchemaUtil.convert(SCHEMA), transforms, properties);\n     SparkTable table = (SparkTable) cat.loadTable(id);\n \n     spark.sql(\"INSERT INTO default.sessioncattest VALUES (1,1,1)\");\n \n     String location = table.table().location().replaceFirst(\"file:\", \"\");\n-    new File(location + \"/data/trashfile\").createNewFile();\n+    String trashFile = \"/data/trashfile\" + ThreadLocalRandom.current().nextInt(1000);\n+    new File(location + trashFile).createNewFile();\n \n     DeleteOrphanFiles.Result results =\n         SparkActions.get()\n@@ -184,7 +183,7 @@ public void testSparkSessionCatalogHiveTable() throws Exception {\n             .execute();\n     assertThat(StreamSupport.stream(results.orphanFileLocations().spliterator(), false))\n         .as(\"trash file should be removed\")\n-        .anyMatch(file -> file.contains(\"file:\" + location + \"/data/trashfile\"));\n+        .anyMatch(file -> file.contains(\"file:\" + location + trashFile));\n   }\n \n   @AfterEach\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\nindex b7ab47f865b5..38c4d32a90d2 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteDataFilesAction.java\n@@ -27,6 +27,7 @@\n import static org.apache.spark.sql.functions.min;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n import static org.mockito.ArgumentMatchers.any;\n import static org.mockito.ArgumentMatchers.argThat;\n import static org.mockito.Mockito.doAnswer;\n@@ -38,7 +39,7 @@\n import java.io.File;\n import java.io.IOException;\n import java.io.UncheckedIOException;\n-import java.nio.file.Path;\n+import java.util.Arrays;\n import java.util.Collections;\n import java.util.Comparator;\n import java.util.List;\n@@ -56,6 +57,9 @@\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.MetadataTableType;\n+import org.apache.iceberg.Parameter;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionData;\n import org.apache.iceberg.PartitionKey;\n import org.apache.iceberg.PartitionSpec;\n@@ -77,6 +81,8 @@\n import org.apache.iceberg.data.GenericAppenderFactory;\n import org.apache.iceberg.data.GenericRecord;\n import org.apache.iceberg.data.Record;\n+import org.apache.iceberg.deletes.BaseDVFileWriter;\n+import org.apache.iceberg.deletes.DVFileWriter;\n import org.apache.iceberg.deletes.EqualityDeleteWriter;\n import org.apache.iceberg.deletes.PositionDelete;\n import org.apache.iceberg.deletes.PositionDeleteWriter;\n@@ -96,7 +102,6 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n-import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.relocated.com.google.common.collect.Streams;\n import org.apache.iceberg.spark.FileRewriteCoordinator;\n import org.apache.iceberg.spark.ScanTaskSetManager;\n@@ -119,11 +124,13 @@\n import org.apache.spark.sql.internal.SQLConf;\n import org.junit.jupiter.api.BeforeAll;\n import org.junit.jupiter.api.BeforeEach;\n-import org.junit.jupiter.api.Test;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n import org.junit.jupiter.api.io.TempDir;\n import org.mockito.ArgumentMatcher;\n import org.mockito.Mockito;\n \n+@ExtendWith(ParameterizedTestExtension.class)\n public class TestRewriteDataFilesAction extends TestBase {\n \n   @TempDir private File tableDir;\n@@ -138,7 +145,12 @@ public class TestRewriteDataFilesAction extends TestBase {\n \n   private static final PartitionSpec SPEC = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n \n-  @TempDir private Path temp;\n+  @Parameter private int formatVersion;\n+\n+  @Parameters(name = \"formatVersion = {0}\")\n+  protected static List<Object> parameters() {\n+    return Arrays.asList(2, 3);\n+  }\n \n   private final FileRewriteCoordinator coordinator = FileRewriteCoordinator.get();\n   private final ScanTaskSetManager manager = ScanTaskSetManager.get();\n@@ -161,10 +173,11 @@ private RewriteDataFilesSparkAction basicRewrite(Table table) {\n     return actions().rewriteDataFiles(table).option(SizeBasedFileRewriter.MIN_INPUT_FILES, \"1\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testEmptyTable() {\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n-    Map<String, String> options = Maps.newHashMap();\n+    Map<String, String> options =\n+        ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion));\n     Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n \n     assertThat(table.currentSnapshot()).as(\"Table must be empty\").isNull();\n@@ -174,7 +187,7 @@ public void testEmptyTable() {\n     assertThat(table.currentSnapshot()).as(\"Table must stay empty\").isNull();\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackUnpartitionedTable() {\n     Table table = createTable(4);\n     shouldHaveFiles(table, 4);\n@@ -194,7 +207,7 @@ public void testBinPackUnpartitionedTable() {\n     assertEquals(\"Rows must match\", expectedRecords, actual);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackPartitionedTable() {\n     Table table = createTablePartitioned(4, 2);\n     shouldHaveFiles(table, 8);\n@@ -214,7 +227,7 @@ public void testBinPackPartitionedTable() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackWithFilter() {\n     Table table = createTablePartitioned(4, 2);\n     shouldHaveFiles(table, 8);\n@@ -239,7 +252,7 @@ public void testBinPackWithFilter() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackWithFilterOnBucketExpression() {\n     Table table = createTablePartitioned(4, 2);\n \n@@ -265,7 +278,7 @@ public void testBinPackWithFilterOnBucketExpression() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackAfterPartitionChange() {\n     Table table = createTable();\n \n@@ -300,10 +313,10 @@ public void testBinPackAfterPartitionChange() {\n     shouldHaveFiles(table, 20);\n   }\n \n-  @Test\n-  public void testBinPackWithDeletes() {\n+  @TestTemplate\n+  public void testBinPackWithDeletes() throws IOException {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n     Table table = createTablePartitioned(4, 2);\n-    table.updateProperties().set(TableProperties.FORMAT_VERSION, \"2\").commit();\n     shouldHaveFiles(table, 8);\n     table.refresh();\n \n@@ -311,14 +324,28 @@ public void testBinPackWithDeletes() {\n     int total = (int) dataFiles.stream().mapToLong(ContentFile::recordCount).sum();\n \n     RowDelta rowDelta = table.newRowDelta();\n-    // add 1 delete file for data files 0, 1, 2\n-    for (int i = 0; i < 3; i++) {\n-      writePosDeletesToFile(table, dataFiles.get(i), 1).forEach(rowDelta::addDeletes);\n-    }\n+    if (formatVersion >= 3) {\n+      // delete 1 position for data files 0, 1, 2\n+      for (int i = 0; i < 3; i++) {\n+        writeDV(table, dataFiles.get(i).partition(), dataFiles.get(i).location(), 1)\n+            .forEach(rowDelta::addDeletes);\n+      }\n \n-    // add 2 delete files for data files 3, 4\n-    for (int i = 3; i < 5; i++) {\n-      writePosDeletesToFile(table, dataFiles.get(i), 2).forEach(rowDelta::addDeletes);\n+      // delete 2 positions for data files 3, 4\n+      for (int i = 3; i < 5; i++) {\n+        writeDV(table, dataFiles.get(i).partition(), dataFiles.get(i).location(), 2)\n+            .forEach(rowDelta::addDeletes);\n+      }\n+    } else {\n+      // add 1 delete file for data files 0, 1, 2\n+      for (int i = 0; i < 3; i++) {\n+        writePosDeletesToFile(table, dataFiles.get(i), 1).forEach(rowDelta::addDeletes);\n+      }\n+\n+      // add 2 delete files for data files 3, 4\n+      for (int i = 3; i < 5; i++) {\n+        writePosDeletesToFile(table, dataFiles.get(i), 2).forEach(rowDelta::addDeletes);\n+      }\n     }\n \n     rowDelta.commit();\n@@ -326,32 +353,49 @@ public void testBinPackWithDeletes() {\n     List<Object[]> expectedRecords = currentData();\n     long dataSizeBefore = testDataSize(table);\n \n-    Result result =\n-        actions()\n-            .rewriteDataFiles(table)\n-            // do not include any file based on bin pack file size configs\n-            .option(SizeBasedFileRewriter.MIN_FILE_SIZE_BYTES, \"0\")\n-            .option(RewriteDataFiles.TARGET_FILE_SIZE_BYTES, Long.toString(Long.MAX_VALUE - 1))\n-            .option(SizeBasedFileRewriter.MAX_FILE_SIZE_BYTES, Long.toString(Long.MAX_VALUE))\n-            .option(SizeBasedDataRewriter.DELETE_FILE_THRESHOLD, \"2\")\n-            .execute();\n-    assertThat(result.rewrittenDataFilesCount())\n-        .as(\"Action should rewrite 2 data files\")\n-        .isEqualTo(2);\n-    assertThat(result.rewrittenBytesCount()).isGreaterThan(0L).isLessThan(dataSizeBefore);\n+    if (formatVersion >= 3) {\n+      Result result =\n+          actions()\n+              .rewriteDataFiles(table)\n+              // do not include any file based on bin pack file size configs\n+              .option(SizeBasedFileRewriter.MIN_FILE_SIZE_BYTES, \"0\")\n+              .option(RewriteDataFiles.TARGET_FILE_SIZE_BYTES, Long.toString(Long.MAX_VALUE - 1))\n+              .option(SizeBasedFileRewriter.MAX_FILE_SIZE_BYTES, Long.toString(Long.MAX_VALUE))\n+              // set DELETE_FILE_THRESHOLD to 1 since DVs only produce one delete file per data file\n+              .option(SizeBasedDataRewriter.DELETE_FILE_THRESHOLD, \"1\")\n+              .execute();\n+      assertThat(result.rewrittenDataFilesCount())\n+          .as(\"Action should rewrite 5 data files\")\n+          .isEqualTo(5);\n+      assertThat(result.rewrittenBytesCount()).isGreaterThan(0L).isLessThan(dataSizeBefore);\n+    } else {\n+      Result result =\n+          actions()\n+              .rewriteDataFiles(table)\n+              // do not include any file based on bin pack file size configs\n+              .option(SizeBasedFileRewriter.MIN_FILE_SIZE_BYTES, \"0\")\n+              .option(RewriteDataFiles.TARGET_FILE_SIZE_BYTES, Long.toString(Long.MAX_VALUE - 1))\n+              .option(SizeBasedFileRewriter.MAX_FILE_SIZE_BYTES, Long.toString(Long.MAX_VALUE))\n+              .option(SizeBasedDataRewriter.DELETE_FILE_THRESHOLD, \"2\")\n+              .execute();\n+      assertThat(result.rewrittenDataFilesCount())\n+          .as(\"Action should rewrite 2 data files\")\n+          .isEqualTo(2);\n+      assertThat(result.rewrittenBytesCount()).isGreaterThan(0L).isLessThan(dataSizeBefore);\n+    }\n \n     List<Object[]> actualRecords = currentData();\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n     assertThat(actualRecords).as(\"7 rows are removed\").hasSize(total - 7);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRemoveDangledEqualityDeletesPartitionEvolution() {\n     Table table =\n         TABLES.create(\n             SCHEMA,\n             SPEC,\n-            Collections.singletonMap(TableProperties.FORMAT_VERSION, \"2\"),\n+            ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion)),\n             tableLocation);\n \n     // data seq = 1, write 4 files in 2 partitions\n@@ -414,13 +458,13 @@ public void testRemoveDangledEqualityDeletesPartitionEvolution() {\n     shouldHaveFiles(table, 5);\n   }\n \n-  @Test\n-  public void testRemoveDangledPositionDeletesPartitionEvolution() {\n+  @TestTemplate\n+  public void testRemoveDangledPositionDeletesPartitionEvolution() throws IOException {\n     Table table =\n         TABLES.create(\n             SCHEMA,\n             SPEC,\n-            Collections.singletonMap(TableProperties.FORMAT_VERSION, \"2\"),\n+            ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion)),\n             tableLocation);\n \n     // data seq = 1, write 4 files in 2 partitions\n@@ -428,11 +472,15 @@ public void testRemoveDangledPositionDeletesPartitionEvolution() {\n     List<DataFile> dataFilesBefore = TestHelpers.dataFiles(table, null);\n     shouldHaveFiles(table, 4);\n \n+    DeleteFile deleteFile;\n     // data seq = 2, write 1 position deletes in c1=1\n-    table\n-        .newRowDelta()\n-        .addDeletes(writePosDeletesToFile(table, dataFilesBefore.get(3), 1).get(0))\n-        .commit();\n+    DataFile dataFile = dataFilesBefore.get(3);\n+    if (formatVersion >= 3) {\n+      deleteFile = writeDV(table, dataFile.partition(), dataFile.location(), 1).get(0);\n+    } else {\n+      deleteFile = writePosDeletesToFile(table, dataFile, 1).get(0);\n+    }\n+    table.newRowDelta().addDeletes(deleteFile).commit();\n \n     // partition evolution\n     table.updateSpec().addField(Expressions.ref(\"c3\")).commit();\n@@ -464,11 +512,10 @@ public void testRemoveDangledPositionDeletesPartitionEvolution() {\n     assertEquals(\"Rows must match\", expectedRecords, currentData());\n   }\n \n-  @Test\n-  public void testBinPackWithDeleteAllData() {\n-    Map<String, String> options = Maps.newHashMap();\n-    options.put(TableProperties.FORMAT_VERSION, \"2\");\n-    Table table = createTablePartitioned(1, 1, 1, options);\n+  @TestTemplate\n+  public void testBinPackWithDeleteAllData() throws IOException {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n+    Table table = createTablePartitioned(1, 1, 1);\n     shouldHaveFiles(table, 1);\n     table.refresh();\n \n@@ -476,8 +523,14 @@ public void testBinPackWithDeleteAllData() {\n     int total = (int) dataFiles.stream().mapToLong(ContentFile::recordCount).sum();\n \n     RowDelta rowDelta = table.newRowDelta();\n+    DataFile dataFile = dataFiles.get(0);\n     // remove all data\n-    writePosDeletesToFile(table, dataFiles.get(0), total).forEach(rowDelta::addDeletes);\n+    if (formatVersion >= 3) {\n+      writeDV(table, dataFile.partition(), dataFile.location(), total)\n+          .forEach(rowDelta::addDeletes);\n+    } else {\n+      writePosDeletesToFile(table, dataFile, total).forEach(rowDelta::addDeletes);\n+    }\n \n     rowDelta.commit();\n     table.refresh();\n@@ -507,12 +560,12 @@ public void testBinPackWithDeleteAllData() {\n         .isEqualTo(total);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackWithStartingSequenceNumber() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n     Table table = createTablePartitioned(4, 2);\n     shouldHaveFiles(table, 8);\n     List<Object[]> expectedRecords = currentData();\n-    table.updateProperties().set(TableProperties.FORMAT_VERSION, \"2\").commit();\n     table.refresh();\n     long oldSequenceNumber = table.currentSnapshot().sequenceNumber();\n     long dataSizeBefore = testDataSize(table);\n@@ -544,7 +597,7 @@ public void testBinPackWithStartingSequenceNumber() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackWithStartingSequenceNumberV1Compatibility() {\n     Map<String, String> properties = ImmutableMap.of(TableProperties.FORMAT_VERSION, \"1\");\n     Table table = createTablePartitioned(4, 2, SCALE, properties);\n@@ -580,11 +633,15 @@ public void testBinPackWithStartingSequenceNumberV1Compatibility() {\n     }\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteLargeTableHasResiduals() {\n     PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).build();\n-    Map<String, String> options = Maps.newHashMap();\n-    options.put(TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES, \"100\");\n+    Map<String, String> options =\n+        ImmutableMap.of(\n+            TableProperties.FORMAT_VERSION,\n+            String.valueOf(formatVersion),\n+            TableProperties.PARQUET_ROW_GROUP_SIZE_BYTES,\n+            \"100\");\n     Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n \n     // all records belong to the same partition\n@@ -622,7 +679,7 @@ public void testRewriteLargeTableHasResiduals() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackSplitLargeFile() {\n     Table table = createTable(1);\n     shouldHaveFiles(table, 1);\n@@ -647,7 +704,7 @@ public void testBinPackSplitLargeFile() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackCombineMixedFiles() {\n     Table table = createTable(1); // 400000\n     shouldHaveFiles(table, 1);\n@@ -683,7 +740,7 @@ public void testBinPackCombineMixedFiles() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackCombineMediumFiles() {\n     Table table = createTable(4);\n     shouldHaveFiles(table, 4);\n@@ -716,7 +773,7 @@ public void testBinPackCombineMediumFiles() {\n     assertEquals(\"Rows must match\", expectedRecords, actualRecords);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartialProgressEnabled() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -747,7 +804,7 @@ public void testPartialProgressEnabled() {\n     assertEquals(\"We shouldn't have changed the data\", originalData, postRewriteData);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testMultipleGroups() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -775,7 +832,7 @@ public void testMultipleGroups() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartialProgressMaxCommits() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -804,7 +861,7 @@ public void testPartialProgressMaxCommits() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSingleCommitWithRewriteFailure() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -838,7 +895,7 @@ public void testSingleCommitWithRewriteFailure() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSingleCommitWithCommitFailure() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -872,7 +929,7 @@ public void testSingleCommitWithCommitFailure() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCommitFailsWithUncleanableFailure() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -906,7 +963,7 @@ public void testCommitFailsWithUncleanableFailure() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testParallelSingleCommitWithRewriteFailure() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -941,7 +998,7 @@ public void testParallelSingleCommitWithRewriteFailure() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testPartialProgressWithRewriteFailure() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -983,7 +1040,7 @@ public void testPartialProgressWithRewriteFailure() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testParallelPartialProgressWithRewriteFailure() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -1026,7 +1083,7 @@ public void testParallelPartialProgressWithRewriteFailure() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testParallelPartialProgressWithCommitFailure() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -1071,7 +1128,7 @@ public void testParallelPartialProgressWithCommitFailure() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testParallelPartialProgressWithMaxFailedCommits() {\n     Table table = createTable(20);\n     int fileSize = averageFileSize(table);\n@@ -1113,7 +1170,7 @@ public void testParallelPartialProgressWithMaxFailedCommits() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidOptions() {\n     Table table = createTable(20);\n \n@@ -1157,7 +1214,7 @@ public void testInvalidOptions() {\n         .hasMessageContaining(\"requires enabling Iceberg Spark session extensions\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortMultipleGroups() {\n     Table table = createTable(20);\n     shouldHaveFiles(table, 20);\n@@ -1189,7 +1246,7 @@ public void testSortMultipleGroups() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSimpleSort() {\n     Table table = createTable(20);\n     shouldHaveFiles(table, 20);\n@@ -1222,7 +1279,7 @@ public void testSimpleSort() {\n     shouldHaveLastCommitSorted(table, \"c2\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortAfterPartitionChange() {\n     Table table = createTable(20);\n     shouldHaveFiles(table, 20);\n@@ -1258,7 +1315,7 @@ public void testSortAfterPartitionChange() {\n     shouldHaveLastCommitSorted(table, \"c2\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortCustomSortOrder() {\n     Table table = createTable(20);\n     shouldHaveLastCommitUnsorted(table, \"c2\");\n@@ -1289,7 +1346,7 @@ public void testSortCustomSortOrder() {\n     shouldHaveLastCommitSorted(table, \"c2\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortCustomSortOrderRequiresRepartition() {\n     int partitions = 4;\n     Table table = createTable();\n@@ -1329,7 +1386,7 @@ public void testSortCustomSortOrderRequiresRepartition() {\n     shouldHaveLastCommitSorted(table, \"c3\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testAutoSortShuffleOutput() {\n     Table table = createTable(20);\n     shouldHaveLastCommitUnsorted(table, \"c2\");\n@@ -1369,7 +1426,7 @@ public void testAutoSortShuffleOutput() {\n     shouldHaveLastCommitSorted(table, \"c2\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testCommitStateUnknownException() {\n     Table table = createTable(20);\n     shouldHaveFiles(table, 20);\n@@ -1401,7 +1458,7 @@ public void testCommitStateUnknownException() {\n     shouldHaveSnapshots(table, 2); // Commit actually Succeeded\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testZOrderSort() {\n     int originalFiles = 20;\n     Table table = createTable(originalFiles);\n@@ -1461,7 +1518,7 @@ public void testZOrderSort() {\n         .isGreaterThan(filesScannedC2C3);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testZOrderAllTypesSort() {\n     Table table = createTypeTestTable();\n     shouldHaveFiles(table, 10);\n@@ -1505,7 +1562,7 @@ public void testZOrderAllTypesSort() {\n     shouldHaveACleanCache(table);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testInvalidAPIUsage() {\n     Table table = createTable(1);\n \n@@ -1524,14 +1581,14 @@ public void testInvalidAPIUsage() {\n         .hasMessage(\"Must use only one rewriter type (bin-pack, sort, zorder)\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteJobOrderBytesAsc() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n     Table table = createTablePartitioned(4, 2);\n     writeRecords(1, SCALE, 1);\n     writeRecords(2, SCALE, 2);\n     writeRecords(3, SCALE, 3);\n     writeRecords(4, SCALE, 4);\n-    table.updateProperties().set(TableProperties.FORMAT_VERSION, \"2\").commit();\n \n     RewriteDataFilesSparkAction basicRewrite = basicRewrite(table).binPack();\n     List<Long> expected =\n@@ -1556,14 +1613,14 @@ public void testRewriteJobOrderBytesAsc() {\n     assertThat(actual).as(\"Size in bytes order should not be descending\").isNotEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteJobOrderBytesDesc() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n     Table table = createTablePartitioned(4, 2);\n     writeRecords(1, SCALE, 1);\n     writeRecords(2, SCALE, 2);\n     writeRecords(3, SCALE, 3);\n     writeRecords(4, SCALE, 4);\n-    table.updateProperties().set(TableProperties.FORMAT_VERSION, \"2\").commit();\n \n     RewriteDataFilesSparkAction basicRewrite = basicRewrite(table).binPack();\n     List<Long> expected =\n@@ -1588,14 +1645,14 @@ public void testRewriteJobOrderBytesDesc() {\n     assertThat(actual).as(\"Size in bytes order should not be ascending\").isNotEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteJobOrderFilesAsc() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n     Table table = createTablePartitioned(4, 2);\n     writeRecords(1, SCALE, 1);\n     writeRecords(2, SCALE, 2);\n     writeRecords(3, SCALE, 3);\n     writeRecords(4, SCALE, 4);\n-    table.updateProperties().set(TableProperties.FORMAT_VERSION, \"2\").commit();\n \n     RewriteDataFilesSparkAction basicRewrite = basicRewrite(table).binPack();\n     List<Long> expected =\n@@ -1620,14 +1677,14 @@ public void testRewriteJobOrderFilesAsc() {\n     assertThat(actual).as(\"Number of files order should not be descending\").isNotEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testRewriteJobOrderFilesDesc() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n     Table table = createTablePartitioned(4, 2);\n     writeRecords(1, SCALE, 1);\n     writeRecords(2, SCALE, 2);\n     writeRecords(3, SCALE, 3);\n     writeRecords(4, SCALE, 4);\n-    table.updateProperties().set(TableProperties.FORMAT_VERSION, \"2\").commit();\n \n     RewriteDataFilesSparkAction basicRewrite = basicRewrite(table).binPack();\n     List<Long> expected =\n@@ -1652,7 +1709,7 @@ public void testRewriteJobOrderFilesDesc() {\n     assertThat(actual).as(\"Number of files order should not be ascending\").isNotEqualTo(expected);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSnapshotProperty() {\n     Table table = createTable(4);\n     Result ignored = basicRewrite(table).snapshotProperty(\"key\", \"value\").execute();\n@@ -1669,7 +1726,7 @@ public void testSnapshotProperty() {\n     assertThat(table.currentSnapshot().summary()).containsKeys(commitMetricsKeys);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackRewriterWithSpecificUnparitionedOutputSpec() {\n     Table table = createTable(10);\n     shouldHaveFiles(table, 10);\n@@ -1691,7 +1748,7 @@ public void testBinPackRewriterWithSpecificUnparitionedOutputSpec() {\n     shouldRewriteDataFilesWithPartitionSpec(table, outputSpecId);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinPackRewriterWithSpecificOutputSpec() {\n     Table table = createTable(10);\n     shouldHaveFiles(table, 10);\n@@ -1714,7 +1771,7 @@ public void testBinPackRewriterWithSpecificOutputSpec() {\n     shouldRewriteDataFilesWithPartitionSpec(table, outputSpecId);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testBinpackRewriteWithInvalidOutputSpecId() {\n     Table table = createTable(10);\n     shouldHaveFiles(table, 10);\n@@ -1730,7 +1787,7 @@ public void testBinpackRewriteWithInvalidOutputSpecId() {\n             \"Cannot use output spec id 1234 because the table does not contain a reference to this spec-id.\");\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testSortRewriterWithSpecificOutputSpecId() {\n     Table table = createTable(10);\n     shouldHaveFiles(table, 10);\n@@ -1753,7 +1810,7 @@ public void testSortRewriterWithSpecificOutputSpecId() {\n     shouldRewriteDataFilesWithPartitionSpec(table, outputSpecId);\n   }\n \n-  @Test\n+  @TestTemplate\n   public void testZOrderRewriteWithSpecificOutputSpecId() {\n     Table table = createTable(10);\n     shouldHaveFiles(table, 10);\n@@ -1956,7 +2013,8 @@ private <T> List<Pair<Pair<T, T>, Pair<T, T>>> checkForOverlappingFiles(\n \n   protected Table createTable() {\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n-    Map<String, String> options = Maps.newHashMap();\n+    Map<String, String> options =\n+        ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion));\n     Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n     table\n         .updateProperties()\n@@ -1989,7 +2047,19 @@ protected Table createTablePartitioned(\n   }\n \n   protected Table createTablePartitioned(int partitions, int files) {\n-    return createTablePartitioned(partitions, files, SCALE, Maps.newHashMap());\n+    return createTablePartitioned(\n+        partitions,\n+        files,\n+        SCALE,\n+        ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion)));\n+  }\n+\n+  protected Table createTablePartitioned(int partitions, int files, int numRecords) {\n+    return createTablePartitioned(\n+        partitions,\n+        files,\n+        numRecords,\n+        ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion)));\n   }\n \n   private Table createTypeTestTable() {\n@@ -2005,7 +2075,8 @@ private Table createTypeTestTable() {\n             optional(8, \"booleanCol\", Types.BooleanType.get()),\n             optional(9, \"binaryCol\", Types.BinaryType.get()));\n \n-    Map<String, String> options = Maps.newHashMap();\n+    Map<String, String> options =\n+        ImmutableMap.of(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion));\n     Table table = TABLES.create(schema, PartitionSpec.unpartitioned(), options, tableLocation);\n \n     spark\n@@ -2123,6 +2194,20 @@ private List<DeleteFile> writePosDeletes(\n     return results;\n   }\n \n+  private List<DeleteFile> writeDV(\n+      Table table, StructLike partition, String path, int numPositionsToDelete) throws IOException {\n+    OutputFileFactory fileFactory =\n+        OutputFileFactory.builderFor(table, 1, 1).format(FileFormat.PUFFIN).build();\n+    DVFileWriter writer = new BaseDVFileWriter(fileFactory, p -> null);\n+    try (DVFileWriter closeableWriter = writer) {\n+      for (int row = 0; row < numPositionsToDelete; row++) {\n+        closeableWriter.delete(path, row, table.spec(), partition);\n+      }\n+    }\n+\n+    return writer.result().deleteFiles();\n+  }\n+\n   private void writeEqDeleteRecord(\n       Table table, String partCol, Object partVal, String delCol, Object delVal) {\n     List<Integer> equalityFieldIds = Lists.newArrayList(table.schema().findField(delCol).fieldId());\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\nindex 11d61e599eba..44971843547b 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\n@@ -37,6 +37,7 @@\n import java.util.List;\n import java.util.Map;\n import java.util.UUID;\n+import java.util.concurrent.ThreadLocalRandom;\n import org.apache.hadoop.conf.Configuration;\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataFiles;\n@@ -107,6 +108,7 @@ public static Object[] parameters() {\n       new Object[] {\"false\", \"true\", true, 1},\n       new Object[] {\"true\", \"false\", false, 2},\n       new Object[] {\"false\", \"false\", false, 2},\n+      new Object[] {\"true\", \"false\", false, 3},\n       new Object[] {\"false\", \"false\", false, 3}\n     };\n   }\n@@ -1108,13 +1110,23 @@ private DeleteFile newDV(Table table, DataFile dataFile) {\n   }\n \n   private DeleteFile newDeleteFile(Table table, String partitionPath) {\n-    return FileMetadata.deleteFileBuilder(table.spec())\n-        .ofPositionDeletes()\n-        .withPath(\"/path/to/pos-deletes-\" + UUID.randomUUID() + \".parquet\")\n-        .withFileSizeInBytes(5)\n-        .withPartitionPath(partitionPath)\n-        .withRecordCount(1)\n-        .build();\n+    return formatVersion >= 3\n+        ? FileMetadata.deleteFileBuilder(table.spec())\n+            .ofPositionDeletes()\n+            .withPath(\"/path/to/pos-deletes-\" + UUID.randomUUID() + \".puffin\")\n+            .withFileSizeInBytes(5)\n+            .withPartitionPath(partitionPath)\n+            .withRecordCount(1)\n+            .withContentOffset(ThreadLocalRandom.current().nextInt())\n+            .withContentSizeInBytes(ThreadLocalRandom.current().nextInt())\n+            .build()\n+        : FileMetadata.deleteFileBuilder(table.spec())\n+            .ofPositionDeletes()\n+            .withPath(\"/path/to/pos-deletes-\" + UUID.randomUUID() + \".parquet\")\n+            .withFileSizeInBytes(5)\n+            .withPartitionPath(partitionPath)\n+            .withRecordCount(1)\n+            .build();\n   }\n \n   private List<Pair<CharSequence, Long>> generatePosDeletes(String predicate) {\n@@ -1145,7 +1157,7 @@ private Pair<DeleteFile, CharSequenceSet> writePosDeletes(\n       Table table, StructLike partition, List<Pair<CharSequence, Long>> deletes)\n       throws IOException {\n     OutputFile outputFile = Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile()));\n-    return FileHelpers.writeDeleteFile(table, outputFile, partition, deletes);\n+    return FileHelpers.writeDeleteFile(table, outputFile, partition, deletes, formatVersion);\n   }\n \n   private DeleteFile writeEqDeletes(Table table, String key, Object... values) throws IOException {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11481",
    "pr_id": 11481,
    "issue_id": 11122,
    "repo": "apache/iceberg",
    "problem_statement": "Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 118,
    "test_files_count": 1,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java",
      "data/src/main/java/org/apache/iceberg/data/BaseDeleteLoader.java",
      "data/src/main/java/org/apache/iceberg/data/DeleteLoader.java",
      "data/src/test/java/org/apache/iceberg/io/TestDVWriters.java"
    ],
    "pr_changed_test_files": [
      "data/src/test/java/org/apache/iceberg/io/TestDVWriters.java"
    ],
    "base_commit": "7938403a437f81a502fd82053dccff10a1531ada",
    "head_commit": "51811dce95809c801f3676881044c694834643f3",
    "repo_url": "https://github.com/apache/iceberg/pull/11481",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11481",
    "dockerfile": "",
    "pr_merged_at": "2024-11-08T17:09:50.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java b/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java\nindex 9e4a65be02ae..beffd3a955c9 100644\n--- a/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java\n@@ -26,6 +26,7 @@\n import org.apache.iceberg.FileContent;\n import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.types.Conversions;\n import org.apache.iceberg.types.Type;\n \n@@ -94,6 +95,10 @@ public static boolean isDV(DeleteFile deleteFile) {\n     return deleteFile.format() == FileFormat.PUFFIN;\n   }\n \n+  public static boolean containsSingleDV(Iterable<DeleteFile> deleteFiles) {\n+    return Iterables.size(deleteFiles) == 1 && Iterables.all(deleteFiles, ContentFileUtil::isDV);\n+  }\n+\n   public static String dvDesc(DeleteFile deleteFile) {\n     return String.format(\n         \"DV{location=%s, offset=%s, length=%s, referencedDataFile=%s}\",\n\ndiff --git a/data/src/main/java/org/apache/iceberg/data/BaseDeleteLoader.java b/data/src/main/java/org/apache/iceberg/data/BaseDeleteLoader.java\nindex 8a1ebf95abeb..796f4f92be33 100644\n--- a/data/src/main/java/org/apache/iceberg/data/BaseDeleteLoader.java\n+++ b/data/src/main/java/org/apache/iceberg/data/BaseDeleteLoader.java\n@@ -42,15 +42,20 @@\n import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.io.DeleteSchemaUtil;\n import org.apache.iceberg.io.InputFile;\n+import org.apache.iceberg.io.RangeReadable;\n+import org.apache.iceberg.io.SeekableInputStream;\n import org.apache.iceberg.orc.ORC;\n import org.apache.iceberg.orc.OrcRowReader;\n import org.apache.iceberg.parquet.Parquet;\n import org.apache.iceberg.parquet.ParquetValueReader;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.relocated.com.google.common.io.ByteStreams;\n import org.apache.iceberg.relocated.com.google.common.math.LongMath;\n import org.apache.iceberg.types.TypeUtil;\n import org.apache.iceberg.util.CharSequenceMap;\n+import org.apache.iceberg.util.ContentFileUtil;\n import org.apache.iceberg.util.StructLikeSet;\n import org.apache.iceberg.util.Tasks;\n import org.apache.iceberg.util.ThreadPools;\n@@ -143,9 +148,48 @@ private <T> Iterable<T> materialize(CloseableIterable<T> iterable) {\n     }\n   }\n \n+  /**\n+   * Loads the content of a deletion vector or position delete files for a given data file path into\n+   * a position index.\n+   *\n+   * <p>The deletion vector is currently loaded without caching as the existing Puffin reader\n+   * requires at least 3 requests to fetch the entire file. Caching a single deletion vector may\n+   * only be useful when multiple data file splits are processed on the same node, which is unlikely\n+   * as task locality is not guaranteed.\n+   *\n+   * <p>For position delete files, however, there is no efficient way to read deletes for a\n+   * particular data file. Therefore, caching may be more effective as such delete files potentially\n+   * apply to many data files, especially in unpartitioned tables and tables with deep partitions.\n+   * If a position delete file qualifies for caching, this method will attempt to cache a position\n+   * index for each referenced data file.\n+   *\n+   * @param deleteFiles a deletion vector or position delete files\n+   * @param filePath the data file path for which to load deletes\n+   * @return a position delete index for the provided data file path\n+   */\n   @Override\n   public PositionDeleteIndex loadPositionDeletes(\n       Iterable<DeleteFile> deleteFiles, CharSequence filePath) {\n+    if (ContentFileUtil.containsSingleDV(deleteFiles)) {\n+      DeleteFile dv = Iterables.getOnlyElement(deleteFiles);\n+      validateDV(dv, filePath);\n+      return readDV(dv);\n+    } else {\n+      return getOrReadPosDeletes(deleteFiles, filePath);\n+    }\n+  }\n+\n+  private PositionDeleteIndex readDV(DeleteFile dv) {\n+    LOG.trace(\"Opening DV file {}\", dv.location());\n+    InputFile inputFile = loadInputFile.apply(dv);\n+    long offset = dv.contentOffset();\n+    int length = dv.contentSizeInBytes().intValue();\n+    byte[] bytes = readBytes(inputFile, offset, length);\n+    return PositionDeleteIndex.deserialize(bytes, dv);\n+  }\n+\n+  private PositionDeleteIndex getOrReadPosDeletes(\n+      Iterable<DeleteFile> deleteFiles, CharSequence filePath) {\n     Iterable<PositionDeleteIndex> deletes =\n         execute(deleteFiles, deleteFile -> getOrReadPosDeletes(deleteFile, filePath));\n     return PositionDeleteIndexUtil.merge(deletes);\n@@ -259,4 +303,42 @@ private long estimateEqDeletesSize(DeleteFile deleteFile, Schema projection) {\n   private int estimateRecordSize(Schema schema) {\n     return schema.columns().stream().mapToInt(TypeUtil::estimateSize).sum();\n   }\n+\n+  private void validateDV(DeleteFile dv, CharSequence filePath) {\n+    Preconditions.checkArgument(\n+        dv.contentOffset() != null,\n+        \"Invalid DV, offset cannot be null: %s\",\n+        ContentFileUtil.dvDesc(dv));\n+    Preconditions.checkArgument(\n+        dv.contentSizeInBytes() != null,\n+        \"Invalid DV, length is null: %s\",\n+        ContentFileUtil.dvDesc(dv));\n+    Preconditions.checkArgument(\n+        dv.contentSizeInBytes() <= Integer.MAX_VALUE,\n+        \"Can't read DV larger than 2GB: %s\",\n+        dv.contentSizeInBytes());\n+    Preconditions.checkArgument(\n+        filePath.toString().equals(dv.referencedDataFile()),\n+        \"DV is expected to reference %s, not %s\",\n+        filePath,\n+        dv.referencedDataFile());\n+  }\n+\n+  private static byte[] readBytes(InputFile inputFile, long offset, int length) {\n+    try (SeekableInputStream stream = inputFile.newStream()) {\n+      byte[] bytes = new byte[length];\n+\n+      if (stream instanceof RangeReadable) {\n+        RangeReadable rangeReadable = (RangeReadable) stream;\n+        rangeReadable.readFully(offset, bytes);\n+      } else {\n+        stream.seek(offset);\n+        ByteStreams.readFully(stream, bytes);\n+      }\n+\n+      return bytes;\n+    } catch (IOException e) {\n+      throw new UncheckedIOException(e);\n+    }\n+  }\n }\n\ndiff --git a/data/src/main/java/org/apache/iceberg/data/DeleteLoader.java b/data/src/main/java/org/apache/iceberg/data/DeleteLoader.java\nindex 07bdce6d836f..0fc0b93f7350 100644\n--- a/data/src/main/java/org/apache/iceberg/data/DeleteLoader.java\n+++ b/data/src/main/java/org/apache/iceberg/data/DeleteLoader.java\n@@ -35,9 +35,10 @@ public interface DeleteLoader {\n   StructLikeSet loadEqualityDeletes(Iterable<DeleteFile> deleteFiles, Schema projection);\n \n   /**\n-   * Loads the content of position delete files for a given data file path into a position index.\n+   * Loads the content of a deletion vector or position delete files for a given data file path into\n+   * a position index.\n    *\n-   * @param deleteFiles position delete files\n+   * @param deleteFiles a deletion vector or position delete files\n    * @param filePath the data file path for which to load deletes\n    * @return a position delete index for the provided data file path\n    */\n",
    "test_patch": "diff --git a/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java b/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java\nindex ce742b1c4685..23e0090ca49f 100644\n--- a/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java\n+++ b/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg.io;\n \n import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.IOException;\n import java.util.Arrays;\n@@ -28,17 +29,25 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.ParameterizedTestExtension;\n import org.apache.iceberg.Parameters;\n import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.RowDelta;\n import org.apache.iceberg.Table;\n+import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.data.BaseDeleteLoader;\n import org.apache.iceberg.data.DeleteLoader;\n import org.apache.iceberg.deletes.BaseDVFileWriter;\n import org.apache.iceberg.deletes.DVFileWriter;\n+import org.apache.iceberg.deletes.PositionDelete;\n import org.apache.iceberg.deletes.PositionDeleteIndex;\n+import org.apache.iceberg.deletes.PositionDeleteWriter;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n+import org.apache.iceberg.util.Pair;\n import org.apache.iceberg.util.StructLikeSet;\n import org.junit.jupiter.api.BeforeEach;\n import org.junit.jupiter.api.TestTemplate;\n@@ -49,10 +58,11 @@ public abstract class TestDVWriters<T> extends WriterTestBase<T> {\n \n   @Parameters(name = \"formatVersion = {0}\")\n   protected static List<Object> parameters() {\n-    return Arrays.asList(new Object[] {3});\n+    return Arrays.asList(new Object[] {2, 3});\n   }\n \n   private OutputFileFactory fileFactory = null;\n+  private OutputFileFactory parquetFileFactory = null;\n \n   protected abstract StructLikeSet toSet(Iterable<T> records);\n \n@@ -65,10 +75,14 @@ protected FileFormat dataFormat() {\n   public void setupTable() throws Exception {\n     this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n     this.fileFactory = OutputFileFactory.builderFor(table, 1, 1).format(FileFormat.PUFFIN).build();\n+    this.parquetFileFactory =\n+        OutputFileFactory.builderFor(table, 1, 1).format(FileFormat.PARQUET).build();\n   }\n \n   @TestTemplate\n   public void testBasicDVs() throws IOException {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(3);\n+\n     FileWriterFactory<T> writerFactory = newWriterFactory(table.schema());\n \n     // add the first data file\n@@ -100,6 +114,211 @@ public void testBasicDVs() throws IOException {\n         .contains(dataFile1.location())\n         .contains(dataFile2.location());\n     assertThat(result.referencesDataFiles()).isTrue();\n+\n+    // commit the deletes\n+    commit(result);\n+\n+    // verify correctness\n+    assertRows(ImmutableList.of(toRow(11, \"aaa\"), toRow(12, \"aaa\")));\n+  }\n+\n+  @TestTemplate\n+  public void testRewriteDVs() throws IOException {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(3);\n+\n+    FileWriterFactory<T> writerFactory = newWriterFactory(table.schema());\n+\n+    // add a data file with 3 data records\n+    List<T> rows = ImmutableList.of(toRow(1, \"aaa\"), toRow(2, \"aaa\"), toRow(3, \"aaa\"));\n+    DataFile dataFile = writeData(writerFactory, parquetFileFactory, rows, table.spec(), null);\n+    table.newFastAppend().appendFile(dataFile).commit();\n+\n+    // write the first DV\n+    DVFileWriter dvWriter1 =\n+        new BaseDVFileWriter(fileFactory, new PreviousDeleteLoader(table, ImmutableMap.of()));\n+    dvWriter1.delete(dataFile.location(), 1L, table.spec(), null);\n+    dvWriter1.close();\n+\n+    // validate the writer result\n+    DeleteWriteResult result1 = dvWriter1.result();\n+    assertThat(result1.deleteFiles()).hasSize(1);\n+    assertThat(result1.referencedDataFiles()).containsOnly(dataFile.location());\n+    assertThat(result1.referencesDataFiles()).isTrue();\n+    assertThat(result1.rewrittenDeleteFiles()).isEmpty();\n+\n+    // commit the first DV\n+    commit(result1);\n+    assertThat(table.currentSnapshot().addedDeleteFiles(table.io())).hasSize(1);\n+    assertThat(table.currentSnapshot().removedDeleteFiles(table.io())).isEmpty();\n+\n+    // verify correctness after committing the first DV\n+    assertRows(ImmutableList.of(toRow(1, \"aaa\"), toRow(3, \"aaa\")));\n+\n+    // write the second DV, merging with the first one\n+    DeleteFile dv1 = Iterables.getOnlyElement(result1.deleteFiles());\n+    DVFileWriter dvWriter2 =\n+        new BaseDVFileWriter(\n+            fileFactory,\n+            new PreviousDeleteLoader(table, ImmutableMap.of(dataFile.location(), dv1)));\n+    dvWriter2.delete(dataFile.location(), 2L, table.spec(), null);\n+    dvWriter2.close();\n+\n+    // validate the writer result\n+    DeleteWriteResult result2 = dvWriter2.result();\n+    assertThat(result2.deleteFiles()).hasSize(1);\n+    assertThat(result2.referencedDataFiles()).containsOnly(dataFile.location());\n+    assertThat(result2.referencesDataFiles()).isTrue();\n+    assertThat(result2.rewrittenDeleteFiles()).hasSize(1);\n+\n+    // replace DVs\n+    commit(result2);\n+    assertThat(table.currentSnapshot().addedDeleteFiles(table.io())).hasSize(1);\n+    assertThat(table.currentSnapshot().removedDeleteFiles(table.io())).hasSize(1);\n+\n+    // verify correctness after replacing DVs\n+    assertRows(ImmutableList.of(toRow(1, \"aaa\")));\n+  }\n+\n+  @TestTemplate\n+  public void testRewriteFileScopedPositionDeletes() throws IOException {\n+    assumeThat(formatVersion).isEqualTo(2);\n+\n+    FileWriterFactory<T> writerFactory = newWriterFactory(table.schema());\n+\n+    // add a data file with 3 records\n+    List<T> rows = ImmutableList.of(toRow(1, \"aaa\"), toRow(2, \"aaa\"), toRow(3, \"aaa\"));\n+    DataFile dataFile = writeData(writerFactory, parquetFileFactory, rows, table.spec(), null);\n+    table.newFastAppend().appendFile(dataFile).commit();\n+\n+    // add a file-scoped position delete file\n+    DeleteFile deleteFile =\n+        writePositionDeletes(writerFactory, ImmutableList.of(Pair.of(dataFile.location(), 0L)));\n+    table.newRowDelta().addDeletes(deleteFile).commit();\n+\n+    // verify correctness after adding the file-scoped position delete\n+    assertRows(ImmutableList.of(toRow(2, \"aaa\"), toRow(3, \"aaa\")));\n+\n+    // upgrade the table to V3 to enable DVs\n+    table.updateProperties().set(TableProperties.FORMAT_VERSION, \"3\").commit();\n+\n+    // write a DV, merging with the file-scoped position delete\n+    DVFileWriter dvWriter =\n+        new BaseDVFileWriter(\n+            fileFactory,\n+            new PreviousDeleteLoader(table, ImmutableMap.of(dataFile.location(), deleteFile)));\n+    dvWriter.delete(dataFile.location(), 1L, table.spec(), null);\n+    dvWriter.close();\n+\n+    // validate the writer result\n+    DeleteWriteResult result = dvWriter.result();\n+    assertThat(result.deleteFiles()).hasSize(1);\n+    assertThat(result.referencedDataFiles()).containsOnly(dataFile.location());\n+    assertThat(result.referencesDataFiles()).isTrue();\n+    assertThat(result.rewrittenDeleteFiles()).hasSize(1);\n+\n+    // replace the position delete file with the DV\n+    commit(result);\n+    assertThat(table.currentSnapshot().addedDeleteFiles(table.io())).hasSize(1);\n+    assertThat(table.currentSnapshot().removedDeleteFiles(table.io())).hasSize(1);\n+\n+    // verify correctness\n+    assertRows(ImmutableList.of(toRow(3, \"aaa\")));\n+  }\n+\n+  @TestTemplate\n+  public void testApplyPartitionScopedPositionDeletes() throws IOException {\n+    assumeThat(formatVersion).isEqualTo(2);\n+\n+    FileWriterFactory<T> writerFactory = newWriterFactory(table.schema());\n+\n+    // add the first data file with 3 records\n+    List<T> rows1 = ImmutableList.of(toRow(1, \"aaa\"), toRow(2, \"aaa\"), toRow(3, \"aaa\"));\n+    DataFile dataFile1 = writeData(writerFactory, parquetFileFactory, rows1, table.spec(), null);\n+    table.newFastAppend().appendFile(dataFile1).commit();\n+\n+    // add the second data file with 3 records\n+    List<T> rows2 = ImmutableList.of(toRow(4, \"aaa\"), toRow(5, \"aaa\"), toRow(6, \"aaa\"));\n+    DataFile dataFile2 = writeData(writerFactory, parquetFileFactory, rows2, table.spec(), null);\n+    table.newFastAppend().appendFile(dataFile2).commit();\n+\n+    // add a position delete file with deletes for both data files\n+    DeleteFile deleteFile =\n+        writePositionDeletes(\n+            writerFactory,\n+            ImmutableList.of(\n+                Pair.of(dataFile1.location(), 0L),\n+                Pair.of(dataFile1.location(), 1L),\n+                Pair.of(dataFile2.location(), 0L)));\n+    table.newRowDelta().addDeletes(deleteFile).commit();\n+\n+    // verify correctness with the position delete file\n+    assertRows(ImmutableList.of(toRow(3, \"aaa\"), toRow(5, \"aaa\"), toRow(6, \"aaa\")));\n+\n+    // upgrade the table to V3 to enable DVs\n+    table.updateProperties().set(TableProperties.FORMAT_VERSION, \"3\").commit();\n+\n+    // write a DV, applying old positions but keeping the position delete file in place\n+    DVFileWriter dvWriter =\n+        new BaseDVFileWriter(\n+            fileFactory,\n+            new PreviousDeleteLoader(table, ImmutableMap.of(dataFile2.location(), deleteFile)));\n+    dvWriter.delete(dataFile2.location(), 1L, table.spec(), null);\n+    dvWriter.close();\n+\n+    // validate the writer result\n+    DeleteWriteResult result = dvWriter.result();\n+    assertThat(result.deleteFiles()).hasSize(1);\n+    assertThat(result.referencedDataFiles()).containsOnly(dataFile2.location());\n+    assertThat(result.referencesDataFiles()).isTrue();\n+    assertThat(result.rewrittenDeleteFiles()).isEmpty();\n+    DeleteFile dv = Iterables.getOnlyElement(result.deleteFiles());\n+\n+    // commit the DV, ensuring the position delete file remains\n+    commit(result);\n+    assertThat(table.currentSnapshot().addedDeleteFiles(table.io())).hasSize(1);\n+    assertThat(table.currentSnapshot().removedDeleteFiles(table.io())).isEmpty();\n+\n+    // verify correctness with DVs and position delete files\n+    assertRows(ImmutableList.of(toRow(3, \"aaa\"), toRow(6, \"aaa\")));\n+\n+    // verify the position delete file applies only to the data file without the DV\n+    try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n+      for (FileScanTask task : tasks) {\n+        DeleteFile taskDeleteFile = Iterables.getOnlyElement(task.deletes());\n+        if (task.file().location().equals(dataFile1.location())) {\n+          assertThat(taskDeleteFile.location()).isEqualTo(deleteFile.location());\n+        } else {\n+          assertThat(taskDeleteFile.location()).isEqualTo(dv.location());\n+        }\n+      }\n+    }\n+  }\n+\n+  private void commit(DeleteWriteResult result) {\n+    RowDelta rowDelta = table.newRowDelta();\n+    result.rewrittenDeleteFiles().forEach(rowDelta::removeDeletes);\n+    result.deleteFiles().forEach(rowDelta::addDeletes);\n+    rowDelta.commit();\n+  }\n+\n+  private void assertRows(Iterable<T> expectedRows) throws IOException {\n+    assertThat(actualRowSet(\"*\")).isEqualTo(toSet(expectedRows));\n+  }\n+\n+  private DeleteFile writePositionDeletes(\n+      FileWriterFactory<T> writerFactory, List<Pair<String, Long>> deletes) throws IOException {\n+    EncryptedOutputFile file = parquetFileFactory.newOutputFile(table.spec(), null);\n+    PositionDeleteWriter<T> writer =\n+        writerFactory.newPositionDeleteWriter(file, table.spec(), null);\n+    PositionDelete<T> posDelete = PositionDelete.create();\n+\n+    try (PositionDeleteWriter<T> closableWriter = writer) {\n+      for (Pair<String, Long> delete : deletes) {\n+        closableWriter.write(posDelete.set(delete.first(), delete.second()));\n+      }\n+    }\n+\n+    return writer.toDeleteFile();\n   }\n \n   private static class PreviousDeleteLoader implements Function<String, PositionDeleteIndex> {\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11476",
    "pr_id": 11476,
    "issue_id": 11122,
    "repo": "apache/iceberg",
    "problem_statement": "Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 118,
    "test_files_count": 2,
    "non_test_files_count": 11,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/deletes/BaseDVFileWriter.java",
      "core/src/main/java/org/apache/iceberg/deletes/DVFileWriter.java",
      "core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java",
      "core/src/main/java/org/apache/iceberg/io/ClusteredWriter.java",
      "core/src/main/java/org/apache/iceberg/io/FanoutWriter.java",
      "core/src/main/java/org/apache/iceberg/io/StructCopy.java",
      "core/src/main/java/org/apache/iceberg/puffin/Puffin.java",
      "core/src/main/java/org/apache/iceberg/puffin/PuffinWriter.java",
      "core/src/main/java/org/apache/iceberg/puffin/StandardBlobTypes.java",
      "core/src/main/java/org/apache/iceberg/util/StructLikeUtil.java",
      "data/src/test/java/org/apache/iceberg/io/TestDVWriters.java",
      "spark/v3.5/spark/src/jmh/java/org/apache/iceberg/spark/source/DVWriterBenchmark.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDVWriters.java"
    ],
    "pr_changed_test_files": [
      "data/src/test/java/org/apache/iceberg/io/TestDVWriters.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDVWriters.java"
    ],
    "base_commit": "9be7f00dd6a9fb480a94c46d49473334908be859",
    "head_commit": "89b310799da8123c07470e5625f226f89f1c0259",
    "repo_url": "https://github.com/apache/iceberg/pull/11476",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11476",
    "dockerfile": "",
    "pr_merged_at": "2024-11-06T20:32:07.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/deletes/BaseDVFileWriter.java b/core/src/main/java/org/apache/iceberg/deletes/BaseDVFileWriter.java\nnew file mode 100644\nindex 000000000000..ab47d8ad78d6\n--- /dev/null\n+++ b/core/src/main/java/org/apache/iceberg/deletes/BaseDVFileWriter.java\n@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.deletes;\n+\n+import java.io.IOException;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileMetadata;\n+import org.apache.iceberg.IcebergBuild;\n+import org.apache.iceberg.MetadataColumns;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n+import org.apache.iceberg.io.DeleteWriteResult;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.puffin.Blob;\n+import org.apache.iceberg.puffin.BlobMetadata;\n+import org.apache.iceberg.puffin.Puffin;\n+import org.apache.iceberg.puffin.PuffinWriter;\n+import org.apache.iceberg.puffin.StandardBlobTypes;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.util.CharSequenceSet;\n+import org.apache.iceberg.util.ContentFileUtil;\n+import org.apache.iceberg.util.StructLikeUtil;\n+\n+public class BaseDVFileWriter implements DVFileWriter {\n+\n+  private static final String REFERENCED_DATA_FILE_KEY = \"referenced-data-file\";\n+  private static final String CARDINALITY_KEY = \"cardinality\";\n+\n+  private final OutputFileFactory fileFactory;\n+  private final Function<String, PositionDeleteIndex> loadPreviousDeletes;\n+  private final Map<String, Deletes> deletesByPath = Maps.newHashMap();\n+  private final Map<String, BlobMetadata> blobsByPath = Maps.newHashMap();\n+  private DeleteWriteResult result = null;\n+\n+  public BaseDVFileWriter(\n+      OutputFileFactory fileFactory, Function<String, PositionDeleteIndex> loadPreviousDeletes) {\n+    this.fileFactory = fileFactory;\n+    this.loadPreviousDeletes = loadPreviousDeletes;\n+  }\n+\n+  @Override\n+  public void delete(String path, long pos, PartitionSpec spec, StructLike partition) {\n+    Deletes deletes =\n+        deletesByPath.computeIfAbsent(path, key -> new Deletes(path, spec, partition));\n+    PositionDeleteIndex positions = deletes.positions();\n+    positions.delete(pos);\n+  }\n+\n+  @Override\n+  public DeleteWriteResult result() {\n+    Preconditions.checkState(result != null, \"Cannot get result from unclosed writer\");\n+    return result;\n+  }\n+\n+  @Override\n+  public void close() throws IOException {\n+    if (result == null) {\n+      List<DeleteFile> dvs = Lists.newArrayList();\n+      CharSequenceSet referencedDataFiles = CharSequenceSet.empty();\n+      List<DeleteFile> rewrittenDeleteFiles = Lists.newArrayList();\n+\n+      PuffinWriter writer = newWriter();\n+\n+      try (PuffinWriter closeableWriter = writer) {\n+        for (Deletes deletes : deletesByPath.values()) {\n+          String path = deletes.path();\n+          PositionDeleteIndex positions = deletes.positions();\n+          PositionDeleteIndex previousPositions = loadPreviousDeletes.apply(path);\n+          if (previousPositions != null) {\n+            positions.merge(previousPositions);\n+            for (DeleteFile previousDeleteFile : previousPositions.deleteFiles()) {\n+              // only DVs and file-scoped deletes can be discarded from the table state\n+              if (ContentFileUtil.isFileScoped(previousDeleteFile)) {\n+                rewrittenDeleteFiles.add(previousDeleteFile);\n+              }\n+            }\n+          }\n+          write(closeableWriter, deletes);\n+          referencedDataFiles.add(path);\n+        }\n+      }\n+\n+      // DVs share the Puffin path and file size but have different offsets\n+      String puffinPath = writer.location();\n+      long puffinFileSize = writer.fileSize();\n+\n+      for (String path : deletesByPath.keySet()) {\n+        DeleteFile dv = createDV(puffinPath, puffinFileSize, path);\n+        dvs.add(dv);\n+      }\n+\n+      this.result = new DeleteWriteResult(dvs, referencedDataFiles, rewrittenDeleteFiles);\n+    }\n+  }\n+\n+  private DeleteFile createDV(String path, long size, String referencedDataFile) {\n+    Deletes deletes = deletesByPath.get(referencedDataFile);\n+    BlobMetadata blobMetadata = blobsByPath.get(referencedDataFile);\n+    return FileMetadata.deleteFileBuilder(deletes.spec())\n+        .ofPositionDeletes()\n+        .withFormat(FileFormat.PUFFIN)\n+        .withPath(path)\n+        .withPartition(deletes.partition())\n+        .withFileSizeInBytes(size)\n+        .withReferencedDataFile(referencedDataFile)\n+        .withContentOffset(blobMetadata.offset())\n+        .withContentSizeInBytes(blobMetadata.length())\n+        .withRecordCount(deletes.positions().cardinality())\n+        .build();\n+  }\n+\n+  private void write(PuffinWriter writer, Deletes deletes) {\n+    String path = deletes.path();\n+    PositionDeleteIndex positions = deletes.positions();\n+    BlobMetadata blobMetadata = writer.write(toBlob(positions, path));\n+    blobsByPath.put(path, blobMetadata);\n+  }\n+\n+  private PuffinWriter newWriter() {\n+    EncryptedOutputFile outputFile = fileFactory.newOutputFile();\n+    String ident = \"Iceberg \" + IcebergBuild.fullVersion();\n+    return Puffin.write(outputFile).createdBy(ident).build();\n+  }\n+\n+  private Blob toBlob(PositionDeleteIndex positions, String path) {\n+    return new Blob(\n+        StandardBlobTypes.DV_V1,\n+        ImmutableList.of(MetadataColumns.ROW_POSITION.fieldId()),\n+        -1 /* snapshot ID is inherited */,\n+        -1 /* sequence number is inherited */,\n+        positions.serialize(),\n+        null /* uncompressed */,\n+        ImmutableMap.of(\n+            REFERENCED_DATA_FILE_KEY,\n+            path,\n+            CARDINALITY_KEY,\n+            String.valueOf(positions.cardinality())));\n+  }\n+\n+  private static class Deletes {\n+    private final String path;\n+    private final PositionDeleteIndex positions;\n+    private final PartitionSpec spec;\n+    private final StructLike partition;\n+\n+    private Deletes(String path, PartitionSpec spec, StructLike partition) {\n+      this.path = path;\n+      this.positions = new BitmapPositionDeleteIndex();\n+      this.spec = spec;\n+      this.partition = StructLikeUtil.copy(partition);\n+    }\n+\n+    public String path() {\n+      return path;\n+    }\n+\n+    public PositionDeleteIndex positions() {\n+      return positions;\n+    }\n+\n+    public PartitionSpec spec() {\n+      return spec;\n+    }\n+\n+    public StructLike partition() {\n+      return partition;\n+    }\n+  }\n+}\n\ndiff --git a/core/src/main/java/org/apache/iceberg/deletes/DVFileWriter.java b/core/src/main/java/org/apache/iceberg/deletes/DVFileWriter.java\nnew file mode 100644\nindex 000000000000..2561f7be3d34\n--- /dev/null\n+++ b/core/src/main/java/org/apache/iceberg/deletes/DVFileWriter.java\n@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.deletes;\n+\n+import java.io.Closeable;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.StructLike;\n+import org.apache.iceberg.io.DeleteWriteResult;\n+\n+/** A deletion vector file writer. */\n+public interface DVFileWriter extends Closeable {\n+  /**\n+   * Marks a position in a given data file as deleted.\n+   *\n+   * @param path the data file path\n+   * @param pos the data file position\n+   * @param spec the data file partition spec\n+   * @param partition the data file partition\n+   */\n+  void delete(String path, long pos, PartitionSpec spec, StructLike partition);\n+\n+  /**\n+   * Returns a result that contains information about written {@link DeleteFile}s. The result is\n+   * valid only after the writer is closed.\n+   *\n+   * @return the writer result\n+   */\n+  DeleteWriteResult result();\n+}\n\ndiff --git a/core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java b/core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java\nindex 471dc3e56035..968db0ab538b 100644\n--- a/core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java\n+++ b/core/src/main/java/org/apache/iceberg/io/BaseTaskWriter.java\n@@ -40,6 +40,7 @@\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.util.CharSequenceSet;\n import org.apache.iceberg.util.StructLikeMap;\n+import org.apache.iceberg.util.StructLikeUtil;\n import org.apache.iceberg.util.StructProjection;\n import org.apache.iceberg.util.Tasks;\n import org.apache.iceberg.util.ThreadPools;\n@@ -149,7 +150,7 @@ public void write(T row) throws IOException {\n       PathOffset pathOffset = PathOffset.of(dataWriter.currentPath(), dataWriter.currentRows());\n \n       // Create a copied key from this row.\n-      StructLike copiedKey = StructCopy.copy(structProjection.wrap(asStructLike(row)));\n+      StructLike copiedKey = StructLikeUtil.copy(structProjection.wrap(asStructLike(row)));\n \n       // Adding a pos-delete to replace the old path-offset.\n       PathOffset previous = insertedRowMap.put(copiedKey, pathOffset);\n\ndiff --git a/core/src/main/java/org/apache/iceberg/io/ClusteredWriter.java b/core/src/main/java/org/apache/iceberg/io/ClusteredWriter.java\nindex 1dc4871f0a12..06093ff2a943 100644\n--- a/core/src/main/java/org/apache/iceberg/io/ClusteredWriter.java\n+++ b/core/src/main/java/org/apache/iceberg/io/ClusteredWriter.java\n@@ -29,6 +29,7 @@\n import org.apache.iceberg.types.Comparators;\n import org.apache.iceberg.types.Types.StructType;\n import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.iceberg.util.StructLikeUtil;\n \n /**\n  * A writer capable of writing to multiple specs and partitions that requires the incoming records\n@@ -81,7 +82,7 @@ public void write(T row, PartitionSpec spec, StructLike partition) {\n       this.partitionComparator = Comparators.forType(partitionType);\n       this.completedPartitions = StructLikeSet.create(partitionType);\n       // copy the partition key as the key object may be reused\n-      this.currentPartition = StructCopy.copy(partition);\n+      this.currentPartition = StructLikeUtil.copy(partition);\n       this.currentWriter = newWriter(currentSpec, currentPartition);\n \n     } else if (partition != currentPartition\n@@ -96,7 +97,7 @@ public void write(T row, PartitionSpec spec, StructLike partition) {\n       }\n \n       // copy the partition key as the key object may be reused\n-      this.currentPartition = StructCopy.copy(partition);\n+      this.currentPartition = StructLikeUtil.copy(partition);\n       this.currentWriter = newWriter(currentSpec, currentPartition);\n     }\n \n\ndiff --git a/core/src/main/java/org/apache/iceberg/io/FanoutWriter.java b/core/src/main/java/org/apache/iceberg/io/FanoutWriter.java\nindex 95d5cc1afcc9..340f4b3558d9 100644\n--- a/core/src/main/java/org/apache/iceberg/io/FanoutWriter.java\n+++ b/core/src/main/java/org/apache/iceberg/io/FanoutWriter.java\n@@ -25,6 +25,7 @@\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n import org.apache.iceberg.util.StructLikeMap;\n+import org.apache.iceberg.util.StructLikeUtil;\n \n /**\n  * A writer capable of writing to multiple specs and partitions that keeps files for each seen\n@@ -59,7 +60,7 @@ private FileWriter<T, R> writer(PartitionSpec spec, StructLike partition) {\n \n     if (writer == null) {\n       // copy the partition key as the key object may be reused\n-      StructLike copiedPartition = StructCopy.copy(partition);\n+      StructLike copiedPartition = StructLikeUtil.copy(partition);\n       writer = newWriter(spec, copiedPartition);\n       specWriters.put(copiedPartition, writer);\n     }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/io/StructCopy.java b/core/src/main/java/org/apache/iceberg/io/StructCopy.java\nindex 229dff371762..f8adbf62a1d6 100644\n--- a/core/src/main/java/org/apache/iceberg/io/StructCopy.java\n+++ b/core/src/main/java/org/apache/iceberg/io/StructCopy.java\n@@ -20,7 +20,13 @@\n \n import org.apache.iceberg.StructLike;\n \n-/** Copy the StructLike's values into a new one. It does not handle list or map values now. */\n+/**\n+ * Copy the StructLike's values into a new one. It does not handle list or map values now.\n+ *\n+ * @deprecated since 1.8.0, will be removed in 1.9.0; use {@link\n+ *     org.apache.iceberg.util.StructLikeUtil#copy(StructLike)} instead.\n+ */\n+@Deprecated\n class StructCopy implements StructLike {\n   static StructLike copy(StructLike struct) {\n     return struct != null ? new StructCopy(struct) : null;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/puffin/Puffin.java b/core/src/main/java/org/apache/iceberg/puffin/Puffin.java\nindex 251486d01e76..d20a5596c4d4 100644\n--- a/core/src/main/java/org/apache/iceberg/puffin/Puffin.java\n+++ b/core/src/main/java/org/apache/iceberg/puffin/Puffin.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg.puffin;\n \n import java.util.Map;\n+import org.apache.iceberg.encryption.EncryptedOutputFile;\n import org.apache.iceberg.io.InputFile;\n import org.apache.iceberg.io.OutputFile;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n@@ -31,6 +32,10 @@ public static WriteBuilder write(OutputFile outputFile) {\n     return new WriteBuilder(outputFile);\n   }\n \n+  public static WriteBuilder write(EncryptedOutputFile outputFile) {\n+    return new WriteBuilder(outputFile.encryptingOutputFile());\n+  }\n+\n   /** A builder for {@link PuffinWriter}. */\n   public static class WriteBuilder {\n     private final OutputFile outputFile;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/puffin/PuffinWriter.java b/core/src/main/java/org/apache/iceberg/puffin/PuffinWriter.java\nindex 5728b7474885..cd44dab03b90 100644\n--- a/core/src/main/java/org/apache/iceberg/puffin/PuffinWriter.java\n+++ b/core/src/main/java/org/apache/iceberg/puffin/PuffinWriter.java\n@@ -44,6 +44,7 @@ public class PuffinWriter implements FileAppender<Blob> {\n   // Must not be modified\n   private static final byte[] MAGIC = PuffinFormat.getMagic();\n \n+  private final OutputFile outputFile;\n   private final PositionOutputStream outputStream;\n   private final Map<String, String> properties;\n   private final PuffinCompressionCodec footerCompression;\n@@ -63,6 +64,7 @@ public class PuffinWriter implements FileAppender<Blob> {\n     Preconditions.checkNotNull(outputFile, \"outputFile is null\");\n     Preconditions.checkNotNull(properties, \"properties is null\");\n     Preconditions.checkNotNull(defaultBlobCompression, \"defaultBlobCompression is null\");\n+    this.outputFile = outputFile;\n     this.outputStream = outputFile.create();\n     this.properties = ImmutableMap.copyOf(properties);\n     this.footerCompression =\n@@ -70,8 +72,16 @@ public class PuffinWriter implements FileAppender<Blob> {\n     this.defaultBlobCompression = defaultBlobCompression;\n   }\n \n+  public String location() {\n+    return outputFile.location();\n+  }\n+\n   @Override\n   public void add(Blob blob) {\n+    write(blob);\n+  }\n+\n+  public BlobMetadata write(Blob blob) {\n     Preconditions.checkNotNull(blob, \"blob is null\");\n     checkNotFinished();\n     try {\n@@ -82,7 +92,7 @@ public void add(Blob blob) {\n       ByteBuffer rawData = PuffinFormat.compress(codec, blob.blobData());\n       int length = rawData.remaining();\n       IOUtil.writeFully(outputStream, rawData);\n-      writtenBlobsMetadata.add(\n+      BlobMetadata blobMetadata =\n           new BlobMetadata(\n               blob.type(),\n               blob.inputFields(),\n@@ -91,7 +101,9 @@ public void add(Blob blob) {\n               fileOffset,\n               length,\n               codec.codecName(),\n-              blob.properties()));\n+              blob.properties());\n+      writtenBlobsMetadata.add(blobMetadata);\n+      return blobMetadata;\n     } catch (IOException e) {\n       throw new UncheckedIOException(e);\n     }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/puffin/StandardBlobTypes.java b/core/src/main/java/org/apache/iceberg/puffin/StandardBlobTypes.java\nindex 2be5df5f88b9..ce78375c4b1a 100644\n--- a/core/src/main/java/org/apache/iceberg/puffin/StandardBlobTypes.java\n+++ b/core/src/main/java/org/apache/iceberg/puffin/StandardBlobTypes.java\n@@ -26,4 +26,7 @@ private StandardBlobTypes() {}\n    * href=\"https://datasketches.apache.org/\">Apache DataSketches</a> library\n    */\n   public static final String APACHE_DATASKETCHES_THETA_V1 = \"apache-datasketches-theta-v1\";\n+\n+  /** A serialized deletion vector according to the Iceberg spec */\n+  public static final String DV_V1 = \"deletion-vector-v1\";\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/util/StructLikeUtil.java b/core/src/main/java/org/apache/iceberg/util/StructLikeUtil.java\nnew file mode 100644\nindex 000000000000..5285793a4aad\n--- /dev/null\n+++ b/core/src/main/java/org/apache/iceberg/util/StructLikeUtil.java\n@@ -0,0 +1,67 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.util;\n+\n+import org.apache.iceberg.StructLike;\n+\n+public class StructLikeUtil {\n+\n+  private StructLikeUtil() {}\n+\n+  public static StructLike copy(StructLike struct) {\n+    return StructCopy.copy(struct);\n+  }\n+\n+  private static class StructCopy implements StructLike {\n+    private static StructLike copy(StructLike struct) {\n+      return struct != null ? new StructCopy(struct) : null;\n+    }\n+\n+    private final Object[] values;\n+\n+    private StructCopy(StructLike toCopy) {\n+      this.values = new Object[toCopy.size()];\n+\n+      for (int i = 0; i < values.length; i += 1) {\n+        Object value = toCopy.get(i, Object.class);\n+\n+        if (value instanceof StructLike) {\n+          values[i] = copy((StructLike) value);\n+        } else {\n+          values[i] = value;\n+        }\n+      }\n+    }\n+\n+    @Override\n+    public int size() {\n+      return values.length;\n+    }\n+\n+    @Override\n+    public <T> T get(int pos, Class<T> javaClass) {\n+      return javaClass.cast(values[pos]);\n+    }\n+\n+    @Override\n+    public <T> void set(int pos, T value) {\n+      throw new UnsupportedOperationException(\"Struct copy cannot be modified\");\n+    }\n+  }\n+}\n\ndiff --git a/spark/v3.5/spark/src/jmh/java/org/apache/iceberg/spark/source/DVWriterBenchmark.java b/spark/v3.5/spark/src/jmh/java/org/apache/iceberg/spark/source/DVWriterBenchmark.java\nnew file mode 100644\nindex 000000000000..ac74fb5a109c\n--- /dev/null\n+++ b/spark/v3.5/spark/src/jmh/java/org/apache/iceberg/spark/source/DVWriterBenchmark.java\n@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.spark.source;\n+\n+import com.google.errorprone.annotations.FormatMethod;\n+import com.google.errorprone.annotations.FormatString;\n+import java.io.IOException;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.UUID;\n+import java.util.concurrent.ThreadLocalRandom;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.FileGenerationUtil;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.deletes.BaseDVFileWriter;\n+import org.apache.iceberg.deletes.DVFileWriter;\n+import org.apache.iceberg.deletes.DeleteGranularity;\n+import org.apache.iceberg.deletes.PositionDelete;\n+import org.apache.iceberg.io.DeleteWriteResult;\n+import org.apache.iceberg.io.FanoutPositionOnlyDeleteWriter;\n+import org.apache.iceberg.io.OutputFileFactory;\n+import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n+import org.apache.iceberg.spark.Spark3Util;\n+import org.apache.iceberg.spark.SparkSessionCatalog;\n+import org.apache.spark.sql.SparkSession;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.analysis.NoSuchTableException;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.catalyst.parser.ParseException;\n+import org.apache.spark.unsafe.types.UTF8String;\n+import org.openjdk.jmh.annotations.Benchmark;\n+import org.openjdk.jmh.annotations.BenchmarkMode;\n+import org.openjdk.jmh.annotations.Fork;\n+import org.openjdk.jmh.annotations.Measurement;\n+import org.openjdk.jmh.annotations.Mode;\n+import org.openjdk.jmh.annotations.Param;\n+import org.openjdk.jmh.annotations.Scope;\n+import org.openjdk.jmh.annotations.Setup;\n+import org.openjdk.jmh.annotations.State;\n+import org.openjdk.jmh.annotations.TearDown;\n+import org.openjdk.jmh.annotations.Threads;\n+import org.openjdk.jmh.annotations.Timeout;\n+import org.openjdk.jmh.annotations.Warmup;\n+import org.openjdk.jmh.infra.Blackhole;\n+\n+/**\n+ * A benchmark that compares the performance of DV and position delete writers.\n+ *\n+ * <p>To run this benchmark for spark-3.5: <code>\n+ *   ./gradlew -DsparkVersions=3.5 :iceberg-spark:iceberg-spark-3.5_2.12:jmh\n+ *       -PjmhIncludeRegex=DVWriterBenchmark\n+ *       -PjmhOutputPath=benchmark/iceberg-dv-writer-benchmark-result.txt\n+ * </code>\n+ */\n+@Fork(1)\n+@State(Scope.Benchmark)\n+@Warmup(iterations = 3)\n+@Measurement(iterations = 10)\n+@Timeout(time = 20, timeUnit = TimeUnit.MINUTES)\n+@BenchmarkMode(Mode.SingleShotTime)\n+public class DVWriterBenchmark {\n+\n+  private static final String TABLE_NAME = \"test_table\";\n+  private static final int DATA_FILE_RECORD_COUNT = 2_000_000;\n+  private static final long TARGET_FILE_SIZE = Long.MAX_VALUE;\n+\n+  @Param({\"5\", \"10\"})\n+  private int referencedDataFileCount;\n+\n+  @Param({\"0.01\", \"0.03\", \"0.05\", \"0.10\", \"0.2\"})\n+  private double deletedRowsRatio;\n+\n+  private final Configuration hadoopConf = new Configuration();\n+  private final Random random = ThreadLocalRandom.current();\n+  private SparkSession spark;\n+  private Table table;\n+  private Iterable<InternalRow> positionDeletes;\n+\n+  @Setup\n+  public void setupBenchmark() throws NoSuchTableException, ParseException {\n+    setupSpark();\n+    initTable();\n+    generatePositionDeletes();\n+  }\n+\n+  @TearDown\n+  public void tearDownBenchmark() {\n+    dropTable();\n+    tearDownSpark();\n+  }\n+\n+  @Benchmark\n+  @Threads(1)\n+  public void dv(Blackhole blackhole) throws IOException {\n+    OutputFileFactory fileFactory = newFileFactory(FileFormat.PUFFIN);\n+    DVFileWriter writer = new BaseDVFileWriter(fileFactory, path -> null);\n+\n+    try (DVFileWriter closableWriter = writer) {\n+      for (InternalRow row : positionDeletes) {\n+        String path = row.getString(0);\n+        long pos = row.getLong(1);\n+        closableWriter.delete(path, pos, table.spec(), null);\n+      }\n+    }\n+\n+    DeleteWriteResult result = writer.result();\n+    blackhole.consume(result);\n+  }\n+\n+  @Benchmark\n+  @Threads(1)\n+  public void fileScopedParquetDeletes(Blackhole blackhole) throws IOException {\n+    FanoutPositionOnlyDeleteWriter<InternalRow> writer = newWriter(DeleteGranularity.FILE);\n+    write(writer, positionDeletes);\n+    DeleteWriteResult result = writer.result();\n+    blackhole.consume(result);\n+  }\n+\n+  @Benchmark\n+  @Threads(1)\n+  public void partitionScopedParquetDeletes(Blackhole blackhole) throws IOException {\n+    FanoutPositionOnlyDeleteWriter<InternalRow> writer = newWriter(DeleteGranularity.PARTITION);\n+    write(writer, positionDeletes);\n+    DeleteWriteResult result = writer.result();\n+    blackhole.consume(result);\n+  }\n+\n+  private FanoutPositionOnlyDeleteWriter<InternalRow> newWriter(DeleteGranularity granularity) {\n+    return new FanoutPositionOnlyDeleteWriter<>(\n+        newWriterFactory(),\n+        newFileFactory(FileFormat.PARQUET),\n+        table.io(),\n+        TARGET_FILE_SIZE,\n+        granularity);\n+  }\n+\n+  private DeleteWriteResult write(\n+      FanoutPositionOnlyDeleteWriter<InternalRow> writer, Iterable<InternalRow> rows)\n+      throws IOException {\n+\n+    try (FanoutPositionOnlyDeleteWriter<InternalRow> closableWriter = writer) {\n+      PositionDelete<InternalRow> positionDelete = PositionDelete.create();\n+\n+      for (InternalRow row : rows) {\n+        String path = row.getString(0);\n+        long pos = row.getLong(1);\n+        positionDelete.set(path, pos, null /* no row */);\n+        closableWriter.write(positionDelete, table.spec(), null);\n+      }\n+    }\n+\n+    return writer.result();\n+  }\n+\n+  private SparkFileWriterFactory newWriterFactory() {\n+    return SparkFileWriterFactory.builderFor(table).dataFileFormat(FileFormat.PARQUET).build();\n+  }\n+\n+  private OutputFileFactory newFileFactory(FileFormat format) {\n+    return OutputFileFactory.builderFor(table, 1, 1).format(format).build();\n+  }\n+\n+  private void generatePositionDeletes() {\n+    int numDeletesPerFile = (int) (DATA_FILE_RECORD_COUNT * deletedRowsRatio);\n+    int numDeletes = referencedDataFileCount * numDeletesPerFile;\n+    List<InternalRow> deletes = Lists.newArrayListWithExpectedSize(numDeletes);\n+\n+    for (int pathIndex = 0; pathIndex < referencedDataFileCount; pathIndex++) {\n+      UTF8String dataFilePath = UTF8String.fromString(generateDataFilePath());\n+      Set<Long> positions = generatePositions(numDeletesPerFile);\n+      for (long pos : positions) {\n+        deletes.add(new GenericInternalRow(new Object[] {dataFilePath, pos}));\n+      }\n+    }\n+\n+    Collections.shuffle(deletes);\n+\n+    this.positionDeletes = deletes;\n+  }\n+\n+  public Set<Long> generatePositions(int numPositions) {\n+    Set<Long> positions = Sets.newHashSet();\n+\n+    while (positions.size() < numPositions) {\n+      long pos = random.nextInt(DATA_FILE_RECORD_COUNT);\n+      positions.add(pos);\n+    }\n+\n+    return positions;\n+  }\n+\n+  private String generateDataFilePath() {\n+    String fileName = FileGenerationUtil.generateFileName();\n+    return table.locationProvider().newDataLocation(table.spec(), null, fileName);\n+  }\n+\n+  private void setupSpark() {\n+    this.spark =\n+        SparkSession.builder()\n+            .config(\"spark.ui.enabled\", false)\n+            .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n+            .config(\"spark.sql.catalog.spark_catalog\", SparkSessionCatalog.class.getName())\n+            .config(\"spark.sql.catalog.spark_catalog.type\", \"hadoop\")\n+            .config(\"spark.sql.catalog.spark_catalog.warehouse\", newWarehouseDir())\n+            .master(\"local[*]\")\n+            .getOrCreate();\n+  }\n+\n+  private void tearDownSpark() {\n+    spark.stop();\n+  }\n+\n+  private void initTable() throws NoSuchTableException, ParseException {\n+    sql(\"CREATE TABLE %s (c1 INT, c2 INT, c3 STRING) USING iceberg\", TABLE_NAME);\n+    this.table = Spark3Util.loadIcebergTable(spark, TABLE_NAME);\n+  }\n+\n+  private void dropTable() {\n+    sql(\"DROP TABLE IF EXISTS %s PURGE\", TABLE_NAME);\n+  }\n+\n+  private String newWarehouseDir() {\n+    return hadoopConf.get(\"hadoop.tmp.dir\") + UUID.randomUUID();\n+  }\n+\n+  @FormatMethod\n+  private void sql(@FormatString String query, Object... args) {\n+    spark.sql(String.format(query, args));\n+  }\n+}\n",
    "test_patch": "diff --git a/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java b/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java\nnew file mode 100644\nindex 000000000000..ce742b1c4685\n--- /dev/null\n+++ b/data/src/test/java/org/apache/iceberg/io/TestDVWriters.java\n@@ -0,0 +1,123 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.io;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import java.io.IOException;\n+import java.util.Arrays;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.function.Function;\n+import org.apache.iceberg.DataFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.ParameterizedTestExtension;\n+import org.apache.iceberg.Parameters;\n+import org.apache.iceberg.PartitionSpec;\n+import org.apache.iceberg.Table;\n+import org.apache.iceberg.data.BaseDeleteLoader;\n+import org.apache.iceberg.data.DeleteLoader;\n+import org.apache.iceberg.deletes.BaseDVFileWriter;\n+import org.apache.iceberg.deletes.DVFileWriter;\n+import org.apache.iceberg.deletes.PositionDeleteIndex;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.TestTemplate;\n+import org.junit.jupiter.api.extension.ExtendWith;\n+\n+@ExtendWith(ParameterizedTestExtension.class)\n+public abstract class TestDVWriters<T> extends WriterTestBase<T> {\n+\n+  @Parameters(name = \"formatVersion = {0}\")\n+  protected static List<Object> parameters() {\n+    return Arrays.asList(new Object[] {3});\n+  }\n+\n+  private OutputFileFactory fileFactory = null;\n+\n+  protected abstract StructLikeSet toSet(Iterable<T> records);\n+\n+  protected FileFormat dataFormat() {\n+    return FileFormat.PARQUET;\n+  }\n+\n+  @Override\n+  @BeforeEach\n+  public void setupTable() throws Exception {\n+    this.table = create(SCHEMA, PartitionSpec.unpartitioned());\n+    this.fileFactory = OutputFileFactory.builderFor(table, 1, 1).format(FileFormat.PUFFIN).build();\n+  }\n+\n+  @TestTemplate\n+  public void testBasicDVs() throws IOException {\n+    FileWriterFactory<T> writerFactory = newWriterFactory(table.schema());\n+\n+    // add the first data file\n+    List<T> rows1 = ImmutableList.of(toRow(1, \"aaa\"), toRow(2, \"aaa\"), toRow(11, \"aaa\"));\n+    DataFile dataFile1 = writeData(writerFactory, fileFactory, rows1, table.spec(), null);\n+    table.newFastAppend().appendFile(dataFile1).commit();\n+\n+    // add the second data file\n+    List<T> rows2 = ImmutableList.of(toRow(3, \"aaa\"), toRow(4, \"aaa\"), toRow(12, \"aaa\"));\n+    DataFile dataFile2 = writeData(writerFactory, fileFactory, rows2, table.spec(), null);\n+    table.newFastAppend().appendFile(dataFile2).commit();\n+\n+    // init the DV writer\n+    DVFileWriter dvWriter =\n+        new BaseDVFileWriter(fileFactory, new PreviousDeleteLoader(table, ImmutableMap.of()));\n+\n+    // write deletes for both data files (the order of records is mixed)\n+    dvWriter.delete(dataFile1.location(), 1L, table.spec(), null);\n+    dvWriter.delete(dataFile2.location(), 0L, table.spec(), null);\n+    dvWriter.delete(dataFile1.location(), 0L, table.spec(), null);\n+    dvWriter.delete(dataFile2.location(), 1L, table.spec(), null);\n+    dvWriter.close();\n+\n+    // verify the writer result\n+    DeleteWriteResult result = dvWriter.result();\n+    assertThat(result.deleteFiles()).hasSize(2);\n+    assertThat(result.referencedDataFiles())\n+        .hasSize(2)\n+        .contains(dataFile1.location())\n+        .contains(dataFile2.location());\n+    assertThat(result.referencesDataFiles()).isTrue();\n+  }\n+\n+  private static class PreviousDeleteLoader implements Function<String, PositionDeleteIndex> {\n+    private final Map<String, DeleteFile> deleteFiles;\n+    private final DeleteLoader deleteLoader;\n+\n+    PreviousDeleteLoader(Table table, Map<String, DeleteFile> deleteFiles) {\n+      this.deleteFiles = deleteFiles;\n+      this.deleteLoader = new BaseDeleteLoader(deleteFile -> table.io().newInputFile(deleteFile));\n+    }\n+\n+    @Override\n+    public PositionDeleteIndex apply(String path) {\n+      DeleteFile deleteFile = deleteFiles.get(path);\n+      if (deleteFile == null) {\n+        return null;\n+      }\n+      return deleteLoader.loadPositionDeletes(ImmutableList.of(deleteFile), path);\n+    }\n+  }\n+}\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDVWriters.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDVWriters.java\nnew file mode 100644\nindex 000000000000..dfc693d3094d\n--- /dev/null\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestSparkDVWriters.java\n@@ -0,0 +1,69 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.spark.source;\n+\n+import java.util.List;\n+import org.apache.iceberg.Schema;\n+import org.apache.iceberg.io.FileWriterFactory;\n+import org.apache.iceberg.io.TestDVWriters;\n+import org.apache.iceberg.spark.SparkSchemaUtil;\n+import org.apache.iceberg.util.ArrayUtil;\n+import org.apache.iceberg.util.StructLikeSet;\n+import org.apache.spark.sql.catalyst.InternalRow;\n+import org.apache.spark.sql.catalyst.expressions.GenericInternalRow;\n+import org.apache.spark.sql.types.StructType;\n+import org.apache.spark.unsafe.types.UTF8String;\n+\n+public class TestSparkDVWriters extends TestDVWriters<InternalRow> {\n+\n+  @Override\n+  protected FileWriterFactory<InternalRow> newWriterFactory(\n+      Schema dataSchema,\n+      List<Integer> equalityFieldIds,\n+      Schema equalityDeleteRowSchema,\n+      Schema positionDeleteRowSchema) {\n+    return SparkFileWriterFactory.builderFor(table)\n+        .dataSchema(table.schema())\n+        .dataFileFormat(dataFormat())\n+        .deleteFileFormat(dataFormat())\n+        .equalityFieldIds(ArrayUtil.toIntArray(equalityFieldIds))\n+        .equalityDeleteRowSchema(equalityDeleteRowSchema)\n+        .positionDeleteRowSchema(positionDeleteRowSchema)\n+        .build();\n+  }\n+\n+  @Override\n+  protected InternalRow toRow(Integer id, String data) {\n+    InternalRow row = new GenericInternalRow(2);\n+    row.update(0, id);\n+    row.update(1, UTF8String.fromString(data));\n+    return row;\n+  }\n+\n+  @Override\n+  protected StructLikeSet toSet(Iterable<InternalRow> rows) {\n+    StructLikeSet set = StructLikeSet.create(table.schema().asStruct());\n+    StructType sparkType = SparkSchemaUtil.convert(table.schema());\n+    for (InternalRow row : rows) {\n+      InternalRowWrapper wrapper = new InternalRowWrapper(sparkType, table.schema().asStruct());\n+      set.add(wrapper.wrap(row));\n+    }\n+    return set;\n+  }\n+}\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11469",
    "pr_id": 11469,
    "issue_id": 11122,
    "repo": "apache/iceberg",
    "problem_statement": "Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 118,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkContentFile.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java"
    ],
    "base_commit": "d0cca384a01172b5133bf7e207d94e374ed0c2ed",
    "head_commit": "da00ad000d9bd2ccf81a55ff46ede102666be8eb",
    "repo_url": "https://github.com/apache/iceberg/pull/11469",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11469",
    "dockerfile": "",
    "pr_merged_at": "2024-11-05T07:50:39.000Z",
    "patch": "diff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkContentFile.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkContentFile.java\nindex af7c4a9b866d..bad31d8d85f4 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkContentFile.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkContentFile.java\n@@ -55,6 +55,8 @@ public abstract class SparkContentFile<F> implements ContentFile<F> {\n   private final int fileSpecIdPosition;\n   private final int equalityIdsPosition;\n   private final int referencedDataFilePosition;\n+  private final int contentOffsetPosition;\n+  private final int contentSizePosition;\n   private final Type lowerBoundsType;\n   private final Type upperBoundsType;\n   private final Type keyMetadataType;\n@@ -105,6 +107,8 @@ public abstract class SparkContentFile<F> implements ContentFile<F> {\n     this.fileSpecIdPosition = positions.get(DataFile.SPEC_ID.name());\n     this.equalityIdsPosition = positions.get(DataFile.EQUALITY_IDS.name());\n     this.referencedDataFilePosition = positions.get(DataFile.REFERENCED_DATA_FILE.name());\n+    this.contentOffsetPosition = positions.get(DataFile.CONTENT_OFFSET.name());\n+    this.contentSizePosition = positions.get(DataFile.CONTENT_SIZE.name());\n   }\n \n   public F wrap(Row row) {\n@@ -240,6 +244,20 @@ public String referencedDataFile() {\n     return wrapped.getString(referencedDataFilePosition);\n   }\n \n+  public Long contentOffset() {\n+    if (wrapped.isNullAt(contentOffsetPosition)) {\n+      return null;\n+    }\n+    return wrapped.getLong(contentOffsetPosition);\n+  }\n+\n+  public Long contentSizeInBytes() {\n+    if (wrapped.isNullAt(contentSizePosition)) {\n+      return null;\n+    }\n+    return wrapped.getLong(contentSizePosition);\n+  }\n+\n   private int fieldPosition(String name, StructType sparkType) {\n     try {\n       return sparkType.fieldIndex(name);\n",
    "test_patch": "diff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\nindex 6cbc53baa349..b86d74415ac8 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\n@@ -106,7 +106,8 @@ public static Object[] parameters() {\n       new Object[] {\"true\", \"true\", false, 1},\n       new Object[] {\"false\", \"true\", true, 1},\n       new Object[] {\"true\", \"false\", false, 2},\n-      new Object[] {\"false\", \"false\", false, 2}\n+      new Object[] {\"false\", \"false\", false, 2},\n+      new Object[] {\"false\", \"false\", false, 3}\n     };\n   }\n \n@@ -150,16 +151,16 @@ public void testRewriteManifestsPreservesOptionalFields() throws IOException {\n         .appendFile(dataFile3)\n         .commit();\n \n-    DeleteFile deleteFile1 = newDeleteFileWithRef(table, dataFile1);\n-    assertThat(deleteFile1.referencedDataFile()).isEqualTo(dataFile1.location());\n+    DeleteFile deleteFile1 = newDeletes(table, dataFile1);\n+    assertDeletes(dataFile1, deleteFile1);\n     table.newRowDelta().addDeletes(deleteFile1).commit();\n \n-    DeleteFile deleteFile2 = newDeleteFileWithRef(table, dataFile2);\n-    assertThat(deleteFile2.referencedDataFile()).isEqualTo(dataFile2.location());\n+    DeleteFile deleteFile2 = newDeletes(table, dataFile2);\n+    assertDeletes(dataFile2, deleteFile2);\n     table.newRowDelta().addDeletes(deleteFile2).commit();\n \n-    DeleteFile deleteFile3 = newDeleteFileWithRef(table, dataFile3);\n-    assertThat(deleteFile3.referencedDataFile()).isEqualTo(dataFile3.location());\n+    DeleteFile deleteFile3 = newDeletes(table, dataFile3);\n+    assertDeletes(dataFile3, deleteFile3);\n     table.newRowDelta().addDeletes(deleteFile3).commit();\n \n     SparkActions actions = SparkActions.get();\n@@ -178,10 +179,13 @@ public void testRewriteManifestsPreservesOptionalFields() throws IOException {\n         DeleteFile deleteFile = Iterables.getOnlyElement(fileTask.deletes());\n         if (dataFile.location().equals(dataFile1.location())) {\n           assertThat(deleteFile.referencedDataFile()).isEqualTo(deleteFile1.referencedDataFile());\n+          assertEqual(deleteFile, deleteFile1);\n         } else if (dataFile.location().equals(dataFile2.location())) {\n           assertThat(deleteFile.referencedDataFile()).isEqualTo(deleteFile2.referencedDataFile());\n+          assertEqual(deleteFile, deleteFile2);\n         } else {\n           assertThat(deleteFile.referencedDataFile()).isEqualTo(deleteFile3.referencedDataFile());\n+          assertEqual(deleteFile, deleteFile3);\n         }\n       }\n     }\n@@ -1035,10 +1039,18 @@ private DataFiles.Builder newDataFileBuilder(Table table) {\n         .withRecordCount(1);\n   }\n \n+  private DeleteFile newDeletes(Table table, DataFile dataFile) {\n+    return formatVersion >= 3 ? newDV(table, dataFile) : newDeleteFileWithRef(table, dataFile);\n+  }\n+\n   private DeleteFile newDeleteFileWithRef(Table table, DataFile dataFile) {\n     return FileGenerationUtil.generatePositionDeleteFileWithRef(table, dataFile);\n   }\n \n+  private DeleteFile newDV(Table table, DataFile dataFile) {\n+    return FileGenerationUtil.generateDV(table, dataFile);\n+  }\n+\n   private DeleteFile newDeleteFile(Table table, String partitionPath) {\n     return FileMetadata.deleteFileBuilder(table.spec())\n         .ofPositionDeletes()\n@@ -1097,4 +1109,26 @@ private DeleteFile writeEqDeletes(Table table, StructLike partition, String key,\n     OutputFile outputFile = Files.localOutput(File.createTempFile(\"junit\", null, temp.toFile()));\n     return FileHelpers.writeDeleteFile(table, outputFile, partition, deletes, deleteSchema);\n   }\n+\n+  private void assertDeletes(DataFile dataFile, DeleteFile deleteFile) {\n+    assertThat(deleteFile.referencedDataFile()).isEqualTo(dataFile.location());\n+    if (formatVersion >= 3) {\n+      assertThat(deleteFile.contentOffset()).isNotNull();\n+      assertThat(deleteFile.contentSizeInBytes()).isNotNull();\n+    } else {\n+      assertThat(deleteFile.contentOffset()).isNull();\n+      assertThat(deleteFile.contentSizeInBytes()).isNull();\n+    }\n+  }\n+\n+  private void assertEqual(DeleteFile deleteFile1, DeleteFile deleteFile2) {\n+    assertThat(deleteFile1.location()).isEqualTo(deleteFile2.location());\n+    assertThat(deleteFile1.content()).isEqualTo(deleteFile2.content());\n+    assertThat(deleteFile1.specId()).isEqualTo(deleteFile2.specId());\n+    assertThat(deleteFile1.partition()).isEqualTo(deleteFile2.partition());\n+    assertThat(deleteFile1.format()).isEqualTo(deleteFile2.format());\n+    assertThat(deleteFile1.referencedDataFile()).isEqualTo(deleteFile2.referencedDataFile());\n+    assertThat(deleteFile1.contentOffset()).isEqualTo(deleteFile2.contentOffset());\n+    assertThat(deleteFile1.contentSizeInBytes()).isEqualTo(deleteFile2.contentSizeInBytes());\n+  }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11467",
    "pr_id": 11467,
    "issue_id": 11122,
    "repo": "apache/iceberg",
    "problem_statement": "Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 118,
    "test_files_count": 1,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/DeleteFileIndex.java",
      "core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java",
      "core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java"
    ],
    "base_commit": "d0cca384a01172b5133bf7e207d94e374ed0c2ed",
    "head_commit": "345cf7cff1eea6a17337ad83adba0d52589f084b",
    "repo_url": "https://github.com/apache/iceberg/pull/11467",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11467",
    "dockerfile": "",
    "pr_merged_at": "2024-11-05T14:52:24.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/DeleteFileIndex.java b/core/src/main/java/org/apache/iceberg/DeleteFileIndex.java\nindex 8444b91eecd4..ab7fec6fb151 100644\n--- a/core/src/main/java/org/apache/iceberg/DeleteFileIndex.java\n+++ b/core/src/main/java/org/apache/iceberg/DeleteFileIndex.java\n@@ -33,6 +33,7 @@\n import java.util.concurrent.ConcurrentLinkedQueue;\n import java.util.concurrent.ExecutorService;\n import org.apache.iceberg.exceptions.RuntimeIOException;\n+import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.expressions.Expression;\n import org.apache.iceberg.expressions.Expressions;\n import org.apache.iceberg.expressions.ManifestEvaluator;\n@@ -70,6 +71,7 @@ class DeleteFileIndex {\n   private final PartitionMap<EqualityDeletes> eqDeletesByPartition;\n   private final PartitionMap<PositionDeletes> posDeletesByPartition;\n   private final Map<String, PositionDeletes> posDeletesByPath;\n+  private final Map<String, DeleteFile> dvByPath;\n   private final boolean hasEqDeletes;\n   private final boolean hasPosDeletes;\n   private final boolean isEmpty;\n@@ -78,13 +80,16 @@ private DeleteFileIndex(\n       EqualityDeletes globalDeletes,\n       PartitionMap<EqualityDeletes> eqDeletesByPartition,\n       PartitionMap<PositionDeletes> posDeletesByPartition,\n-      Map<String, PositionDeletes> posDeletesByPath) {\n+      Map<String, PositionDeletes> posDeletesByPath,\n+      Map<String, DeleteFile> dvByPath) {\n     this.globalDeletes = globalDeletes;\n     this.eqDeletesByPartition = eqDeletesByPartition;\n     this.posDeletesByPartition = posDeletesByPartition;\n     this.posDeletesByPath = posDeletesByPath;\n+    this.dvByPath = dvByPath;\n     this.hasEqDeletes = globalDeletes != null || eqDeletesByPartition != null;\n-    this.hasPosDeletes = posDeletesByPartition != null || posDeletesByPath != null;\n+    this.hasPosDeletes =\n+        posDeletesByPartition != null || posDeletesByPath != null || dvByPath != null;\n     this.isEmpty = !hasEqDeletes && !hasPosDeletes;\n   }\n \n@@ -125,6 +130,10 @@ public Iterable<DeleteFile> referencedDeleteFiles() {\n       }\n     }\n \n+    if (dvByPath != null) {\n+      deleteFiles = Iterables.concat(deleteFiles, dvByPath.values());\n+    }\n+\n     return deleteFiles;\n   }\n \n@@ -143,9 +152,16 @@ DeleteFile[] forDataFile(long sequenceNumber, DataFile file) {\n \n     DeleteFile[] global = findGlobalDeletes(sequenceNumber, file);\n     DeleteFile[] eqPartition = findEqPartitionDeletes(sequenceNumber, file);\n-    DeleteFile[] posPartition = findPosPartitionDeletes(sequenceNumber, file);\n-    DeleteFile[] posPath = findPathDeletes(sequenceNumber, file);\n-    return concat(global, eqPartition, posPartition, posPath);\n+    DeleteFile dv = findDV(sequenceNumber, file);\n+    if (dv != null && global == null && eqPartition == null) {\n+      return new DeleteFile[] {dv};\n+    } else if (dv != null) {\n+      return concat(global, eqPartition, new DeleteFile[] {dv});\n+    } else {\n+      DeleteFile[] posPartition = findPosPartitionDeletes(sequenceNumber, file);\n+      DeleteFile[] posPath = findPathDeletes(sequenceNumber, file);\n+      return concat(global, eqPartition, posPartition, posPath);\n+    }\n   }\n \n   private DeleteFile[] findGlobalDeletes(long seq, DataFile dataFile) {\n@@ -180,6 +196,22 @@ private DeleteFile[] findPathDeletes(long seq, DataFile dataFile) {\n     return deletes == null ? EMPTY_DELETES : deletes.filter(seq);\n   }\n \n+  private DeleteFile findDV(long seq, DataFile dataFile) {\n+    if (dvByPath == null) {\n+      return null;\n+    }\n+\n+    DeleteFile dv = dvByPath.get(dataFile.location());\n+    if (dv != null) {\n+      ValidationException.check(\n+          dv.dataSequenceNumber() >= seq,\n+          \"DV data sequence number (%s) must be greater than or equal to data file sequence number (%s)\",\n+          dv.dataSequenceNumber(),\n+          seq);\n+    }\n+    return dv;\n+  }\n+\n   @SuppressWarnings(\"checkstyle:CyclomaticComplexity\")\n   private static boolean canContainEqDeletesForFile(\n       DataFile dataFile, EqualityDeleteFile deleteFile) {\n@@ -434,11 +466,16 @@ DeleteFileIndex build() {\n       PartitionMap<EqualityDeletes> eqDeletesByPartition = PartitionMap.create(specsById);\n       PartitionMap<PositionDeletes> posDeletesByPartition = PartitionMap.create(specsById);\n       Map<String, PositionDeletes> posDeletesByPath = Maps.newHashMap();\n+      Map<String, DeleteFile> dvByPath = Maps.newHashMap();\n \n       for (DeleteFile file : files) {\n         switch (file.content()) {\n           case POSITION_DELETES:\n-            add(posDeletesByPath, posDeletesByPartition, file);\n+            if (ContentFileUtil.isDV(file)) {\n+              add(dvByPath, file);\n+            } else {\n+              add(posDeletesByPath, posDeletesByPartition, file);\n+            }\n             break;\n           case EQUALITY_DELETES:\n             add(globalDeletes, eqDeletesByPartition, file);\n@@ -453,7 +490,18 @@ DeleteFileIndex build() {\n           globalDeletes.isEmpty() ? null : globalDeletes,\n           eqDeletesByPartition.isEmpty() ? null : eqDeletesByPartition,\n           posDeletesByPartition.isEmpty() ? null : posDeletesByPartition,\n-          posDeletesByPath.isEmpty() ? null : posDeletesByPath);\n+          posDeletesByPath.isEmpty() ? null : posDeletesByPath,\n+          dvByPath.isEmpty() ? null : dvByPath);\n+    }\n+\n+    private void add(Map<String, DeleteFile> dvByPath, DeleteFile dv) {\n+      String path = dv.referencedDataFile();\n+      DeleteFile existingDV = dvByPath.putIfAbsent(path, dv);\n+      if (existingDV != null) {\n+        throw new ValidationException(\n+            \"Can't index multiple DVs for %s: %s and %s\",\n+            path, ContentFileUtil.dvDesc(dv), ContentFileUtil.dvDesc(existingDV));\n+      }\n     }\n \n     private void add(\n\ndiff --git a/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java b/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java\nindex c82b3ff828cf..e4666bd1bd8f 100644\n--- a/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java\n@@ -24,6 +24,7 @@\n import org.apache.iceberg.ContentFile;\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.MetadataColumns;\n import org.apache.iceberg.types.Conversions;\n import org.apache.iceberg.types.Type;\n@@ -84,4 +85,17 @@ public static String referencedDataFileLocation(DeleteFile deleteFile) {\n     CharSequence location = referencedDataFile(deleteFile);\n     return location != null ? location.toString() : null;\n   }\n+\n+  public static boolean isDV(DeleteFile deleteFile) {\n+    return deleteFile.format() == FileFormat.PUFFIN;\n+  }\n+\n+  public static String dvDesc(DeleteFile deleteFile) {\n+    return String.format(\n+        \"DV{location=%s, offset=%s, length=%s, referencedDataFile=%s}\",\n+        deleteFile.location(),\n+        deleteFile.contentOffset(),\n+        deleteFile.contentSizeInBytes(),\n+        deleteFile.referencedDataFile());\n+  }\n }\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java b/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java\nindex 986e8608c082..28c7715a73ff 100644\n--- a/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/DeleteFileIndexTestBase.java\n@@ -22,20 +22,24 @@\n import static org.apache.iceberg.expressions.Expressions.equal;\n import static org.assertj.core.api.Assertions.assertThat;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n+import static org.assertj.core.api.Assumptions.assumeThat;\n \n import java.io.File;\n import java.io.IOException;\n import java.nio.file.Files;\n import java.util.Arrays;\n+import java.util.Collections;\n import java.util.List;\n import java.util.UUID;\n import org.apache.iceberg.DeleteFileIndex.EqualityDeletes;\n import org.apache.iceberg.DeleteFileIndex.PositionDeletes;\n+import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.relocated.com.google.common.collect.Sets;\n import org.apache.iceberg.util.CharSequenceSet;\n+import org.apache.iceberg.util.ContentFileUtil;\n import org.junit.jupiter.api.TestTemplate;\n import org.junit.jupiter.api.extension.ExtendWith;\n \n@@ -624,4 +628,52 @@ public void testEqualityDeletesGroup() {\n     // it should not be possible to add more elements upon indexing\n     assertThatThrownBy(() -> group.add(SPEC, file1)).isInstanceOf(IllegalStateException.class);\n   }\n+\n+  @TestTemplate\n+  public void testMixDeleteFilesAndDVs() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(3);\n+\n+    List<DeleteFile> deletes =\n+        Arrays.asList(\n+            withDataSequenceNumber(1, partitionedPosDeletes(SPEC, FILE_A.partition())),\n+            withDataSequenceNumber(2, newDV(FILE_A)),\n+            withDataSequenceNumber(1, partitionedPosDeletes(SPEC, FILE_B.partition())),\n+            withDataSequenceNumber(2, partitionedPosDeletes(SPEC, FILE_B.partition())));\n+\n+    DeleteFileIndex index = DeleteFileIndex.builderFor(deletes).specsById(table.specs()).build();\n+\n+    DeleteFile[] fileADeletes = index.forDataFile(0, FILE_A);\n+    assertThat(fileADeletes).as(\"Only DV should apply to FILE_A\").hasSize(1);\n+    assertThat(ContentFileUtil.isDV(fileADeletes[0])).isTrue();\n+    assertThat(fileADeletes[0].referencedDataFile()).isEqualTo(FILE_A.location());\n+\n+    DeleteFile[] fileBDeletes = index.forDataFile(0, FILE_B);\n+    assertThat(fileBDeletes).as(\"Two delete files should apply to FILE_B\").hasSize(2);\n+    assertThat(ContentFileUtil.isDV(fileBDeletes[0])).isFalse();\n+    assertThat(ContentFileUtil.isDV(fileBDeletes[1])).isFalse();\n+  }\n+\n+  @TestTemplate\n+  public void testMultipleDVs() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(3);\n+\n+    DeleteFile dv1 = withDataSequenceNumber(1, newDV(FILE_A));\n+    DeleteFile dv2 = withDataSequenceNumber(2, newDV(FILE_A));\n+    List<DeleteFile> dvs = Arrays.asList(dv1, dv2);\n+\n+    assertThatThrownBy(() -> DeleteFileIndex.builderFor(dvs).specsById(table.specs()).build())\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessageContaining(\"Can't index multiple DVs for %s\", FILE_A.location());\n+  }\n+\n+  @TestTemplate\n+  public void testInvalidDVSequenceNumber() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(3);\n+    DeleteFile dv = withDataSequenceNumber(1, newDV(FILE_A));\n+    List<DeleteFile> dvs = Collections.singletonList(dv);\n+    DeleteFileIndex index = DeleteFileIndex.builderFor(dvs).specsById(table.specs()).build();\n+    assertThatThrownBy(() -> index.forDataFile(2, FILE_A))\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessageContaining(\"must be greater than or equal to data file sequence number\");\n+  }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11464",
    "pr_id": 11464,
    "issue_id": 11122,
    "repo": "apache/iceberg",
    "problem_statement": "Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 118,
    "test_files_count": 4,
    "non_test_files_count": 7,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/SnapshotSummary.java",
      "core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResult.java",
      "core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResultParser.java",
      "core/src/main/java/org/apache/iceberg/metrics/ScanMetrics.java",
      "core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResult.java",
      "core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResultParser.java",
      "core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java",
      "core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java",
      "core/src/test/java/org/apache/iceberg/metrics/TestCommitMetricsResultParser.java",
      "core/src/test/java/org/apache/iceberg/metrics/TestScanMetricsResultParser.java",
      "core/src/test/java/org/apache/iceberg/metrics/TestScanReportParser.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java",
      "core/src/test/java/org/apache/iceberg/metrics/TestCommitMetricsResultParser.java",
      "core/src/test/java/org/apache/iceberg/metrics/TestScanMetricsResultParser.java",
      "core/src/test/java/org/apache/iceberg/metrics/TestScanReportParser.java"
    ],
    "base_commit": "5bd314bdf6c3b5e0e5346d0f7408353bdf31bc81",
    "head_commit": "c64b6593d9f89b4274193fdecba8038d72339b8b",
    "repo_url": "https://github.com/apache/iceberg/pull/11464",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11464",
    "dockerfile": "",
    "pr_merged_at": "2024-11-05T16:02:00.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/SnapshotSummary.java b/core/src/main/java/org/apache/iceberg/SnapshotSummary.java\nindex ad832a5e78e2..6043424cd7fc 100644\n--- a/core/src/main/java/org/apache/iceberg/SnapshotSummary.java\n+++ b/core/src/main/java/org/apache/iceberg/SnapshotSummary.java\n@@ -25,6 +25,7 @@\n import org.apache.iceberg.relocated.com.google.common.base.Strings;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.util.ContentFileUtil;\n import org.apache.iceberg.util.ScanTaskUtil;\n \n public class SnapshotSummary {\n@@ -36,6 +37,8 @@ public class SnapshotSummary {\n   public static final String REMOVED_EQ_DELETE_FILES_PROP = \"removed-equality-delete-files\";\n   public static final String ADD_POS_DELETE_FILES_PROP = \"added-position-delete-files\";\n   public static final String REMOVED_POS_DELETE_FILES_PROP = \"removed-position-delete-files\";\n+  public static final String ADDED_DVS_PROP = \"added-dvs\";\n+  public static final String REMOVED_DVS_PROP = \"removed-dvs\";\n   public static final String REMOVED_DELETE_FILES_PROP = \"removed-delete-files\";\n   public static final String TOTAL_DELETE_FILES_PROP = \"total-delete-files\";\n   public static final String ADDED_RECORDS_PROP = \"added-records\";\n@@ -222,6 +225,8 @@ private static class UpdateMetrics {\n     private int removedEqDeleteFiles = 0;\n     private int addedPosDeleteFiles = 0;\n     private int removedPosDeleteFiles = 0;\n+    private int addedDVs = 0;\n+    private int removedDVs = 0;\n     private int addedDeleteFiles = 0;\n     private int removedDeleteFiles = 0;\n     private long addedRecords = 0L;\n@@ -243,6 +248,8 @@ void clear() {\n       this.removedPosDeleteFiles = 0;\n       this.addedDeleteFiles = 0;\n       this.removedDeleteFiles = 0;\n+      this.addedDVs = 0;\n+      this.removedDVs = 0;\n       this.addedRecords = 0L;\n       this.deletedRecords = 0L;\n       this.addedPosDeletes = 0L;\n@@ -262,6 +269,8 @@ void addTo(ImmutableMap.Builder<String, String> builder) {\n           removedPosDeleteFiles > 0, builder, REMOVED_POS_DELETE_FILES_PROP, removedPosDeleteFiles);\n       setIf(addedDeleteFiles > 0, builder, ADDED_DELETE_FILES_PROP, addedDeleteFiles);\n       setIf(removedDeleteFiles > 0, builder, REMOVED_DELETE_FILES_PROP, removedDeleteFiles);\n+      setIf(addedDVs > 0, builder, ADDED_DVS_PROP, addedDVs);\n+      setIf(removedDVs > 0, builder, REMOVED_DVS_PROP, removedDVs);\n       setIf(addedRecords > 0, builder, ADDED_RECORDS_PROP, addedRecords);\n       setIf(deletedRecords > 0, builder, DELETED_RECORDS_PROP, deletedRecords);\n \n@@ -283,8 +292,13 @@ void addedFile(ContentFile<?> file) {\n           this.addedRecords += file.recordCount();\n           break;\n         case POSITION_DELETES:\n+          DeleteFile deleteFile = (DeleteFile) file;\n+          if (ContentFileUtil.isDV(deleteFile)) {\n+            this.addedDVs += 1;\n+          } else {\n+            this.addedPosDeleteFiles += 1;\n+          }\n           this.addedDeleteFiles += 1;\n-          this.addedPosDeleteFiles += 1;\n           this.addedPosDeletes += file.recordCount();\n           break;\n         case EQUALITY_DELETES:\n@@ -306,8 +320,13 @@ void removedFile(ContentFile<?> file) {\n           this.deletedRecords += file.recordCount();\n           break;\n         case POSITION_DELETES:\n+          DeleteFile deleteFile = (DeleteFile) file;\n+          if (ContentFileUtil.isDV(deleteFile)) {\n+            this.removedDVs += 1;\n+          } else {\n+            this.removedPosDeleteFiles += 1;\n+          }\n           this.removedDeleteFiles += 1;\n-          this.removedPosDeleteFiles += 1;\n           this.removedPosDeletes += file.recordCount();\n           break;\n         case EQUALITY_DELETES:\n@@ -344,6 +363,8 @@ void merge(UpdateMetrics other) {\n       this.removedEqDeleteFiles += other.removedEqDeleteFiles;\n       this.addedPosDeleteFiles += other.addedPosDeleteFiles;\n       this.removedPosDeleteFiles += other.removedPosDeleteFiles;\n+      this.addedDVs += other.addedDVs;\n+      this.removedDVs += other.removedDVs;\n       this.addedDeleteFiles += other.addedDeleteFiles;\n       this.removedDeleteFiles += other.removedDeleteFiles;\n       this.addedSize += other.addedSize;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResult.java b/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResult.java\nindex ad66e8d32408..7a87172708f6 100644\n--- a/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResult.java\n+++ b/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResult.java\n@@ -34,7 +34,9 @@ public interface CommitMetricsResult {\n   String ADDED_DELETE_FILES = \"added-delete-files\";\n   String ADDED_EQ_DELETE_FILES = \"added-equality-delete-files\";\n   String ADDED_POS_DELETE_FILES = \"added-positional-delete-files\";\n+  String ADDED_DVS = \"added-dvs\";\n   String REMOVED_POS_DELETE_FILES = \"removed-positional-delete-files\";\n+  String REMOVED_DVS = \"removed-dvs\";\n   String REMOVED_EQ_DELETE_FILES = \"removed-equality-delete-files\";\n   String REMOVED_DELETE_FILES = \"removed-delete-files\";\n   String TOTAL_DELETE_FILES = \"total-delete-files\";\n@@ -75,6 +77,12 @@ public interface CommitMetricsResult {\n   @Nullable\n   CounterResult addedPositionalDeleteFiles();\n \n+  @Nullable\n+  @Value.Default\n+  default CounterResult addedDVs() {\n+    return null;\n+  }\n+\n   @Nullable\n   CounterResult removedDeleteFiles();\n \n@@ -84,6 +92,12 @@ public interface CommitMetricsResult {\n   @Nullable\n   CounterResult removedPositionalDeleteFiles();\n \n+  @Nullable\n+  @Value.Default\n+  default CounterResult removedDVs() {\n+    return null;\n+  }\n+\n   @Nullable\n   CounterResult totalDeleteFiles();\n \n@@ -136,6 +150,7 @@ static CommitMetricsResult from(\n         .addedDeleteFiles(counterFrom(snapshotSummary, SnapshotSummary.ADDED_DELETE_FILES_PROP))\n         .addedPositionalDeleteFiles(\n             counterFrom(snapshotSummary, SnapshotSummary.ADD_POS_DELETE_FILES_PROP))\n+        .addedDVs(counterFrom(snapshotSummary, SnapshotSummary.ADDED_DVS_PROP))\n         .addedEqualityDeleteFiles(\n             counterFrom(snapshotSummary, SnapshotSummary.ADD_EQ_DELETE_FILES_PROP))\n         .removedDeleteFiles(counterFrom(snapshotSummary, SnapshotSummary.REMOVED_DELETE_FILES_PROP))\n@@ -143,6 +158,7 @@ static CommitMetricsResult from(\n             counterFrom(snapshotSummary, SnapshotSummary.REMOVED_EQ_DELETE_FILES_PROP))\n         .removedPositionalDeleteFiles(\n             counterFrom(snapshotSummary, SnapshotSummary.REMOVED_POS_DELETE_FILES_PROP))\n+        .removedDVs(counterFrom(snapshotSummary, SnapshotSummary.REMOVED_DVS_PROP))\n         .totalDeleteFiles(counterFrom(snapshotSummary, SnapshotSummary.TOTAL_DELETE_FILES_PROP))\n         .addedRecords(counterFrom(snapshotSummary, SnapshotSummary.ADDED_RECORDS_PROP))\n         .removedRecords(counterFrom(snapshotSummary, SnapshotSummary.DELETED_RECORDS_PROP))\n\ndiff --git a/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResultParser.java b/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResultParser.java\nindex d4fd883c4375..2c45581ba5d6 100644\n--- a/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResultParser.java\n+++ b/core/src/main/java/org/apache/iceberg/metrics/CommitMetricsResultParser.java\n@@ -81,6 +81,11 @@ static void toJson(CommitMetricsResult metrics, JsonGenerator gen) throws IOExce\n       CounterResultParser.toJson(metrics.addedPositionalDeleteFiles(), gen);\n     }\n \n+    if (null != metrics.addedDVs()) {\n+      gen.writeFieldName(CommitMetricsResult.ADDED_DVS);\n+      CounterResultParser.toJson(metrics.addedDVs(), gen);\n+    }\n+\n     if (null != metrics.removedDeleteFiles()) {\n       gen.writeFieldName(CommitMetricsResult.REMOVED_DELETE_FILES);\n       CounterResultParser.toJson(metrics.removedDeleteFiles(), gen);\n@@ -91,6 +96,11 @@ static void toJson(CommitMetricsResult metrics, JsonGenerator gen) throws IOExce\n       CounterResultParser.toJson(metrics.removedPositionalDeleteFiles(), gen);\n     }\n \n+    if (null != metrics.removedDVs()) {\n+      gen.writeFieldName(CommitMetricsResult.REMOVED_DVS);\n+      CounterResultParser.toJson(metrics.removedDVs(), gen);\n+    }\n+\n     if (null != metrics.removedEqualityDeleteFiles()) {\n       gen.writeFieldName(CommitMetricsResult.REMOVED_EQ_DELETE_FILES);\n       CounterResultParser.toJson(metrics.removedEqualityDeleteFiles(), gen);\n@@ -186,10 +196,12 @@ static CommitMetricsResult fromJson(JsonNode json) {\n             CounterResultParser.fromJson(CommitMetricsResult.ADDED_EQ_DELETE_FILES, json))\n         .addedPositionalDeleteFiles(\n             CounterResultParser.fromJson(CommitMetricsResult.ADDED_POS_DELETE_FILES, json))\n+        .addedDVs(CounterResultParser.fromJson(CommitMetricsResult.ADDED_DVS, json))\n         .removedEqualityDeleteFiles(\n             CounterResultParser.fromJson(CommitMetricsResult.REMOVED_EQ_DELETE_FILES, json))\n         .removedPositionalDeleteFiles(\n             CounterResultParser.fromJson(CommitMetricsResult.REMOVED_POS_DELETE_FILES, json))\n+        .removedDVs(CounterResultParser.fromJson(CommitMetricsResult.REMOVED_DVS, json))\n         .removedDeleteFiles(\n             CounterResultParser.fromJson(CommitMetricsResult.REMOVED_DELETE_FILES, json))\n         .totalDeleteFiles(\n\ndiff --git a/core/src/main/java/org/apache/iceberg/metrics/ScanMetrics.java b/core/src/main/java/org/apache/iceberg/metrics/ScanMetrics.java\nindex 421466f0fa85..0f7def37638e 100644\n--- a/core/src/main/java/org/apache/iceberg/metrics/ScanMetrics.java\n+++ b/core/src/main/java/org/apache/iceberg/metrics/ScanMetrics.java\n@@ -40,6 +40,7 @@ public abstract class ScanMetrics {\n   public static final String INDEXED_DELETE_FILES = \"indexed-delete-files\";\n   public static final String EQUALITY_DELETE_FILES = \"equality-delete-files\";\n   public static final String POSITIONAL_DELETE_FILES = \"positional-delete-files\";\n+  public static final String DVS = \"dvs\";\n \n   public static ScanMetrics noop() {\n     return ScanMetrics.of(MetricsContext.nullMetrics());\n@@ -127,6 +128,11 @@ public Counter positionalDeleteFiles() {\n     return metricsContext().counter(POSITIONAL_DELETE_FILES);\n   }\n \n+  @Value.Derived\n+  public Counter dvs() {\n+    return metricsContext().counter(DVS);\n+  }\n+\n   public static ScanMetrics of(MetricsContext metricsContext) {\n     return ImmutableScanMetrics.builder().metricsContext(metricsContext).build();\n   }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResult.java b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResult.java\nindex b930dd83adef..2137e52e0a89 100644\n--- a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResult.java\n+++ b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResult.java\n@@ -73,6 +73,12 @@ public interface ScanMetricsResult {\n   @Nullable\n   CounterResult positionalDeleteFiles();\n \n+  @Nullable\n+  @Value.Default\n+  default CounterResult dvs() {\n+    return null;\n+  }\n+\n   static ScanMetricsResult fromScanMetrics(ScanMetrics scanMetrics) {\n     Preconditions.checkArgument(null != scanMetrics, \"Invalid scan metrics: null\");\n     return ImmutableScanMetricsResult.builder()\n@@ -93,6 +99,7 @@ static ScanMetricsResult fromScanMetrics(ScanMetrics scanMetrics) {\n         .indexedDeleteFiles(CounterResult.fromCounter(scanMetrics.indexedDeleteFiles()))\n         .equalityDeleteFiles(CounterResult.fromCounter(scanMetrics.equalityDeleteFiles()))\n         .positionalDeleteFiles(CounterResult.fromCounter(scanMetrics.positionalDeleteFiles()))\n+        .dvs(CounterResult.fromCounter(scanMetrics.dvs()))\n         .build();\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResultParser.java b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResultParser.java\nindex 5cff1ae8e0db..f85c26753211 100644\n--- a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResultParser.java\n+++ b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsResultParser.java\n@@ -121,6 +121,11 @@ static void toJson(ScanMetricsResult metrics, JsonGenerator gen) throws IOExcept\n       CounterResultParser.toJson(metrics.positionalDeleteFiles(), gen);\n     }\n \n+    if (null != metrics.dvs()) {\n+      gen.writeFieldName(ScanMetrics.DVS);\n+      CounterResultParser.toJson(metrics.dvs(), gen);\n+    }\n+\n     gen.writeEndObject();\n   }\n \n@@ -159,6 +164,7 @@ static ScanMetricsResult fromJson(JsonNode json) {\n         .equalityDeleteFiles(CounterResultParser.fromJson(ScanMetrics.EQUALITY_DELETE_FILES, json))\n         .positionalDeleteFiles(\n             CounterResultParser.fromJson(ScanMetrics.POSITIONAL_DELETE_FILES, json))\n+        .dvs(CounterResultParser.fromJson(ScanMetrics.DVS, json))\n         .build();\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java\nindex 1ba891f58474..6e6aa25636bd 100644\n--- a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java\n@@ -21,6 +21,7 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.util.ContentFileUtil;\n import org.apache.iceberg.util.ScanTaskUtil;\n \n public class ScanMetricsUtil {\n@@ -31,7 +32,11 @@ public static void indexedDeleteFile(ScanMetrics metrics, DeleteFile deleteFile)\n     metrics.indexedDeleteFiles().increment();\n \n     if (deleteFile.content() == FileContent.POSITION_DELETES) {\n-      metrics.positionalDeleteFiles().increment();\n+      if (ContentFileUtil.isDV(deleteFile)) {\n+        metrics.dvs().increment();\n+      } else {\n+        metrics.positionalDeleteFiles().increment();\n+      }\n     } else if (deleteFile.content() == FileContent.EQUALITY_DELETES) {\n       metrics.equalityDeleteFiles().increment();\n     }\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java b/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java\nindex 529e0cc614f6..b0b9d003e35b 100644\n--- a/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java\n+++ b/core/src/test/java/org/apache/iceberg/TestSnapshotSummary.java\n@@ -358,4 +358,76 @@ public void rewriteWithDeletesAndDuplicates() {\n         .containsEntry(SnapshotSummary.TOTAL_FILE_SIZE_PROP, \"20\")\n         .containsEntry(SnapshotSummary.TOTAL_RECORDS_PROP, \"1\");\n   }\n+\n+  @TestTemplate\n+  public void testFileSizeSummaryWithDVs() {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(3);\n+\n+    DeleteFile dv1 = newDV(FILE_A);\n+    table.newRowDelta().addDeletes(dv1).commit();\n+\n+    DeleteFile dv2 = newDV(FILE_B);\n+    table.newRowDelta().addDeletes(dv2).commit();\n+\n+    Map<String, String> summary1 = table.currentSnapshot().summary();\n+    long addedPosDeletes1 = dv2.recordCount();\n+    long addedFileSize1 = dv2.contentSizeInBytes();\n+    long totalPosDeletes1 = dv1.recordCount() + dv2.recordCount();\n+    long totalFileSize1 = dv1.contentSizeInBytes() + dv2.contentSizeInBytes();\n+    assertThat(summary1)\n+        .hasSize(12)\n+        .doesNotContainKey(SnapshotSummary.ADD_POS_DELETE_FILES_PROP)\n+        .doesNotContainKey(SnapshotSummary.REMOVED_POS_DELETE_FILES_PROP)\n+        .containsEntry(SnapshotSummary.ADDED_DELETE_FILES_PROP, \"1\")\n+        .doesNotContainKey(SnapshotSummary.REMOVED_DELETE_FILES_PROP)\n+        .containsEntry(SnapshotSummary.ADDED_DVS_PROP, \"1\")\n+        .doesNotContainKey(SnapshotSummary.REMOVED_DVS_PROP)\n+        .containsEntry(SnapshotSummary.ADDED_POS_DELETES_PROP, String.valueOf(addedPosDeletes1))\n+        .doesNotContainKey(SnapshotSummary.REMOVED_POS_DELETES_PROP)\n+        .containsEntry(SnapshotSummary.ADDED_FILE_SIZE_PROP, String.valueOf(addedFileSize1))\n+        .doesNotContainKey(SnapshotSummary.REMOVED_FILE_SIZE_PROP)\n+        .containsEntry(SnapshotSummary.TOTAL_DELETE_FILES_PROP, \"2\")\n+        .containsEntry(SnapshotSummary.TOTAL_POS_DELETES_PROP, String.valueOf(totalPosDeletes1))\n+        .containsEntry(SnapshotSummary.TOTAL_FILE_SIZE_PROP, String.valueOf(totalFileSize1))\n+        .containsEntry(SnapshotSummary.TOTAL_DATA_FILES_PROP, \"0\")\n+        .containsEntry(SnapshotSummary.TOTAL_EQ_DELETES_PROP, \"0\")\n+        .containsEntry(SnapshotSummary.TOTAL_RECORDS_PROP, \"0\")\n+        .containsEntry(SnapshotSummary.CHANGED_PARTITION_COUNT_PROP, \"1\");\n+\n+    DeleteFile dv3 = newDV(FILE_A);\n+    table\n+        .newRowDelta()\n+        .removeDeletes(dv1)\n+        .removeDeletes(dv2)\n+        .addDeletes(dv3)\n+        .validateFromSnapshot(table.currentSnapshot().snapshotId())\n+        .commit();\n+\n+    Map<String, String> summary2 = table.currentSnapshot().summary();\n+    long addedPosDeletes2 = dv3.recordCount();\n+    long removedPosDeletes2 = dv1.recordCount() + dv2.recordCount();\n+    long addedFileSize2 = dv3.contentSizeInBytes();\n+    long removedFileSize2 = dv1.contentSizeInBytes() + dv2.contentSizeInBytes();\n+    long totalPosDeletes2 = dv3.recordCount();\n+    long totalFileSize2 = dv3.contentSizeInBytes();\n+    assertThat(summary2)\n+        .hasSize(16)\n+        .doesNotContainKey(SnapshotSummary.ADD_POS_DELETE_FILES_PROP)\n+        .doesNotContainKey(SnapshotSummary.REMOVED_POS_DELETE_FILES_PROP)\n+        .containsEntry(SnapshotSummary.ADDED_DELETE_FILES_PROP, \"1\")\n+        .containsEntry(SnapshotSummary.REMOVED_DELETE_FILES_PROP, \"2\")\n+        .containsEntry(SnapshotSummary.ADDED_DVS_PROP, \"1\")\n+        .containsEntry(SnapshotSummary.REMOVED_DVS_PROP, \"2\")\n+        .containsEntry(SnapshotSummary.ADDED_POS_DELETES_PROP, String.valueOf(addedPosDeletes2))\n+        .containsEntry(SnapshotSummary.REMOVED_POS_DELETES_PROP, String.valueOf(removedPosDeletes2))\n+        .containsEntry(SnapshotSummary.ADDED_FILE_SIZE_PROP, String.valueOf(addedFileSize2))\n+        .containsEntry(SnapshotSummary.REMOVED_FILE_SIZE_PROP, String.valueOf(removedFileSize2))\n+        .containsEntry(SnapshotSummary.TOTAL_DELETE_FILES_PROP, \"1\")\n+        .containsEntry(SnapshotSummary.TOTAL_POS_DELETES_PROP, String.valueOf(totalPosDeletes2))\n+        .containsEntry(SnapshotSummary.TOTAL_FILE_SIZE_PROP, String.valueOf(totalFileSize2))\n+        .containsEntry(SnapshotSummary.TOTAL_DATA_FILES_PROP, \"0\")\n+        .containsEntry(SnapshotSummary.TOTAL_EQ_DELETES_PROP, \"0\")\n+        .containsEntry(SnapshotSummary.TOTAL_RECORDS_PROP, \"0\")\n+        .containsEntry(SnapshotSummary.CHANGED_PARTITION_COUNT_PROP, \"2\");\n+  }\n }\n\ndiff --git a/core/src/test/java/org/apache/iceberg/metrics/TestCommitMetricsResultParser.java b/core/src/test/java/org/apache/iceberg/metrics/TestCommitMetricsResultParser.java\nindex 5aa2660143a4..1b51066cf15c 100644\n--- a/core/src/test/java/org/apache/iceberg/metrics/TestCommitMetricsResultParser.java\n+++ b/core/src/test/java/org/apache/iceberg/metrics/TestCommitMetricsResultParser.java\n@@ -74,6 +74,8 @@ public void roundTripSerde() {\n             .put(SnapshotSummary.ADDED_DELETE_FILES_PROP, \"4\")\n             .put(SnapshotSummary.ADD_EQ_DELETE_FILES_PROP, \"5\")\n             .put(SnapshotSummary.ADD_POS_DELETE_FILES_PROP, \"6\")\n+            .put(SnapshotSummary.ADDED_DVS_PROP, \"1\")\n+            .put(SnapshotSummary.REMOVED_DVS_PROP, \"4\")\n             .put(SnapshotSummary.REMOVED_POS_DELETE_FILES_PROP, \"7\")\n             .put(SnapshotSummary.REMOVED_EQ_DELETE_FILES_PROP, \"8\")\n             .put(SnapshotSummary.REMOVED_DELETE_FILES_PROP, \"9\")\n@@ -101,6 +103,8 @@ public void roundTripSerde() {\n     assertThat(result.addedDeleteFiles().value()).isEqualTo(4L);\n     assertThat(result.addedEqualityDeleteFiles().value()).isEqualTo(5L);\n     assertThat(result.addedPositionalDeleteFiles().value()).isEqualTo(6L);\n+    assertThat(result.addedDVs().value()).isEqualTo(1L);\n+    assertThat(result.removedDVs().value()).isEqualTo(4L);\n     assertThat(result.removedPositionalDeleteFiles().value()).isEqualTo(7L);\n     assertThat(result.removedEqualityDeleteFiles().value()).isEqualTo(8L);\n     assertThat(result.removedDeleteFiles().value()).isEqualTo(9L);\n@@ -153,6 +157,10 @@ public void roundTripSerde() {\n             + \"    \\\"unit\\\" : \\\"count\\\",\\n\"\n             + \"    \\\"value\\\" : 6\\n\"\n             + \"  },\\n\"\n+            + \"  \\\"added-dvs\\\" : {\\n\"\n+            + \"    \\\"unit\\\" : \\\"count\\\",\\n\"\n+            + \"    \\\"value\\\" : 1\\n\"\n+            + \"  },\\n\"\n             + \"  \\\"removed-delete-files\\\" : {\\n\"\n             + \"    \\\"unit\\\" : \\\"count\\\",\\n\"\n             + \"    \\\"value\\\" : 9\\n\"\n@@ -161,6 +169,10 @@ public void roundTripSerde() {\n             + \"    \\\"unit\\\" : \\\"count\\\",\\n\"\n             + \"    \\\"value\\\" : 7\\n\"\n             + \"  },\\n\"\n+            + \"  \\\"removed-dvs\\\" : {\\n\"\n+            + \"    \\\"unit\\\" : \\\"count\\\",\\n\"\n+            + \"    \\\"value\\\" : 4\\n\"\n+            + \"  },\\n\"\n             + \"  \\\"removed-equality-delete-files\\\" : {\\n\"\n             + \"    \\\"unit\\\" : \\\"count\\\",\\n\"\n             + \"    \\\"value\\\" : 8\\n\"\n\ndiff --git a/core/src/test/java/org/apache/iceberg/metrics/TestScanMetricsResultParser.java b/core/src/test/java/org/apache/iceberg/metrics/TestScanMetricsResultParser.java\nindex 44d5803c4a3a..f5cb1e237307 100644\n--- a/core/src/test/java/org/apache/iceberg/metrics/TestScanMetricsResultParser.java\n+++ b/core/src/test/java/org/apache/iceberg/metrics/TestScanMetricsResultParser.java\n@@ -178,6 +178,7 @@ public void extraFields() {\n     scanMetrics.skippedDeleteManifests().increment(3L);\n     scanMetrics.indexedDeleteFiles().increment(10L);\n     scanMetrics.positionalDeleteFiles().increment(6L);\n+    scanMetrics.dvs().increment();\n     scanMetrics.equalityDeleteFiles().increment(4L);\n \n     ScanMetricsResult scanMetricsResult = ScanMetricsResult.fromScanMetrics(scanMetrics);\n@@ -199,6 +200,7 @@ public void extraFields() {\n                     + \"\\\"indexed-delete-files\\\":{\\\"unit\\\":\\\"count\\\",\\\"value\\\":10},\"\n                     + \"\\\"equality-delete-files\\\":{\\\"unit\\\":\\\"count\\\",\\\"value\\\":4},\"\n                     + \"\\\"positional-delete-files\\\":{\\\"unit\\\":\\\"count\\\",\\\"value\\\":6},\"\n+                    + \"\\\"dvs\\\":{\\\"unit\\\":\\\"count\\\",\\\"value\\\":1},\"\n                     + \"\\\"extra\\\": \\\"value\\\",\\\"extra2\\\":23}\"))\n         .isEqualTo(scanMetricsResult);\n   }\n@@ -242,6 +244,7 @@ public void roundTripSerde() {\n     scanMetrics.skippedDeleteManifests().increment(3L);\n     scanMetrics.indexedDeleteFiles().increment(10L);\n     scanMetrics.positionalDeleteFiles().increment(6L);\n+    scanMetrics.dvs().increment(3L);\n     scanMetrics.equalityDeleteFiles().increment(4L);\n \n     ScanMetricsResult scanMetricsResult = ScanMetricsResult.fromScanMetrics(scanMetrics);\n@@ -312,6 +315,10 @@ public void roundTripSerde() {\n             + \"  \\\"positional-delete-files\\\" : {\\n\"\n             + \"    \\\"unit\\\" : \\\"count\\\",\\n\"\n             + \"    \\\"value\\\" : 6\\n\"\n+            + \"  },\\n\"\n+            + \"  \\\"dvs\\\" : {\\n\"\n+            + \"    \\\"unit\\\" : \\\"count\\\",\\n\"\n+            + \"    \\\"value\\\" : 3\\n\"\n             + \"  }\\n\"\n             + \"}\";\n \n\ndiff --git a/core/src/test/java/org/apache/iceberg/metrics/TestScanReportParser.java b/core/src/test/java/org/apache/iceberg/metrics/TestScanReportParser.java\nindex 51e21ad9bf01..caac0704cd3f 100644\n--- a/core/src/test/java/org/apache/iceberg/metrics/TestScanReportParser.java\n+++ b/core/src/test/java/org/apache/iceberg/metrics/TestScanReportParser.java\n@@ -84,6 +84,7 @@ public void extraFields() {\n     scanMetrics.skippedDeleteManifests().increment(3L);\n     scanMetrics.indexedDeleteFiles().increment(10L);\n     scanMetrics.positionalDeleteFiles().increment(6L);\n+    scanMetrics.dvs().increment();\n     scanMetrics.equalityDeleteFiles().increment(4L);\n \n     String tableName = \"roundTripTableName\";\n@@ -118,6 +119,7 @@ public void extraFields() {\n                     + \"\\\"indexed-delete-files\\\":{\\\"unit\\\":\\\"count\\\",\\\"value\\\":10},\"\n                     + \"\\\"equality-delete-files\\\":{\\\"unit\\\":\\\"count\\\",\\\"value\\\":4},\"\n                     + \"\\\"positional-delete-files\\\":{\\\"unit\\\":\\\"count\\\",\\\"value\\\":6},\"\n+                    + \"\\\"dvs\\\":{\\\"unit\\\":\\\"count\\\",\\\"value\\\":1},\"\n                     + \"\\\"extra-metric\\\":\\\"extra-val\\\"},\"\n                     + \"\\\"extra\\\":\\\"extraVal\\\"}\"))\n         .isEqualTo(scanReport);\n@@ -279,6 +281,10 @@ public void roundTripSerde() {\n             + \"    \\\"positional-delete-files\\\" : {\\n\"\n             + \"      \\\"unit\\\" : \\\"count\\\",\\n\"\n             + \"      \\\"value\\\" : 6\\n\"\n+            + \"    },\\n\"\n+            + \"    \\\"dvs\\\" : {\\n\"\n+            + \"      \\\"unit\\\" : \\\"count\\\",\\n\"\n+            + \"      \\\"value\\\" : 0\\n\"\n             + \"    }\\n\"\n             + \"  }\\n\"\n             + \"}\";\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11463",
    "pr_id": 11463,
    "issue_id": 11122,
    "repo": "apache/iceberg",
    "problem_statement": "Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 118,
    "test_files_count": 5,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/deletes/BitmapPositionDeleteIndex.java",
      "core/src/main/java/org/apache/iceberg/deletes/PositionDeleteIndex.java",
      "core/src/test/java/org/apache/iceberg/deletes/TestBitmapPositionDeleteIndex.java",
      "core/src/test/resources/org/apache/iceberg/deletes/all-container-types-position-index.bin",
      "core/src/test/resources/org/apache/iceberg/deletes/empty-position-index.bin",
      "core/src/test/resources/org/apache/iceberg/deletes/small-alternating-values-position-index.bin",
      "core/src/test/resources/org/apache/iceberg/deletes/small-and-large-values-position-index.bin"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/deletes/TestBitmapPositionDeleteIndex.java",
      "core/src/test/resources/org/apache/iceberg/deletes/all-container-types-position-index.bin",
      "core/src/test/resources/org/apache/iceberg/deletes/empty-position-index.bin",
      "core/src/test/resources/org/apache/iceberg/deletes/small-alternating-values-position-index.bin",
      "core/src/test/resources/org/apache/iceberg/deletes/small-and-large-values-position-index.bin"
    ],
    "base_commit": "ec269ee3ec0de4184eb536a6ef4f3523dc91332a",
    "head_commit": "af6bc8d32c014a1c3f7781b99b4cea717e87b8c6",
    "repo_url": "https://github.com/apache/iceberg/pull/11463",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11463",
    "dockerfile": "",
    "pr_merged_at": "2024-11-05T07:35:42.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/deletes/BitmapPositionDeleteIndex.java b/core/src/main/java/org/apache/iceberg/deletes/BitmapPositionDeleteIndex.java\nindex cfb163e8379c..376b391d9c24 100644\n--- a/core/src/main/java/org/apache/iceberg/deletes/BitmapPositionDeleteIndex.java\n+++ b/core/src/main/java/org/apache/iceberg/deletes/BitmapPositionDeleteIndex.java\n@@ -18,13 +18,23 @@\n  */\n package org.apache.iceberg.deletes;\n \n+import java.nio.ByteBuffer;\n+import java.nio.ByteOrder;\n import java.util.Collection;\n import java.util.List;\n import java.util.function.LongConsumer;\n+import java.util.zip.CRC32;\n import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n \n class BitmapPositionDeleteIndex implements PositionDeleteIndex {\n+  private static final int LENGTH_SIZE_BYTES = 4;\n+  private static final int MAGIC_NUMBER_SIZE_BYTES = 4;\n+  private static final int CRC_SIZE_BYTES = 4;\n+  private static final int BITMAP_DATA_OFFSET = 4;\n+  private static final int MAGIC_NUMBER = 1681511377;\n+\n   private final RoaringPositionBitmap bitmap;\n   private final List<DeleteFile> deleteFiles;\n \n@@ -43,6 +53,11 @@ class BitmapPositionDeleteIndex implements PositionDeleteIndex {\n     this.deleteFiles = deleteFile != null ? Lists.newArrayList(deleteFile) : Lists.newArrayList();\n   }\n \n+  BitmapPositionDeleteIndex(RoaringPositionBitmap bitmap, DeleteFile deleteFile) {\n+    this.bitmap = bitmap;\n+    this.deleteFiles = deleteFile != null ? Lists.newArrayList(deleteFile) : Lists.newArrayList();\n+  }\n+\n   void merge(BitmapPositionDeleteIndex that) {\n     bitmap.setAll(that.bitmap);\n     deleteFiles.addAll(that.deleteFiles);\n@@ -92,4 +107,113 @@ public Collection<DeleteFile> deleteFiles() {\n   public long cardinality() {\n     return bitmap.cardinality();\n   }\n+\n+  /**\n+   * Serializes the index using the following format:\n+   *\n+   * <ul>\n+   *   <li>The length of the magic bytes and bitmap stored as 4 bytes (big-endian).\n+   *   <li>A 4-byte {@link #MAGIC_NUMBER} (little-endian).\n+   *   <li>The bitmap serialized using the portable Roaring spec (little-endian).\n+   *   <li>A CRC-32 checksum of the magic bytes and bitmap as 4-bytes (big-endian).\n+   * </ul>\n+   *\n+   * Note that the length and the checksum are computed for the bitmap data, which includes the\n+   * magic bytes and bitmap for compatibility with Delta.\n+   */\n+  @Override\n+  public ByteBuffer serialize() {\n+    bitmap.runLengthEncode(); // run-length encode the bitmap before serializing\n+    int bitmapDataLength = computeBitmapDataLength(bitmap); // magic bytes + bitmap\n+    byte[] bytes = new byte[LENGTH_SIZE_BYTES + bitmapDataLength + CRC_SIZE_BYTES];\n+    ByteBuffer buffer = ByteBuffer.wrap(bytes);\n+    buffer.putInt(bitmapDataLength);\n+    serializeBitmapData(bytes, bitmapDataLength, bitmap);\n+    int crcOffset = LENGTH_SIZE_BYTES + bitmapDataLength;\n+    int crc = computeChecksum(bytes, bitmapDataLength);\n+    buffer.putInt(crcOffset, crc);\n+    buffer.rewind();\n+    return buffer;\n+  }\n+\n+  /**\n+   * Deserializes the index from bytes, assuming the format described in {@link #serialize()}.\n+   *\n+   * @param bytes an array containing the serialized index\n+   * @param deleteFile the DV file\n+   * @return the deserialized index\n+   */\n+  public static PositionDeleteIndex deserialize(byte[] bytes, DeleteFile deleteFile) {\n+    ByteBuffer buffer = ByteBuffer.wrap(bytes);\n+    int bitmapDataLength = readBitmapDataLength(buffer, deleteFile);\n+    RoaringPositionBitmap bitmap = deserializeBitmap(bytes, bitmapDataLength, deleteFile);\n+    int crc = computeChecksum(bytes, bitmapDataLength);\n+    int crcOffset = LENGTH_SIZE_BYTES + bitmapDataLength;\n+    int expectedCrc = buffer.getInt(crcOffset);\n+    Preconditions.checkArgument(crc == expectedCrc, \"Invalid CRC\");\n+    return new BitmapPositionDeleteIndex(bitmap, deleteFile);\n+  }\n+\n+  // computes and validates the length of the bitmap data (magic bytes + bitmap)\n+  private static int computeBitmapDataLength(RoaringPositionBitmap bitmap) {\n+    long length = MAGIC_NUMBER_SIZE_BYTES + bitmap.serializedSizeInBytes();\n+    long bufferSize = LENGTH_SIZE_BYTES + length + CRC_SIZE_BYTES;\n+    Preconditions.checkState(bufferSize <= Integer.MAX_VALUE, \"Can't serialize index > 2GB\");\n+    return (int) length;\n+  }\n+\n+  // serializes the bitmap data (magic bytes + bitmap) using the little-endian byte order\n+  private static void serializeBitmapData(\n+      byte[] bytes, int bitmapDataLength, RoaringPositionBitmap bitmap) {\n+    ByteBuffer bitmapData = pointToBitmapData(bytes, bitmapDataLength);\n+    bitmapData.putInt(MAGIC_NUMBER);\n+    bitmap.serialize(bitmapData);\n+  }\n+\n+  // points to the bitmap data in the blob\n+  private static ByteBuffer pointToBitmapData(byte[] bytes, int bitmapDataLength) {\n+    ByteBuffer bitmapData = ByteBuffer.wrap(bytes, BITMAP_DATA_OFFSET, bitmapDataLength);\n+    bitmapData.order(ByteOrder.LITTLE_ENDIAN);\n+    return bitmapData;\n+  }\n+\n+  // checks the blob size is equal to the bitmap data length + extra bytes for length and CRC\n+  private static int readBitmapDataLength(ByteBuffer buffer, DeleteFile deleteFile) {\n+    int length = buffer.getInt();\n+    long expectedLength = deleteFile.contentSizeInBytes() - LENGTH_SIZE_BYTES - CRC_SIZE_BYTES;\n+    Preconditions.checkArgument(\n+        length == expectedLength,\n+        \"Invalid bitmap data length: %s, expected %s\",\n+        length,\n+        expectedLength);\n+    return length;\n+  }\n+\n+  // validates magic bytes and deserializes the bitmap\n+  private static RoaringPositionBitmap deserializeBitmap(\n+      byte[] bytes, int bitmapDataLength, DeleteFile deleteFile) {\n+    ByteBuffer bitmapData = pointToBitmapData(bytes, bitmapDataLength);\n+    int magicNumber = bitmapData.getInt();\n+    Preconditions.checkArgument(\n+        magicNumber == MAGIC_NUMBER,\n+        \"Invalid magic number: %s, expected %s\",\n+        magicNumber,\n+        MAGIC_NUMBER);\n+    RoaringPositionBitmap bitmap = RoaringPositionBitmap.deserialize(bitmapData);\n+    long cardinality = bitmap.cardinality();\n+    long expectedCardinality = deleteFile.recordCount();\n+    Preconditions.checkArgument(\n+        cardinality == expectedCardinality,\n+        \"Invalid cardinality: %s, expected %s\",\n+        cardinality,\n+        expectedCardinality);\n+    return bitmap;\n+  }\n+\n+  // generates a 32-bit unsigned checksum for the magic bytes and serialized bitmap\n+  private static int computeChecksum(byte[] bytes, int bitmapDataLength) {\n+    CRC32 crc = new CRC32();\n+    crc.update(bytes, BITMAP_DATA_OFFSET, bitmapDataLength);\n+    return (int) crc.getValue();\n+  }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/deletes/PositionDeleteIndex.java b/core/src/main/java/org/apache/iceberg/deletes/PositionDeleteIndex.java\nindex 8ccfc03d1a26..6f97b3a6ac87 100644\n--- a/core/src/main/java/org/apache/iceberg/deletes/PositionDeleteIndex.java\n+++ b/core/src/main/java/org/apache/iceberg/deletes/PositionDeleteIndex.java\n@@ -18,6 +18,7 @@\n  */\n package org.apache.iceberg.deletes;\n \n+import java.nio.ByteBuffer;\n import java.util.Collection;\n import java.util.function.LongConsumer;\n import org.apache.iceberg.DeleteFile;\n@@ -92,6 +93,26 @@ default long cardinality() {\n     throw new UnsupportedOperationException(getClass().getName() + \" does not support cardinality\");\n   }\n \n+  /**\n+   * Serializes this index.\n+   *\n+   * @return a buffer containing the serialized index\n+   */\n+  default ByteBuffer serialize() {\n+    throw new UnsupportedOperationException(getClass().getName() + \" does not support serialize\");\n+  }\n+\n+  /**\n+   * Deserializes a position delete index.\n+   *\n+   * @param bytes an array containing the serialized index\n+   * @param deleteFile the delete file that the index is created for\n+   * @return the deserialized index\n+   */\n+  static PositionDeleteIndex deserialize(byte[] bytes, DeleteFile deleteFile) {\n+    return BitmapPositionDeleteIndex.deserialize(bytes, deleteFile);\n+  }\n+\n   /** Returns an empty immutable position delete index. */\n   static PositionDeleteIndex empty() {\n     return EmptyPositionDeleteIndex.get();\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/deletes/TestBitmapPositionDeleteIndex.java b/core/src/test/java/org/apache/iceberg/deletes/TestBitmapPositionDeleteIndex.java\nindex c8fc723deb9e..76b294f80611 100644\n--- a/core/src/test/java/org/apache/iceberg/deletes/TestBitmapPositionDeleteIndex.java\n+++ b/core/src/test/java/org/apache/iceberg/deletes/TestBitmapPositionDeleteIndex.java\n@@ -20,12 +20,21 @@\n \n import static org.assertj.core.api.Assertions.assertThat;\n \n+import java.io.IOException;\n+import java.net.URL;\n+import java.nio.ByteBuffer;\n import java.util.List;\n+import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n+import org.apache.iceberg.relocated.com.google.common.io.Resources;\n import org.junit.jupiter.api.Test;\n+import org.mockito.Mockito;\n \n public class TestBitmapPositionDeleteIndex {\n \n+  private static final long BITMAP_OFFSET = 0xFFFFFFFFL + 1L;\n+  private static final long CONTAINER_OFFSET = Character.MAX_VALUE + 1L;\n+\n   @Test\n   public void testForEach() {\n     long pos1 = 10L; // Container 0 (high bits = 0)\n@@ -105,6 +114,102 @@ public void testMergeBitmapIndexWithEmpty() {\n     assertThat(positions).containsExactly(pos1, pos2, pos3, pos4);\n   }\n \n+  @Test\n+  public void testEmptyIndexSerialization() throws Exception {\n+    PositionDeleteIndex index = new BitmapPositionDeleteIndex();\n+    validate(index, \"empty-position-index.bin\");\n+  }\n+\n+  @Test\n+  public void testSmallAlternatingValuesIndexSerialization() throws Exception {\n+    PositionDeleteIndex index = new BitmapPositionDeleteIndex();\n+    index.delete(1L);\n+    index.delete(3L);\n+    index.delete(5L);\n+    index.delete(7L);\n+    index.delete(9L);\n+    validate(index, \"small-alternating-values-position-index.bin\");\n+  }\n+\n+  @Test\n+  public void testSmallAndLargeValuesIndexSerialization() throws Exception {\n+    PositionDeleteIndex index = new BitmapPositionDeleteIndex();\n+    index.delete(100L);\n+    index.delete(101L);\n+    index.delete(Integer.MAX_VALUE + 100L);\n+    index.delete(Integer.MAX_VALUE + 101L);\n+    validate(index, \"small-and-large-values-position-index.bin\");\n+  }\n+\n+  @Test\n+  public void testAllContainerTypesIndexSerialization() throws Exception {\n+    PositionDeleteIndex index = new BitmapPositionDeleteIndex();\n+\n+    // bitmap 0, container 0 (array)\n+    index.delete(position(0 /* bitmap */, 0 /* container */, 5L));\n+    index.delete(position(0 /* bitmap */, 0 /* container */, 7L));\n+\n+    // bitmap 0, container 1 (array that can be compressed)\n+    index.delete(\n+        position(0 /* bitmap */, 1 /* container */, 1L),\n+        position(0 /* bitmap */, 1 /* container */, 1000L));\n+\n+    // bitmap 1, container 2 (bitset)\n+    index.delete(\n+        position(0 /* bitmap */, 2 /* container */, 1L),\n+        position(0 /* bitmap */, 2 /* container */, CONTAINER_OFFSET - 1L));\n+\n+    // bitmap 1, container 0 (array)\n+    index.delete(position(1 /* bitmap */, 0 /* container */, 10L));\n+    index.delete(position(1 /* bitmap */, 0 /* container */, 20L));\n+\n+    // bitmap 1, container 1 (array that can be compressed)\n+    index.delete(\n+        position(1 /* bitmap */, 1 /* container */, 10L),\n+        position(1 /* bitmap */, 1 /* container */, 500L));\n+\n+    // bitmap 1, container 2 (bitset)\n+    index.delete(\n+        position(1 /* bitmap */, 2 /* container */, 1L),\n+        position(1 /* bitmap */, 2 /* container */, CONTAINER_OFFSET - 1));\n+\n+    validate(index, \"all-container-types-position-index.bin\");\n+  }\n+\n+  private static void validate(PositionDeleteIndex index, String goldenFile) throws Exception {\n+    ByteBuffer buffer = index.serialize();\n+    byte[] bytes = buffer.array();\n+    DeleteFile dv = mockDV(bytes.length, index.cardinality());\n+    PositionDeleteIndex indexCopy = PositionDeleteIndex.deserialize(bytes, dv);\n+    assertEqual(index, indexCopy);\n+    byte[] goldenBytes = readTestResource(goldenFile);\n+    assertThat(bytes).isEqualTo(goldenBytes);\n+    PositionDeleteIndex goldenIndex = PositionDeleteIndex.deserialize(goldenBytes, dv);\n+    assertEqual(index, goldenIndex);\n+  }\n+\n+  private static DeleteFile mockDV(long contentSize, long cardinality) {\n+    DeleteFile mock = Mockito.mock(DeleteFile.class);\n+    Mockito.when(mock.contentSizeInBytes()).thenReturn(contentSize);\n+    Mockito.when(mock.recordCount()).thenReturn(cardinality);\n+    return mock;\n+  }\n+\n+  private static void assertEqual(PositionDeleteIndex index, PositionDeleteIndex thatIndex) {\n+    assertThat(index.cardinality()).isEqualTo(thatIndex.cardinality());\n+    index.forEach(position -> assertThat(thatIndex.isDeleted(position)).isTrue());\n+    thatIndex.forEach(position -> assertThat(index.isDeleted(position)).isTrue());\n+  }\n+\n+  private static long position(int bitmapIndex, int containerIndex, long value) {\n+    return bitmapIndex * BITMAP_OFFSET + containerIndex * CONTAINER_OFFSET + value;\n+  }\n+\n+  private static byte[] readTestResource(String resourceName) throws IOException {\n+    URL resource = Resources.getResource(TestRoaringPositionBitmap.class, resourceName);\n+    return Resources.toByteArray(resource);\n+  }\n+\n   private List<Long> collect(PositionDeleteIndex index) {\n     List<Long> positions = Lists.newArrayList();\n     index.forEach(positions::add);\n\ndiff --git a/core/src/test/resources/org/apache/iceberg/deletes/all-container-types-position-index.bin b/core/src/test/resources/org/apache/iceberg/deletes/all-container-types-position-index.bin\nnew file mode 100644\nindex 000000000000..00d47303b11b\nBinary files /dev/null and b/core/src/test/resources/org/apache/iceberg/deletes/all-container-types-position-index.bin differ\n\ndiff --git a/core/src/test/resources/org/apache/iceberg/deletes/empty-position-index.bin b/core/src/test/resources/org/apache/iceberg/deletes/empty-position-index.bin\nnew file mode 100644\nindex 000000000000..8bbc1265dc1d\nBinary files /dev/null and b/core/src/test/resources/org/apache/iceberg/deletes/empty-position-index.bin differ\n\ndiff --git a/core/src/test/resources/org/apache/iceberg/deletes/small-alternating-values-position-index.bin b/core/src/test/resources/org/apache/iceberg/deletes/small-alternating-values-position-index.bin\nnew file mode 100644\nindex 000000000000..80829fae22c3\nBinary files /dev/null and b/core/src/test/resources/org/apache/iceberg/deletes/small-alternating-values-position-index.bin differ\n\ndiff --git a/core/src/test/resources/org/apache/iceberg/deletes/small-and-large-values-position-index.bin b/core/src/test/resources/org/apache/iceberg/deletes/small-and-large-values-position-index.bin\nnew file mode 100644\nindex 000000000000..989dabf6ad53\nBinary files /dev/null and b/core/src/test/resources/org/apache/iceberg/deletes/small-and-large-values-position-index.bin differ\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11457",
    "pr_id": 11457,
    "issue_id": 11122,
    "repo": "apache/iceberg",
    "problem_statement": "Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 118,
    "test_files_count": 2,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "core/src/test/java/org/apache/iceberg/FileGenerationUtil.java",
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkContentFile.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/FileGenerationUtil.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java"
    ],
    "base_commit": "ec269ee3ec0de4184eb536a6ef4f3523dc91332a",
    "head_commit": "3fc424154e2d045d26edb50dd348651e96bfa9c6",
    "repo_url": "https://github.com/apache/iceberg/pull/11457",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11457",
    "dockerfile": "",
    "pr_merged_at": "2024-11-04T20:22:50.000Z",
    "patch": "diff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkContentFile.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkContentFile.java\nindex 99586f2503c2..af7c4a9b866d 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkContentFile.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/SparkContentFile.java\n@@ -54,6 +54,7 @@ public abstract class SparkContentFile<F> implements ContentFile<F> {\n   private final int sortOrderIdPosition;\n   private final int fileSpecIdPosition;\n   private final int equalityIdsPosition;\n+  private final int referencedDataFilePosition;\n   private final Type lowerBoundsType;\n   private final Type upperBoundsType;\n   private final Type keyMetadataType;\n@@ -103,6 +104,7 @@ public abstract class SparkContentFile<F> implements ContentFile<F> {\n     this.sortOrderIdPosition = positions.get(DataFile.SORT_ORDER_ID.name());\n     this.fileSpecIdPosition = positions.get(DataFile.SPEC_ID.name());\n     this.equalityIdsPosition = positions.get(DataFile.EQUALITY_IDS.name());\n+    this.referencedDataFilePosition = positions.get(DataFile.REFERENCED_DATA_FILE.name());\n   }\n \n   public F wrap(Row row) {\n@@ -231,6 +233,13 @@ public List<Integer> equalityFieldIds() {\n     return wrapped.isNullAt(equalityIdsPosition) ? null : wrapped.getList(equalityIdsPosition);\n   }\n \n+  public String referencedDataFile() {\n+    if (wrapped.isNullAt(referencedDataFilePosition)) {\n+      return null;\n+    }\n+    return wrapped.getString(referencedDataFilePosition);\n+  }\n+\n   private int fieldPosition(String name, StructType sparkType) {\n     try {\n       return sparkType.fieldIndex(name);\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/FileGenerationUtil.java b/core/src/test/java/org/apache/iceberg/FileGenerationUtil.java\nindex b210cfcd4fa7..4f85151c80da 100644\n--- a/core/src/test/java/org/apache/iceberg/FileGenerationUtil.java\n+++ b/core/src/test/java/org/apache/iceberg/FileGenerationUtil.java\n@@ -136,6 +136,23 @@ public static DeleteFile generatePositionDeleteFile(Table table, DataFile dataFi\n         .build();\n   }\n \n+  public static DeleteFile generatePositionDeleteFileWithRef(Table table, DataFile dataFile) {\n+    PartitionSpec spec = table.specs().get(dataFile.specId());\n+    StructLike partition = dataFile.partition();\n+    LocationProvider locations = table.locationProvider();\n+    String path = locations.newDataLocation(spec, partition, generateFileName());\n+    long fileSize = generateFileSize();\n+    return FileMetadata.deleteFileBuilder(spec)\n+        .ofPositionDeletes()\n+        .withPath(path)\n+        .withPartition(partition)\n+        .withFileSizeInBytes(fileSize)\n+        .withFormat(FileFormat.PARQUET)\n+        .withReferencedDataFile(dataFile.location())\n+        .withRecordCount(3)\n+        .build();\n+  }\n+\n   // mimics the behavior of OutputFileFactory\n   public static String generateFileName() {\n     int partitionId = random().nextInt(100_000);\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\nindex a449de414a10..6cbc53baa349 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/actions/TestRewriteManifestsAction.java\n@@ -41,7 +41,9 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataFiles;\n import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileGenerationUtil;\n import org.apache.iceberg.FileMetadata;\n+import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.Files;\n import org.apache.iceberg.ManifestContent;\n import org.apache.iceberg.ManifestFile;\n@@ -64,6 +66,7 @@\n import org.apache.iceberg.data.Record;\n import org.apache.iceberg.exceptions.CommitStateUnknownException;\n import org.apache.iceberg.hadoop.HadoopTables;\n+import org.apache.iceberg.io.CloseableIterable;\n import org.apache.iceberg.io.OutputFile;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Iterables;\n@@ -128,6 +131,62 @@ public void setupTableLocation() throws Exception {\n     this.tableLocation = tableDir.toURI().toString();\n   }\n \n+  @TestTemplate\n+  public void testRewriteManifestsPreservesOptionalFields() throws IOException {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n+\n+    PartitionSpec spec = PartitionSpec.builderFor(SCHEMA).identity(\"c1\").build();\n+    Map<String, String> options = Maps.newHashMap();\n+    options.put(TableProperties.FORMAT_VERSION, String.valueOf(formatVersion));\n+    Table table = TABLES.create(SCHEMA, spec, options, tableLocation);\n+\n+    DataFile dataFile1 = newDataFile(table, \"c1=0\");\n+    DataFile dataFile2 = newDataFile(table, \"c1=0\");\n+    DataFile dataFile3 = newDataFile(table, \"c1=0\");\n+    table\n+        .newFastAppend()\n+        .appendFile(dataFile1)\n+        .appendFile(dataFile2)\n+        .appendFile(dataFile3)\n+        .commit();\n+\n+    DeleteFile deleteFile1 = newDeleteFileWithRef(table, dataFile1);\n+    assertThat(deleteFile1.referencedDataFile()).isEqualTo(dataFile1.location());\n+    table.newRowDelta().addDeletes(deleteFile1).commit();\n+\n+    DeleteFile deleteFile2 = newDeleteFileWithRef(table, dataFile2);\n+    assertThat(deleteFile2.referencedDataFile()).isEqualTo(dataFile2.location());\n+    table.newRowDelta().addDeletes(deleteFile2).commit();\n+\n+    DeleteFile deleteFile3 = newDeleteFileWithRef(table, dataFile3);\n+    assertThat(deleteFile3.referencedDataFile()).isEqualTo(dataFile3.location());\n+    table.newRowDelta().addDeletes(deleteFile3).commit();\n+\n+    SparkActions actions = SparkActions.get();\n+\n+    actions\n+        .rewriteManifests(table)\n+        .rewriteIf(manifest -> true)\n+        .option(RewriteManifestsSparkAction.USE_CACHING, useCaching)\n+        .execute();\n+\n+    table.refresh();\n+\n+    try (CloseableIterable<FileScanTask> tasks = table.newScan().planFiles()) {\n+      for (FileScanTask fileTask : tasks) {\n+        DataFile dataFile = fileTask.file();\n+        DeleteFile deleteFile = Iterables.getOnlyElement(fileTask.deletes());\n+        if (dataFile.location().equals(dataFile1.location())) {\n+          assertThat(deleteFile.referencedDataFile()).isEqualTo(deleteFile1.referencedDataFile());\n+        } else if (dataFile.location().equals(dataFile2.location())) {\n+          assertThat(deleteFile.referencedDataFile()).isEqualTo(deleteFile2.referencedDataFile());\n+        } else {\n+          assertThat(deleteFile.referencedDataFile()).isEqualTo(deleteFile3.referencedDataFile());\n+        }\n+      }\n+    }\n+  }\n+\n   @TestTemplate\n   public void testRewriteManifestsEmptyTable() throws IOException {\n     PartitionSpec spec = PartitionSpec.unpartitioned();\n@@ -976,6 +1035,10 @@ private DataFiles.Builder newDataFileBuilder(Table table) {\n         .withRecordCount(1);\n   }\n \n+  private DeleteFile newDeleteFileWithRef(Table table, DataFile dataFile) {\n+    return FileGenerationUtil.generatePositionDeleteFileWithRef(table, dataFile);\n+  }\n+\n   private DeleteFile newDeleteFile(Table table, String partitionPath) {\n     return FileMetadata.deleteFileBuilder(table.spec())\n         .ofPositionDeletes()\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11446",
    "pr_id": 11446,
    "issue_id": 11122,
    "repo": "apache/iceberg",
    "problem_statement": "Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 118,
    "test_files_count": 8,
    "non_test_files_count": 28,
    "pr_changed_files": [
      "api/src/main/java/org/apache/iceberg/AddedRowsScanTask.java",
      "api/src/main/java/org/apache/iceberg/DataFile.java",
      "api/src/main/java/org/apache/iceberg/DeleteFile.java",
      "api/src/main/java/org/apache/iceberg/DeletedDataFileScanTask.java",
      "api/src/main/java/org/apache/iceberg/DeletedRowsScanTask.java",
      "api/src/main/java/org/apache/iceberg/FileFormat.java",
      "api/src/main/java/org/apache/iceberg/FileScanTask.java",
      "api/src/main/java/org/apache/iceberg/util/DeleteFileSet.java",
      "api/src/main/java/org/apache/iceberg/util/ScanTaskUtil.java",
      "api/src/test/java/org/apache/iceberg/util/TestScanTaskUtil.java",
      "core/src/main/java/org/apache/iceberg/BaseFile.java",
      "core/src/main/java/org/apache/iceberg/BaseFileScanTask.java",
      "core/src/main/java/org/apache/iceberg/BaseScan.java",
      "core/src/main/java/org/apache/iceberg/ContentFileParser.java",
      "core/src/main/java/org/apache/iceberg/FileMetadata.java",
      "core/src/main/java/org/apache/iceberg/GenericDataFile.java",
      "core/src/main/java/org/apache/iceberg/GenericDeleteFile.java",
      "core/src/main/java/org/apache/iceberg/ScanSummary.java",
      "core/src/main/java/org/apache/iceberg/SnapshotProducer.java",
      "core/src/main/java/org/apache/iceberg/SnapshotSummary.java",
      "core/src/main/java/org/apache/iceberg/V3Metadata.java",
      "core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java",
      "core/src/main/java/org/apache/iceberg/util/TableScanUtil.java",
      "core/src/test/java/org/apache/iceberg/FileGenerationUtil.java",
      "core/src/test/java/org/apache/iceberg/TestBase.java",
      "core/src/test/java/org/apache/iceberg/TestContentFileParser.java",
      "core/src/test/java/org/apache/iceberg/TestManifestEncryption.java",
      "core/src/test/java/org/apache/iceberg/TestManifestReader.java",
      "core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java",
      "core/src/test/java/org/apache/iceberg/util/TestTableScanUtil.java",
      "flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java",
      "flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java",
      "flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java",
      "flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java",
      "flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java",
      "flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java"
    ],
    "pr_changed_test_files": [
      "api/src/test/java/org/apache/iceberg/util/TestScanTaskUtil.java",
      "core/src/test/java/org/apache/iceberg/FileGenerationUtil.java",
      "core/src/test/java/org/apache/iceberg/TestBase.java",
      "core/src/test/java/org/apache/iceberg/TestContentFileParser.java",
      "core/src/test/java/org/apache/iceberg/TestManifestEncryption.java",
      "core/src/test/java/org/apache/iceberg/TestManifestReader.java",
      "core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java",
      "core/src/test/java/org/apache/iceberg/util/TestTableScanUtil.java"
    ],
    "base_commit": "d9b9768766b359adf696f5dc9e321507bd0213d2",
    "head_commit": "a3617beedab69511aca4d342b3c3c9f8372f9a2c",
    "repo_url": "https://github.com/apache/iceberg/pull/11446",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11446",
    "dockerfile": "",
    "pr_merged_at": "2024-11-04T18:35:08.000Z",
    "patch": "diff --git a/api/src/main/java/org/apache/iceberg/AddedRowsScanTask.java b/api/src/main/java/org/apache/iceberg/AddedRowsScanTask.java\nindex d48b268287c3..506e344d3660 100644\n--- a/api/src/main/java/org/apache/iceberg/AddedRowsScanTask.java\n+++ b/api/src/main/java/org/apache/iceberg/AddedRowsScanTask.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg;\n \n import java.util.List;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n /**\n  * A scan task for inserts generated by adding a data file to the table.\n@@ -55,7 +56,7 @@ default ChangelogOperation operation() {\n \n   @Override\n   default long sizeBytes() {\n-    return length() + deletes().stream().mapToLong(ContentFile::fileSizeInBytes).sum();\n+    return length() + ScanTaskUtil.contentSizeInBytes(deletes());\n   }\n \n   @Override\n\ndiff --git a/api/src/main/java/org/apache/iceberg/DataFile.java b/api/src/main/java/org/apache/iceberg/DataFile.java\nindex 3c6d77f34d8f..ea6262afac85 100644\n--- a/api/src/main/java/org/apache/iceberg/DataFile.java\n+++ b/api/src/main/java/org/apache/iceberg/DataFile.java\n@@ -104,12 +104,21 @@ public interface DataFile extends ContentFile<DataFile> {\n           \"referenced_data_file\",\n           StringType.get(),\n           \"Fully qualified location (URI with FS scheme) of a data file that all deletes reference\");\n+  Types.NestedField CONTENT_OFFSET =\n+      optional(\n+          144, \"content_offset\", LongType.get(), \"The offset in the file where the content starts\");\n+  Types.NestedField CONTENT_SIZE =\n+      optional(\n+          145,\n+          \"content_size_in_bytes\",\n+          LongType.get(),\n+          \"The length of referenced content stored in the file\");\n \n   int PARTITION_ID = 102;\n   String PARTITION_NAME = \"partition\";\n   String PARTITION_DOC = \"Partition data tuple, schema based on the partition spec\";\n \n-  // NEXT ID TO ASSIGN: 144\n+  // NEXT ID TO ASSIGN: 146\n \n   static StructType getType(StructType partitionType) {\n     // IDs start at 100 to leave room for changes to ManifestEntry\n@@ -131,7 +140,9 @@ static StructType getType(StructType partitionType) {\n         SPLIT_OFFSETS,\n         EQUALITY_IDS,\n         SORT_ORDER_ID,\n-        REFERENCED_DATA_FILE);\n+        REFERENCED_DATA_FILE,\n+        CONTENT_OFFSET,\n+        CONTENT_SIZE);\n   }\n \n   /**\n\ndiff --git a/api/src/main/java/org/apache/iceberg/DeleteFile.java b/api/src/main/java/org/apache/iceberg/DeleteFile.java\nindex 8e17e60fcccf..340a00e36b17 100644\n--- a/api/src/main/java/org/apache/iceberg/DeleteFile.java\n+++ b/api/src/main/java/org/apache/iceberg/DeleteFile.java\n@@ -42,4 +42,26 @@ default List<Long> splitOffsets() {\n   default String referencedDataFile() {\n     return null;\n   }\n+\n+  /**\n+   * Returns the offset in the file where the content starts.\n+   *\n+   * <p>The content offset is required for deletion vectors and points to the start of the deletion\n+   * vector blob in the Puffin file, enabling direct access. This method always returns null for\n+   * equality and position delete files.\n+   */\n+  default Long contentOffset() {\n+    return null;\n+  }\n+\n+  /**\n+   * Returns the length of referenced content stored in the file.\n+   *\n+   * <p>The content size is required for deletion vectors and indicates the size of the deletion\n+   * vector blob in the Puffin file, enabling direct access. This method always returns null for\n+   * equality and position delete files.\n+   */\n+  default Long contentSizeInBytes() {\n+    return null;\n+  }\n }\n\ndiff --git a/api/src/main/java/org/apache/iceberg/DeletedDataFileScanTask.java b/api/src/main/java/org/apache/iceberg/DeletedDataFileScanTask.java\nindex 9edd6afd0cea..4b9c1704b9d2 100644\n--- a/api/src/main/java/org/apache/iceberg/DeletedDataFileScanTask.java\n+++ b/api/src/main/java/org/apache/iceberg/DeletedDataFileScanTask.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg;\n \n import java.util.List;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n /**\n  * A scan task for deletes generated by removing a data file from the table.\n@@ -54,7 +55,7 @@ default ChangelogOperation operation() {\n \n   @Override\n   default long sizeBytes() {\n-    return length() + existingDeletes().stream().mapToLong(ContentFile::fileSizeInBytes).sum();\n+    return length() + ScanTaskUtil.contentSizeInBytes(existingDeletes());\n   }\n \n   @Override\n\ndiff --git a/api/src/main/java/org/apache/iceberg/DeletedRowsScanTask.java b/api/src/main/java/org/apache/iceberg/DeletedRowsScanTask.java\nindex 131edfddd349..1e0a52a53241 100644\n--- a/api/src/main/java/org/apache/iceberg/DeletedRowsScanTask.java\n+++ b/api/src/main/java/org/apache/iceberg/DeletedRowsScanTask.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg;\n \n import java.util.List;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n /**\n  * A scan task for deletes generated by adding delete files to the table.\n@@ -63,9 +64,9 @@ default ChangelogOperation operation() {\n \n   @Override\n   default long sizeBytes() {\n-    return length()\n-        + addedDeletes().stream().mapToLong(ContentFile::fileSizeInBytes).sum()\n-        + existingDeletes().stream().mapToLong(ContentFile::fileSizeInBytes).sum();\n+    long addedDeletesSize = ScanTaskUtil.contentSizeInBytes(addedDeletes());\n+    long existingDeletesSize = ScanTaskUtil.contentSizeInBytes(existingDeletes());\n+    return length() + addedDeletesSize + existingDeletesSize;\n   }\n \n   @Override\n\ndiff --git a/api/src/main/java/org/apache/iceberg/FileFormat.java b/api/src/main/java/org/apache/iceberg/FileFormat.java\nindex d662437d5ddb..6b41aec42c3e 100644\n--- a/api/src/main/java/org/apache/iceberg/FileFormat.java\n+++ b/api/src/main/java/org/apache/iceberg/FileFormat.java\n@@ -24,6 +24,7 @@\n \n /** Enum of supported file formats. */\n public enum FileFormat {\n+  PUFFIN(\"puffin\", false),\n   ORC(\"orc\", true),\n   PARQUET(\"parquet\", true),\n   AVRO(\"avro\", true),\n\ndiff --git a/api/src/main/java/org/apache/iceberg/FileScanTask.java b/api/src/main/java/org/apache/iceberg/FileScanTask.java\nindex 5fb4b55459e3..94f153e56052 100644\n--- a/api/src/main/java/org/apache/iceberg/FileScanTask.java\n+++ b/api/src/main/java/org/apache/iceberg/FileScanTask.java\n@@ -19,6 +19,7 @@\n package org.apache.iceberg;\n \n import java.util.List;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n /** A scan task over a range of bytes in a single data file. */\n public interface FileScanTask extends ContentScanTask<DataFile>, SplittableScanTask<FileScanTask> {\n@@ -36,7 +37,7 @@ default Schema schema() {\n \n   @Override\n   default long sizeBytes() {\n-    return length() + deletes().stream().mapToLong(ContentFile::fileSizeInBytes).sum();\n+    return length() + ScanTaskUtil.contentSizeInBytes(deletes());\n   }\n \n   @Override\n\ndiff --git a/api/src/main/java/org/apache/iceberg/util/DeleteFileSet.java b/api/src/main/java/org/apache/iceberg/util/DeleteFileSet.java\nindex bbe9824963fc..06ddd1869ace 100644\n--- a/api/src/main/java/org/apache/iceberg/util/DeleteFileSet.java\n+++ b/api/src/main/java/org/apache/iceberg/util/DeleteFileSet.java\n@@ -97,13 +97,14 @@ public boolean equals(Object o) {\n       }\n \n       DeleteFileWrapper that = (DeleteFileWrapper) o;\n-      // this needs to be updated once deletion vector support is added\n-      return Objects.equals(file.location(), that.file.location());\n+      return Objects.equals(file.location(), that.file.location())\n+          && Objects.equals(file.contentOffset(), that.file.contentOffset())\n+          && Objects.equals(file.contentSizeInBytes(), that.file.contentSizeInBytes());\n     }\n \n     @Override\n     public int hashCode() {\n-      return Objects.hashCode(file.location());\n+      return Objects.hash(file.location(), file.contentOffset(), file.contentSizeInBytes());\n     }\n \n     @Override\n\ndiff --git a/api/src/main/java/org/apache/iceberg/util/ScanTaskUtil.java b/api/src/main/java/org/apache/iceberg/util/ScanTaskUtil.java\nnew file mode 100644\nindex 000000000000..276aae6e2caf\n--- /dev/null\n+++ b/api/src/main/java/org/apache/iceberg/util/ScanTaskUtil.java\n@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.util;\n+\n+import org.apache.iceberg.ContentFile;\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.FileFormat;\n+\n+public class ScanTaskUtil {\n+\n+  private ScanTaskUtil() {}\n+\n+  public static long contentSizeInBytes(ContentFile<?> file) {\n+    if (file.content() == FileContent.DATA) {\n+      return file.fileSizeInBytes();\n+    } else {\n+      DeleteFile deleteFile = (DeleteFile) file;\n+      return isDV(deleteFile) ? deleteFile.contentSizeInBytes() : deleteFile.fileSizeInBytes();\n+    }\n+  }\n+\n+  public static long contentSizeInBytes(Iterable<? extends ContentFile<?>> files) {\n+    long size = 0L;\n+    for (ContentFile<?> file : files) {\n+      size += contentSizeInBytes(file);\n+    }\n+    return size;\n+  }\n+\n+  private static boolean isDV(DeleteFile deleteFile) {\n+    return deleteFile.format() == FileFormat.PUFFIN;\n+  }\n+}\n\ndiff --git a/core/src/main/java/org/apache/iceberg/BaseFile.java b/core/src/main/java/org/apache/iceberg/BaseFile.java\nindex f4fd94724e95..e9724637dfa3 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseFile.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseFile.java\n@@ -81,6 +81,8 @@ public PartitionData copy() {\n   private byte[] keyMetadata = null;\n   private Integer sortOrderId;\n   private String referencedDataFile = null;\n+  private Long contentOffset = null;\n+  private Long contentSizeInBytes = null;\n \n   // cached schema\n   private transient Schema avroSchema = null;\n@@ -110,6 +112,8 @@ public PartitionData copy() {\n           DataFile.EQUALITY_IDS,\n           DataFile.SORT_ORDER_ID,\n           DataFile.REFERENCED_DATA_FILE,\n+          DataFile.CONTENT_OFFSET,\n+          DataFile.CONTENT_SIZE,\n           MetadataColumns.ROW_POSITION);\n \n   /** Used by Avro reflection to instantiate this class when reading manifest files. */\n@@ -152,7 +156,9 @@ public PartitionData copy() {\n       int[] equalityFieldIds,\n       Integer sortOrderId,\n       ByteBuffer keyMetadata,\n-      String referencedDataFile) {\n+      String referencedDataFile,\n+      Long contentOffset,\n+      Long contentSizeInBytes) {\n     super(BASE_TYPE.fields().size());\n     this.partitionSpecId = specId;\n     this.content = content;\n@@ -182,6 +188,8 @@ public PartitionData copy() {\n     this.sortOrderId = sortOrderId;\n     this.keyMetadata = ByteBuffers.toByteArray(keyMetadata);\n     this.referencedDataFile = referencedDataFile;\n+    this.contentOffset = contentOffset;\n+    this.contentSizeInBytes = contentSizeInBytes;\n   }\n \n   /**\n@@ -235,6 +243,8 @@ public PartitionData copy() {\n     this.dataSequenceNumber = toCopy.dataSequenceNumber;\n     this.fileSequenceNumber = toCopy.fileSequenceNumber;\n     this.referencedDataFile = toCopy.referencedDataFile;\n+    this.contentOffset = toCopy.contentOffset;\n+    this.contentSizeInBytes = toCopy.contentSizeInBytes;\n   }\n \n   /** Constructor for Java serialization. */\n@@ -347,6 +357,12 @@ protected <T> void internalSet(int pos, T value) {\n         this.referencedDataFile = value != null ? value.toString() : null;\n         return;\n       case 18:\n+        this.contentOffset = (Long) value;\n+        return;\n+      case 19:\n+        this.contentSizeInBytes = (Long) value;\n+        return;\n+      case 20:\n         this.fileOrdinal = (long) value;\n         return;\n       default:\n@@ -398,6 +414,10 @@ private Object getByPos(int basePos) {\n       case 17:\n         return referencedDataFile;\n       case 18:\n+        return contentOffset;\n+      case 19:\n+        return contentSizeInBytes;\n+      case 20:\n         return fileOrdinal;\n       default:\n         throw new UnsupportedOperationException(\"Unknown field ordinal: \" + basePos);\n@@ -528,6 +548,14 @@ public String referencedDataFile() {\n     return referencedDataFile;\n   }\n \n+  public Long contentOffset() {\n+    return contentOffset;\n+  }\n+\n+  public Long contentSizeInBytes() {\n+    return contentSizeInBytes;\n+  }\n+\n   private static <K, V> Map<K, V> copyMap(Map<K, V> map, Set<K> keys) {\n     return keys == null ? SerializableMap.copyOf(map) : SerializableMap.filteredCopyOf(map, keys);\n   }\n@@ -580,6 +608,8 @@ public String toString() {\n         .add(\"data_sequence_number\", dataSequenceNumber == null ? \"null\" : dataSequenceNumber)\n         .add(\"file_sequence_number\", fileSequenceNumber == null ? \"null\" : fileSequenceNumber)\n         .add(\"referenced_data_file\", referencedDataFile == null ? \"null\" : referencedDataFile)\n+        .add(\"content_offset\", contentOffset == null ? \"null\" : contentOffset)\n+        .add(\"content_size_in_bytes\", contentSizeInBytes == null ? \"null\" : contentSizeInBytes)\n         .toString();\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/BaseFileScanTask.java b/core/src/main/java/org/apache/iceberg/BaseFileScanTask.java\nindex 2469395021d4..aa37f40be7c0 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseFileScanTask.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseFileScanTask.java\n@@ -23,6 +23,7 @@\n import org.apache.iceberg.expressions.ResidualEvaluator;\n import org.apache.iceberg.relocated.com.google.common.annotations.VisibleForTesting;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n public class BaseFileScanTask extends BaseContentScanTask<FileScanTask, DataFile>\n     implements FileScanTask {\n@@ -79,7 +80,7 @@ private long deletesSizeBytes() {\n     if (deletesSizeBytes == 0L && deletes.length > 0) {\n       long size = 0L;\n       for (DeleteFile deleteFile : deletes) {\n-        size += deleteFile.fileSizeInBytes();\n+        size += ScanTaskUtil.contentSizeInBytes(deleteFile);\n       }\n       this.deletesSizeBytes = size;\n     }\n@@ -180,11 +181,7 @@ public SplitScanTask merge(ScanTask other) {\n \n     private long deletesSizeBytes() {\n       if (deletesSizeBytes == 0L && fileScanTask.filesCount() > 1) {\n-        long size = 0L;\n-        for (DeleteFile deleteFile : fileScanTask.deletes()) {\n-          size += deleteFile.fileSizeInBytes();\n-        }\n-        this.deletesSizeBytes = size;\n+        this.deletesSizeBytes = ScanTaskUtil.contentSizeInBytes(fileScanTask.deletes());\n       }\n \n       return deletesSizeBytes;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/BaseScan.java b/core/src/main/java/org/apache/iceberg/BaseScan.java\nindex a011d03d59ad..618b2e95f29f 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseScan.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseScan.java\n@@ -78,6 +78,8 @@ abstract class BaseScan<ThisT, T extends ScanTask, G extends ScanTaskGroup<T>>\n           \"key_metadata\",\n           \"split_offsets\",\n           \"referenced_data_file\",\n+          \"content_offset\",\n+          \"content_size_in_bytes\",\n           \"equality_ids\");\n \n   protected static final List<String> DELETE_SCAN_WITH_STATS_COLUMNS =\n\ndiff --git a/core/src/main/java/org/apache/iceberg/ContentFileParser.java b/core/src/main/java/org/apache/iceberg/ContentFileParser.java\nindex 96dfa5586c31..e6d7c8043f3f 100644\n--- a/core/src/main/java/org/apache/iceberg/ContentFileParser.java\n+++ b/core/src/main/java/org/apache/iceberg/ContentFileParser.java\n@@ -46,6 +46,8 @@ class ContentFileParser {\n   private static final String EQUALITY_IDS = \"equality-ids\";\n   private static final String SORT_ORDER_ID = \"sort-order-id\";\n   private static final String REFERENCED_DATA_FILE = \"referenced-data-file\";\n+  private static final String CONTENT_OFFSET = \"content-offset\";\n+  private static final String CONTENT_SIZE = \"content-size-in-bytes\";\n \n   private ContentFileParser() {}\n \n@@ -116,6 +118,14 @@ static void toJson(ContentFile<?> contentFile, PartitionSpec spec, JsonGenerator\n       if (deleteFile.referencedDataFile() != null) {\n         generator.writeStringField(REFERENCED_DATA_FILE, deleteFile.referencedDataFile());\n       }\n+\n+      if (deleteFile.contentOffset() != null) {\n+        generator.writeNumberField(CONTENT_OFFSET, deleteFile.contentOffset());\n+      }\n+\n+      if (deleteFile.contentSizeInBytes() != null) {\n+        generator.writeNumberField(CONTENT_SIZE, deleteFile.contentSizeInBytes());\n+      }\n     }\n \n     generator.writeEndObject();\n@@ -155,6 +165,8 @@ static ContentFile<?> fromJson(JsonNode jsonNode, PartitionSpec spec) {\n     int[] equalityFieldIds = JsonUtil.getIntArrayOrNull(EQUALITY_IDS, jsonNode);\n     Integer sortOrderId = JsonUtil.getIntOrNull(SORT_ORDER_ID, jsonNode);\n     String referencedDataFile = JsonUtil.getStringOrNull(REFERENCED_DATA_FILE, jsonNode);\n+    Long contentOffset = JsonUtil.getLongOrNull(CONTENT_OFFSET, jsonNode);\n+    Long contentSizeInBytes = JsonUtil.getLongOrNull(CONTENT_SIZE, jsonNode);\n \n     if (fileContent == FileContent.DATA) {\n       return new GenericDataFile(\n@@ -180,7 +192,9 @@ static ContentFile<?> fromJson(JsonNode jsonNode, PartitionSpec spec) {\n           sortOrderId,\n           splitOffsets,\n           keyMetadata,\n-          referencedDataFile);\n+          referencedDataFile,\n+          contentOffset,\n+          contentSizeInBytes);\n     }\n   }\n \n\ndiff --git a/core/src/main/java/org/apache/iceberg/FileMetadata.java b/core/src/main/java/org/apache/iceberg/FileMetadata.java\nindex ef229593bcab..7bb8d886dd16 100644\n--- a/core/src/main/java/org/apache/iceberg/FileMetadata.java\n+++ b/core/src/main/java/org/apache/iceberg/FileMetadata.java\n@@ -60,6 +60,8 @@ public static class Builder {\n     private Integer sortOrderId = null;\n     private List<Long> splitOffsets = null;\n     private String referencedDataFile = null;\n+    private Long contentOffset = null;\n+    private Long contentSizeInBytes = null;\n \n     Builder(PartitionSpec spec) {\n       this.spec = spec;\n@@ -230,6 +232,16 @@ public Builder withReferencedDataFile(CharSequence newReferencedDataFile) {\n       return this;\n     }\n \n+    public Builder withContentOffset(long newContentOffset) {\n+      this.contentOffset = newContentOffset;\n+      return this;\n+    }\n+\n+    public Builder withContentSizeInBytes(long newContentSizeInBytes) {\n+      this.contentSizeInBytes = newContentSizeInBytes;\n+      return this;\n+    }\n+\n     public DeleteFile build() {\n       Preconditions.checkArgument(filePath != null, \"File path is required\");\n       if (format == null) {\n@@ -240,6 +252,15 @@ public DeleteFile build() {\n       Preconditions.checkArgument(fileSizeInBytes >= 0, \"File size is required\");\n       Preconditions.checkArgument(recordCount >= 0, \"Record count is required\");\n \n+      if (format == FileFormat.PUFFIN) {\n+        Preconditions.checkArgument(contentOffset != null, \"Content offset is required for DV\");\n+        Preconditions.checkArgument(contentSizeInBytes != null, \"Content size is required for DV\");\n+      } else {\n+        Preconditions.checkArgument(contentOffset == null, \"Content offset can only be set for DV\");\n+        Preconditions.checkArgument(\n+            contentSizeInBytes == null, \"Content size can only be set for DV\");\n+      }\n+\n       switch (content) {\n         case POSITION_DELETES:\n           Preconditions.checkArgument(\n@@ -273,7 +294,9 @@ public DeleteFile build() {\n           sortOrderId,\n           splitOffsets,\n           keyMetadata,\n-          referencedDataFile);\n+          referencedDataFile,\n+          contentOffset,\n+          contentSizeInBytes);\n     }\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/GenericDataFile.java b/core/src/main/java/org/apache/iceberg/GenericDataFile.java\nindex aa34cd22cdaa..a61cc1e0fb72 100644\n--- a/core/src/main/java/org/apache/iceberg/GenericDataFile.java\n+++ b/core/src/main/java/org/apache/iceberg/GenericDataFile.java\n@@ -65,7 +65,9 @@ class GenericDataFile extends BaseFile<DataFile> implements DataFile {\n         null /* no equality field IDs */,\n         sortOrderId,\n         keyMetadata,\n-        null /* no referenced data file */);\n+        null /* no referenced data file */,\n+        null /* no content offset */,\n+        null /* no content size */);\n   }\n \n   /**\n\ndiff --git a/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java b/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java\nindex 05eb7c97dbab..9205551f24b3 100644\n--- a/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java\n+++ b/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java\n@@ -49,7 +49,9 @@ class GenericDeleteFile extends BaseFile<DeleteFile> implements DeleteFile {\n       Integer sortOrderId,\n       List<Long> splitOffsets,\n       ByteBuffer keyMetadata,\n-      String referencedDataFile) {\n+      String referencedDataFile,\n+      Long contentOffset,\n+      Long contentSizeInBytes) {\n     super(\n         specId,\n         content,\n@@ -68,7 +70,9 @@ class GenericDeleteFile extends BaseFile<DeleteFile> implements DeleteFile {\n         equalityFieldIds,\n         sortOrderId,\n         keyMetadata,\n-        referencedDataFile);\n+        referencedDataFile,\n+        contentOffset,\n+        contentSizeInBytes);\n   }\n \n   /**\n\ndiff --git a/core/src/main/java/org/apache/iceberg/ScanSummary.java b/core/src/main/java/org/apache/iceberg/ScanSummary.java\nindex 1ea171c5b2c3..5f8e66c0b450 100644\n--- a/core/src/main/java/org/apache/iceberg/ScanSummary.java\n+++ b/core/src/main/java/org/apache/iceberg/ScanSummary.java\n@@ -47,6 +47,7 @@\n import org.apache.iceberg.types.Types;\n import org.apache.iceberg.util.DateTimeUtil;\n import org.apache.iceberg.util.Pair;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n public class ScanSummary {\n   private ScanSummary() {}\n@@ -294,7 +295,7 @@ PartitionMetrics updateFromCounts(\n     private PartitionMetrics updateFromFile(ContentFile<?> file, Long timestampMillis) {\n       this.fileCount += 1;\n       this.recordCount += file.recordCount();\n-      this.totalSize += file.fileSizeInBytes();\n+      this.totalSize += ScanTaskUtil.contentSizeInBytes(file);\n       if (timestampMillis != null\n           && (dataTimestampMillis == null || dataTimestampMillis < timestampMillis)) {\n         this.dataTimestampMillis = timestampMillis;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/SnapshotProducer.java b/core/src/main/java/org/apache/iceberg/SnapshotProducer.java\nindex daf1c3d72b89..45b71d654344 100644\n--- a/core/src/main/java/org/apache/iceberg/SnapshotProducer.java\n+++ b/core/src/main/java/org/apache/iceberg/SnapshotProducer.java\n@@ -928,5 +928,15 @@ public Integer sortOrderId() {\n     public String referencedDataFile() {\n       return deleteFile.referencedDataFile();\n     }\n+\n+    @Override\n+    public Long contentOffset() {\n+      return deleteFile.contentOffset();\n+    }\n+\n+    @Override\n+    public Long contentSizeInBytes() {\n+      return deleteFile.contentSizeInBytes();\n+    }\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/SnapshotSummary.java b/core/src/main/java/org/apache/iceberg/SnapshotSummary.java\nindex 22c9df2a8eaf..ad832a5e78e2 100644\n--- a/core/src/main/java/org/apache/iceberg/SnapshotSummary.java\n+++ b/core/src/main/java/org/apache/iceberg/SnapshotSummary.java\n@@ -25,6 +25,7 @@\n import org.apache.iceberg.relocated.com.google.common.base.Strings;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n public class SnapshotSummary {\n   public static final String ADDED_FILES_PROP = \"added-data-files\";\n@@ -275,7 +276,7 @@ void addTo(ImmutableMap.Builder<String, String> builder) {\n     }\n \n     void addedFile(ContentFile<?> file) {\n-      this.addedSize += file.fileSizeInBytes();\n+      this.addedSize += ScanTaskUtil.contentSizeInBytes(file);\n       switch (file.content()) {\n         case DATA:\n           this.addedFiles += 1;\n@@ -298,7 +299,7 @@ void addedFile(ContentFile<?> file) {\n     }\n \n     void removedFile(ContentFile<?> file) {\n-      this.removedSize += file.fileSizeInBytes();\n+      this.removedSize += ScanTaskUtil.contentSizeInBytes(file);\n       switch (file.content()) {\n         case DATA:\n           this.removedFiles += 1;\n\ndiff --git a/core/src/main/java/org/apache/iceberg/V3Metadata.java b/core/src/main/java/org/apache/iceberg/V3Metadata.java\nindex a418a868564e..70461ac74a70 100644\n--- a/core/src/main/java/org/apache/iceberg/V3Metadata.java\n+++ b/core/src/main/java/org/apache/iceberg/V3Metadata.java\n@@ -275,7 +275,9 @@ static Types.StructType fileType(Types.StructType partitionType) {\n         DataFile.SPLIT_OFFSETS,\n         DataFile.EQUALITY_IDS,\n         DataFile.SORT_ORDER_ID,\n-        DataFile.REFERENCED_DATA_FILE);\n+        DataFile.REFERENCED_DATA_FILE,\n+        DataFile.CONTENT_OFFSET,\n+        DataFile.CONTENT_SIZE);\n   }\n \n   static class IndexedManifestEntry<F extends ContentFile<F>>\n@@ -455,6 +457,18 @@ public Object get(int pos) {\n           } else {\n             return null;\n           }\n+        case 17:\n+          if (wrapped.content() == FileContent.POSITION_DELETES) {\n+            return ((DeleteFile) wrapped).contentOffset();\n+          } else {\n+            return null;\n+          }\n+        case 18:\n+          if (wrapped.content() == FileContent.POSITION_DELETES) {\n+            return ((DeleteFile) wrapped).contentSizeInBytes();\n+          } else {\n+            return null;\n+          }\n       }\n       throw new IllegalArgumentException(\"Unknown field ordinal: \" + pos);\n     }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java\nindex c5aa6e1dd673..1ba891f58474 100644\n--- a/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/metrics/ScanMetricsUtil.java\n@@ -21,6 +21,7 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DeleteFile;\n import org.apache.iceberg.FileContent;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n public class ScanMetricsUtil {\n \n@@ -43,7 +44,7 @@ public static void fileTask(ScanMetrics metrics, DataFile dataFile, DeleteFile[]\n \n     long deletesSizeInBytes = 0L;\n     for (DeleteFile deleteFile : deleteFiles) {\n-      deletesSizeInBytes += deleteFile.fileSizeInBytes();\n+      deletesSizeInBytes += ScanTaskUtil.contentSizeInBytes(deleteFile);\n     }\n \n     metrics.totalDeleteFileSizeInBytes().increment(deletesSizeInBytes);\n\ndiff --git a/core/src/main/java/org/apache/iceberg/util/TableScanUtil.java b/core/src/main/java/org/apache/iceberg/util/TableScanUtil.java\nindex e2dbcb61e9b7..2d80e88ae328 100644\n--- a/core/src/main/java/org/apache/iceberg/util/TableScanUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/util/TableScanUtil.java\n@@ -25,7 +25,6 @@\n import org.apache.iceberg.BaseCombinedScanTask;\n import org.apache.iceberg.BaseScanTaskGroup;\n import org.apache.iceberg.CombinedScanTask;\n-import org.apache.iceberg.ContentFile;\n import org.apache.iceberg.FileContent;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.MergeableScanTask;\n@@ -92,8 +91,7 @@ public static CloseableIterable<CombinedScanTask> planTasks(\n     Function<FileScanTask, Long> weightFunc =\n         file ->\n             Math.max(\n-                file.length()\n-                    + file.deletes().stream().mapToLong(ContentFile::fileSizeInBytes).sum(),\n+                file.length() + ScanTaskUtil.contentSizeInBytes(file.deletes()),\n                 (1 + file.deletes().size()) * openFileCost);\n \n     return CloseableIterable.transform(\n\ndiff --git a/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java b/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\nindex 9a2f57181708..2109c91bddf7 100644\n--- a/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\n+++ b/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\n@@ -23,6 +23,7 @@\n import java.util.concurrent.atomic.AtomicLong;\n import org.apache.iceberg.io.WriteResult;\n import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n class CommitSummary {\n \n@@ -50,7 +51,8 @@ class CommitSummary {\n                   .forEach(\n                       deleteFile -> {\n                         deleteFilesRecordCount.addAndGet(deleteFile.recordCount());\n-                        deleteFilesByteCount.addAndGet(deleteFile.fileSizeInBytes());\n+                        long deleteBytes = ScanTaskUtil.contentSizeInBytes(deleteFile);\n+                        deleteFilesByteCount.addAndGet(deleteBytes);\n                       });\n             });\n   }\n\ndiff --git a/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java b/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\nindex ce2a6c583fdf..ab458ad2e7cb 100644\n--- a/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\n+++ b/flink/v1.18/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\n@@ -26,6 +26,7 @@\n import org.apache.flink.metrics.Histogram;\n import org.apache.flink.metrics.MetricGroup;\n import org.apache.iceberg.io.WriteResult;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n class IcebergStreamWriterMetrics {\n   // 1,024 reservoir size should cost about 8KB, which is quite small.\n@@ -79,7 +80,7 @@ void updateFlushResult(WriteResult result) {\n     Arrays.stream(result.deleteFiles())\n         .forEach(\n             deleteFile -> {\n-              deleteFilesSizeHistogram.update(deleteFile.fileSizeInBytes());\n+              deleteFilesSizeHistogram.update(ScanTaskUtil.contentSizeInBytes(deleteFile));\n             });\n   }\n \n\ndiff --git a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\nindex 9a2f57181708..2109c91bddf7 100644\n--- a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\n+++ b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\n@@ -23,6 +23,7 @@\n import java.util.concurrent.atomic.AtomicLong;\n import org.apache.iceberg.io.WriteResult;\n import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n class CommitSummary {\n \n@@ -50,7 +51,8 @@ class CommitSummary {\n                   .forEach(\n                       deleteFile -> {\n                         deleteFilesRecordCount.addAndGet(deleteFile.recordCount());\n-                        deleteFilesByteCount.addAndGet(deleteFile.fileSizeInBytes());\n+                        long deleteBytes = ScanTaskUtil.contentSizeInBytes(deleteFile);\n+                        deleteFilesByteCount.addAndGet(deleteBytes);\n                       });\n             });\n   }\n\ndiff --git a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\nindex ce2a6c583fdf..ab458ad2e7cb 100644\n--- a/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\n+++ b/flink/v1.19/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\n@@ -26,6 +26,7 @@\n import org.apache.flink.metrics.Histogram;\n import org.apache.flink.metrics.MetricGroup;\n import org.apache.iceberg.io.WriteResult;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n class IcebergStreamWriterMetrics {\n   // 1,024 reservoir size should cost about 8KB, which is quite small.\n@@ -79,7 +80,7 @@ void updateFlushResult(WriteResult result) {\n     Arrays.stream(result.deleteFiles())\n         .forEach(\n             deleteFile -> {\n-              deleteFilesSizeHistogram.update(deleteFile.fileSizeInBytes());\n+              deleteFilesSizeHistogram.update(ScanTaskUtil.contentSizeInBytes(deleteFile));\n             });\n   }\n \n\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\nindex 9a2f57181708..2109c91bddf7 100644\n--- a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/CommitSummary.java\n@@ -23,6 +23,7 @@\n import java.util.concurrent.atomic.AtomicLong;\n import org.apache.iceberg.io.WriteResult;\n import org.apache.iceberg.relocated.com.google.common.base.MoreObjects;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n class CommitSummary {\n \n@@ -50,7 +51,8 @@ class CommitSummary {\n                   .forEach(\n                       deleteFile -> {\n                         deleteFilesRecordCount.addAndGet(deleteFile.recordCount());\n-                        deleteFilesByteCount.addAndGet(deleteFile.fileSizeInBytes());\n+                        long deleteBytes = ScanTaskUtil.contentSizeInBytes(deleteFile);\n+                        deleteFilesByteCount.addAndGet(deleteBytes);\n                       });\n             });\n   }\n\ndiff --git a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\nindex ce2a6c583fdf..ab458ad2e7cb 100644\n--- a/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\n+++ b/flink/v1.20/flink/src/main/java/org/apache/iceberg/flink/sink/IcebergStreamWriterMetrics.java\n@@ -26,6 +26,7 @@\n import org.apache.flink.metrics.Histogram;\n import org.apache.flink.metrics.MetricGroup;\n import org.apache.iceberg.io.WriteResult;\n+import org.apache.iceberg.util.ScanTaskUtil;\n \n class IcebergStreamWriterMetrics {\n   // 1,024 reservoir size should cost about 8KB, which is quite small.\n@@ -79,7 +80,7 @@ void updateFlushResult(WriteResult result) {\n     Arrays.stream(result.deleteFiles())\n         .forEach(\n             deleteFile -> {\n-              deleteFilesSizeHistogram.update(deleteFile.fileSizeInBytes());\n+              deleteFilesSizeHistogram.update(ScanTaskUtil.contentSizeInBytes(deleteFile));\n             });\n   }\n \n",
    "test_patch": "diff --git a/api/src/test/java/org/apache/iceberg/util/TestScanTaskUtil.java b/api/src/test/java/org/apache/iceberg/util/TestScanTaskUtil.java\nnew file mode 100644\nindex 000000000000..a449cf20a65b\n--- /dev/null\n+++ b/api/src/test/java/org/apache/iceberg/util/TestScanTaskUtil.java\n@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *   http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing,\n+ * software distributed under the License is distributed on an\n+ * \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+ * KIND, either express or implied.  See the License for the\n+ * specific language governing permissions and limitations\n+ * under the License.\n+ */\n+package org.apache.iceberg.util;\n+\n+import static org.assertj.core.api.Assertions.assertThat;\n+\n+import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileFormat;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n+import org.junit.jupiter.api.Test;\n+import org.mockito.Mockito;\n+\n+public class TestScanTaskUtil {\n+\n+  @Test\n+  public void testContentSize() {\n+    DeleteFile dv1 = mockDV(\"dv1.puffin\", 20L, 25L, \"data1.parquet\");\n+    DeleteFile dv2 = mockDV(\"dv2.puffin\", 4L, 15L, \"data2.parquet\");\n+\n+    long size1 = ScanTaskUtil.contentSizeInBytes(ImmutableList.of());\n+    assertThat(size1).isEqualTo(0);\n+\n+    long size2 = ScanTaskUtil.contentSizeInBytes(ImmutableList.of(dv1));\n+    assertThat(size2).isEqualTo(25L);\n+\n+    long size3 = ScanTaskUtil.contentSizeInBytes(ImmutableList.of(dv1, dv2));\n+    assertThat(size3).isEqualTo(40L);\n+  }\n+\n+  private static DeleteFile mockDV(\n+      String location, long contentOffset, long contentSize, String referencedDataFile) {\n+    DeleteFile mockFile = Mockito.mock(DeleteFile.class);\n+    Mockito.when(mockFile.format()).thenReturn(FileFormat.PUFFIN);\n+    Mockito.when(mockFile.location()).thenReturn(location);\n+    Mockito.when(mockFile.contentOffset()).thenReturn(contentOffset);\n+    Mockito.when(mockFile.contentSizeInBytes()).thenReturn(contentSize);\n+    Mockito.when(mockFile.referencedDataFile()).thenReturn(referencedDataFile);\n+    return mockFile;\n+  }\n+}\n\ndiff --git a/core/src/test/java/org/apache/iceberg/FileGenerationUtil.java b/core/src/test/java/org/apache/iceberg/FileGenerationUtil.java\nindex f66496ae6624..e1c8ce9ccfed 100644\n--- a/core/src/test/java/org/apache/iceberg/FileGenerationUtil.java\n+++ b/core/src/test/java/org/apache/iceberg/FileGenerationUtil.java\n@@ -101,6 +101,24 @@ public static DeleteFile generateEqualityDeleteFile(Table table, StructLike part\n         .build();\n   }\n \n+  public static DeleteFile generateDV(Table table, DataFile dataFile) {\n+    PartitionSpec spec = table.specs().get(dataFile.specId());\n+    long fileSize = generateFileSize();\n+    long cardinality = generateRowCount();\n+    long offset = generateContentOffset();\n+    long length = generateContentLength();\n+    return FileMetadata.deleteFileBuilder(spec)\n+        .ofPositionDeletes()\n+        .withPath(\"/path/to/delete-\" + UUID.randomUUID() + \".puffin\")\n+        .withFileSizeInBytes(fileSize)\n+        .withPartition(dataFile.partition())\n+        .withRecordCount(cardinality)\n+        .withReferencedDataFile(dataFile.location())\n+        .withContentOffset(offset)\n+        .withContentSizeInBytes(length)\n+        .build();\n+  }\n+\n   public static DeleteFile generatePositionDeleteFile(Table table, DataFile dataFile) {\n     PartitionSpec spec = table.spec();\n     StructLike partition = dataFile.partition();\n@@ -229,6 +247,14 @@ private static long generateFileSize() {\n     return random().nextInt(50_000);\n   }\n \n+  private static long generateContentOffset() {\n+    return random().nextInt(1_000_000);\n+  }\n+\n+  private static long generateContentLength() {\n+    return random().nextInt(10_000);\n+  }\n+\n   private static Pair<ByteBuffer, ByteBuffer> generateBounds(PrimitiveType type, MetricsMode mode) {\n     Comparator<Object> cmp = Comparators.forType(type);\n     Object value1 = generateBound(type, mode);\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestBase.java b/core/src/test/java/org/apache/iceberg/TestBase.java\nindex 45441631900c..9813d02910a6 100644\n--- a/core/src/test/java/org/apache/iceberg/TestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/TestBase.java\n@@ -666,6 +666,10 @@ protected DeleteFile newDeleteFileWithRef(DataFile dataFile) {\n         .build();\n   }\n \n+  protected DeleteFile newDV(DataFile dataFile) {\n+    return FileGenerationUtil.generateDV(table, dataFile);\n+  }\n+\n   protected DeleteFile newEqualityDeleteFile(int specId, String partitionPath, int... fieldIds) {\n     PartitionSpec spec = table.specs().get(specId);\n     return FileMetadata.deleteFileBuilder(spec)\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestContentFileParser.java b/core/src/test/java/org/apache/iceberg/TestContentFileParser.java\nindex fbe473931659..0c98e8448745 100644\n--- a/core/src/test/java/org/apache/iceberg/TestContentFileParser.java\n+++ b/core/src/test/java/org/apache/iceberg/TestContentFileParser.java\n@@ -198,6 +198,7 @@ private static DataFile dataFileWithAllOptional(PartitionSpec spec) {\n \n   private static Stream<Arguments> provideSpecAndDeleteFile() {\n     return Stream.of(\n+        Arguments.of(TestBase.SPEC, dv(TestBase.SPEC), dvJson()),\n         Arguments.of(\n             PartitionSpec.unpartitioned(),\n             deleteFileWithRequiredOnly(PartitionSpec.unpartitioned()),\n@@ -233,7 +234,9 @@ private static DeleteFile deleteFileWithDataRef(PartitionSpec spec) {\n         null,\n         null,\n         null,\n-        \"/path/to/data/file.parquet\");\n+        \"/path/to/data/file.parquet\",\n+        null,\n+        null);\n   }\n \n   private static String deleteFileWithDataRefJson() {\n@@ -242,6 +245,32 @@ private static String deleteFileWithDataRefJson() {\n         + \"\\\"record-count\\\":10,\\\"referenced-data-file\\\":\\\"/path/to/data/file.parquet\\\"}\";\n   }\n \n+  private static DeleteFile dv(PartitionSpec spec) {\n+    PartitionData partitionData = new PartitionData(spec.partitionType());\n+    partitionData.set(0, 4);\n+    return new GenericDeleteFile(\n+        spec.specId(),\n+        FileContent.POSITION_DELETES,\n+        \"/path/to/delete.puffin\",\n+        FileFormat.PUFFIN,\n+        partitionData,\n+        1234,\n+        new Metrics(10L, null, null, null, null),\n+        null,\n+        null,\n+        null,\n+        null,\n+        \"/path/to/data/file.parquet\",\n+        4L,\n+        40L);\n+  }\n+\n+  private static String dvJson() {\n+    return \"{\\\"spec-id\\\":0,\\\"content\\\":\\\"POSITION_DELETES\\\",\\\"file-path\\\":\\\"/path/to/delete.puffin\\\",\"\n+        + \"\\\"file-format\\\":\\\"PUFFIN\\\",\\\"partition\\\":{\\\"1000\\\":4},\\\"file-size-in-bytes\\\":1234,\\\"record-count\\\":10,\"\n+        + \"\\\"referenced-data-file\\\":\\\"/path/to/data/file.parquet\\\",\\\"content-offset\\\":4,\\\"content-size-in-bytes\\\":40}\";\n+  }\n+\n   private static DeleteFile deleteFileWithRequiredOnly(PartitionSpec spec) {\n     PartitionData partitionData = null;\n     if (spec.isPartitioned()) {\n@@ -261,6 +290,8 @@ private static DeleteFile deleteFileWithRequiredOnly(PartitionSpec spec) {\n         null,\n         null,\n         null,\n+        null,\n+        null,\n         null);\n   }\n \n@@ -301,6 +332,8 @@ private static DeleteFile deleteFileWithAllOptional(PartitionSpec spec) {\n         1,\n         Collections.singletonList(128L),\n         ByteBuffer.wrap(new byte[16]),\n+        null,\n+        null,\n         null);\n   }\n \n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java b/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java\nindex 1f29c0e5b85c..01d38dc129c9 100644\n--- a/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java\n+++ b/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java\n@@ -111,6 +111,8 @@ public class TestManifestEncryption {\n           SORT_ORDER_ID,\n           null,\n           CONTENT_KEY_METADATA,\n+          null,\n+          null,\n           null);\n \n   private static final EncryptionManager ENCRYPTION_MANAGER =\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestManifestReader.java b/core/src/test/java/org/apache/iceberg/TestManifestReader.java\nindex 4652da943003..63c6779298e0 100644\n--- a/core/src/test/java/org/apache/iceberg/TestManifestReader.java\n+++ b/core/src/test/java/org/apache/iceberg/TestManifestReader.java\n@@ -130,7 +130,7 @@ public void testDataFilePositions() throws IOException {\n       long expectedPos = 0L;\n       for (DataFile file : reader) {\n         assertThat(file.pos()).as(\"Position should match\").isEqualTo(expectedPos);\n-        assertThat(((BaseFile) file).get(18))\n+        assertThat(((BaseFile) file).get(20))\n             .as(\"Position from field index should match\")\n             .isEqualTo(expectedPos);\n         expectedPos += 1;\n@@ -158,7 +158,7 @@ public void testDeleteFilePositions() throws IOException {\n       long expectedPos = 0L;\n       for (DeleteFile file : reader) {\n         assertThat(file.pos()).as(\"Position should match\").isEqualTo(expectedPos);\n-        assertThat(((BaseFile) file).get(18))\n+        assertThat(((BaseFile) file).get(20))\n             .as(\"Position from field index should match\")\n             .isEqualTo(expectedPos);\n         expectedPos += 1;\n@@ -199,6 +199,30 @@ public void testDeleteFilesWithReferences() throws IOException {\n     }\n   }\n \n+  @TestTemplate\n+  public void testDVs() throws IOException {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(3);\n+    DeleteFile dv1 = newDV(FILE_A);\n+    DeleteFile dv2 = newDV(FILE_B);\n+    ManifestFile manifest = writeDeleteManifest(formatVersion, 1000L, dv1, dv2);\n+    try (ManifestReader<DeleteFile> reader =\n+        ManifestFiles.readDeleteManifest(manifest, FILE_IO, table.specs())) {\n+      for (DeleteFile dv : reader) {\n+        if (dv.location().equals(dv1.location())) {\n+          assertThat(dv.location()).isEqualTo(dv1.location());\n+          assertThat(dv.referencedDataFile()).isEqualTo(FILE_A.location());\n+          assertThat(dv.contentOffset()).isEqualTo(dv1.contentOffset());\n+          assertThat(dv.contentSizeInBytes()).isEqualTo(dv1.contentSizeInBytes());\n+        } else {\n+          assertThat(dv.location()).isEqualTo(dv2.location());\n+          assertThat(dv.referencedDataFile()).isEqualTo(FILE_B.location());\n+          assertThat(dv.contentOffset()).isEqualTo(dv2.contentOffset());\n+          assertThat(dv.contentSizeInBytes()).isEqualTo(dv2.contentSizeInBytes());\n+        }\n+      }\n+    }\n+  }\n+\n   @TestTemplate\n   public void testDataFileSplitOffsetsNullWhenInvalid() throws IOException {\n     DataFile invalidOffset =\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java b/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java\nindex 88dcc6ff9ca4..9abe7c426f32 100644\n--- a/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java\n+++ b/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java\n@@ -97,6 +97,8 @@ public class TestManifestWriterVersions {\n           SORT_ORDER_ID,\n           null,\n           null,\n+          null,\n+          null,\n           null);\n \n   @TempDir private Path temp;\n\ndiff --git a/core/src/test/java/org/apache/iceberg/util/TestTableScanUtil.java b/core/src/test/java/org/apache/iceberg/util/TestTableScanUtil.java\nindex eb713a4d2e0b..8f8343733525 100644\n--- a/core/src/test/java/org/apache/iceberg/util/TestTableScanUtil.java\n+++ b/core/src/test/java/org/apache/iceberg/util/TestTableScanUtil.java\n@@ -31,6 +31,7 @@\n import org.apache.iceberg.DataFile;\n import org.apache.iceberg.DataFiles;\n import org.apache.iceberg.DeleteFile;\n+import org.apache.iceberg.FileFormat;\n import org.apache.iceberg.FileScanTask;\n import org.apache.iceberg.MergeableScanTask;\n import org.apache.iceberg.MockFileScanTask;\n@@ -74,6 +75,13 @@ private DataFile dataFileWithSize(long size) {\n     return mockFile;\n   }\n \n+  private DeleteFile dvWithSize(long size) {\n+    DeleteFile mockDeleteFile = Mockito.mock(DeleteFile.class);\n+    Mockito.when(mockDeleteFile.format()).thenReturn(FileFormat.PUFFIN);\n+    Mockito.when(mockDeleteFile.contentSizeInBytes()).thenReturn(size);\n+    return mockDeleteFile;\n+  }\n+\n   private DeleteFile[] deleteFilesWithSizes(long... sizes) {\n     return Arrays.stream(sizes)\n         .mapToObj(\n@@ -85,6 +93,14 @@ private DeleteFile[] deleteFilesWithSizes(long... sizes) {\n         .toArray(DeleteFile[]::new);\n   }\n \n+  @Test\n+  public void testFileScanTaskSizeEstimation() {\n+    DataFile dataFile = dataFileWithSize(100L);\n+    DeleteFile dv = dvWithSize(20L);\n+    MockFileScanTask task = new MockFileScanTask(dataFile, new DeleteFile[] {dv});\n+    assertThat(task.sizeBytes()).isEqualTo(120L);\n+  }\n+\n   @Test\n   public void testPlanTaskWithDeleteFiles() {\n     List<FileScanTask> testFiles =\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11443",
    "pr_id": 11443,
    "issue_id": 11122,
    "repo": "apache/iceberg",
    "problem_statement": "Improve Position Deletes in V3\n### Proposed Change\n\nThis proposal aims to enhance the handling of position deletes in Iceberg. It builds on lessons learned from deploying the current approach at scale and addresses all unresolved questions from past community discussions and proposals. This effort primarily targets the V3 spec but some parts can be completed independently.\r\n\r\nShortcomings of position deletes we observe today:\r\n- Choosing between fewer delete files on disk (partition granularity) or targeted deletes (file granularity).\r\n- Dependence on external maintenance for consistent write and read performance.\r\n- Writing and reading overhead as in-memory and on-disk representations differ.\r\n\n\n### Proposal document\n\nhttps://docs.google.com/document/d/18Bqhr-vnzFfQk1S4AgRISkA_5_m5m32Nnc2Cw0zn2XM\n\n### Specifications\n\n- [X] Table\n- [ ] View\n- [ ] REST\n- [X] Puffin\n- [ ] Encryption\n- [ ] Other",
    "issue_word_count": 118,
    "test_files_count": 5,
    "non_test_files_count": 12,
    "pr_changed_files": [
      "api/src/main/java/org/apache/iceberg/DataFile.java",
      "api/src/main/java/org/apache/iceberg/DeleteFile.java",
      "core/src/main/java/org/apache/iceberg/BaseFile.java",
      "core/src/main/java/org/apache/iceberg/BaseScan.java",
      "core/src/main/java/org/apache/iceberg/ContentFileParser.java",
      "core/src/main/java/org/apache/iceberg/FileMetadata.java",
      "core/src/main/java/org/apache/iceberg/GenericDataFile.java",
      "core/src/main/java/org/apache/iceberg/GenericDeleteFile.java",
      "core/src/main/java/org/apache/iceberg/SnapshotProducer.java",
      "core/src/main/java/org/apache/iceberg/V2Metadata.java",
      "core/src/main/java/org/apache/iceberg/V3Metadata.java",
      "core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java",
      "core/src/test/java/org/apache/iceberg/TestBase.java",
      "core/src/test/java/org/apache/iceberg/TestContentFileParser.java",
      "core/src/test/java/org/apache/iceberg/TestManifestEncryption.java",
      "core/src/test/java/org/apache/iceberg/TestManifestReader.java",
      "core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/TestBase.java",
      "core/src/test/java/org/apache/iceberg/TestContentFileParser.java",
      "core/src/test/java/org/apache/iceberg/TestManifestEncryption.java",
      "core/src/test/java/org/apache/iceberg/TestManifestReader.java",
      "core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java"
    ],
    "base_commit": "caf424a373fa125d427401acda7079b08abea9de",
    "head_commit": "03e60e527a43223142b208f3ca96db8145ab9058",
    "repo_url": "https://github.com/apache/iceberg/pull/11443",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11443",
    "dockerfile": "",
    "pr_merged_at": "2024-11-02T16:23:29.000Z",
    "patch": "diff --git a/api/src/main/java/org/apache/iceberg/DataFile.java b/api/src/main/java/org/apache/iceberg/DataFile.java\nindex 02ad0aff3128..3c6d77f34d8f 100644\n--- a/api/src/main/java/org/apache/iceberg/DataFile.java\n+++ b/api/src/main/java/org/apache/iceberg/DataFile.java\n@@ -98,12 +98,18 @@ public interface DataFile extends ContentFile<DataFile> {\n   Types.NestedField SORT_ORDER_ID =\n       optional(140, \"sort_order_id\", IntegerType.get(), \"Sort order ID\");\n   Types.NestedField SPEC_ID = optional(141, \"spec_id\", IntegerType.get(), \"Partition spec ID\");\n+  Types.NestedField REFERENCED_DATA_FILE =\n+      optional(\n+          143,\n+          \"referenced_data_file\",\n+          StringType.get(),\n+          \"Fully qualified location (URI with FS scheme) of a data file that all deletes reference\");\n \n   int PARTITION_ID = 102;\n   String PARTITION_NAME = \"partition\";\n   String PARTITION_DOC = \"Partition data tuple, schema based on the partition spec\";\n \n-  // NEXT ID TO ASSIGN: 142\n+  // NEXT ID TO ASSIGN: 144\n \n   static StructType getType(StructType partitionType) {\n     // IDs start at 100 to leave room for changes to ManifestEntry\n@@ -124,7 +130,8 @@ static StructType getType(StructType partitionType) {\n         KEY_METADATA,\n         SPLIT_OFFSETS,\n         EQUALITY_IDS,\n-        SORT_ORDER_ID);\n+        SORT_ORDER_ID,\n+        REFERENCED_DATA_FILE);\n   }\n \n   /**\n\ndiff --git a/api/src/main/java/org/apache/iceberg/DeleteFile.java b/api/src/main/java/org/apache/iceberg/DeleteFile.java\nindex 0f8087e6a055..8e17e60fcccf 100644\n--- a/api/src/main/java/org/apache/iceberg/DeleteFile.java\n+++ b/api/src/main/java/org/apache/iceberg/DeleteFile.java\n@@ -31,4 +31,15 @@ public interface DeleteFile extends ContentFile<DeleteFile> {\n   default List<Long> splitOffsets() {\n     return null;\n   }\n+\n+  /**\n+   * Returns the location of a data file that all deletes reference.\n+   *\n+   * <p>The referenced data file is required for deletion vectors and may be optionally captured for\n+   * position delete files that apply to only one data file. This method always returns null for\n+   * equality delete files.\n+   */\n+  default String referencedDataFile() {\n+    return null;\n+  }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/BaseFile.java b/core/src/main/java/org/apache/iceberg/BaseFile.java\nindex 8f84eb5737b9..f4fd94724e95 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseFile.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseFile.java\n@@ -80,6 +80,7 @@ public PartitionData copy() {\n   private int[] equalityIds = null;\n   private byte[] keyMetadata = null;\n   private Integer sortOrderId;\n+  private String referencedDataFile = null;\n \n   // cached schema\n   private transient Schema avroSchema = null;\n@@ -108,6 +109,7 @@ public PartitionData copy() {\n           DataFile.SPLIT_OFFSETS,\n           DataFile.EQUALITY_IDS,\n           DataFile.SORT_ORDER_ID,\n+          DataFile.REFERENCED_DATA_FILE,\n           MetadataColumns.ROW_POSITION);\n \n   /** Used by Avro reflection to instantiate this class when reading manifest files. */\n@@ -149,7 +151,8 @@ public PartitionData copy() {\n       List<Long> splitOffsets,\n       int[] equalityFieldIds,\n       Integer sortOrderId,\n-      ByteBuffer keyMetadata) {\n+      ByteBuffer keyMetadata,\n+      String referencedDataFile) {\n     super(BASE_TYPE.fields().size());\n     this.partitionSpecId = specId;\n     this.content = content;\n@@ -178,6 +181,7 @@ public PartitionData copy() {\n     this.equalityIds = equalityFieldIds;\n     this.sortOrderId = sortOrderId;\n     this.keyMetadata = ByteBuffers.toByteArray(keyMetadata);\n+    this.referencedDataFile = referencedDataFile;\n   }\n \n   /**\n@@ -230,6 +234,7 @@ public PartitionData copy() {\n     this.sortOrderId = toCopy.sortOrderId;\n     this.dataSequenceNumber = toCopy.dataSequenceNumber;\n     this.fileSequenceNumber = toCopy.fileSequenceNumber;\n+    this.referencedDataFile = toCopy.referencedDataFile;\n   }\n \n   /** Constructor for Java serialization. */\n@@ -339,6 +344,9 @@ protected <T> void internalSet(int pos, T value) {\n         this.sortOrderId = (Integer) value;\n         return;\n       case 17:\n+        this.referencedDataFile = value != null ? value.toString() : null;\n+        return;\n+      case 18:\n         this.fileOrdinal = (long) value;\n         return;\n       default:\n@@ -388,6 +396,8 @@ private Object getByPos(int basePos) {\n       case 16:\n         return sortOrderId;\n       case 17:\n+        return referencedDataFile;\n+      case 18:\n         return fileOrdinal;\n       default:\n         throw new UnsupportedOperationException(\"Unknown field ordinal: \" + basePos);\n@@ -514,6 +524,10 @@ public Integer sortOrderId() {\n     return sortOrderId;\n   }\n \n+  public String referencedDataFile() {\n+    return referencedDataFile;\n+  }\n+\n   private static <K, V> Map<K, V> copyMap(Map<K, V> map, Set<K> keys) {\n     return keys == null ? SerializableMap.copyOf(map) : SerializableMap.filteredCopyOf(map, keys);\n   }\n@@ -565,6 +579,7 @@ public String toString() {\n         .add(\"sort_order_id\", sortOrderId)\n         .add(\"data_sequence_number\", dataSequenceNumber == null ? \"null\" : dataSequenceNumber)\n         .add(\"file_sequence_number\", fileSequenceNumber == null ? \"null\" : fileSequenceNumber)\n+        .add(\"referenced_data_file\", referencedDataFile == null ? \"null\" : referencedDataFile)\n         .toString();\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/BaseScan.java b/core/src/main/java/org/apache/iceberg/BaseScan.java\nindex 804df01d31ba..a011d03d59ad 100644\n--- a/core/src/main/java/org/apache/iceberg/BaseScan.java\n+++ b/core/src/main/java/org/apache/iceberg/BaseScan.java\n@@ -77,6 +77,7 @@ abstract class BaseScan<ThisT, T extends ScanTask, G extends ScanTaskGroup<T>>\n           \"partition\",\n           \"key_metadata\",\n           \"split_offsets\",\n+          \"referenced_data_file\",\n           \"equality_ids\");\n \n   protected static final List<String> DELETE_SCAN_WITH_STATS_COLUMNS =\n\ndiff --git a/core/src/main/java/org/apache/iceberg/ContentFileParser.java b/core/src/main/java/org/apache/iceberg/ContentFileParser.java\nindex dd08c5c69e7d..96dfa5586c31 100644\n--- a/core/src/main/java/org/apache/iceberg/ContentFileParser.java\n+++ b/core/src/main/java/org/apache/iceberg/ContentFileParser.java\n@@ -45,6 +45,7 @@ class ContentFileParser {\n   private static final String SPLIT_OFFSETS = \"split-offsets\";\n   private static final String EQUALITY_IDS = \"equality-ids\";\n   private static final String SORT_ORDER_ID = \"sort-order-id\";\n+  private static final String REFERENCED_DATA_FILE = \"referenced-data-file\";\n \n   private ContentFileParser() {}\n \n@@ -109,6 +110,14 @@ static void toJson(ContentFile<?> contentFile, PartitionSpec spec, JsonGenerator\n       generator.writeNumberField(SORT_ORDER_ID, contentFile.sortOrderId());\n     }\n \n+    if (contentFile instanceof DeleteFile) {\n+      DeleteFile deleteFile = (DeleteFile) contentFile;\n+\n+      if (deleteFile.referencedDataFile() != null) {\n+        generator.writeStringField(REFERENCED_DATA_FILE, deleteFile.referencedDataFile());\n+      }\n+    }\n+\n     generator.writeEndObject();\n   }\n \n@@ -145,6 +154,7 @@ static ContentFile<?> fromJson(JsonNode jsonNode, PartitionSpec spec) {\n     List<Long> splitOffsets = JsonUtil.getLongListOrNull(SPLIT_OFFSETS, jsonNode);\n     int[] equalityFieldIds = JsonUtil.getIntArrayOrNull(EQUALITY_IDS, jsonNode);\n     Integer sortOrderId = JsonUtil.getIntOrNull(SORT_ORDER_ID, jsonNode);\n+    String referencedDataFile = JsonUtil.getStringOrNull(REFERENCED_DATA_FILE, jsonNode);\n \n     if (fileContent == FileContent.DATA) {\n       return new GenericDataFile(\n@@ -169,7 +179,8 @@ static ContentFile<?> fromJson(JsonNode jsonNode, PartitionSpec spec) {\n           equalityFieldIds,\n           sortOrderId,\n           splitOffsets,\n-          keyMetadata);\n+          keyMetadata,\n+          referencedDataFile);\n     }\n   }\n \n\ndiff --git a/core/src/main/java/org/apache/iceberg/FileMetadata.java b/core/src/main/java/org/apache/iceberg/FileMetadata.java\nindex 9a201d1b3b6f..ef229593bcab 100644\n--- a/core/src/main/java/org/apache/iceberg/FileMetadata.java\n+++ b/core/src/main/java/org/apache/iceberg/FileMetadata.java\n@@ -59,6 +59,7 @@ public static class Builder {\n     private ByteBuffer keyMetadata = null;\n     private Integer sortOrderId = null;\n     private List<Long> splitOffsets = null;\n+    private String referencedDataFile = null;\n \n     Builder(PartitionSpec spec) {\n       this.spec = spec;\n@@ -220,6 +221,15 @@ public Builder withSortOrder(SortOrder newSortOrder) {\n       return this;\n     }\n \n+    public Builder withReferencedDataFile(CharSequence newReferencedDataFile) {\n+      if (newReferencedDataFile != null) {\n+        this.referencedDataFile = newReferencedDataFile.toString();\n+      } else {\n+        this.referencedDataFile = null;\n+      }\n+      return this;\n+    }\n+\n     public DeleteFile build() {\n       Preconditions.checkArgument(filePath != null, \"File path is required\");\n       if (format == null) {\n@@ -262,7 +272,8 @@ public DeleteFile build() {\n           equalityFieldIds,\n           sortOrderId,\n           splitOffsets,\n-          keyMetadata);\n+          keyMetadata,\n+          referencedDataFile);\n     }\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/GenericDataFile.java b/core/src/main/java/org/apache/iceberg/GenericDataFile.java\nindex 7b99e7b60ab8..aa34cd22cdaa 100644\n--- a/core/src/main/java/org/apache/iceberg/GenericDataFile.java\n+++ b/core/src/main/java/org/apache/iceberg/GenericDataFile.java\n@@ -64,7 +64,8 @@ class GenericDataFile extends BaseFile<DataFile> implements DataFile {\n         splitOffsets,\n         null /* no equality field IDs */,\n         sortOrderId,\n-        keyMetadata);\n+        keyMetadata,\n+        null /* no referenced data file */);\n   }\n \n   /**\n\ndiff --git a/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java b/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java\nindex 77e0d8505af6..05eb7c97dbab 100644\n--- a/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java\n+++ b/core/src/main/java/org/apache/iceberg/GenericDeleteFile.java\n@@ -48,7 +48,8 @@ class GenericDeleteFile extends BaseFile<DeleteFile> implements DeleteFile {\n       int[] equalityFieldIds,\n       Integer sortOrderId,\n       List<Long> splitOffsets,\n-      ByteBuffer keyMetadata) {\n+      ByteBuffer keyMetadata,\n+      String referencedDataFile) {\n     super(\n         specId,\n         content,\n@@ -66,7 +67,8 @@ class GenericDeleteFile extends BaseFile<DeleteFile> implements DeleteFile {\n         splitOffsets,\n         equalityFieldIds,\n         sortOrderId,\n-        keyMetadata);\n+        keyMetadata,\n+        referencedDataFile);\n   }\n \n   /**\n\ndiff --git a/core/src/main/java/org/apache/iceberg/SnapshotProducer.java b/core/src/main/java/org/apache/iceberg/SnapshotProducer.java\nindex 89f9eab7192a..daf1c3d72b89 100644\n--- a/core/src/main/java/org/apache/iceberg/SnapshotProducer.java\n+++ b/core/src/main/java/org/apache/iceberg/SnapshotProducer.java\n@@ -923,5 +923,10 @@ public List<Integer> equalityFieldIds() {\n     public Integer sortOrderId() {\n       return deleteFile.sortOrderId();\n     }\n+\n+    @Override\n+    public String referencedDataFile() {\n+      return deleteFile.referencedDataFile();\n+    }\n   }\n }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/V2Metadata.java b/core/src/main/java/org/apache/iceberg/V2Metadata.java\nindex be4c3734e40b..20b2169b8dad 100644\n--- a/core/src/main/java/org/apache/iceberg/V2Metadata.java\n+++ b/core/src/main/java/org/apache/iceberg/V2Metadata.java\n@@ -274,7 +274,8 @@ static Types.StructType fileType(Types.StructType partitionType) {\n         DataFile.KEY_METADATA,\n         DataFile.SPLIT_OFFSETS,\n         DataFile.EQUALITY_IDS,\n-        DataFile.SORT_ORDER_ID);\n+        DataFile.SORT_ORDER_ID,\n+        DataFile.REFERENCED_DATA_FILE);\n   }\n \n   static class IndexedManifestEntry<F extends ContentFile<F>>\n@@ -448,6 +449,12 @@ public Object get(int pos) {\n           return wrapped.equalityFieldIds();\n         case 15:\n           return wrapped.sortOrderId();\n+        case 16:\n+          if (wrapped instanceof DeleteFile) {\n+            return ((DeleteFile) wrapped).referencedDataFile();\n+          } else {\n+            return null;\n+          }\n       }\n       throw new IllegalArgumentException(\"Unknown field ordinal: \" + pos);\n     }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/V3Metadata.java b/core/src/main/java/org/apache/iceberg/V3Metadata.java\nindex f295af3e109d..a418a868564e 100644\n--- a/core/src/main/java/org/apache/iceberg/V3Metadata.java\n+++ b/core/src/main/java/org/apache/iceberg/V3Metadata.java\n@@ -274,7 +274,8 @@ static Types.StructType fileType(Types.StructType partitionType) {\n         DataFile.KEY_METADATA,\n         DataFile.SPLIT_OFFSETS,\n         DataFile.EQUALITY_IDS,\n-        DataFile.SORT_ORDER_ID);\n+        DataFile.SORT_ORDER_ID,\n+        DataFile.REFERENCED_DATA_FILE);\n   }\n \n   static class IndexedManifestEntry<F extends ContentFile<F>>\n@@ -448,6 +449,12 @@ public Object get(int pos) {\n           return wrapped.equalityFieldIds();\n         case 15:\n           return wrapped.sortOrderId();\n+        case 16:\n+          if (wrapped.content() == FileContent.POSITION_DELETES) {\n+            return ((DeleteFile) wrapped).referencedDataFile();\n+          } else {\n+            return null;\n+          }\n       }\n       throw new IllegalArgumentException(\"Unknown field ordinal: \" + pos);\n     }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java b/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java\nindex 04fc077d10ea..c82b3ff828cf 100644\n--- a/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/util/ContentFileUtil.java\n@@ -54,6 +54,10 @@ public static CharSequence referencedDataFile(DeleteFile deleteFile) {\n       return null;\n     }\n \n+    if (deleteFile.referencedDataFile() != null) {\n+      return deleteFile.referencedDataFile();\n+    }\n+\n     int pathId = MetadataColumns.DELETE_FILE_PATH.fieldId();\n     Type pathType = MetadataColumns.DELETE_FILE_PATH.type();\n \n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/TestBase.java b/core/src/test/java/org/apache/iceberg/TestBase.java\nindex f3bbb7979547..45441631900c 100644\n--- a/core/src/test/java/org/apache/iceberg/TestBase.java\n+++ b/core/src/test/java/org/apache/iceberg/TestBase.java\n@@ -654,6 +654,18 @@ protected DeleteFile newDeleteFile(int specId, String partitionPath) {\n         .build();\n   }\n \n+  protected DeleteFile newDeleteFileWithRef(DataFile dataFile) {\n+    PartitionSpec spec = table.specs().get(dataFile.specId());\n+    return FileMetadata.deleteFileBuilder(spec)\n+        .ofPositionDeletes()\n+        .withPath(\"/path/to/delete-\" + UUID.randomUUID() + \".parquet\")\n+        .withFileSizeInBytes(10)\n+        .withPartition(dataFile.partition())\n+        .withReferencedDataFile(dataFile.location())\n+        .withRecordCount(1)\n+        .build();\n+  }\n+\n   protected DeleteFile newEqualityDeleteFile(int specId, String partitionPath, int... fieldIds) {\n     PartitionSpec spec = table.specs().get(specId);\n     return FileMetadata.deleteFileBuilder(spec)\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestContentFileParser.java b/core/src/test/java/org/apache/iceberg/TestContentFileParser.java\nindex 83f7fc1f6220..fbe473931659 100644\n--- a/core/src/test/java/org/apache/iceberg/TestContentFileParser.java\n+++ b/core/src/test/java/org/apache/iceberg/TestContentFileParser.java\n@@ -213,7 +213,33 @@ private static Stream<Arguments> provideSpecAndDeleteFile() {\n         Arguments.of(\n             TestBase.SPEC,\n             deleteFileWithAllOptional(TestBase.SPEC),\n-            deleteFileJsonWithAllOptional(TestBase.SPEC)));\n+            deleteFileJsonWithAllOptional(TestBase.SPEC)),\n+        Arguments.of(\n+            TestBase.SPEC, deleteFileWithDataRef(TestBase.SPEC), deleteFileWithDataRefJson()));\n+  }\n+\n+  private static DeleteFile deleteFileWithDataRef(PartitionSpec spec) {\n+    PartitionData partitionData = new PartitionData(spec.partitionType());\n+    partitionData.set(0, 4);\n+    return new GenericDeleteFile(\n+        spec.specId(),\n+        FileContent.POSITION_DELETES,\n+        \"/path/to/delete.parquet\",\n+        FileFormat.PARQUET,\n+        partitionData,\n+        1234,\n+        new Metrics(10L, null, null, null, null),\n+        null,\n+        null,\n+        null,\n+        null,\n+        \"/path/to/data/file.parquet\");\n+  }\n+\n+  private static String deleteFileWithDataRefJson() {\n+    return \"{\\\"spec-id\\\":0,\\\"content\\\":\\\"POSITION_DELETES\\\",\\\"file-path\\\":\\\"/path/to/delete.parquet\\\",\"\n+        + \"\\\"file-format\\\":\\\"PARQUET\\\",\\\"partition\\\":{\\\"1000\\\":4},\\\"file-size-in-bytes\\\":1234,\"\n+        + \"\\\"record-count\\\":10,\\\"referenced-data-file\\\":\\\"/path/to/data/file.parquet\\\"}\";\n   }\n \n   private static DeleteFile deleteFileWithRequiredOnly(PartitionSpec spec) {\n@@ -234,6 +260,7 @@ private static DeleteFile deleteFileWithRequiredOnly(PartitionSpec spec) {\n         null,\n         null,\n         null,\n+        null,\n         null);\n   }\n \n@@ -273,7 +300,8 @@ private static DeleteFile deleteFileWithAllOptional(PartitionSpec spec) {\n         new int[] {3},\n         1,\n         Collections.singletonList(128L),\n-        ByteBuffer.wrap(new byte[16]));\n+        ByteBuffer.wrap(new byte[16]),\n+        null);\n   }\n \n   private static String deleteFileJsonWithRequiredOnly(PartitionSpec spec) {\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java b/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java\nindex 13e8985cdb56..1f29c0e5b85c 100644\n--- a/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java\n+++ b/core/src/test/java/org/apache/iceberg/TestManifestEncryption.java\n@@ -110,7 +110,8 @@ public class TestManifestEncryption {\n           EQUALITY_ID_ARR,\n           SORT_ORDER_ID,\n           null,\n-          CONTENT_KEY_METADATA);\n+          CONTENT_KEY_METADATA,\n+          null);\n \n   private static final EncryptionManager ENCRYPTION_MANAGER =\n       EncryptionTestHelpers.createEncryptionManager();\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestManifestReader.java b/core/src/test/java/org/apache/iceberg/TestManifestReader.java\nindex e45415f1f2d2..4652da943003 100644\n--- a/core/src/test/java/org/apache/iceberg/TestManifestReader.java\n+++ b/core/src/test/java/org/apache/iceberg/TestManifestReader.java\n@@ -130,7 +130,7 @@ public void testDataFilePositions() throws IOException {\n       long expectedPos = 0L;\n       for (DataFile file : reader) {\n         assertThat(file.pos()).as(\"Position should match\").isEqualTo(expectedPos);\n-        assertThat(((BaseFile) file).get(17))\n+        assertThat(((BaseFile) file).get(18))\n             .as(\"Position from field index should match\")\n             .isEqualTo(expectedPos);\n         expectedPos += 1;\n@@ -158,7 +158,7 @@ public void testDeleteFilePositions() throws IOException {\n       long expectedPos = 0L;\n       for (DeleteFile file : reader) {\n         assertThat(file.pos()).as(\"Position should match\").isEqualTo(expectedPos);\n-        assertThat(((BaseFile) file).get(17))\n+        assertThat(((BaseFile) file).get(18))\n             .as(\"Position from field index should match\")\n             .isEqualTo(expectedPos);\n         expectedPos += 1;\n@@ -181,6 +181,24 @@ public void testDeleteFileManifestPaths() throws IOException {\n     }\n   }\n \n+  @TestTemplate\n+  public void testDeleteFilesWithReferences() throws IOException {\n+    assumeThat(formatVersion).isGreaterThanOrEqualTo(2);\n+    DeleteFile deleteFile1 = newDeleteFileWithRef(FILE_A);\n+    DeleteFile deleteFile2 = newDeleteFileWithRef(FILE_B);\n+    ManifestFile manifest = writeDeleteManifest(formatVersion, 1000L, deleteFile1, deleteFile2);\n+    try (ManifestReader<DeleteFile> reader =\n+        ManifestFiles.readDeleteManifest(manifest, FILE_IO, table.specs())) {\n+      for (DeleteFile deleteFile : reader) {\n+        if (deleteFile.location().equals(deleteFile1.location())) {\n+          assertThat(deleteFile.referencedDataFile()).isEqualTo(FILE_A.location());\n+        } else {\n+          assertThat(deleteFile.referencedDataFile()).isEqualTo(FILE_B.location());\n+        }\n+      }\n+    }\n+  }\n+\n   @TestTemplate\n   public void testDataFileSplitOffsetsNullWhenInvalid() throws IOException {\n     DataFile invalidOffset =\n\ndiff --git a/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java b/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java\nindex 1d5c34fa4b16..88dcc6ff9ca4 100644\n--- a/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java\n+++ b/core/src/test/java/org/apache/iceberg/TestManifestWriterVersions.java\n@@ -96,6 +96,7 @@ public class TestManifestWriterVersions {\n           EQUALITY_ID_ARR,\n           SORT_ORDER_ID,\n           null,\n+          null,\n           null);\n \n   @TempDir private Path temp;\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11439",
    "pr_id": 11439,
    "issue_id": 11438,
    "repo": "apache/iceberg",
    "problem_statement": "the `where` sql in rewriteDataFilesAction is always case sensitive\n### Apache Iceberg version\r\n\r\n1.6.1 (latest release)\r\n\r\n### Query engine\r\n\r\nSpark\r\n\r\n### Please describe the bug üêû\r\n\r\ngiven  an iceberg table created by the following sql:\r\n```sql\r\nCREATE TABLE icbtest.test0(\r\n`pk_id` int,\r\n val string,\r\n start_dt string, \r\nend_dt string\r\n) USING iceberg \r\npartitioned by (truncate(6, end_dt))\r\n;\r\n```\r\nafter that, we run the `rewriteDataFiles` procedure by the sql:\r\n```sql\r\nset spark.sql.caseSensitive=false;\r\ncall my_catalog.system.rewrite_data_files(table=>'icbtest.test0', where => \" END_DT  > '202008' \");\r\n```\r\nthe following exception is raised \r\n![ice-bug](https://github.com/user-attachments/assets/5af39e87-40fb-4090-8fef-9925c581fe79)\r\n\r\nsince the `where` sql is always case sensitive, and `set spark.sql.caseSensitive=false` does not take effects.\r\n\r\n\r\n### Willingness to contribute\r\n\r\n- [X] I can contribute a fix for this bug independently\r\n- [X] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\r\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 157,
    "test_files_count": 1,
    "non_test_files_count": 1,
    "pr_changed_files": [
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java",
      "spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java"
    ],
    "pr_changed_test_files": [
      "spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java"
    ],
    "base_commit": "e770facc3e7cbccb719b3ae5263cd1ece181f9ea",
    "head_commit": "9e766911a3b8044db37909de860ceb62458fea81",
    "repo_url": "https://github.com/apache/iceberg/pull/11439",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11439",
    "dockerfile": "",
    "pr_merged_at": "2024-12-02T21:58:57.000Z",
    "patch": "diff --git a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java\nindex 4e381a7bd362..e04a0c88b4bb 100644\n--- a/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java\n+++ b/spark/v3.5/spark/src/main/java/org/apache/iceberg/spark/actions/RewriteDataFilesSparkAction.java\n@@ -62,6 +62,7 @@\n import org.apache.iceberg.relocated.com.google.common.math.IntMath;\n import org.apache.iceberg.relocated.com.google.common.util.concurrent.MoreExecutors;\n import org.apache.iceberg.relocated.com.google.common.util.concurrent.ThreadFactoryBuilder;\n+import org.apache.iceberg.spark.SparkUtil;\n import org.apache.iceberg.types.Types.StructType;\n import org.apache.iceberg.util.PropertyUtil;\n import org.apache.iceberg.util.StructLikeMap;\n@@ -102,11 +103,13 @@ public class RewriteDataFilesSparkAction\n   private boolean useStartingSequenceNumber;\n   private RewriteJobOrder rewriteJobOrder;\n   private FileRewriter<FileScanTask, DataFile> rewriter = null;\n+  private boolean caseSensitive;\n \n   RewriteDataFilesSparkAction(SparkSession spark, Table table) {\n     super(spark.cloneSession());\n     // Disable Adaptive Query Execution as this may change the output partitioning of our write\n     spark().conf().set(SQLConf.ADAPTIVE_EXECUTION_ENABLED().key(), false);\n+    this.caseSensitive = SparkUtil.caseSensitive(spark);\n     this.table = table;\n   }\n \n@@ -198,6 +201,7 @@ StructLikeMap<List<List<FileScanTask>>> planFileGroups(long startingSnapshotId)\n         table\n             .newScan()\n             .useSnapshot(startingSnapshotId)\n+            .caseSensitive(caseSensitive)\n             .filter(filter)\n             .ignoreResiduals()\n             .planFiles();\n",
    "test_patch": "diff --git a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\nindex 93198825e326..3d3a105a14be 100644\n--- a/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\n+++ b/spark/v3.5/spark-extensions/src/test/java/org/apache/iceberg/spark/extensions/TestRewriteDataFilesProcedure.java\n@@ -69,6 +69,29 @@ public void removeTable() {\n     sql(\"DROP TABLE IF EXISTS %s\", tableName(QUOTED_SPECIAL_CHARS_TABLE_NAME));\n   }\n \n+  @TestTemplate\n+  public void testFilterCaseSensitivity() {\n+    createTable();\n+    insertData(10);\n+    sql(\"set %s = false\", SQLConf.CASE_SENSITIVE().key());\n+    List<Object[]> expectedRecords = currentData();\n+    List<Object[]> output =\n+        sql(\n+            \"CALL %s.system.rewrite_data_files(table=>'%s', where=>'C1 > 0')\",\n+            catalogName, tableIdent);\n+    assertEquals(\n+        \"Action should rewrite 10 data files and add 1 data files\",\n+        row(10, 1),\n+        Arrays.copyOf(output.get(0), 2));\n+    // verify rewritten bytes separately\n+    assertThat(output.get(0)).hasSize(4);\n+    assertThat(output.get(0)[2])\n+        .isInstanceOf(Long.class)\n+        .isEqualTo(Long.valueOf(snapshotSummary().get(SnapshotSummary.REMOVED_FILE_SIZE_PROP)));\n+    List<Object[]> actualRecords = currentData();\n+    assertEquals(\"Data after compaction should not change\", expectedRecords, actualRecords);\n+  }\n+\n   @TestTemplate\n   public void testZOrderSortExpression() {\n     List<ExtendedParser.RawOrderField> order =\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11437",
    "pr_id": 11437,
    "issue_id": 11435,
    "repo": "apache/iceberg",
    "problem_statement": "Bad Table Properties cause commit failure\n### Apache Iceberg version\n\n1.6.1 (latest release)\n\n### Query engine\n\nSpark\n\n### Please describe the bug üêû\n\nToday iceberg does not validate the type of table properties values on and this can be problematic if those table properties value are used for commit. \r\n\r\nExample setup\r\n```\r\n                sql(\r\n                    \"CREATE TABLE foo.bar \"\r\n                        + \"(id BIGINT NOT NULL, data STRING) \"\r\n                        + \"USING iceberg \"\r\n                        + \"TBLPROPERTIES ('commit.retry.num-retries'='x', p2='x')\",\r\n```\r\n\r\nSee the value for `commit.retry.num-retries` is accidentally set to some non-integer value, we will unable to rectify this through either spark SQL or iceberg API \r\n\r\n```\r\ntable.updateProperties.remove(\"commit.retry.max-wait-ms\").apply.commit\r\n\r\ntable.updateProperties.remove(\"commit.retry.max-wait-ms\").commit\r\njava.lang.NumberFormatException: For input string: \"commit.retry.max-wait-ms\"\r\n  at java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\r\n  at java.base/java.lang.Integer.parseInt(Integer.java:668)\r\n  at java.base/java.lang.Integer.parseInt(Integer.java:786)\r\n  at org.apache.iceberg.util.PropertyUtil.propertyAsInt(PropertyUtil.java:64)\r\n  at org.apache.iceberg.TableMetadata.propertyAsInt(TableMetadata.java:472)\r\n  at org.apache.iceberg.PropertiesUpdate.commit(PropertiesUpdate.java:105)\r\n  ... 48 elided\r\n```\r\n\r\nAfter some look, I believe this block of code might be the problem: https://github.com/apache/iceberg/blob/main/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java#L100-L114, where  propertyAsInt will throw NumberFormatException before commit can proceed to rectify or remove incorrectly set table properties. \r\n\r\n\r\nCurrent workaround is move back to previous table metadata before the table properties change, but I plan to contribute the patch to fix the problem in 2 ways\r\n1. add validation for new table where commit related table properties need to have value type checked (as integer)\r\n2. relax the condition in `PropertiesUpdate` class to allow update when existing value is corrupted, this help with existing table to move forward. \r\n\r\n\n\n### Willingness to contribute\n\n- [X] I can contribute a fix for this bug independently\n- [ ] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 348,
    "test_files_count": 2,
    "non_test_files_count": 3,
    "pr_changed_files": [
      "core/src/main/java/org/apache/iceberg/PropertiesUpdate.java",
      "core/src/main/java/org/apache/iceberg/TableMetadata.java",
      "core/src/main/java/org/apache/iceberg/util/PropertyUtil.java",
      "core/src/test/java/org/apache/iceberg/TestTransaction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/TestTransaction.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java"
    ],
    "base_commit": "91e04c9c88b63dc01d6c8e69dfdc8cd27ee811cc",
    "head_commit": "38efa71e028a14b77079cdcf8bc9953367950f48",
    "repo_url": "https://github.com/apache/iceberg/pull/11437",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11437",
    "dockerfile": "",
    "pr_merged_at": "2024-11-02T00:22:44.000Z",
    "patch": "diff --git a/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java b/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java\nindex 35338a689205..9389aec50c0a 100644\n--- a/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java\n+++ b/core/src/main/java/org/apache/iceberg/PropertiesUpdate.java\n@@ -98,12 +98,13 @@ public Map<String, String> apply() {\n \n   @Override\n   public void commit() {\n+    // If existing table commit properties in base are corrupted, allow rectification\n     Tasks.foreach(ops)\n-        .retry(base.propertyAsInt(COMMIT_NUM_RETRIES, COMMIT_NUM_RETRIES_DEFAULT))\n+        .retry(base.propertyTryAsInt(COMMIT_NUM_RETRIES, COMMIT_NUM_RETRIES_DEFAULT))\n         .exponentialBackoff(\n-            base.propertyAsInt(COMMIT_MIN_RETRY_WAIT_MS, COMMIT_MIN_RETRY_WAIT_MS_DEFAULT),\n-            base.propertyAsInt(COMMIT_MAX_RETRY_WAIT_MS, COMMIT_MAX_RETRY_WAIT_MS_DEFAULT),\n-            base.propertyAsInt(COMMIT_TOTAL_RETRY_TIME_MS, COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT),\n+            base.propertyTryAsInt(COMMIT_MIN_RETRY_WAIT_MS, COMMIT_MIN_RETRY_WAIT_MS_DEFAULT),\n+            base.propertyTryAsInt(COMMIT_MAX_RETRY_WAIT_MS, COMMIT_MAX_RETRY_WAIT_MS_DEFAULT),\n+            base.propertyTryAsInt(COMMIT_TOTAL_RETRY_TIME_MS, COMMIT_TOTAL_RETRY_TIME_MS_DEFAULT),\n             2.0 /* exponential */)\n         .onlyRetryOn(CommitFailedException.class)\n         .run(\n\ndiff --git a/core/src/main/java/org/apache/iceberg/TableMetadata.java b/core/src/main/java/org/apache/iceberg/TableMetadata.java\nindex d20dd59d2b97..3cdc53995dce 100644\n--- a/core/src/main/java/org/apache/iceberg/TableMetadata.java\n+++ b/core/src/main/java/org/apache/iceberg/TableMetadata.java\n@@ -134,6 +134,8 @@ static TableMetadata newTableMetadata(\n     // break existing tables.\n     MetricsConfig.fromProperties(properties).validateReferencedColumns(schema);\n \n+    PropertyUtil.validateCommitProperties(properties);\n+\n     return new Builder()\n         .setInitialFormatVersion(formatVersion)\n         .setCurrentSchema(freshSchema, lastColumnId.get())\n@@ -486,6 +488,10 @@ public int propertyAsInt(String property, int defaultValue) {\n     return PropertyUtil.propertyAsInt(properties, property, defaultValue);\n   }\n \n+  public int propertyTryAsInt(String property, int defaultValue) {\n+    return PropertyUtil.propertyTryAsInt(properties, property, defaultValue);\n+  }\n+\n   public long propertyAsLong(String property, long defaultValue) {\n     return PropertyUtil.propertyAsLong(properties, property, defaultValue);\n   }\n\ndiff --git a/core/src/main/java/org/apache/iceberg/util/PropertyUtil.java b/core/src/main/java/org/apache/iceberg/util/PropertyUtil.java\nindex 68c8f3e9efda..633b0a6ae739 100644\n--- a/core/src/main/java/org/apache/iceberg/util/PropertyUtil.java\n+++ b/core/src/main/java/org/apache/iceberg/util/PropertyUtil.java\n@@ -24,10 +24,23 @@\n import java.util.Set;\n import java.util.function.Predicate;\n import java.util.stream.Collectors;\n+import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n+import org.apache.iceberg.relocated.com.google.common.collect.ImmutableSet;\n import org.apache.iceberg.relocated.com.google.common.collect.Maps;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n \n public class PropertyUtil {\n+  private static final Logger LOG = LoggerFactory.getLogger(PropertyUtil.class);\n+\n+  private static final Set<String> COMMIT_PROPERTIES =\n+      ImmutableSet.of(\n+          TableProperties.COMMIT_NUM_RETRIES,\n+          TableProperties.COMMIT_MIN_RETRY_WAIT_MS,\n+          TableProperties.COMMIT_MAX_RETRY_WAIT_MS,\n+          TableProperties.COMMIT_TOTAL_RETRY_TIME_MS);\n \n   private PropertyUtil() {}\n \n@@ -57,6 +70,20 @@ public static double propertyAsDouble(\n     return defaultValue;\n   }\n \n+  public static int propertyTryAsInt(\n+      Map<String, String> properties, String property, int defaultValue) {\n+    String value = properties.get(property);\n+    if (value == null) {\n+      return defaultValue;\n+    }\n+    try {\n+      return Integer.parseInt(value);\n+    } catch (NumberFormatException e) {\n+      LOG.warn(\"Failed to parse value of {} as integer, default to {}\", property, defaultValue, e);\n+      return defaultValue;\n+    }\n+  }\n+\n   public static int propertyAsInt(\n       Map<String, String> properties, String property, int defaultValue) {\n     String value = properties.get(property);\n@@ -100,6 +127,29 @@ public static String propertyAsString(\n     return defaultValue;\n   }\n \n+  /**\n+   * Validate the table commit related properties to have non-negative integer on table creation to\n+   * prevent commit failure\n+   */\n+  public static void validateCommitProperties(Map<String, String> properties) {\n+    for (String commitProperty : COMMIT_PROPERTIES) {\n+      String value = properties.get(commitProperty);\n+      if (value != null) {\n+        int parsedValue;\n+        try {\n+          parsedValue = Integer.parseInt(value);\n+        } catch (NumberFormatException e) {\n+          throw new ValidationException(\n+              \"Table property %s must have integer value\", commitProperty);\n+        }\n+        ValidationException.check(\n+            parsedValue >= 0,\n+            \"Table property %s must have non negative integer value\",\n+            commitProperty);\n+      }\n+    }\n+  }\n+\n   /**\n    * Returns subset of provided map with keys matching the provided prefix. Matching is\n    * case-sensitive and the matching prefix is removed from the keys in returned map.\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/TestTransaction.java b/core/src/test/java/org/apache/iceberg/TestTransaction.java\nindex 8fed7134fae1..8770e24f8e40 100644\n--- a/core/src/test/java/org/apache/iceberg/TestTransaction.java\n+++ b/core/src/test/java/org/apache/iceberg/TestTransaction.java\n@@ -714,4 +714,22 @@ public void testTransactionRecommit() {\n     assertThat(paths).isEqualTo(expectedPaths);\n     assertThat(table.currentSnapshot().allManifests(table.io())).hasSize(2);\n   }\n+\n+  @TestTemplate\n+  public void testCommitProperties() {\n+    table\n+        .updateProperties()\n+        .set(TableProperties.COMMIT_MAX_RETRY_WAIT_MS, \"foo\")\n+        .set(TableProperties.COMMIT_NUM_RETRIES, \"bar\")\n+        .set(TableProperties.COMMIT_TOTAL_RETRY_TIME_MS, Integer.toString(60 * 60 * 1000))\n+        .commit();\n+    table.updateProperties().remove(TableProperties.COMMIT_MAX_RETRY_WAIT_MS).commit();\n+    table.updateProperties().remove(TableProperties.COMMIT_NUM_RETRIES).commit();\n+\n+    assertThat(table.properties())\n+        .doesNotContainKey(TableProperties.COMMIT_NUM_RETRIES)\n+        .doesNotContainKey(TableProperties.COMMIT_MAX_RETRY_WAIT_MS)\n+        .containsEntry(\n+            TableProperties.COMMIT_TOTAL_RETRY_TIME_MS, Integer.toString(60 * 60 * 1000));\n+  }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java\nindex ae0aa2cda49b..11d4cfebfea6 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/sql/TestCreateTable.java\n@@ -31,6 +31,7 @@\n import org.apache.iceberg.Table;\n import org.apache.iceberg.TableOperations;\n import org.apache.iceberg.TableProperties;\n+import org.apache.iceberg.exceptions.ValidationException;\n import org.apache.iceberg.hadoop.HadoopCatalog;\n import org.apache.iceberg.spark.CatalogTestBase;\n import org.apache.iceberg.types.Types;\n@@ -348,6 +349,47 @@ public void testCreateTableProperties() {\n     assertThat(table.properties()).containsEntry(\"p1\", \"2\").containsEntry(\"p2\", \"x\");\n   }\n \n+  @TestTemplate\n+  public void testCreateTableCommitProperties() {\n+    assertThat(validationCatalog.tableExists(tableIdent))\n+        .as(\"Table should not already exist\")\n+        .isFalse();\n+\n+    assertThatThrownBy(\n+            () ->\n+                sql(\n+                    \"CREATE TABLE %s \"\n+                        + \"(id BIGINT NOT NULL, data STRING) \"\n+                        + \"USING iceberg \"\n+                        + \"TBLPROPERTIES ('commit.retry.num-retries'='x', p2='x')\",\n+                    tableName))\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessage(\"Table property commit.retry.num-retries must have integer value\");\n+\n+    assertThatThrownBy(\n+            () ->\n+                sql(\n+                    \"CREATE TABLE %s \"\n+                        + \"(id BIGINT NOT NULL, data STRING) \"\n+                        + \"USING iceberg \"\n+                        + \"TBLPROPERTIES ('commit.retry.max-wait-ms'='-1')\",\n+                    tableName))\n+        .isInstanceOf(ValidationException.class)\n+        .hasMessage(\"Table property commit.retry.max-wait-ms must have non negative integer value\");\n+\n+    sql(\n+        \"CREATE TABLE %s \"\n+            + \"(id BIGINT NOT NULL, data STRING) \"\n+            + \"USING iceberg \"\n+            + \"TBLPROPERTIES ('commit.retry.num-retries'='1', 'commit.retry.max-wait-ms'='3000')\",\n+        tableName);\n+\n+    Table table = validationCatalog.loadTable(tableIdent);\n+    assertThat(table.properties())\n+        .containsEntry(TableProperties.COMMIT_NUM_RETRIES, \"1\")\n+        .containsEntry(TableProperties.COMMIT_MAX_RETRY_WAIT_MS, \"3000\");\n+  }\n+\n   @TestTemplate\n   public void testCreateTableWithFormatV2ThroughTableProperty() {\n     assertThat(validationCatalog.tableExists(tableIdent))\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11419",
    "pr_id": 11419,
    "issue_id": 4849,
    "repo": "apache/iceberg",
    "problem_statement": "Ignore downcasting of column types when \"mergeSchema\" is set.\nWe encountered an error while writing to iceberg table\r\n`java.lang.IllegalArgumentException: Cannot change column type: myCol: long->int`\r\nThe table was created with long type for myCol. We are writing to Iceberg table from Spark application like this :\r\n`df.writeTo(icebergTable).option(\"mergeSchema\", \"true\").overwritePartitions()`\r\n \r\nSince we have datatype for myCol as int in the dataframe and we are using 'mergeSchema' option, I believe the writer is trying to downcast the column type which is failing. Would it be better if we ignore the downcasting of column types with 'mergeSchema' option?",
    "issue_word_count": 101,
    "test_files_count": 4,
    "non_test_files_count": 2,
    "pr_changed_files": [
      "api/src/main/java/org/apache/iceberg/UpdateSchema.java",
      "core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java",
      "core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java",
      "spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java"
    ],
    "pr_changed_test_files": [
      "core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java",
      "spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java",
      "spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java",
      "spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java"
    ],
    "base_commit": "1e3ee1e4e80873018af716a190e541925f09c285",
    "head_commit": "9424d16a018bd007bb45968527c1fe3de32e74a0",
    "repo_url": "https://github.com/apache/iceberg/pull/11419",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11419",
    "dockerfile": "",
    "pr_merged_at": "2024-11-15T06:07:04.000Z",
    "patch": "diff --git a/api/src/main/java/org/apache/iceberg/UpdateSchema.java b/api/src/main/java/org/apache/iceberg/UpdateSchema.java\nindex afe1891cd530..c84c237f8d8f 100644\n--- a/api/src/main/java/org/apache/iceberg/UpdateSchema.java\n+++ b/api/src/main/java/org/apache/iceberg/UpdateSchema.java\n@@ -369,7 +369,9 @@ default UpdateSchema updateColumn(String name, Type.PrimitiveType newType, Strin\n    * to create a union schema.\n    *\n    * <p>For fields with same canonical names in both schemas it is required that the widen types is\n-   * supported using {@link UpdateSchema#updateColumn(String, Type.PrimitiveType)}\n+   * supported using {@link UpdateSchema#updateColumn(String, Type.PrimitiveType)}. Differences in\n+   * type are ignored if the new type is narrower than the existing type (e.g. long to int, double\n+   * to float).\n    *\n    * <p>Only supports turning a previously required field into an optional one if it is marked\n    * optional in the provided new schema using {@link UpdateSchema#makeColumnOptional(String)}\n\ndiff --git a/core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java b/core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java\nindex 1497ba59c582..68172b7062a6 100644\n--- a/core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java\n+++ b/core/src/main/java/org/apache/iceberg/schema/UnionByNameVisitor.java\n@@ -24,6 +24,7 @@\n import org.apache.iceberg.UpdateSchema;\n import org.apache.iceberg.relocated.com.google.common.base.Preconditions;\n import org.apache.iceberg.types.Type;\n+import org.apache.iceberg.types.TypeUtil;\n import org.apache.iceberg.types.Types;\n \n /**\n@@ -163,8 +164,7 @@ private void updateColumn(Types.NestedField field, Types.NestedField existingFie\n     String fullName = partnerSchema.findColumnName(existingField.fieldId());\n \n     boolean needsOptionalUpdate = field.isOptional() && existingField.isRequired();\n-    boolean needsTypeUpdate =\n-        field.type().isPrimitiveType() && !field.type().equals(existingField.type());\n+    boolean needsTypeUpdate = !isIgnorableTypeUpdate(existingField.type(), field.type());\n     boolean needsDocUpdate = field.doc() != null && !field.doc().equals(existingField.doc());\n \n     if (needsOptionalUpdate) {\n@@ -180,6 +180,23 @@ private void updateColumn(Types.NestedField field, Types.NestedField existingFie\n     }\n   }\n \n+  private boolean isIgnorableTypeUpdate(Type existingType, Type newType) {\n+    if (existingType.isPrimitiveType()) {\n+      // TypeUtil.isPromotionAllowed is used to check whether type promotion is allowed in the\n+      // reverse order, newType to existingType. A true result implies that the newType is more\n+      // narrow than the existingType, which translates in this context as an ignorable update when\n+      // evaluating the existingType to newType order. A false result implies the opposite.\n+      // Examples:\n+      // existingType:long -> newType:int returns true, meaning it is ignorable\n+      // existingType:int -> newType:long returns false, meaning it is not ignorable\n+      return newType.isPrimitiveType()\n+          && TypeUtil.isPromotionAllowed(newType, existingType.asPrimitiveType());\n+    } else {\n+      // Complex -> Complex\n+      return !newType.isPrimitiveType();\n+    }\n+  }\n+\n   private static class PartnerIdByNameAccessors implements PartnerAccessors<Integer> {\n     private final Schema partnerSchema;\n     private boolean caseSensitive = true;\n",
    "test_patch": "diff --git a/core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java b/core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java\nindex bda76469e1fa..656e72a0c19c 100644\n--- a/core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java\n+++ b/core/src/test/java/org/apache/iceberg/TestSchemaUnionByFieldName.java\n@@ -303,13 +303,33 @@ public void testTypePromoteFloatToDouble() {\n   }\n \n   @Test\n-  public void testInvalidTypePromoteDoubleToFloat() {\n+  public void testIgnoreTypePromoteDoubleToFloat() {\n     Schema currentSchema = new Schema(required(1, \"aCol\", DoubleType.get()));\n     Schema newSchema = new Schema(required(1, \"aCol\", FloatType.get()));\n \n-    assertThatThrownBy(() -> new SchemaUpdate(currentSchema, 1).unionByNameWith(newSchema).apply())\n-        .isInstanceOf(IllegalArgumentException.class)\n-        .hasMessage(\"Cannot change column type: aCol: double -> float\");\n+    Schema applied = new SchemaUpdate(currentSchema, 1).unionByNameWith(newSchema).apply();\n+    assertThat(applied.asStruct()).isEqualTo(currentSchema.asStruct());\n+    assertThat(applied.asStruct().fields()).hasSize(1);\n+    assertThat(applied.asStruct().fields().get(0).type()).isEqualTo(DoubleType.get());\n+  }\n+\n+  @Test\n+  public void testIgnoreTypePromoteLongToInt() {\n+    Schema currentSchema = new Schema(required(1, \"aCol\", LongType.get()));\n+    Schema newSchema = new Schema(required(1, \"aCol\", IntegerType.get()));\n+\n+    Schema applied = new SchemaUpdate(currentSchema, 1).unionByNameWith(newSchema).apply();\n+    assertThat(applied.asStruct().fields()).hasSize(1);\n+    assertThat(applied.asStruct().fields().get(0).type()).isEqualTo(LongType.get());\n+  }\n+\n+  @Test\n+  public void testIgnoreTypePromoteDecimalToNarrowerPrecision() {\n+    Schema currentSchema = new Schema(required(1, \"aCol\", DecimalType.of(20, 1)));\n+    Schema newSchema = new Schema(required(1, \"aCol\", DecimalType.of(10, 1)));\n+\n+    Schema applied = new SchemaUpdate(currentSchema, 1).unionByNameWith(newSchema).apply();\n+    assertThat(applied.asStruct()).isEqualTo(currentSchema.asStruct());\n   }\n \n   @Test\n\ndiff --git a/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\nindex 59a32bd239df..190f434e2949 100644\n--- a/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\n+++ b/spark/v3.3/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\n@@ -18,13 +18,17 @@\n  */\n package org.apache.iceberg.spark.source;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatCode;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import java.math.BigDecimal;\n import java.util.List;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.spark.Spark3Util;\n import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.AnalysisException;\n import org.apache.spark.sql.Dataset;\n@@ -226,4 +230,135 @@ public void testWriteWithCaseSensitiveOption() throws NoSuchTableException, Pars\n     fields = Spark3Util.loadIcebergTable(sparkSession, tableName).schema().asStruct().fields();\n     Assert.assertEquals(4, fields.size());\n   }\n+\n+  @Test\n+  public void testMergeSchemaIgnoreCastingLongToInt() throws Exception {\n+    sql(\n+        \"ALTER TABLE %s SET TBLPROPERTIES ('%s'='true')\",\n+        tableName, TableProperties.SPARK_WRITE_ACCEPT_ANY_SCHEMA);\n+\n+    Dataset<Row> bigintDF =\n+        jsonToDF(\n+            \"id bigint, data string\",\n+            \"{ \\\"id\\\": 1, \\\"data\\\": \\\"a\\\" }\",\n+            \"{ \\\"id\\\": 2, \\\"data\\\": \\\"b\\\" }\");\n+\n+    bigintDF.writeTo(tableName).append();\n+\n+    assertEquals(\n+        \"Should have initial rows with long column\",\n+        ImmutableList.of(row(1L, \"a\"), row(2L, \"b\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    Dataset<Row> intDF =\n+        jsonToDF(\n+            \"id int, data string\",\n+            \"{ \\\"id\\\": 3, \\\"data\\\": \\\"c\\\" }\",\n+            \"{ \\\"id\\\": 4, \\\"data\\\": \\\"d\\\" }\");\n+\n+    // merge-schema=true on writes allows table schema updates when incoming data has schema changes\n+    assertThatCode(() -> intDF.writeTo(tableName).option(\"merge-schema\", \"true\").append())\n+        .doesNotThrowAnyException();\n+\n+    assertEquals(\n+        \"Should include new rows with unchanged long column type\",\n+        ImmutableList.of(row(1L, \"a\"), row(2L, \"b\"), row(3L, \"c\"), row(4L, \"d\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    // verify the column type did not change\n+    Types.NestedField idField =\n+        Spark3Util.loadIcebergTable(spark, tableName).schema().findField(\"id\");\n+    assertThat(idField.type().typeId()).isEqualTo(Type.TypeID.LONG);\n+  }\n+\n+  @Test\n+  public void testMergeSchemaIgnoreCastingDoubleToFloat() throws Exception {\n+    removeTables();\n+    sql(\"CREATE TABLE %s (id double, data string) USING iceberg\", tableName);\n+    sql(\n+        \"ALTER TABLE %s SET TBLPROPERTIES ('%s'='true')\",\n+        tableName, TableProperties.SPARK_WRITE_ACCEPT_ANY_SCHEMA);\n+\n+    Dataset<Row> doubleDF =\n+        jsonToDF(\n+            \"id double, data string\",\n+            \"{ \\\"id\\\": 1.0, \\\"data\\\": \\\"a\\\" }\",\n+            \"{ \\\"id\\\": 2.0, \\\"data\\\": \\\"b\\\" }\");\n+\n+    doubleDF.writeTo(tableName).append();\n+\n+    assertEquals(\n+        \"Should have initial rows with double column\",\n+        ImmutableList.of(row(1.0, \"a\"), row(2.0, \"b\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    Dataset<Row> floatDF =\n+        jsonToDF(\n+            \"id float, data string\",\n+            \"{ \\\"id\\\": 3.0, \\\"data\\\": \\\"c\\\" }\",\n+            \"{ \\\"id\\\": 4.0, \\\"data\\\": \\\"d\\\" }\");\n+\n+    // merge-schema=true on writes allows table schema updates when incoming data has schema changes\n+    assertThatCode(() -> floatDF.writeTo(tableName).option(\"merge-schema\", \"true\").append())\n+        .doesNotThrowAnyException();\n+\n+    assertEquals(\n+        \"Should include new rows with unchanged double column type\",\n+        ImmutableList.of(row(1.0, \"a\"), row(2.0, \"b\"), row(3.0, \"c\"), row(4.0, \"d\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    // verify the column type did not change\n+    Types.NestedField idField =\n+        Spark3Util.loadIcebergTable(spark, tableName).schema().findField(\"id\");\n+    assertThat(idField.type().typeId()).isEqualTo(Type.TypeID.DOUBLE);\n+  }\n+\n+  @Test\n+  public void testMergeSchemaIgnoreCastingDecimalToDecimalWithNarrowerPrecision() throws Exception {\n+    removeTables();\n+    sql(\"CREATE TABLE %s (id decimal(6,2), data string) USING iceberg\", tableName);\n+    sql(\n+        \"ALTER TABLE %s SET TBLPROPERTIES ('%s'='true')\",\n+        tableName, TableProperties.SPARK_WRITE_ACCEPT_ANY_SCHEMA);\n+\n+    Dataset<Row> decimalPrecision6DF =\n+        jsonToDF(\n+            \"id decimal(6,2), data string\",\n+            \"{ \\\"id\\\": 1.0, \\\"data\\\": \\\"a\\\" }\",\n+            \"{ \\\"id\\\": 2.0, \\\"data\\\": \\\"b\\\" }\");\n+\n+    decimalPrecision6DF.writeTo(tableName).append();\n+\n+    assertEquals(\n+        \"Should have initial rows with decimal column with precision 6\",\n+        ImmutableList.of(row(new BigDecimal(\"1.00\"), \"a\"), row(new BigDecimal(\"2.00\"), \"b\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    Dataset<Row> decimalPrecision4DF =\n+        jsonToDF(\n+            \"id decimal(4,2), data string\",\n+            \"{ \\\"id\\\": 3.0, \\\"data\\\": \\\"c\\\" }\",\n+            \"{ \\\"id\\\": 4.0, \\\"data\\\": \\\"d\\\" }\");\n+\n+    // merge-schema=true on writes allows table schema updates when incoming data has schema changes\n+    assertThatCode(\n+            () -> decimalPrecision4DF.writeTo(tableName).option(\"merge-schema\", \"true\").append())\n+        .doesNotThrowAnyException();\n+\n+    assertEquals(\n+        \"Should include new rows with unchanged decimal precision\",\n+        ImmutableList.of(\n+            row(new BigDecimal(\"1.00\"), \"a\"),\n+            row(new BigDecimal(\"2.00\"), \"b\"),\n+            row(new BigDecimal(\"3.00\"), \"c\"),\n+            row(new BigDecimal(\"4.00\"), \"d\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    // verify the decimal column precision did not change\n+    Type idFieldType =\n+        Spark3Util.loadIcebergTable(spark, tableName).schema().findField(\"id\").type();\n+    assertThat(idFieldType.typeId()).isEqualTo(Type.TypeID.DECIMAL);\n+    Types.DecimalType decimalType = (Types.DecimalType) idFieldType;\n+    assertThat(decimalType.precision()).isEqualTo(6);\n+  }\n }\n\ndiff --git a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\nindex 824b0a17daef..47a0e87b9398 100644\n--- a/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\n+++ b/spark/v3.4/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\n@@ -18,13 +18,17 @@\n  */\n package org.apache.iceberg.spark.source;\n \n+import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatCode;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import java.math.BigDecimal;\n import java.util.List;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.spark.Spark3Util;\n import org.apache.iceberg.spark.SparkTestBaseWithCatalog;\n+import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.AnalysisException;\n import org.apache.spark.sql.Dataset;\n@@ -209,4 +213,135 @@ public void testWriteWithCaseSensitiveOption() throws NoSuchTableException, Pars\n     fields = Spark3Util.loadIcebergTable(sparkSession, tableName).schema().asStruct().fields();\n     Assert.assertEquals(4, fields.size());\n   }\n+\n+  @Test\n+  public void testMergeSchemaIgnoreCastingLongToInt() throws Exception {\n+    sql(\n+        \"ALTER TABLE %s SET TBLPROPERTIES ('%s'='true')\",\n+        tableName, TableProperties.SPARK_WRITE_ACCEPT_ANY_SCHEMA);\n+\n+    Dataset<Row> bigintDF =\n+        jsonToDF(\n+            \"id bigint, data string\",\n+            \"{ \\\"id\\\": 1, \\\"data\\\": \\\"a\\\" }\",\n+            \"{ \\\"id\\\": 2, \\\"data\\\": \\\"b\\\" }\");\n+\n+    bigintDF.writeTo(tableName).append();\n+\n+    assertEquals(\n+        \"Should have initial rows with long column\",\n+        ImmutableList.of(row(1L, \"a\"), row(2L, \"b\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    Dataset<Row> intDF =\n+        jsonToDF(\n+            \"id int, data string\",\n+            \"{ \\\"id\\\": 3, \\\"data\\\": \\\"c\\\" }\",\n+            \"{ \\\"id\\\": 4, \\\"data\\\": \\\"d\\\" }\");\n+\n+    // merge-schema=true on writes allows table schema updates when incoming data has schema changes\n+    assertThatCode(() -> intDF.writeTo(tableName).option(\"merge-schema\", \"true\").append())\n+        .doesNotThrowAnyException();\n+\n+    assertEquals(\n+        \"Should include new rows with unchanged long column type\",\n+        ImmutableList.of(row(1L, \"a\"), row(2L, \"b\"), row(3L, \"c\"), row(4L, \"d\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    // verify the column type did not change\n+    Types.NestedField idField =\n+        Spark3Util.loadIcebergTable(spark, tableName).schema().findField(\"id\");\n+    assertThat(idField.type().typeId()).isEqualTo(Type.TypeID.LONG);\n+  }\n+\n+  @Test\n+  public void testMergeSchemaIgnoreCastingDoubleToFloat() throws Exception {\n+    removeTables();\n+    sql(\"CREATE TABLE %s (id double, data string) USING iceberg\", tableName);\n+    sql(\n+        \"ALTER TABLE %s SET TBLPROPERTIES ('%s'='true')\",\n+        tableName, TableProperties.SPARK_WRITE_ACCEPT_ANY_SCHEMA);\n+\n+    Dataset<Row> doubleDF =\n+        jsonToDF(\n+            \"id double, data string\",\n+            \"{ \\\"id\\\": 1.0, \\\"data\\\": \\\"a\\\" }\",\n+            \"{ \\\"id\\\": 2.0, \\\"data\\\": \\\"b\\\" }\");\n+\n+    doubleDF.writeTo(tableName).append();\n+\n+    assertEquals(\n+        \"Should have initial rows with double column\",\n+        ImmutableList.of(row(1.0, \"a\"), row(2.0, \"b\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    Dataset<Row> floatDF =\n+        jsonToDF(\n+            \"id float, data string\",\n+            \"{ \\\"id\\\": 3.0, \\\"data\\\": \\\"c\\\" }\",\n+            \"{ \\\"id\\\": 4.0, \\\"data\\\": \\\"d\\\" }\");\n+\n+    // merge-schema=true on writes allows table schema updates when incoming data has schema changes\n+    assertThatCode(() -> floatDF.writeTo(tableName).option(\"merge-schema\", \"true\").append())\n+        .doesNotThrowAnyException();\n+\n+    assertEquals(\n+        \"Should include new rows with unchanged double column type\",\n+        ImmutableList.of(row(1.0, \"a\"), row(2.0, \"b\"), row(3.0, \"c\"), row(4.0, \"d\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    // verify the column type did not change\n+    Types.NestedField idField =\n+        Spark3Util.loadIcebergTable(spark, tableName).schema().findField(\"id\");\n+    assertThat(idField.type().typeId()).isEqualTo(Type.TypeID.DOUBLE);\n+  }\n+\n+  @Test\n+  public void testMergeSchemaIgnoreCastingDecimalToDecimalWithNarrowerPrecision() throws Exception {\n+    removeTables();\n+    sql(\"CREATE TABLE %s (id decimal(6,2), data string) USING iceberg\", tableName);\n+    sql(\n+        \"ALTER TABLE %s SET TBLPROPERTIES ('%s'='true')\",\n+        tableName, TableProperties.SPARK_WRITE_ACCEPT_ANY_SCHEMA);\n+\n+    Dataset<Row> decimalPrecision6DF =\n+        jsonToDF(\n+            \"id decimal(6,2), data string\",\n+            \"{ \\\"id\\\": 1.0, \\\"data\\\": \\\"a\\\" }\",\n+            \"{ \\\"id\\\": 2.0, \\\"data\\\": \\\"b\\\" }\");\n+\n+    decimalPrecision6DF.writeTo(tableName).append();\n+\n+    assertEquals(\n+        \"Should have initial rows with decimal column with precision 6\",\n+        ImmutableList.of(row(new BigDecimal(\"1.00\"), \"a\"), row(new BigDecimal(\"2.00\"), \"b\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    Dataset<Row> decimalPrecision4DF =\n+        jsonToDF(\n+            \"id decimal(4,2), data string\",\n+            \"{ \\\"id\\\": 3.0, \\\"data\\\": \\\"c\\\" }\",\n+            \"{ \\\"id\\\": 4.0, \\\"data\\\": \\\"d\\\" }\");\n+\n+    // merge-schema=true on writes allows table schema updates when incoming data has schema changes\n+    assertThatCode(\n+            () -> decimalPrecision4DF.writeTo(tableName).option(\"merge-schema\", \"true\").append())\n+        .doesNotThrowAnyException();\n+\n+    assertEquals(\n+        \"Should include new rows with unchanged decimal precision\",\n+        ImmutableList.of(\n+            row(new BigDecimal(\"1.00\"), \"a\"),\n+            row(new BigDecimal(\"2.00\"), \"b\"),\n+            row(new BigDecimal(\"3.00\"), \"c\"),\n+            row(new BigDecimal(\"4.00\"), \"d\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    // verify the decimal column precision did not change\n+    Type idFieldType =\n+        Spark3Util.loadIcebergTable(spark, tableName).schema().findField(\"id\").type();\n+    assertThat(idFieldType.typeId()).isEqualTo(Type.TypeID.DECIMAL);\n+    Types.DecimalType decimalType = (Types.DecimalType) idFieldType;\n+    assertThat(decimalType.precision()).isEqualTo(6);\n+  }\n }\n\ndiff --git a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\nindex 1c87886241bf..7404b18d14b2 100644\n--- a/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\n+++ b/spark/v3.5/spark/src/test/java/org/apache/iceberg/spark/source/TestDataFrameWriterV2.java\n@@ -19,13 +19,16 @@\n package org.apache.iceberg.spark.source;\n \n import static org.assertj.core.api.Assertions.assertThat;\n+import static org.assertj.core.api.Assertions.assertThatCode;\n import static org.assertj.core.api.Assertions.assertThatThrownBy;\n \n+import java.math.BigDecimal;\n import java.util.List;\n import org.apache.iceberg.TableProperties;\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableList;\n import org.apache.iceberg.spark.Spark3Util;\n import org.apache.iceberg.spark.TestBaseWithCatalog;\n+import org.apache.iceberg.types.Type;\n import org.apache.iceberg.types.Types;\n import org.apache.spark.sql.AnalysisException;\n import org.apache.spark.sql.Dataset;\n@@ -244,4 +247,135 @@ public void testMergeSchemaSparkConfiguration() throws Exception {\n             row(4L, \"d\", 140000.56F)),\n         sql(\"select * from %s order by id\", tableName));\n   }\n+\n+  @TestTemplate\n+  public void testMergeSchemaIgnoreCastingLongToInt() throws Exception {\n+    sql(\n+        \"ALTER TABLE %s SET TBLPROPERTIES ('%s'='true')\",\n+        tableName, TableProperties.SPARK_WRITE_ACCEPT_ANY_SCHEMA);\n+\n+    Dataset<Row> bigintDF =\n+        jsonToDF(\n+            \"id bigint, data string\",\n+            \"{ \\\"id\\\": 1, \\\"data\\\": \\\"a\\\" }\",\n+            \"{ \\\"id\\\": 2, \\\"data\\\": \\\"b\\\" }\");\n+\n+    bigintDF.writeTo(tableName).append();\n+\n+    assertEquals(\n+        \"Should have initial rows with long column\",\n+        ImmutableList.of(row(1L, \"a\"), row(2L, \"b\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    Dataset<Row> intDF =\n+        jsonToDF(\n+            \"id int, data string\",\n+            \"{ \\\"id\\\": 3, \\\"data\\\": \\\"c\\\" }\",\n+            \"{ \\\"id\\\": 4, \\\"data\\\": \\\"d\\\" }\");\n+\n+    // merge-schema=true on writes allows table schema updates when incoming data has schema changes\n+    assertThatCode(() -> intDF.writeTo(tableName).option(\"merge-schema\", \"true\").append())\n+        .doesNotThrowAnyException();\n+\n+    assertEquals(\n+        \"Should include new rows with unchanged long column type\",\n+        ImmutableList.of(row(1L, \"a\"), row(2L, \"b\"), row(3L, \"c\"), row(4L, \"d\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    // verify the column type did not change\n+    Types.NestedField idField =\n+        Spark3Util.loadIcebergTable(spark, tableName).schema().findField(\"id\");\n+    assertThat(idField.type().typeId()).isEqualTo(Type.TypeID.LONG);\n+  }\n+\n+  @TestTemplate\n+  public void testMergeSchemaIgnoreCastingDoubleToFloat() throws Exception {\n+    removeTables();\n+    sql(\"CREATE TABLE %s (id double, data string) USING iceberg\", tableName);\n+    sql(\n+        \"ALTER TABLE %s SET TBLPROPERTIES ('%s'='true')\",\n+        tableName, TableProperties.SPARK_WRITE_ACCEPT_ANY_SCHEMA);\n+\n+    Dataset<Row> doubleDF =\n+        jsonToDF(\n+            \"id double, data string\",\n+            \"{ \\\"id\\\": 1.0, \\\"data\\\": \\\"a\\\" }\",\n+            \"{ \\\"id\\\": 2.0, \\\"data\\\": \\\"b\\\" }\");\n+\n+    doubleDF.writeTo(tableName).append();\n+\n+    assertEquals(\n+        \"Should have initial rows with double column\",\n+        ImmutableList.of(row(1.0, \"a\"), row(2.0, \"b\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    Dataset<Row> floatDF =\n+        jsonToDF(\n+            \"id float, data string\",\n+            \"{ \\\"id\\\": 3.0, \\\"data\\\": \\\"c\\\" }\",\n+            \"{ \\\"id\\\": 4.0, \\\"data\\\": \\\"d\\\" }\");\n+\n+    // merge-schema=true on writes allows table schema updates when incoming data has schema changes\n+    assertThatCode(() -> floatDF.writeTo(tableName).option(\"merge-schema\", \"true\").append())\n+        .doesNotThrowAnyException();\n+\n+    assertEquals(\n+        \"Should include new rows with unchanged double column type\",\n+        ImmutableList.of(row(1.0, \"a\"), row(2.0, \"b\"), row(3.0, \"c\"), row(4.0, \"d\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    // verify the column type did not change\n+    Types.NestedField idField =\n+        Spark3Util.loadIcebergTable(spark, tableName).schema().findField(\"id\");\n+    assertThat(idField.type().typeId()).isEqualTo(Type.TypeID.DOUBLE);\n+  }\n+\n+  @TestTemplate\n+  public void testMergeSchemaIgnoreCastingDecimalToDecimalWithNarrowerPrecision() throws Exception {\n+    removeTables();\n+    sql(\"CREATE TABLE %s (id decimal(6,2), data string) USING iceberg\", tableName);\n+    sql(\n+        \"ALTER TABLE %s SET TBLPROPERTIES ('%s'='true')\",\n+        tableName, TableProperties.SPARK_WRITE_ACCEPT_ANY_SCHEMA);\n+\n+    Dataset<Row> decimalPrecision6DF =\n+        jsonToDF(\n+            \"id decimal(6,2), data string\",\n+            \"{ \\\"id\\\": 1.0, \\\"data\\\": \\\"a\\\" }\",\n+            \"{ \\\"id\\\": 2.0, \\\"data\\\": \\\"b\\\" }\");\n+\n+    decimalPrecision6DF.writeTo(tableName).append();\n+\n+    assertEquals(\n+        \"Should have initial rows with decimal column with precision 6\",\n+        ImmutableList.of(row(new BigDecimal(\"1.00\"), \"a\"), row(new BigDecimal(\"2.00\"), \"b\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    Dataset<Row> decimalPrecision4DF =\n+        jsonToDF(\n+            \"id decimal(4,2), data string\",\n+            \"{ \\\"id\\\": 3.0, \\\"data\\\": \\\"c\\\" }\",\n+            \"{ \\\"id\\\": 4.0, \\\"data\\\": \\\"d\\\" }\");\n+\n+    // merge-schema=true on writes allows table schema updates when incoming data has schema changes\n+    assertThatCode(\n+            () -> decimalPrecision4DF.writeTo(tableName).option(\"merge-schema\", \"true\").append())\n+        .doesNotThrowAnyException();\n+\n+    assertEquals(\n+        \"Should include new rows with unchanged decimal precision\",\n+        ImmutableList.of(\n+            row(new BigDecimal(\"1.00\"), \"a\"),\n+            row(new BigDecimal(\"2.00\"), \"b\"),\n+            row(new BigDecimal(\"3.00\"), \"c\"),\n+            row(new BigDecimal(\"4.00\"), \"d\")),\n+        sql(\"select * from %s order by id\", tableName));\n+\n+    // verify the decimal column precision did not change\n+    Type idFieldType =\n+        Spark3Util.loadIcebergTable(spark, tableName).schema().findField(\"id\").type();\n+    assertThat(idFieldType.typeId()).isEqualTo(Type.TypeID.DECIMAL);\n+    Types.DecimalType decimalType = (Types.DecimalType) idFieldType;\n+    assertThat(decimalType.precision()).isEqualTo(6);\n+  }\n }\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  },
  {
    "instance_id": "apache__iceberg-11410",
    "pr_id": 11410,
    "issue_id": 11397,
    "repo": "apache/iceberg",
    "problem_statement": "Flaky test `TestFlinkIcebergSinkRangeDistributionBucketing > testBucketNumberHigherThanWriterParallelismNotDivisible()`\n### Apache Iceberg version\n\nmain (development)\n\n### Query engine\n\nFlink\n\n### Please describe the bug üêû\n\nhttps://github.com/apache/iceberg/actions/runs/11525609495/job/32088339013\r\n\r\n```\r\njava.lang.AssertionError: \r\n    Expecting size of:\r\n      [GenericDataFile{content=data, file_path=file:/tmp/junit5_hadoop_catalog-14525356505085993747/fc6fcd29-95ad-4477-8066-cd11f59528ba/default/t/data/ts_hour=2024-10-25-21/uuid_bucket=3/00002-0-2c5978f8-a440-4c4f-be50-86c438ff63b7-00017.parquet, file_format=PARQUET, spec_id=0, partition=PartitionData{ts_hour=480525, uuid_bucket=3}, record_count=11, file_size_in_bytes=1278, column_sizes=org.apache.iceberg.util.SerializableMap@184, value_counts=org.apache.iceberg.util.SerializableMap@1b, null_value_counts=org.apache.iceberg.util.SerializableMap@6, nan_value_counts=org.apache.iceberg.util.SerializableMap@0, lower_bounds=org.apache.iceberg.SerializableByteBufferMap@cb1481a6, upper_bounds=org.apache.iceberg.SerializableByteBufferMap@d4f5ff3d, key_metadata=null, split_offsets=[4], equality_ids=null, sort_order_id=0, data_sequence_number=7, file_sequence_number=7},\r\n        GenericDataFile{content=data, file_path=file:/tmp/junit5_hadoop_catalog-14525356505085993747/fc6fcd29-95ad-4477-8066-cd11f59528ba/default/t/data/ts_hour=2024-10-25-21/uuid_bucket=2/00002-0-2c5978f8-a440-4c4f-be50-86c438ff63b7-00018.parquet, file_format=PARQUET, spec_id=0, partition=PartitionData{ts_hour=480525, uuid_bucket=2}, record_count=4, file_size_in_bytes=1138, column_sizes=org.apache.iceberg.util.SerializableMap@fe, value_counts=org.apache.iceberg.util.SerializableMap@12, null_value_counts=org.apache.iceberg.util.SerializableMap@6, nan_value_counts=org.apache.iceberg.util.SerializableMap@0, lower_bounds=org.apache.iceberg.SerializableByteBufferMap@969add9a, upper_bounds=org.apache.iceberg.SerializableByteBufferMap@76639efe, key_metadata=null, split_offsets=[4], equality_ids=null, sort_order_id=0, data_sequence_number=7, file_sequence_number=7},\r\n        GenericDataFile{content=data, file_path=file:/tmp/junit5_hadoop_catalog-14525356505085993747/fc6fcd29-95ad-4477-8066-cd11f59528ba/default/t/data/ts_hour=2024-10-25-21/uuid_bucket=1/00000-0-0861575b-99f3-410e-bf09-19007260ee22-00017.parquet, file_format=PARQUET, spec_id=0, partition=PartitionData{ts_hour=480525, uuid_bucket=1}, record_count=7, file_size_in_bytes=1197, column_sizes=org.apache.iceberg.util.SerializableMap@137, value_counts=org.apache.iceberg.util.SerializableMap@f, null_value_counts=org.apache.iceberg.util.SerializableMap@6, nan_value_counts=org.apache.iceberg.util.SerializableMap@0, lower_bounds=org.apache.iceberg.SerializableByteBufferMap@8edc3f28, upper_bounds=org.apache.iceberg.SerializableByteBufferMap@36d4a917, key_metadata=null, split_offsets=[4], equality_ids=null, sort_order_id=0, data_sequence_number=7, file_sequence_number=7},\r\n        GenericDataFile{content=data, file_path=file:/tmp/junit5_hadoop_catalog-14525356505085993747/fc6fcd29-95ad-4477-8066-cd11f59528ba/default/t/data/ts_hour=2024-10-25-21/uuid_bucket=0/00000-0-0861575b-99f3-410e-bf09-19007260ee22-00018.parquet, file_format=PARQUET, spec_id=0, partition=PartitionData{ts_hour=480525, uuid_bucket=0}, record_count=11, file_size_in_bytes=1280, column_sizes=org.apache.iceberg.util.SerializableMap@18a, value_counts=org.apache.iceberg.util.SerializableMap@1b, null_value_counts=org.apache.iceberg.util.SerializableMap@6, nan_value_counts=org.apache.iceberg.util.SerializableMap@0, lower_bounds=org.apache.iceberg.SerializableByteBufferMap@1c065f83, upper_bounds=org.apache.iceberg.SerializableByteBufferMap@575e24b0, key_metadata=null, split_offsets=[4], equality_ids=null, sort_order_id=0, data_sequence_number=7, file_sequence_number=7},\r\n        GenericDataFile{content=data, file_path=file:/tmp/junit5_hadoop_catalog-14525356505085993747/fc6fcd29-95ad-4477-8066-cd11f59528ba/default/t/data/ts_hour=2024-10-25-21/uuid_bucket=2/00001-0-c258fabc-b05b-4d5a-9930-714bdb254951-00018.parquet, file_format=PARQUET, spec_id=0, partition=PartitionData{ts_hour=480525, uuid_bucket=2}, record_count=8, file_size_in_bytes=1218, column_sizes=org.apache.iceberg.util.SerializableMap@14e, value_counts=org.apache.iceberg.util.SerializableMap@1e, null_value_counts=org.apache.iceberg.util.SerializableMap@6, nan_value_counts=org.apache.iceberg.util.SerializableMap@0, lower_bounds=org.apache.iceberg.SerializableByteBufferMap@4f1d4eba, upper_bounds=org.apache.iceberg.SerializableByteBufferMap@f741ceee, key_metadata=null, split_offsets=[4], equality_ids=null, sort_order_id=0, data_sequence_number=7, file_sequence_number=7},\r\n        GenericDataFile{content=data, file_path=file:/tmp/junit5_hadoop_catalog-14525356505085993747/fc6fcd29-95ad-4477-8066-cd11f59528ba/default/t/data/ts_hour=2024-10-25-21/uuid_bucket=1/00001-0-c258fabc-b05b-4d5a-9930-714bdb254951-00017.parquet, file_format=PARQUET, spec_id=0, partition=PartitionData{ts_hour=480525, uuid_bucket=1}, record_count=10, file_size_in_bytes=1266, column_sizes=org.apache.iceberg.util.SerializableMap@178, value_counts=org.apache.iceberg.util.SerializableMap@1c, null_value_counts=org.apache.iceberg.util.SerializableMap@6, nan_value_counts=org.apache.iceberg.util.SerializableMap@0, lower_bounds=org.apache.iceberg.SerializableByteBufferMap@ca85bf2b, upper_bounds=org.apache.iceberg.SerializableByteBufferMap@123a5e0e, key_metadata=null, split_offsets=[4], equality_ids=null, sort_order_id=0, data_sequence_number=7, file_sequence_number=7},\r\n        GenericDataFile{content=data, file_path=file:/tmp/junit5_hadoop_catalog-14525356505085993747/fc6fcd29-95ad-4477-8066-cd11f59528ba/default/t/data/ts_hour=2024-10-25-21/uuid_bucket=1/00000-0-0861575b-99f3-410e-bf09-19007260ee22-00019.parquet, file_format=PARQUET, spec_id=0, partition=PartitionData{ts_hour=480525, uuid_bucket=1}, record_count=1, file_size_in_bytes=1084, column_sizes=org.apache.iceberg.util.SerializableMap@a1, value_counts=org.apache.iceberg.util.SerializableMap@5, null_value_counts=org.apache.iceberg.util.SerializableMap@6, nan_value_counts=org.apache.iceberg.util.SerializableMap@0, lower_bounds=org.apache.iceberg.SerializableByteBufferMap@b38647c, upper_bounds=org.apache.iceberg.SerializableByteBufferMap@b38647c, key_metadata=null, split_offsets=[4], equality_ids=null, sort_order_id=0, data_sequence_number=7, file_sequence_number=7},\r\n        GenericDataFile{content=data, file_path=file:/tmp/junit5_hadoop_catalog-14525356505085993747/fc6fcd29-95ad-4477-8066-cd11f59528ba/default/t/data/ts_hour=2024-10-25-21/uuid_bucket=1/00001-0-c258fabc-b05b-4d5a-9930-714bdb254951-00019.parquet, file_format=PARQUET, spec_id=0, partition=PartitionData{ts_hour=480525, uuid_bucket=1}, record_count=2, file_size_in_bytes=1099, column_sizes=org.apache.iceberg.util.SerializableMap@db, value_counts=org.apache.iceberg.util.SerializableMap@4, null_value_counts=org.apache.iceberg.util.SerializableMap@6, nan_value_counts=org.apache.iceberg.util.SerializableMap@0, lower_bounds=org.apache.iceberg.SerializableByteBufferMap@961f8456, upper_bounds=org.apache.iceberg.SerializableByteBufferMap@b4913cb9, key_metadata=null, split_offsets=[4], equality_ids=null, sort_order_id=0, data_sequence_number=7, file_sequence_number=7},\r\n        GenericDataFile{content=data, file_path=file:/tmp/junit5_hadoop_catalog-14525356505085993747/fc6fcd29-95ad-4477-8066-cd11f59528ba/default/t/data/ts_hour=2024-10-25-21/uuid_bucket=3/00002-0-2c5978f8-a440-4c4f-be50-86c438ff63b7-00019.parquet, file_format=PARQUET, spec_id=0, partition=PartitionData{ts_hour=480525, uuid_bucket=3}, record_count=1, file_size_in_bytes=1084, column_sizes=org.apache.iceberg.util.SerializableMap@a1, value_counts=org.apache.iceberg.util.SerializableMap@5, null_value_counts=org.apache.iceberg.util.SerializableMap@6, nan_value_counts=org.apache.iceberg.util.SerializableMap@0, lower_bounds=org.apache.iceberg.SerializableByteBufferMap@49f6be09, upper_bounds=org.apache.iceberg.SerializableByteBufferMap@49f6be09, key_metadata=null, split_offsets=[4], equality_ids=null, sort_order_id=0, data_sequence_number=7, file_sequence_number=7}]\r\n    to be less than or equal to 7 but was 9\r\n        at org.apache.iceberg.flink.sink.TestFlinkIcebergSinkRangeDistributionBucketing.testParallelism(TestFlinkIcebergSinkRangeDistributionBucketing.java:222)\r\n        at org.apache.iceberg.flink.sink.TestFlinkIcebergSinkRangeDistributionBucketing.testBucketNumberHigherThanWriterParallelismNotDivisible(TestFlinkIcebergSinkRangeDistributionBucketing.java:166)\r\n[TaskExecutorFileMergingManager shutdown hook] INFO org.apache.flink.runtime.state.TaskExecutorFileMergingManager - Shutting down TaskExecutorFileMergingManager.\r\n```\n\n### Willingness to contribute\n\n- [ ] I can contribute a fix for this bug independently\n- [X] I would be willing to contribute a fix for this bug with guidance from the Iceberg community\n- [ ] I cannot contribute a fix for this bug at this time",
    "issue_word_count": 1001,
    "test_files_count": 2,
    "non_test_files_count": 0,
    "pr_changed_files": [
      "flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java"
    ],
    "pr_changed_test_files": [
      "flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java",
      "flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java"
    ],
    "base_commit": "48acaadcf94f43572f11b7b589d1fb3857fc6b9d",
    "head_commit": "e0ddd10922f75c1f5e41e7752a75acbe6a7b05ab",
    "repo_url": "https://github.com/apache/iceberg/pull/11410",
    "swe_url": "https://swe-bench-plus.turing.com/repos/apache__iceberg/11410",
    "dockerfile": "",
    "pr_merged_at": "2024-10-28T16:42:58.000Z",
    "patch": "",
    "test_patch": "diff --git a/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java b/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java\nindex 084c66317e2b..a5799288b5e3 100644\n--- a/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java\n+++ b/flink/v1.19/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java\n@@ -58,9 +58,9 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.types.Types;\n-import org.junit.Ignore;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Disabled;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.Timeout;\n import org.junit.jupiter.api.extension.RegisterExtension;\n@@ -78,7 +78,7 @@\n  * </ul>\n  */\n @Timeout(value = 30)\n-@Ignore // https://github.com/apache/iceberg/pull/11305#issuecomment-2415207097\n+@Disabled // https://github.com/apache/iceberg/pull/11305#issuecomment-2415207097\n public class TestFlinkIcebergSinkRangeDistributionBucketing {\n   private static final Configuration DISABLE_CLASSLOADER_CHECK_CONFIG =\n       new Configuration()\n\ndiff --git a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java\nindex 084c66317e2b..a5799288b5e3 100644\n--- a/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java\n+++ b/flink/v1.20/flink/src/test/java/org/apache/iceberg/flink/sink/TestFlinkIcebergSinkRangeDistributionBucketing.java\n@@ -58,9 +58,9 @@\n import org.apache.iceberg.relocated.com.google.common.collect.ImmutableMap;\n import org.apache.iceberg.relocated.com.google.common.collect.Lists;\n import org.apache.iceberg.types.Types;\n-import org.junit.Ignore;\n import org.junit.jupiter.api.AfterEach;\n import org.junit.jupiter.api.BeforeEach;\n+import org.junit.jupiter.api.Disabled;\n import org.junit.jupiter.api.Test;\n import org.junit.jupiter.api.Timeout;\n import org.junit.jupiter.api.extension.RegisterExtension;\n@@ -78,7 +78,7 @@\n  * </ul>\n  */\n @Timeout(value = 30)\n-@Ignore // https://github.com/apache/iceberg/pull/11305#issuecomment-2415207097\n+@Disabled // https://github.com/apache/iceberg/pull/11305#issuecomment-2415207097\n public class TestFlinkIcebergSinkRangeDistributionBucketing {\n   private static final Configuration DISABLE_CLASSLOADER_CHECK_CONFIG =\n       new Configuration()\n",
    "agent_patch": null,
    "FAIL_TO_PASS": [],
    "PASS_TO_PASS": [],
    "test_output_before": null,
    "errors_before": [],
    "failed_before": [],
    "test_output_after": null,
    "errors_after": [],
    "failed_after": []
  }
]